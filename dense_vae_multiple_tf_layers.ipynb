{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 80\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 80\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0.0, stddev=0.05, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = y))\n",
    "\n",
    "            loss = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.001\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.optimizer = train_op.minimize(self.regularized_loss)  \n",
    "            \n",
    "        # add op for merging summary\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(y, 1)\n",
    "        self.actual = tf.argmax(self.y_, 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = collections.namedtuple(\"result\", [\"epochs\", \"hidden_layers\", \"feature_count\",\n",
    "                                  \"Train_Accuracy\", \"Test_Accuracy\"])\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 100\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.4)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss, summary_str = sess.run([net.optimizer, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           net.summary_op],\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:0.8})\n",
    "                    summary_writer_train.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                valid_accuracy, summary_str = sess.run([net.tf_accuracy, net.summary_op], \n",
    "                                                      feed_dict={net.x: x_valid, \n",
    "                                                                 net.y_: y_valid, \n",
    "                                                                 net.keep_prob:1})\n",
    "                summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, h, f,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:2\n",
      "Accuracy on Test data: 0.7159332633018494\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:4\n",
      "Accuracy on Test data: 0.4838537871837616\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:8\n",
      "Accuracy on Test data: 0.49889105558395386\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:16\n",
      "Accuracy on Test data: 0.8440826535224915\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:32\n",
      "Accuracy on Test data: 0.7944907546043396\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:64\n",
      "Accuracy on Test data: 0.8049591779708862\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:128\n",
      "Accuracy on Test data: 0.7340756058692932\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:256\n",
      "Accuracy on Test data: 0.8760645985603333\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:2\n",
      "Accuracy on Test data: 0.6251774430274963\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:4\n",
      "Accuracy on Test data: 0.7281759977340698\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:8\n",
      "Accuracy on Test data: 0.44362136721611023\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:16\n",
      "Accuracy on Test data: 0.7667672038078308\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:32\n",
      "Accuracy on Test data: 0.4910397529602051\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:64\n",
      "Accuracy on Test data: 0.6845723986625671\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:128\n",
      "Accuracy on Test data: 0.5947924256324768\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:256\n",
      "Accuracy on Test data: 0.6149308085441589\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:2\n",
      "Accuracy on Test data: 0.7338981628417969\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:4\n",
      "Accuracy on Test data: 0.7392210960388184\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:8\n",
      "Accuracy on Test data: 0.7912083268165588\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:16\n",
      "Accuracy on Test data: 0.5687100887298584\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:32\n",
      "Accuracy on Test data: 0.5248846411705017\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:64\n",
      "Accuracy on Test data: 0.6552519798278809\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:128\n",
      "Accuracy on Test data: 0.5892477035522461\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:256\n",
      "Accuracy on Test data: 0.5156582593917847\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:2\n",
      "Accuracy on Test data: 0.836408793926239\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:4\n",
      "Accuracy on Test data: 0.43080198764801025\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:8\n",
      "Accuracy on Test data: 0.5032824873924255\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:16\n",
      "Accuracy on Test data: 0.4415809214115143\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:32\n",
      "Accuracy on Test data: 0.4692601263523102\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:64\n",
      "Accuracy on Test data: 0.445262610912323\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:128\n",
      "Accuracy on Test data: 0.5922639966011047\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:256\n",
      "Accuracy on Test data: 0.4443754553794861\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    epochs = [1]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>feature_count</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.715933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.478567</td>\n",
       "      <td>0.483854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.616749</td>\n",
       "      <td>0.498891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.807958</td>\n",
       "      <td>0.844083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.805934</td>\n",
       "      <td>0.794491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.862631</td>\n",
       "      <td>0.804959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.861996</td>\n",
       "      <td>0.734076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.896329</td>\n",
       "      <td>0.876065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.836297</td>\n",
       "      <td>0.625177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.771463</td>\n",
       "      <td>0.728176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.560349</td>\n",
       "      <td>0.443621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.915598</td>\n",
       "      <td>0.766767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.613138</td>\n",
       "      <td>0.491040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.606390</td>\n",
       "      <td>0.684572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.786267</td>\n",
       "      <td>0.594792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908057</td>\n",
       "      <td>0.739221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.742469</td>\n",
       "      <td>0.791208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.464040</td>\n",
       "      <td>0.568710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0.456896</td>\n",
       "      <td>0.524885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>0.829192</td>\n",
       "      <td>0.655252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>0.742628</td>\n",
       "      <td>0.589248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>0.698472</td>\n",
       "      <td>0.515658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.727962</td>\n",
       "      <td>0.836409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.532427</td>\n",
       "      <td>0.430802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.717563</td>\n",
       "      <td>0.503282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.548303</td>\n",
       "      <td>0.441581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.484243</td>\n",
       "      <td>0.469260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.538440</td>\n",
       "      <td>0.445263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.633439</td>\n",
       "      <td>0.592264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.522782</td>\n",
       "      <td>0.444375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  hidden_layers  feature_count  Train_Accuracy  Test_Accuracy\n",
       "0        1              2              2        0.866700       0.715933\n",
       "1        1              2              4        0.478567       0.483854\n",
       "2        1              2              8        0.616749       0.498891\n",
       "3        1              2             16        0.807958       0.844083\n",
       "4        1              2             32        0.805934       0.794491\n",
       "5        1              2             64        0.862631       0.804959\n",
       "6        1              2            128        0.861996       0.734076\n",
       "7        1              2            256        0.896329       0.876065\n",
       "8        1              4              2        0.836297       0.625177\n",
       "9        1              4              4        0.771463       0.728176\n",
       "10       1              4              8        0.560349       0.443621\n",
       "11       1              4             16        0.915598       0.766767\n",
       "12       1              4             32        0.613138       0.491040\n",
       "13       1              4             64        0.606390       0.684572\n",
       "14       1              4            128        0.786267       0.594792\n",
       "..     ...            ...            ...             ...            ...\n",
       "17       1              6              4        0.908057       0.739221\n",
       "18       1              6              8        0.742469       0.791208\n",
       "19       1              6             16        0.464040       0.568710\n",
       "20       1              6             32        0.456896       0.524885\n",
       "21       1              6             64        0.829192       0.655252\n",
       "22       1              6            128        0.742628       0.589248\n",
       "23       1              6            256        0.698472       0.515658\n",
       "24       1             10              2        0.727962       0.836409\n",
       "25       1             10              4        0.532427       0.430802\n",
       "26       1             10              8        0.717563       0.503282\n",
       "27       1             10             16        0.548303       0.441581\n",
       "28       1             10             32        0.484243       0.469260\n",
       "29       1             10             64        0.538440       0.445263\n",
       "30       1             10            128        0.633439       0.592264\n",
       "31       1             10            256        0.522782       0.444375\n",
       "\n",
       "[32 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8233  0.1767]\n",
      " [ 0.0543  0.9457]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAGXCAYAAABrxf4dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf9dfd6d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = Train.pred_value, y_true = Train.actual_value)\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                             title = \"Confusion Matrix for {} having Accuracy{}\".format(Train.best_parameters, Train.best_acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
