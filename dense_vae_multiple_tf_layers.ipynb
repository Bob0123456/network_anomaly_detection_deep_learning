{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 80\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 80\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0.0, stddev=0.05, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = y))\n",
    "\n",
    "            loss = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.001\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.optimizer = train_op.minimize(self.regularized_loss)  \n",
    "            \n",
    "        # add op for merging summary\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(y, 1)\n",
    "        self.actual = tf.argmax(self.y_, 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = pd.DataFrame()\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 100\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.4)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss, summary_str = sess.run([net.optimizer, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           net.summary_op],\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:0.8})\n",
    "                    summary_writer_train.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                valid_accuracy, summary_str = sess.run([net.tf_accuracy, net.summary_op], \n",
    "                                                      feed_dict={net.x: x_valid, \n",
    "                                                                 net.y_: y_valid, \n",
    "                                                                 net.keep_prob:1})\n",
    "                summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"{}_{}_{}\".format(epochs,f,h):pred_value},)\n",
    "            Train.predictions = pd.concat([Train.predictions, curr_pred], axis = 1)\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, h, f,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:2\n",
      "Step 10 | Training Loss: 0.551219 | Validation Accuracy: 0.590316\n",
      "Accuracy on Test data: 0.5769606232643127\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:4\n",
      "Step 10 | Training Loss: 1.404011 | Validation Accuracy: 0.897916\n",
      "Accuracy on Test data: 0.7184173464775085\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:8\n",
      "Step 10 | Training Loss: 0.423343 | Validation Accuracy: 0.872832\n",
      "Accuracy on Test data: 0.7243168950080872\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:16\n",
      "Step 10 | Training Loss: 1.438968 | Validation Accuracy: 0.604267\n",
      "Accuracy on Test data: 0.673349916934967\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:32\n",
      "Step 10 | Training Loss: 0.671368 | Validation Accuracy: 0.815737\n",
      "Accuracy on Test data: 0.6816447973251343\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:64\n",
      "Step 10 | Training Loss: 0.500838 | Validation Accuracy: 0.878547\n",
      "Accuracy on Test data: 0.7152235507965088\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:128\n",
      "Step 10 | Training Loss: 1.275712 | Validation Accuracy: 0.856837\n",
      "Accuracy on Test data: 0.7367814183235168\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:256\n",
      "Step 10 | Training Loss: 0.983916 | Validation Accuracy: 0.897698\n",
      "Accuracy on Test data: 0.777102530002594\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:2\n",
      "Step 10 | Training Loss: 2.535775 | Validation Accuracy: 0.458623\n",
      "Accuracy on Test data: 0.47844216227531433\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:4\n",
      "Step 10 | Training Loss: 0.195876 | Validation Accuracy: 0.747093\n",
      "Accuracy on Test data: 0.8038502335548401\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:8\n",
      "Step 10 | Training Loss: 0.285918 | Validation Accuracy: 0.830760\n",
      "Accuracy on Test data: 0.6623935699462891\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:16\n",
      "Step 10 | Training Loss: 0.724005 | Validation Accuracy: 0.870351\n",
      "Accuracy on Test data: 0.7279985547065735\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:32\n",
      "Step 10 | Training Loss: 1.217490 | Validation Accuracy: 0.782040\n",
      "Accuracy on Test data: 0.7455642223358154\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:64\n",
      "Step 10 | Training Loss: 0.854436 | Validation Accuracy: 0.872475\n",
      "Accuracy on Test data: 0.8189762234687805\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:128\n",
      "Step 10 | Training Loss: 0.966902 | Validation Accuracy: 0.464616\n",
      "Accuracy on Test data: 0.5427608489990234\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:256\n",
      "Step 10 | Training Loss: 0.332727 | Validation Accuracy: 0.856519\n",
      "Accuracy on Test data: 0.7574964761734009\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:2\n",
      "Step 10 | Training Loss: 0.389031 | Validation Accuracy: 0.865926\n",
      "Accuracy on Test data: 0.7113644480705261\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:4\n",
      "Step 10 | Training Loss: 0.759043 | Validation Accuracy: 0.512086\n",
      "Accuracy on Test data: 0.5120209455490112\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:8\n",
      "Step 10 | Training Loss: 0.426523 | Validation Accuracy: 0.857650\n",
      "Accuracy on Test data: 0.7370032072067261\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:16\n",
      "Step 10 | Training Loss: 0.382053 | Validation Accuracy: 0.852451\n",
      "Accuracy on Test data: 0.7141589522361755\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:32\n",
      "Step 10 | Training Loss: 0.287272 | Validation Accuracy: 0.844076\n",
      "Accuracy on Test data: 0.7713360786437988\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:64\n",
      "Step 10 | Training Loss: 1.681805 | Validation Accuracy: 0.650625\n",
      "Accuracy on Test data: 0.589292049407959\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:128\n",
      "Step 10 | Training Loss: 0.047063 | Validation Accuracy: 0.646140\n",
      "Accuracy on Test data: 0.6622161269187927\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:256\n",
      "Step 10 | Training Loss: 0.996017 | Validation Accuracy: 0.614467\n",
      "Accuracy on Test data: 0.6742814183235168\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:2\n",
      "Step 10 | Training Loss: 0.387576 | Validation Accuracy: 0.535622\n",
      "Accuracy on Test data: 0.4657558500766754\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:4\n",
      "Step 10 | Training Loss: 0.017733 | Validation Accuracy: 0.735781\n",
      "Accuracy on Test data: 0.5131742358207703\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:8\n",
      "Step 10 | Training Loss: 1.498951 | Validation Accuracy: 0.588252\n",
      "Accuracy on Test data: 0.5555358529090881\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:16\n",
      "Step 10 | Training Loss: 0.754320 | Validation Accuracy: 0.552788\n",
      "Accuracy on Test data: 0.4688608944416046\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:32\n",
      "Step 10 | Training Loss: 0.302495 | Validation Accuracy: 0.478984\n",
      "Accuracy on Test data: 0.5415188074111938\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:64\n",
      "Step 10 | Training Loss: 0.716529 | Validation Accuracy: 0.513237\n",
      "Accuracy on Test data: 0.5083392262458801\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:128\n",
      "Step 10 | Training Loss: 0.399014 | Validation Accuracy: 0.541893\n",
      "Accuracy on Test data: 0.48270049691200256\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:256\n",
      "Step 10 | Training Loss: 1.143684 | Validation Accuracy: 0.482735\n",
      "Accuracy on Test data: 0.5231990814208984\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    epochs = [10]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.590316</td>\n",
       "      <td>0.576961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.897916</td>\n",
       "      <td>0.718417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.724317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.604267</td>\n",
       "      <td>0.673350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.815737</td>\n",
       "      <td>0.681645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.878547</td>\n",
       "      <td>0.715224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.856837</td>\n",
       "      <td>0.736781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.897698</td>\n",
       "      <td>0.777103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.458623</td>\n",
       "      <td>0.478442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.747093</td>\n",
       "      <td>0.803850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.830760</td>\n",
       "      <td>0.662394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.870351</td>\n",
       "      <td>0.727999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.782040</td>\n",
       "      <td>0.745564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.872475</td>\n",
       "      <td>0.818976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.464616</td>\n",
       "      <td>0.542761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.856519</td>\n",
       "      <td>0.757496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865926</td>\n",
       "      <td>0.711364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.512086</td>\n",
       "      <td>0.512021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.857650</td>\n",
       "      <td>0.737003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.852451</td>\n",
       "      <td>0.714159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0.844076</td>\n",
       "      <td>0.771336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>0.650625</td>\n",
       "      <td>0.589292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>0.646140</td>\n",
       "      <td>0.662216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>0.614467</td>\n",
       "      <td>0.674281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.535622</td>\n",
       "      <td>0.465756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.735781</td>\n",
       "      <td>0.513174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.588252</td>\n",
       "      <td>0.555536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.552788</td>\n",
       "      <td>0.468861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.478984</td>\n",
       "      <td>0.541519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.513237</td>\n",
       "      <td>0.508339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.541893</td>\n",
       "      <td>0.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.482735</td>\n",
       "      <td>0.523199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0      10               2              2     0.590316    0.576961\n",
       "1      10               2              4     0.897916    0.718417\n",
       "2      10               2              8     0.872832    0.724317\n",
       "3      10               2             16     0.604267    0.673350\n",
       "4      10               2             32     0.815737    0.681645\n",
       "5      10               2             64     0.878547    0.715224\n",
       "6      10               2            128     0.856837    0.736781\n",
       "7      10               2            256     0.897698    0.777103\n",
       "8      10               4              2     0.458623    0.478442\n",
       "9      10               4              4     0.747093    0.803850\n",
       "10     10               4              8     0.830760    0.662394\n",
       "11     10               4             16     0.870351    0.727999\n",
       "12     10               4             32     0.782040    0.745564\n",
       "13     10               4             64     0.872475    0.818976\n",
       "14     10               4            128     0.464616    0.542761\n",
       "15     10               4            256     0.856519    0.757496\n",
       "16     10               6              2     0.865926    0.711364\n",
       "17     10               6              4     0.512086    0.512021\n",
       "18     10               6              8     0.857650    0.737003\n",
       "19     10               6             16     0.852451    0.714159\n",
       "20     10               6             32     0.844076    0.771336\n",
       "21     10               6             64     0.650625    0.589292\n",
       "22     10               6            128     0.646140    0.662216\n",
       "23     10               6            256     0.614467    0.674281\n",
       "24     10              10              2     0.535622    0.465756\n",
       "25     10              10              4     0.735781    0.513174\n",
       "26     10              10              8     0.588252    0.555536\n",
       "27     10              10             16     0.552788    0.468861\n",
       "28     10              10             32     0.478984    0.541519\n",
       "29     10              10             64     0.513237    0.508339\n",
       "30     10              10            128     0.541893    0.482700\n",
       "31     10              10            256     0.482735    0.523199"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions.to_pickle(\"dataset/vae_dense_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/vae_dense_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
