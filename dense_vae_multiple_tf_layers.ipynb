{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 80\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 80\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0.0, stddev=0.005, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = y))\n",
    "\n",
    "            loss = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.001\n",
    "            grad_clip=1\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            \n",
    "        # add op for merging summary\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(y, 1)\n",
    "        self.actual = tf.argmax(self.y_, 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = pd.DataFrame()\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 100\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.4)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss, summary_str = sess.run([net.optimizer, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           net.summary_op],\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:0.8})\n",
    "                    summary_writer_train.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                valid_accuracy, summary_str = sess.run([net.tf_accuracy, net.summary_op], \n",
    "                                                      feed_dict={net.x: x_valid, \n",
    "                                                                 net.y_: y_valid, \n",
    "                                                                 net.keep_prob:1})\n",
    "                summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"{}_{}_{}\".format(epochs,f,h):pred_value},)\n",
    "            Train.predictions = pd.concat([Train.predictions, curr_pred], axis = 1)\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:2\n",
      "Accuracy on Test data: 0.3824077248573303\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:4\n",
      "Accuracy on Test data: 0.7326561212539673\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:8\n",
      "Accuracy on Test data: 0.7519960999488831\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:16\n",
      "Accuracy on Test data: 0.5812633037567139\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:32\n",
      "Accuracy on Test data: 0.5964336395263672\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:64\n",
      "Accuracy on Test data: 0.655518114566803\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:128\n",
      "Accuracy on Test data: 0.7377129197120667\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:256\n",
      "Accuracy on Test data: 0.8074876070022583\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:2\n",
      "Accuracy on Test data: 0.5534954071044922\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:4\n",
      "Accuracy on Test data: 0.542317271232605\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:8\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:16\n",
      "Accuracy on Test data: 0.5668026804924011\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:32\n",
      "Accuracy on Test data: 0.512641966342926\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:64\n",
      "Accuracy on Test data: 0.6778743863105774\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:128\n",
      "Accuracy on Test data: 0.7247604727745056\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:256\n",
      "Accuracy on Test data: 0.753193736076355\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:2\n",
      "Accuracy on Test data: 0.8396469354629517\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:4\n",
      "Accuracy on Test data: 0.6999201774597168\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:8\n",
      "Accuracy on Test data: 0.5773154497146606\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:16\n",
      "Accuracy on Test data: 0.7830021381378174\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:32\n",
      "Accuracy on Test data: 0.8407114744186401\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:64\n",
      "Accuracy on Test data: 0.6625709533691406\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:128\n",
      "Accuracy on Test data: 0.5274130702018738\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:256\n",
      "Accuracy on Test data: 0.7759935855865479\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:2\n",
      "Accuracy on Test data: 0.5571770668029785\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:4\n",
      "Accuracy on Test data: 0.5691536664962769\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:8\n",
      "Accuracy on Test data: 0.5217796564102173\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:16\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:32\n",
      "Accuracy on Test data: 0.5362846255302429\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:64\n",
      "Accuracy on Test data: 0.19974271953105927\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:128\n",
      "Accuracy on Test data: 0.4449964463710785\n",
      "Current Layer Attributes - epochs:1 hidden layers:10 features count:256\n",
      "Accuracy on Test data: 0.43275371193885803\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    epochs = [1]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428002</td>\n",
       "      <td>0.382408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.926335</td>\n",
       "      <td>0.732656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.907363</td>\n",
       "      <td>0.751996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.648799</td>\n",
       "      <td>0.581263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.596434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855348</td>\n",
       "      <td>0.655518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.877396</td>\n",
       "      <td>0.737713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846041</td>\n",
       "      <td>0.807488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.580770</td>\n",
       "      <td>0.553495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.822326</td>\n",
       "      <td>0.542317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.536039</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.465886</td>\n",
       "      <td>0.566803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.638797</td>\n",
       "      <td>0.512642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.797678</td>\n",
       "      <td>0.677874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.814348</td>\n",
       "      <td>0.724760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.836952</td>\n",
       "      <td>0.753194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.875769</td>\n",
       "      <td>0.839647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.879659</td>\n",
       "      <td>0.699920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.577315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.783002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.743838</td>\n",
       "      <td>0.840711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>0.787478</td>\n",
       "      <td>0.662571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.737269</td>\n",
       "      <td>0.527413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>0.888946</td>\n",
       "      <td>0.775994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.467057</td>\n",
       "      <td>0.557177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.466601</td>\n",
       "      <td>0.569154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.468406</td>\n",
       "      <td>0.521780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.475928</td>\n",
       "      <td>0.536285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.258841</td>\n",
       "      <td>0.199743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.534392</td>\n",
       "      <td>0.444996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>0.533657</td>\n",
       "      <td>0.432754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0       1               2              2     0.428002    0.382408\n",
       "1       1               4              2     0.926335    0.732656\n",
       "2       1               8              2     0.907363    0.751996\n",
       "3       1              16              2     0.648799    0.581263\n",
       "4       1              32              2     0.820798    0.596434\n",
       "5       1              64              2     0.855348    0.655518\n",
       "6       1             128              2     0.877396    0.737713\n",
       "7       1             256              2     0.846041    0.807488\n",
       "8       1               2              4     0.580770    0.553495\n",
       "9       1               4              4     0.822326    0.542317\n",
       "10      1               8              4     0.536039    0.430758\n",
       "11      1              16              4     0.465886    0.566803\n",
       "12      1              32              4     0.638797    0.512642\n",
       "13      1              64              4     0.797678    0.677874\n",
       "14      1             128              4     0.814348    0.724760\n",
       "15      1             256              4     0.836952    0.753194\n",
       "16      1               2              6     0.875769    0.839647\n",
       "17      1               4              6     0.879659    0.699920\n",
       "18      1               8              6     0.522366    0.577315\n",
       "19      1              16              6     0.889165    0.783002\n",
       "20      1              32              6     0.743838    0.840711\n",
       "21      1              64              6     0.787478    0.662571\n",
       "22      1             128              6     0.737269    0.527413\n",
       "23      1             256              6     0.888946    0.775994\n",
       "24      1               2             10     0.467057    0.557177\n",
       "25      1               4             10     0.466601    0.569154\n",
       "26      1               8             10     0.468406    0.521780\n",
       "27      1              16             10     0.463286    0.569242\n",
       "28      1              32             10     0.475928    0.536285\n",
       "29      1              64             10     0.258841    0.199743\n",
       "30      1             128             10     0.534392    0.444996\n",
       "31      1             256             10     0.533657    0.432754"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions.to_pickle(\"dataset/vae_dense_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/vae_dense_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.9613  0.0387]\n",
      " [ 0.3186  0.6814]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLhlHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36dc6d1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
