{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    #x_train = x_input.values\n",
    "    #x_test = x_test_input.values\n",
    "\n",
    "    y_train = y_output.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            #self.x = tf.nn.softmax(self.x)\n",
    "            \n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=0.01, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            #z = tf.clip_by_value(z, -1, 1)\n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            #BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            #KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            \n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = y))\n",
    "\n",
    "            #ls = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            #self.regularized_loss = tf.abs(ls, name = \"Regularized_loss\")\n",
    "            self.regularized_loss = tf.clip_by_value(softmax_loss, -1, 1)\n",
    "            #self.regularized_loss = tf.cond(self.regularized_loss is None, lambda: tf.constant(0.5), lambda: self.regularized_loss )\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-3\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            #self.grad = optimizer.compute_gradients(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(y, 1)\n",
    "        self.actual = tf.argmax(self.y_, 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = pd.DataFrame()\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                \n",
    "                #batch_iterations = x_train.shape[0]\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    #, summary_str\n",
    "                    _, train_loss = sess.run([net.train_op,  \n",
    "                                               net.regularized_loss, \n",
    "                                               #net.summary_op\n",
    "                                              ],\n",
    "                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                         net.y_: y_train[i,:], \n",
    "                                                         net.keep_prob:0.5})\n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e1):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "                # summary_str\n",
    "                valid_accuracy = sess.run(net.tf_accuracy, #, net.summary_op \n",
    "                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                     net.y_: preprocess.y_test, \n",
    "                                                     net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"{}_{}_{}\".format(epochs,f,h):pred_value},)\n",
    "            Train.predictions = pd.concat([Train.predictions, curr_pred], axis = 1)\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:2\n",
      "Step 1 | Training Loss: 0.337434 | Validation Accuracy: 0.754702\n",
      "Step 2 | Training Loss: 0.334368 | Validation Accuracy: 0.757984\n",
      "Step 3 | Training Loss: 0.338247 | Validation Accuracy: 0.757718\n",
      "Step 4 | Training Loss: 0.335411 | Validation Accuracy: 0.757674\n",
      "Step 5 | Training Loss: 0.334477 | Validation Accuracy: 0.767300\n",
      "Step 6 | Training Loss: 0.333484 | Validation Accuracy: 0.759005\n",
      "Step 7 | Training Loss: 0.328826 | Validation Accuracy: 0.760380\n",
      "Step 8 | Training Loss: 0.335302 | Validation Accuracy: 0.762997\n",
      "Step 9 | Training Loss: 0.331089 | Validation Accuracy: 0.761799\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.328595 | Validation Accuracy: 0.756698\n",
      "Step 2 | Training Loss: 0.328897 | Validation Accuracy: 0.756875\n",
      "Step 3 | Training Loss: 0.341080 | Validation Accuracy: 0.753371\n",
      "Step 4 | Training Loss: 0.334813 | Validation Accuracy: 0.757940\n",
      "Step 5 | Training Loss: 0.332745 | Validation Accuracy: 0.755589\n",
      "Step 6 | Training Loss: 0.335213 | Validation Accuracy: 0.753859\n",
      "Step 7 | Training Loss: 0.327298 | Validation Accuracy: 0.753371\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.330636 | Validation Accuracy: 0.756654\n",
      "Step 2 | Training Loss: 0.342216 | Validation Accuracy: 0.758738\n",
      "Step 3 | Training Loss: 0.327127 | Validation Accuracy: 0.760779\n",
      "Step 4 | Training Loss: 0.341902 | Validation Accuracy: 0.756299\n",
      "Step 5 | Training Loss: 0.337856 | Validation Accuracy: 0.758251\n",
      "Step 6 | Training Loss: 0.334363 | Validation Accuracy: 0.764106\n",
      "Step 7 | Training Loss: 0.329611 | Validation Accuracy: 0.764061\n",
      "Step 8 | Training Loss: 0.324349 | Validation Accuracy: 0.761799\n",
      "Step 9 | Training Loss: 0.327634 | Validation Accuracy: 0.761577\n",
      "Step 10 | Training Loss: 0.338033 | Validation Accuracy: 0.761533\n",
      "Accuracy on Test data: 0.7615773677825928\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.336469 | Validation Accuracy: 0.749113\n",
      "Step 2 | Training Loss: 0.339085 | Validation Accuracy: 0.756742\n",
      "Step 3 | Training Loss: 0.343450 | Validation Accuracy: 0.760247\n",
      "Step 4 | Training Loss: 0.332800 | Validation Accuracy: 0.762465\n",
      "Step 5 | Training Loss: 0.329816 | Validation Accuracy: 0.757541\n",
      "Step 6 | Training Loss: 0.338350 | Validation Accuracy: 0.753859\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:2\n",
      "Step 1 | Training Loss: 0.334440 | Validation Accuracy: 0.741350\n",
      "Step 2 | Training Loss: 0.339028 | Validation Accuracy: 0.761799\n",
      "Step 3 | Training Loss: 0.340241 | Validation Accuracy: 0.757896\n",
      "Step 4 | Training Loss: 0.329243 | Validation Accuracy: 0.769606\n",
      "Step 5 | Training Loss: 0.335954 | Validation Accuracy: 0.760912\n",
      "Step 6 | Training Loss: 0.328731 | Validation Accuracy: 0.762908\n",
      "Step 7 | Training Loss: 0.339730 | Validation Accuracy: 0.762864\n",
      "Step 8 | Training Loss: 0.321015 | Validation Accuracy: 0.759803\n",
      "Step 9 | Training Loss: 0.341606 | Validation Accuracy: 0.758206\n",
      "Step 10 | Training Loss: 0.335961 | Validation Accuracy: 0.761001\n",
      "Accuracy on Test data: 0.7610006928443909\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.340861 | Validation Accuracy: 0.766013\n",
      "Step 2 | Training Loss: 0.335098 | Validation Accuracy: 0.758029\n",
      "Step 3 | Training Loss: 0.332819 | Validation Accuracy: 0.760291\n",
      "Step 4 | Training Loss: 0.334062 | Validation Accuracy: 0.760690\n",
      "Step 5 | Training Loss: 0.332873 | Validation Accuracy: 0.762420\n",
      "Step 6 | Training Loss: 0.322718 | Validation Accuracy: 0.766102\n",
      "Step 7 | Training Loss: 0.333729 | Validation Accuracy: 0.763130\n",
      "Step 8 | Training Loss: 0.328343 | Validation Accuracy: 0.755012\n",
      "Step 9 | Training Loss: 0.338111 | Validation Accuracy: 0.761844\n",
      "Step 10 | Training Loss: 0.325714 | Validation Accuracy: 0.763352\n",
      "Accuracy on Test data: 0.7633073329925537\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.332476 | Validation Accuracy: 0.760335\n",
      "Step 2 | Training Loss: 0.345746 | Validation Accuracy: 0.767876\n",
      "Step 3 | Training Loss: 0.332508 | Validation Accuracy: 0.761267\n",
      "Step 4 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.338901 | Validation Accuracy: 0.738245\n",
      "Step 2 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 3 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 4 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:2\n",
      "Step 1 | Training Loss: 0.359633 | Validation Accuracy: 0.770626\n",
      "Step 2 | Training Loss: 0.341589 | Validation Accuracy: 0.776304\n",
      "Step 3 | Training Loss: 0.328312 | Validation Accuracy: 0.774840\n",
      "Step 4 | Training Loss: 0.338184 | Validation Accuracy: 0.775949\n",
      "Step 5 | Training Loss: 0.336564 | Validation Accuracy: 0.760956\n",
      "Step 6 | Training Loss: 0.325802 | Validation Accuracy: 0.756565\n",
      "Step 7 | Training Loss: 0.336428 | Validation Accuracy: 0.764061\n",
      "Step 8 | Training Loss: 0.335257 | Validation Accuracy: 0.760690\n",
      "Step 9 | Training Loss: 0.325464 | Validation Accuracy: 0.759936\n",
      "Step 10 | Training Loss: 0.329128 | Validation Accuracy: 0.761666\n",
      "Accuracy on Test data: 0.7616660594940186\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.351692 | Validation Accuracy: 0.754968\n",
      "Step 2 | Training Loss: 0.326623 | Validation Accuracy: 0.759404\n",
      "Step 3 | Training Loss: 0.327426 | Validation Accuracy: 0.759005\n",
      "Step 4 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.342618 | Validation Accuracy: 0.762509\n",
      "Step 2 | Training Loss: 0.335667 | Validation Accuracy: 0.759271\n",
      "Step 3 | Training Loss: 0.333167 | Validation Accuracy: 0.761755\n",
      "Step 4 | Training Loss: 0.334128 | Validation Accuracy: 0.758783\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.348964 | Validation Accuracy: 0.761799\n",
      "Step 2 | Training Loss: 0.341551 | Validation Accuracy: 0.749246\n",
      "Step 3 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 4 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [2, 4, 8, 16]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [10]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.761533</td>\n",
       "      <td>0.761577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.761001</td>\n",
       "      <td>0.761001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.763352</td>\n",
       "      <td>0.763307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.761666</td>\n",
       "      <td>0.761666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0      10               2              2     0.569242    0.569242\n",
       "1      10               4              2     0.569242    0.569242\n",
       "2      10               8              2     0.761533    0.761577\n",
       "3      10              16              2     0.569242    0.569242\n",
       "4      10               2              4     0.761001    0.761001\n",
       "5      10               4              4     0.763352    0.763307\n",
       "6      10               8              4     0.569242    0.569242\n",
       "7      10              16              4     0.569242    0.569242\n",
       "8      10               2              6     0.761666    0.761666\n",
       "9      10               4              6     0.569242    0.569242\n",
       "10     10               8              6     0.569242    0.569242\n",
       "11     10              16              6     0.569242    0.569242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions.to_pickle(\"dataset/tf_vae_dense_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.6402  0.3598]\n",
      " [ 0.074   0.926 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe04de06eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
