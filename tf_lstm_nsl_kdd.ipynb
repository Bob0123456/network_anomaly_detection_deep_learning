{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:54.764193Z",
     "start_time": "2017-06-01T01:19:54.354761Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:54.848726Z",
     "start_time": "2017-06-01T01:19:54.765831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:54.854933Z",
     "start_time": "2017-06-01T01:19:54.850323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:54.881182Z",
     "start_time": "2017-06-01T01:19:54.856323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:55.629898Z",
     "start_time": "2017-06-01T01:19:54.882606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:57.446843Z",
     "start_time": "2017-06-01T01:19:55.631326Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import basic_rnn_seq2seq\n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMCell, MultiRNNCell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:19:57.622469Z",
     "start_time": "2017-06-01T01:19:57.448731Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x_input = tf.placeholder(\"float\", shape=[None, 1, input_dim])\n",
    "            self.y_input_ = tf.placeholder(\"float\", shape=[None, 1, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.x_list = tf.unstack(self.x_input, axis= 1)\n",
    "            self.y_list_ = tf.unstack(self.y_input_, axis = 1)\n",
    "            self.y_ = self.y_list_[0]\n",
    "            \n",
    "            #GO = tf.fill((tf.shape(self.x)[0], 1), 0.5)\n",
    "            \n",
    "            #y_with_GO = tf.stack([self.y_, GO])\n",
    "            \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            multi_cell = MultiRNNCell([LSTMCell(input_dim) for i in range(hidden_layers)] )\n",
    "            \n",
    "            self.y, states = basic_rnn_seq2seq(self.x_list, self.y_list_, multi_cell)\n",
    "            #self.y = tf.slice(self.y, [0, 0], [-1,2])\n",
    "            \n",
    "            #self.out = tf.squeeze(self.y)\n",
    "            \n",
    "            self.y = tf.layers.dense(self.y[0], classes, activation = tf.nn.softmax)\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.y_, self.y)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:59:00.684124Z",
     "start_time": "2017-06-01T00:58:59.843181Z"
    }
   },
   "source": [
    "batch_iterations = 200\n",
    "\n",
    "x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "for i in batch_indices:\n",
    "    print(x_train[i,np.newaxis,:])\n",
    "    print(y_train[i,np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T01:20:58.486264Z",
     "start_time": "2017-06-01T01:20:58.352326Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    \n",
    "                    _, train_loss = sess.run([net.train_op, net.regularized_loss], #net.summary_op\n",
    "                                                          feed_dict={net.x_input: x_train[i,np.newaxis,:], \n",
    "                                                                     net.y_input_: y_train[i,np.newaxis,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e9):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "\n",
    "                valid_accuracy,valid_loss = sess.run([net.tf_accuracy, net.regularized_loss], #net.summary_op \n",
    "                                                      feed_dict={net.x_input: x_valid[:,np.newaxis,:], \n",
    "                                                                 net.y_input_: y_valid[:,np.newaxis,:], \n",
    "                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:.6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, valid_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual, net.y], \n",
    "                                                          feed_dict={net.x_input: preprocess.x_test[:,np.newaxis,:], \n",
    "                                                                     net.y_input_: preprocess.y_test[:,np.newaxis,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "            Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.367576Z",
     "start_time": "2017-06-01T01:20:59.498994Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.000001 | Test Loss: 0.000875 | Test Accuracy: 0.999127\n",
      "Step 2 | Training Loss: 0.000024 | Test Loss: 0.000957 | Test Accuracy: 0.998889\n",
      "Step 3 | Training Loss: 0.000000 | Test Loss: 0.000874 | Test Accuracy: 0.999127\n",
      "Step 4 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 5 | Training Loss: 0.000000 | Test Loss: 0.000556 | Test Accuracy: 0.999444\n",
      "Step 6 | Training Loss: 0.001767 | Test Loss: 0.001032 | Test Accuracy: 0.998968\n",
      "Step 7 | Training Loss: 0.000000 | Test Loss: 0.000318 | Test Accuracy: 0.999682\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 9 | Training Loss: 0.000000 | Test Loss: 0.000477 | Test Accuracy: 0.999524\n",
      "Step 10 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.000318 | Test Accuracy: 0.999682\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.000714 | Test Accuracy: 0.999286\n",
      "Step 13 | Training Loss: 0.001767 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 14 | Training Loss: 0.003534 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 15 | Training Loss: 0.000000 | Test Loss: 0.000318 | Test Accuracy: 0.999682\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 17 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 18 | Training Loss: 0.000000 | Test Loss: 0.000159 | Test Accuracy: 0.999841\n",
      "Step 19 | Training Loss: 0.001767 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 20 | Training Loss: 0.001767 | Test Loss: 0.000556 | Test Accuracy: 0.999444\n",
      "Accuracy on Test data: 0.9803938865661621\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.008836 | Test Loss: 0.009572 | Test Accuracy: 0.990395\n",
      "Step 2 | Training Loss: 0.010603 | Test Loss: 0.005363 | Test Accuracy: 0.994602\n",
      "Step 3 | Training Loss: 0.000004 | Test Loss: 0.000489 | Test Accuracy: 0.999524\n",
      "Step 4 | Training Loss: 0.000000 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 5 | Training Loss: 0.001767 | Test Loss: 0.001112 | Test Accuracy: 0.998889\n",
      "Step 6 | Training Loss: 0.007067 | Test Loss: 0.001270 | Test Accuracy: 0.998730\n",
      "Step 7 | Training Loss: 0.000000 | Test Loss: 0.002064 | Test Accuracy: 0.997936\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.001826 | Test Accuracy: 0.998174\n",
      "Step 9 | Training Loss: 0.001767 | Test Loss: 0.001270 | Test Accuracy: 0.998730\n",
      "Step 10 | Training Loss: 0.003534 | Test Loss: 0.001667 | Test Accuracy: 0.998333\n",
      "Step 11 | Training Loss: 0.001767 | Test Loss: 0.000953 | Test Accuracy: 0.999047\n",
      "Step 12 | Training Loss: 0.001767 | Test Loss: 0.001826 | Test Accuracy: 0.998174\n",
      "Step 13 | Training Loss: 0.001767 | Test Loss: 0.002381 | Test Accuracy: 0.997619\n",
      "Step 14 | Training Loss: 0.005300 | Test Loss: 0.001746 | Test Accuracy: 0.998254\n",
      "Step 15 | Training Loss: 0.000000 | Test Loss: 0.002064 | Test Accuracy: 0.997936\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.001746 | Test Accuracy: 0.998254\n",
      "Step 17 | Training Loss: 0.003534 | Test Loss: 0.001905 | Test Accuracy: 0.998095\n",
      "Step 18 | Training Loss: 0.003534 | Test Loss: 0.001588 | Test Accuracy: 0.998412\n",
      "Step 19 | Training Loss: 0.000000 | Test Loss: 0.001905 | Test Accuracy: 0.998095\n",
      "Step 20 | Training Loss: 0.001767 | Test Loss: 0.001111 | Test Accuracy: 0.998889\n",
      "Accuracy on Test data: 0.9981369972229004\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.001672 | Test Loss: 0.000771 | Test Accuracy: 0.999206\n",
      "Step 2 | Training Loss: 0.001768 | Test Loss: 0.000636 | Test Accuracy: 0.999365\n",
      "Step 3 | Training Loss: 0.000000 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 4 | Training Loss: 0.000000 | Test Loss: 0.000715 | Test Accuracy: 0.999286\n",
      "Step 5 | Training Loss: 0.000000 | Test Loss: 0.000159 | Test Accuracy: 0.999841\n",
      "Step 6 | Training Loss: 0.000000 | Test Loss: 0.000318 | Test Accuracy: 0.999682\n",
      "Step 7 | Training Loss: 0.001767 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.000714 | Test Accuracy: 0.999286\n",
      "Step 9 | Training Loss: 0.000000 | Test Loss: 0.000159 | Test Accuracy: 0.999841\n",
      "Step 10 | Training Loss: 0.000000 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.000000 | Test Accuracy: 1.000000\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 13 | Training Loss: 0.000000 | Test Loss: 0.000318 | Test Accuracy: 0.999682\n",
      "Step 14 | Training Loss: 0.000000 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 15 | Training Loss: 0.000000 | Test Loss: 0.000159 | Test Accuracy: 0.999841\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 17 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 18 | Training Loss: 0.000000 | Test Loss: 0.000556 | Test Accuracy: 0.999444\n",
      "Step 19 | Training Loss: 0.001767 | Test Loss: 0.000238 | Test Accuracy: 0.999762\n",
      "Step 20 | Training Loss: 0.000000 | Test Loss: 0.000635 | Test Accuracy: 0.999365\n",
      "Accuracy on Test data: 0.998846709728241\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.000005 | Test Loss: 0.000879 | Test Accuracy: 0.999127\n",
      "Step 2 | Training Loss: 0.000001 | Test Loss: 0.001112 | Test Accuracy: 0.998889\n",
      "Step 3 | Training Loss: 0.001767 | Test Loss: 0.000635 | Test Accuracy: 0.999365\n",
      "Step 4 | Training Loss: 0.000000 | Test Loss: 0.000715 | Test Accuracy: 0.999286\n",
      "Step 5 | Training Loss: 0.000000 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 6 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 7 | Training Loss: 0.000000 | Test Loss: 0.001111 | Test Accuracy: 0.998889\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 9 | Training Loss: 0.000000 | Test Loss: 0.000635 | Test Accuracy: 0.999365\n",
      "Step 10 | Training Loss: 0.001767 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.000794 | Test Accuracy: 0.999206\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.000556 | Test Accuracy: 0.999444\n",
      "Step 13 | Training Loss: 0.000000 | Test Loss: 0.000635 | Test Accuracy: 0.999365\n",
      "Step 14 | Training Loss: 0.000000 | Test Loss: 0.000714 | Test Accuracy: 0.999286\n",
      "Step 15 | Training Loss: 0.003534 | Test Loss: 0.000794 | Test Accuracy: 0.999206\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 17 | Training Loss: 0.001767 | Test Loss: 0.000635 | Test Accuracy: 0.999365\n",
      "Step 18 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 19 | Training Loss: 0.001767 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Step 20 | Training Loss: 0.000000 | Test Loss: 0.000397 | Test Accuracy: 0.999603\n",
      "Accuracy on Test data: 0.9819907546043396\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.018363 | Test Loss: 0.016579 | Test Accuracy: 0.982219\n",
      "Step 2 | Training Loss: 0.003473 | Test Loss: 0.001708 | Test Accuracy: 0.998254\n",
      "Step 3 | Training Loss: 0.000001 | Test Loss: 0.001350 | Test Accuracy: 0.998651\n",
      "Step 4 | Training Loss: 0.005300 | Test Loss: 0.001667 | Test Accuracy: 0.998333\n",
      "Step 5 | Training Loss: 0.000000 | Test Loss: 0.001429 | Test Accuracy: 0.998571\n",
      "Step 6 | Training Loss: 0.001767 | Test Loss: 0.001588 | Test Accuracy: 0.998412\n",
      "Step 7 | Training Loss: 0.000000 | Test Loss: 0.001429 | Test Accuracy: 0.998571\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.001429 | Test Accuracy: 0.998571\n",
      "Step 9 | Training Loss: 0.005300 | Test Loss: 0.001508 | Test Accuracy: 0.998492\n",
      "Step 10 | Training Loss: 0.001767 | Test Loss: 0.001191 | Test Accuracy: 0.998809\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.001508 | Test Accuracy: 0.998492\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.001826 | Test Accuracy: 0.998174\n",
      "Step 13 | Training Loss: 0.003534 | Test Loss: 0.001349 | Test Accuracy: 0.998651\n",
      "Step 14 | Training Loss: 0.001767 | Test Loss: 0.001984 | Test Accuracy: 0.998016\n",
      "Step 15 | Training Loss: 0.003534 | Test Loss: 0.001191 | Test Accuracy: 0.998809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000953 | Test Accuracy: 0.999047\n",
      "Step 17 | Training Loss: 0.003534 | Test Loss: 0.001826 | Test Accuracy: 0.998174\n",
      "Step 18 | Training Loss: 0.000000 | Test Loss: 0.001270 | Test Accuracy: 0.998730\n",
      "Step 19 | Training Loss: 0.000000 | Test Loss: 0.001349 | Test Accuracy: 0.998651\n",
      "Step 20 | Training Loss: 0.001767 | Test Loss: 0.001429 | Test Accuracy: 0.998571\n",
      "Accuracy on Test data: 0.9861160516738892\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.014138 | Test Loss: 0.012775 | Test Accuracy: 0.987220\n",
      "Step 2 | Training Loss: 0.012326 | Test Loss: 0.013728 | Test Accuracy: 0.986268\n",
      "Step 3 | Training Loss: 0.019437 | Test Loss: 0.013259 | Test Accuracy: 0.986744\n",
      "Step 4 | Training Loss: 0.003570 | Test Loss: 0.003678 | Test Accuracy: 0.996269\n",
      "Step 5 | Training Loss: 0.008816 | Test Loss: 0.002896 | Test Accuracy: 0.997063\n",
      "Step 6 | Training Loss: 0.003535 | Test Loss: 0.003575 | Test Accuracy: 0.996428\n",
      "Step 7 | Training Loss: 0.003534 | Test Loss: 0.003655 | Test Accuracy: 0.996349\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.003414 | Test Accuracy: 0.996587\n",
      "Step 9 | Training Loss: 0.000000 | Test Loss: 0.002937 | Test Accuracy: 0.997063\n",
      "Step 10 | Training Loss: 0.001767 | Test Loss: 0.003255 | Test Accuracy: 0.996746\n",
      "Step 11 | Training Loss: 0.003534 | Test Loss: 0.002302 | Test Accuracy: 0.997698\n",
      "Step 12 | Training Loss: 0.001767 | Test Loss: 0.003334 | Test Accuracy: 0.996666\n",
      "Step 13 | Training Loss: 0.005300 | Test Loss: 0.002620 | Test Accuracy: 0.997381\n",
      "Step 14 | Training Loss: 0.005300 | Test Loss: 0.003414 | Test Accuracy: 0.996587\n",
      "Step 15 | Training Loss: 0.003534 | Test Loss: 0.002778 | Test Accuracy: 0.997222\n",
      "Step 16 | Training Loss: 0.003534 | Test Loss: 0.002223 | Test Accuracy: 0.997777\n",
      "Step 17 | Training Loss: 0.000000 | Test Loss: 0.004128 | Test Accuracy: 0.995872\n",
      "Step 18 | Training Loss: 0.003534 | Test Loss: 0.002858 | Test Accuracy: 0.997142\n",
      "Step 19 | Training Loss: 0.000000 | Test Loss: 0.002858 | Test Accuracy: 0.997142\n",
      "Step 20 | Training Loss: 0.001767 | Test Loss: 0.003413 | Test Accuracy: 0.996587\n",
      "Accuracy on Test data: 0.9784865379333496\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.038866 | Test Loss: 0.041433 | Test Accuracy: 0.958565\n",
      "Step 2 | Training Loss: 0.015580 | Test Loss: 0.021411 | Test Accuracy: 0.978727\n",
      "Step 3 | Training Loss: 0.016550 | Test Loss: 0.025342 | Test Accuracy: 0.974361\n",
      "Step 4 | Training Loss: 0.008183 | Test Loss: 0.004317 | Test Accuracy: 0.995634\n",
      "Step 5 | Training Loss: 0.005302 | Test Loss: 0.003709 | Test Accuracy: 0.996269\n",
      "Step 6 | Training Loss: 0.001770 | Test Loss: 0.003497 | Test Accuracy: 0.996507\n",
      "Step 7 | Training Loss: 0.000000 | Test Loss: 0.000556 | Test Accuracy: 0.999444\n",
      "Step 8 | Training Loss: 0.003575 | Test Loss: 0.001026 | Test Accuracy: 0.998968\n",
      "Step 9 | Training Loss: 0.000000 | Test Loss: 0.000080 | Test Accuracy: 0.999921\n",
      "Step 10 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 13 | Training Loss: 0.000000 | Test Loss: 0.000000 | Test Accuracy: 1.000000\n",
      "Step 14 | Training Loss: 0.000000 | Test Loss: 0.000000 | Test Accuracy: 1.000000\n",
      "Step 15 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000000 | Test Accuracy: 1.000000\n",
      "Step 17 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 18 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 19 | Training Loss: 0.000000 | Test Loss: 0.000079 | Test Accuracy: 0.999921\n",
      "Step 20 | Training Loss: 0.001767 | Test Loss: 0.000000 | Test Accuracy: 1.000000\n",
      "Accuracy on Test data: 0.9995120763778687\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.051237 | Test Loss: 0.040232 | Test Accuracy: 0.959756\n",
      "Step 2 | Training Loss: 0.028071 | Test Loss: 0.028136 | Test Accuracy: 0.970154\n",
      "Step 3 | Training Loss: 0.030242 | Test Loss: 0.042308 | Test Accuracy: 0.957374\n",
      "Step 4 | Training Loss: 0.020013 | Test Loss: 0.022613 | Test Accuracy: 0.976742\n",
      "Step 5 | Training Loss: 0.017587 | Test Loss: 0.022870 | Test Accuracy: 0.976663\n",
      "Step 6 | Training Loss: 0.012152 | Test Loss: 0.021822 | Test Accuracy: 0.977615\n",
      "Step 7 | Training Loss: 0.024422 | Test Loss: 0.021514 | Test Accuracy: 0.977854\n",
      "Step 8 | Training Loss: 0.022974 | Test Loss: 0.014868 | Test Accuracy: 0.985077\n",
      "Step 9 | Training Loss: 0.017671 | Test Loss: 0.017021 | Test Accuracy: 0.983013\n",
      "Step 10 | Training Loss: 0.015912 | Test Loss: 0.013972 | Test Accuracy: 0.986030\n",
      "Step 11 | Training Loss: 0.008835 | Test Loss: 0.014685 | Test Accuracy: 0.985315\n",
      "Step 12 | Training Loss: 0.010601 | Test Loss: 0.013654 | Test Accuracy: 0.986347\n",
      "Step 13 | Training Loss: 0.014134 | Test Loss: 0.013574 | Test Accuracy: 0.986426\n",
      "Step 14 | Training Loss: 0.012368 | Test Loss: 0.014764 | Test Accuracy: 0.985236\n",
      "Step 15 | Training Loss: 0.014134 | Test Loss: 0.013177 | Test Accuracy: 0.986823\n",
      "Step 16 | Training Loss: 0.017758 | Test Loss: 0.015097 | Test Accuracy: 0.984760\n",
      "Step 17 | Training Loss: 0.017668 | Test Loss: 0.014104 | Test Accuracy: 0.985871\n",
      "Step 18 | Training Loss: 0.014135 | Test Loss: 0.014973 | Test Accuracy: 0.984998\n",
      "Step 19 | Training Loss: 0.014141 | Test Loss: 0.014373 | Test Accuracy: 0.985633\n",
      "Step 20 | Training Loss: 0.017668 | Test Loss: 0.012780 | Test Accuracy: 0.987220\n",
      "Accuracy on Test data: 0.8909243941307068\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.048913 | Test Loss: 0.050344 | Test Accuracy: 0.949198\n",
      "Step 2 | Training Loss: 0.067359 | Test Loss: 0.055008 | Test Accuracy: 0.944753\n",
      "Step 3 | Training Loss: 0.058190 | Test Loss: 0.058185 | Test Accuracy: 0.940943\n",
      "Step 4 | Training Loss: 0.020390 | Test Loss: 0.027054 | Test Accuracy: 0.971742\n",
      "Step 5 | Training Loss: 0.026753 | Test Loss: 0.026510 | Test Accuracy: 0.971900\n",
      "Step 6 | Training Loss: 0.017599 | Test Loss: 0.021674 | Test Accuracy: 0.977695\n",
      "Step 7 | Training Loss: 0.018891 | Test Loss: 0.019219 | Test Accuracy: 0.980314\n",
      "Step 8 | Training Loss: 0.026172 | Test Loss: 0.022937 | Test Accuracy: 0.976028\n",
      "Step 9 | Training Loss: 0.028513 | Test Loss: 0.019015 | Test Accuracy: 0.980156\n",
      "Step 10 | Training Loss: 0.033098 | Test Loss: 0.020759 | Test Accuracy: 0.976425\n",
      "Step 11 | Training Loss: 0.008092 | Test Loss: 0.016032 | Test Accuracy: 0.983886\n",
      "Step 12 | Training Loss: 0.013996 | Test Loss: 0.013412 | Test Accuracy: 0.986426\n",
      "Step 13 | Training Loss: 0.019236 | Test Loss: 0.014753 | Test Accuracy: 0.984680\n",
      "Step 14 | Training Loss: 0.007084 | Test Loss: 0.012822 | Test Accuracy: 0.986982\n",
      "Step 15 | Training Loss: 0.001769 | Test Loss: 0.006512 | Test Accuracy: 0.993412\n",
      "Step 16 | Training Loss: 0.005236 | Test Loss: 0.005264 | Test Accuracy: 0.994682\n",
      "Step 17 | Training Loss: 0.008199 | Test Loss: 0.006208 | Test Accuracy: 0.993491\n",
      "Step 18 | Training Loss: 0.000010 | Test Loss: 0.000164 | Test Accuracy: 0.999841\n",
      "Step 19 | Training Loss: 0.000003 | Test Loss: 0.000160 | Test Accuracy: 0.999841\n",
      "Step 20 | Training Loss: 0.000001 | Test Loss: 0.000398 | Test Accuracy: 0.999603\n",
      "Accuracy on Test data: 0.9869588613510132\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.060023 | Test Loss: 0.039549 | Test Accuracy: 0.960391\n",
      "Step 2 | Training Loss: 0.150177 | Test Loss: 0.146636 | Test Accuracy: 0.853389\n",
      "Step 3 | Training Loss: 0.038255 | Test Loss: 0.035764 | Test Accuracy: 0.959279\n",
      "Step 4 | Training Loss: 0.009725 | Test Loss: 0.015171 | Test Accuracy: 0.982616\n",
      "Step 5 | Training Loss: 0.020673 | Test Loss: 0.014853 | Test Accuracy: 0.984760\n",
      "Step 6 | Training Loss: 0.009865 | Test Loss: 0.012523 | Test Accuracy: 0.985871\n",
      "Step 7 | Training Loss: 0.000501 | Test Loss: 0.011480 | Test Accuracy: 0.987141\n",
      "Step 8 | Training Loss: 0.011641 | Test Loss: 0.010508 | Test Accuracy: 0.988570\n",
      "Step 9 | Training Loss: 0.011620 | Test Loss: 0.009668 | Test Accuracy: 0.989125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.011542 | Test Loss: 0.009639 | Test Accuracy: 0.989760\n",
      "Step 11 | Training Loss: 0.016302 | Test Loss: 0.009798 | Test Accuracy: 0.989840\n",
      "Step 12 | Training Loss: 0.008870 | Test Loss: 0.009132 | Test Accuracy: 0.989681\n",
      "Step 13 | Training Loss: 0.022026 | Test Loss: 0.010958 | Test Accuracy: 0.988411\n",
      "Step 14 | Training Loss: 0.015188 | Test Loss: 0.011386 | Test Accuracy: 0.988014\n",
      "Step 15 | Training Loss: 0.011669 | Test Loss: 0.014885 | Test Accuracy: 0.983331\n",
      "Step 16 | Training Loss: 0.013022 | Test Loss: 0.012898 | Test Accuracy: 0.987141\n",
      "Step 17 | Training Loss: 0.049470 | Test Loss: 0.057711 | Test Accuracy: 0.942292\n",
      "Step 18 | Training Loss: 0.064950 | Test Loss: 0.060126 | Test Accuracy: 0.939752\n",
      "Step 19 | Training Loss: 0.044118 | Test Loss: 0.050733 | Test Accuracy: 0.948960\n",
      "Step 20 | Training Loss: 0.003728 | Test Loss: 0.003824 | Test Accuracy: 0.995634\n",
      "Accuracy on Test data: 0.8834723234176636\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.041351 | Test Loss: 0.026145 | Test Accuracy: 0.972218\n",
      "Step 2 | Training Loss: 0.032925 | Test Loss: 0.023676 | Test Accuracy: 0.974520\n",
      "Step 3 | Training Loss: 0.016493 | Test Loss: 0.019639 | Test Accuracy: 0.979600\n",
      "Step 4 | Training Loss: 0.024342 | Test Loss: 0.016752 | Test Accuracy: 0.981902\n",
      "Step 5 | Training Loss: 0.021126 | Test Loss: 0.020642 | Test Accuracy: 0.979044\n",
      "Step 6 | Training Loss: 0.001760 | Test Loss: 0.002059 | Test Accuracy: 0.997936\n",
      "Step 7 | Training Loss: 0.001769 | Test Loss: 0.001337 | Test Accuracy: 0.998651\n",
      "Step 8 | Training Loss: 0.001762 | Test Loss: 0.001938 | Test Accuracy: 0.998016\n",
      "Step 9 | Training Loss: 0.001761 | Test Loss: 0.001270 | Test Accuracy: 0.998730\n",
      "Step 10 | Training Loss: 0.000003 | Test Loss: 0.000909 | Test Accuracy: 0.999047\n",
      "Step 11 | Training Loss: 0.000005 | Test Loss: 0.001160 | Test Accuracy: 0.998809\n",
      "Step 12 | Training Loss: 0.001767 | Test Loss: 0.000977 | Test Accuracy: 0.998968\n",
      "Step 13 | Training Loss: 0.000001 | Test Loss: 0.000547 | Test Accuracy: 0.999444\n",
      "Step 14 | Training Loss: 0.000001 | Test Loss: 0.000387 | Test Accuracy: 0.999603\n",
      "Step 15 | Training Loss: 0.001767 | Test Loss: 0.000299 | Test Accuracy: 0.999682\n",
      "Step 16 | Training Loss: 0.003047 | Test Loss: 0.000465 | Test Accuracy: 0.999524\n",
      "Step 17 | Training Loss: 0.000000 | Test Loss: 0.000356 | Test Accuracy: 0.999603\n",
      "Step 18 | Training Loss: 0.001761 | Test Loss: 0.000953 | Test Accuracy: 0.999047\n",
      "Step 19 | Training Loss: 0.001768 | Test Loss: 0.000476 | Test Accuracy: 0.999524\n",
      "Step 20 | Training Loss: 0.000001 | Test Loss: 0.000610 | Test Accuracy: 0.999365\n",
      "Accuracy on Test data: 0.9956973195075989\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.056668 | Test Loss: 0.055979 | Test Accuracy: 0.943959\n",
      "Step 2 | Training Loss: 0.023965 | Test Loss: 0.026512 | Test Accuracy: 0.972773\n",
      "Step 3 | Training Loss: 0.024657 | Test Loss: 0.035345 | Test Accuracy: 0.963328\n",
      "Step 4 | Training Loss: 0.025590 | Test Loss: 0.026395 | Test Accuracy: 0.972853\n",
      "Step 5 | Training Loss: 0.031902 | Test Loss: 0.025410 | Test Accuracy: 0.974361\n",
      "Step 6 | Training Loss: 0.030687 | Test Loss: 0.023191 | Test Accuracy: 0.976822\n",
      "Step 7 | Training Loss: 0.012511 | Test Loss: 0.023813 | Test Accuracy: 0.976107\n",
      "Step 8 | Training Loss: 0.029255 | Test Loss: 0.026704 | Test Accuracy: 0.972615\n",
      "Step 9 | Training Loss: 0.054732 | Test Loss: 0.050981 | Test Accuracy: 0.944039\n",
      "Step 10 | Training Loss: 0.022968 | Test Loss: 0.026616 | Test Accuracy: 0.973329\n",
      "Step 11 | Training Loss: 0.033048 | Test Loss: 0.027551 | Test Accuracy: 0.972535\n",
      "Step 12 | Training Loss: 0.053004 | Test Loss: 0.052851 | Test Accuracy: 0.947134\n",
      "Step 13 | Training Loss: 0.048972 | Test Loss: 0.049799 | Test Accuracy: 0.950071\n",
      "Step 14 | Training Loss: 0.028303 | Test Loss: 0.033098 | Test Accuracy: 0.966820\n",
      "Step 15 | Training Loss: 0.036924 | Test Loss: 0.031120 | Test Accuracy: 0.968566\n",
      "Step 16 | Training Loss: 0.065189 | Test Loss: 0.057182 | Test Accuracy: 0.941816\n",
      "Step 17 | Training Loss: 0.038825 | Test Loss: 0.028495 | Test Accuracy: 0.971107\n",
      "Step 18 | Training Loss: 0.054770 | Test Loss: 0.054355 | Test Accuracy: 0.945626\n",
      "Step 19 | Training Loss: 0.058304 | Test Loss: 0.055363 | Test Accuracy: 0.944674\n",
      "Step 20 | Training Loss: 0.056537 | Test Loss: 0.051344 | Test Accuracy: 0.948643\n",
      "Accuracy on Test data: 0.8218594789505005\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [20]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.374486Z",
     "start_time": "2017-06-01T02:38:32.370016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.440492Z",
     "start_time": "2017-06-01T02:38:32.376167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.998847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>0.998137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.995697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>0.986959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>0.986116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999603</td>\n",
       "      <td>0.981991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999444</td>\n",
       "      <td>0.980394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.996587</td>\n",
       "      <td>0.978487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987220</td>\n",
       "      <td>0.890924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>0.883472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.948643</td>\n",
       "      <td>0.821859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "6      20              16              4     1.000000    0.999512\n",
       "2      20              16              2     0.999365    0.998847\n",
       "1      20               8              2     0.998889    0.998137\n",
       "10     20              16              6     0.999365    0.995697\n",
       "8      20               4              6     0.999603    0.986959\n",
       "4      20               4              4     0.998571    0.986116\n",
       "3      20              32              2     0.999603    0.981991\n",
       "0      20               4              2     0.999444    0.980394\n",
       "5      20               8              4     0.996587    0.978487\n",
       "7      20              32              4     0.987220    0.890924\n",
       "9      20               8              6     0.995634    0.883472\n",
       "11     20              32              6     0.948643    0.821859"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.452585Z",
     "start_time": "2017-06-01T02:38:32.442359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_lstm_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_lstm_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.516492Z",
     "start_time": "2017-06-01T02:38:32.454061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:38:32.814963Z",
     "start_time": "2017-06-01T02:38:32.518367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 1.      0.    ]\n",
      " [ 0.0011  0.9989]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac7c64deb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T02:42:25.395449Z",
     "start_time": "2017-06-01T02:42:25.358093Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.5 GB\n",
    "pd.Series(Train.pred_value).to_csv('LSTM_prediction_values.csv')"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
