{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:14.135606Z",
     "start_time": "2017-07-23T20:56:13.724224Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:14.146344Z",
     "start_time": "2017-07-23T20:56:14.137036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/tf_vae_dense_trained_together_nsl_kdd_all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_dense_trained_together_nsl_kdd_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:16.674886Z",
     "start_time": "2017-07-23T20:56:14.148064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:16.681598Z",
     "start_time": "2017-07-23T20:56:16.676379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:16.689072Z",
     "start_time": "2017-07-23T20:56:16.682835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:17.484350Z",
     "start_time": "2017-07-23T20:56:16.690453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:25.678982Z",
     "start_time": "2017-07-23T20:56:17.485864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T20:56:26.124234Z",
     "start_time": "2017-07-23T20:56:25.680674Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:00:27.036001Z",
     "start_time": "2017-07-23T21:00:26.639449Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}\".format(h,f),\n",
    "                    exist_ok = True)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            Train.best_acc = 0\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.2)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                                   net.regularized_loss, \n",
    "                                                                   ], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                                             net.y_: y_train[i,:], \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        count = 10\n",
    "                        \n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                            print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                         .format(h,f)))\n",
    "                            train_batch()\n",
    "                            count -= 1\n",
    "\n",
    "                    valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                      net.actual, net.y], #net.summary_op \n",
    "                                                                                      feed_dict={net.x: preprocess.x_test, \n",
    "                                                                                     net.y_: preprocess.y_test, \n",
    "                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    test_accuracy_, test_loss_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                      net.actual, net.y], #net.summary_op \n",
    "                                                                                      feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                                     net.y_: preprocess.y_test_, \n",
    "                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(test_accuracy, test_accuracy_))\n",
    "\n",
    "                    if test_accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = test_accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        \n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        \n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if test_accuracy > Train.best_acc:\n",
    "                        Train.best_acc = test_accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, test_accuracy, f1_score, test_accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, test_accuracy, f1_score, test_accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "            print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:00:27.427722Z",
     "start_time": "2017-07-23T21:00:27.340943Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "        Train.results = []\n",
    "\n",
    "        epochs = [15]\n",
    "        lrs = [1e-2, 1e-2]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        \n",
    "        if not os.path.isfile('dataset/scores/tf_vae_dense_trained_together_nsl_kdd_all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_all.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "                \n",
    "        past_scores.to_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_all.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:14:36.111869Z",
     "start_time": "2017-07-23T21:00:28.086969Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.000105 | Validation Accuracy: 0.779917\n",
      "Accuracy on Test data: 0.6815117001533508, 0.5146835446357727\n",
      "Step 2 | Training Loss: 0.000215 | Validation Accuracy: 0.881524\n",
      "Accuracy on Test data: 0.761799156665802, 0.5572152137756348\n",
      "Step 3 | Training Loss: 0.000090 | Validation Accuracy: 0.808533\n",
      "Accuracy on Test data: 0.7109208703041077, 0.5145147442817688\n",
      "Step 4 | Training Loss: 0.000043 | Validation Accuracy: 0.788490\n",
      "Accuracy on Test data: 0.6199432015419006, 0.40708860754966736\n",
      "Step 5 | Training Loss: 0.000306 | Validation Accuracy: 0.778647\n",
      "Accuracy on Test data: 0.7403743863105774, 0.6514768004417419\n",
      "Step 6 | Training Loss: 0.000425 | Validation Accuracy: 0.824846\n",
      "Accuracy on Test data: 0.7391323447227478, 0.5559493899345398\n",
      "Step 7 | Training Loss: 0.000102 | Validation Accuracy: 0.796706\n",
      "Accuracy on Test data: 0.6808019876480103, 0.4925738275051117\n",
      "Step 8 | Training Loss: 0.000069 | Validation Accuracy: 0.824132\n",
      "Accuracy on Test data: 0.7008073329925537, 0.5202531814575195\n",
      "Step 9 | Training Loss: 0.000006 | Validation Accuracy: 0.848502\n",
      "Accuracy on Test data: 0.7631298899650574, 0.60472571849823\n",
      "Step 10 | Training Loss: 0.000210 | Validation Accuracy: 0.819806\n",
      "Accuracy on Test data: 0.719171404838562, 0.5349367260932922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 | Training Loss: 0.598608 | Validation Accuracy: 0.539829\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.407515 | Validation Accuracy: 0.533915\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.512440 | Validation Accuracy: 0.533876\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.383999 | Validation Accuracy: 0.529867\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.487952 | Validation Accuracy: 0.530185\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.855318 | Validation Accuracy: 0.534392\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.833317 | Validation Accuracy: 0.530105\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.762139 | Validation Accuracy: 0.533320\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.486080 | Validation Accuracy: 0.536059\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.462135 | Validation Accuracy: 0.538718\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.412023 | Validation Accuracy: 0.534193\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.456503 | Validation Accuracy: 0.536773\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.229875 | Validation Accuracy: 0.537130\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.206023 | Validation Accuracy: 0.531931\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.372092 | Validation Accuracy: 0.538242\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.524235 | Validation Accuracy: 0.536257\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.507455 | Validation Accuracy: 0.530105\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.717263 | Validation Accuracy: 0.537051\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 1.450042 | Validation Accuracy: 0.530978\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.159706 | Validation Accuracy: 0.536297\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.7631298899650574\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.000283 | Validation Accuracy: 0.907561\n",
      "Accuracy on Test data: 0.8423970937728882, 0.7108016610145569\n",
      "Step 2 | Training Loss: 0.000308 | Validation Accuracy: 0.912244\n",
      "Accuracy on Test data: 0.7743967175483704, 0.5726582407951355\n",
      "Step 3 | Training Loss: 0.512295 | Validation Accuracy: 0.899464\n",
      "Accuracy on Test data: 0.7870386838912964, 0.6084387898445129\n",
      "Step 4 | Training Loss: 0.000042 | Validation Accuracy: 0.908117\n",
      "Accuracy on Test data: 0.7806955575942993, 0.5907173156738281\n",
      "Step 5 | Training Loss: 0.000247 | Validation Accuracy: 0.914705\n",
      "Accuracy on Test data: 0.783401370048523, 0.596540093421936\n",
      "Step 6 | Training Loss: 0.490228 | Validation Accuracy: 0.911451\n",
      "Accuracy on Test data: 0.8189762234687805, 0.6613501906394958\n",
      "Step 7 | Training Loss: 1.068962 | Validation Accuracy: 0.534789\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.006739 | Validation Accuracy: 0.537091\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.074578 | Validation Accuracy: 0.531693\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.751071 | Validation Accuracy: 0.532606\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 1.019314 | Validation Accuracy: 0.533558\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.578699 | Validation Accuracy: 0.533757\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 1.384635 | Validation Accuracy: 0.537011\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.271055 | Validation Accuracy: 0.533995\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.088443 | Validation Accuracy: 0.530383\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.808341 | Validation Accuracy: 0.533479\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.233517 | Validation Accuracy: 0.535424\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.325851 | Validation Accuracy: 0.528835\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.184011 | Validation Accuracy: 0.533677\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.879355 | Validation Accuracy: 0.530343\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.286230 | Validation Accuracy: 0.533519\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.695318 | Validation Accuracy: 0.532487\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 1.898309 | Validation Accuracy: 0.533280\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.264839 | Validation Accuracy: 0.534749\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.602111 | Validation Accuracy: 0.537249\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.615690 | Validation Accuracy: 0.531217\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.076663 | Validation Accuracy: 0.535860\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.814122 | Validation Accuracy: 0.537329\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.133343 | Validation Accuracy: 0.533876\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.706984 | Validation Accuracy: 0.536178\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.8423970937728882\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.000079 | Validation Accuracy: 0.889303\n",
      "Accuracy on Test data: 0.7514194250106812, 0.5342615842819214\n",
      "Step 2 | Training Loss: 1.391636 | Validation Accuracy: 0.532804\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.963884 | Validation Accuracy: 0.526335\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 1.991958 | Validation Accuracy: 0.532804\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.283592 | Validation Accuracy: 0.536297\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.824139 | Validation Accuracy: 0.535344\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.166301 | Validation Accuracy: 0.536456\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 1.036381 | Validation Accuracy: 0.530462\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.666918 | Validation Accuracy: 0.533955\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.059618 | Validation Accuracy: 0.535066\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.708252 | Validation Accuracy: 0.533320\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.991342 | Validation Accuracy: 0.526295\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 | Training Loss: 0.649616 | Validation Accuracy: 0.534908\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 1.372472 | Validation Accuracy: 0.527605\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.521095 | Validation Accuracy: 0.539909\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.282591 | Validation Accuracy: 0.534114\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.567270 | Validation Accuracy: 0.534312\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.391793 | Validation Accuracy: 0.532288\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 2.305556 | Validation Accuracy: 0.529788\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.192790 | Validation Accuracy: 0.539909\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.178322 | Validation Accuracy: 0.536932\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.263125 | Validation Accuracy: 0.536098\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.519965 | Validation Accuracy: 0.534590\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.065175 | Validation Accuracy: 0.531256\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.283050 | Validation Accuracy: 0.534590\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.362700 | Validation Accuracy: 0.532884\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.403375 | Validation Accuracy: 0.536138\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.361964 | Validation Accuracy: 0.537805\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 1.435013 | Validation Accuracy: 0.533082\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.328835 | Validation Accuracy: 0.529708\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.7514194250106812\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 2.599756 | Validation Accuracy: 0.532884\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 1.657728 | Validation Accuracy: 0.537289\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.467931 | Validation Accuracy: 0.530502\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.734667 | Validation Accuracy: 0.530145\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 1.043036 | Validation Accuracy: 0.535027\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.021370 | Validation Accuracy: 0.537210\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 1.043934 | Validation Accuracy: 0.528240\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.737309 | Validation Accuracy: 0.532963\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 1.598725 | Validation Accuracy: 0.537249\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.189645 | Validation Accuracy: 0.533638\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 1.675554 | Validation Accuracy: 0.533439\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.609000 | Validation Accuracy: 0.531534\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.704782 | Validation Accuracy: 0.537210\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.193881 | Validation Accuracy: 0.532328\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.475317 | Validation Accuracy: 0.538797\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.510727 | Validation Accuracy: 0.541655\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.823664 | Validation Accuracy: 0.527684\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.593812 | Validation Accuracy: 0.537448\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 1.228738 | Validation Accuracy: 0.534550\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.098134 | Validation Accuracy: 0.532368\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.616636 | Validation Accuracy: 0.537011\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 2.150686 | Validation Accuracy: 0.534590\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.166524 | Validation Accuracy: 0.533836\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.651595 | Validation Accuracy: 0.533519\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.061141 | Validation Accuracy: 0.532685\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.292590 | Validation Accuracy: 0.538202\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.214442 | Validation Accuracy: 0.537527\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.199889 | Validation Accuracy: 0.532923\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.964735 | Validation Accuracy: 0.530502\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.996989 | Validation Accuracy: 0.536614\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.000490 | Validation Accuracy: 0.753403\n",
      "Accuracy on Test data: 0.5652945637702942, 0.2329113930463791\n",
      "Step 2 | Training Loss: 0.240252 | Validation Accuracy: 0.529550\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.945947 | Validation Accuracy: 0.536337\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.019427 | Validation Accuracy: 0.534154\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 1.391309 | Validation Accuracy: 0.535741\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 1.478997 | Validation Accuracy: 0.538678\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 1.188855 | Validation Accuracy: 0.538956\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.839642 | Validation Accuracy: 0.536376\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.541445 | Validation Accuracy: 0.535781\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.286773 | Validation Accuracy: 0.530542\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.325222 | Validation Accuracy: 0.538559\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.274262 | Validation Accuracy: 0.539472\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 1.271192 | Validation Accuracy: 0.535027\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.453955 | Validation Accuracy: 0.538361\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 | Training Loss: 1.404501 | Validation Accuracy: 0.535344\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.574517 | Validation Accuracy: 0.539353\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.343881 | Validation Accuracy: 0.535463\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.909692 | Validation Accuracy: 0.534947\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.471563 | Validation Accuracy: 0.532129\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.439323 | Validation Accuracy: 0.535305\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.997143 | Validation Accuracy: 0.534312\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 1.349683 | Validation Accuracy: 0.534273\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.586275 | Validation Accuracy: 0.536654\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 1.610457 | Validation Accuracy: 0.534908\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.007753 | Validation Accuracy: 0.535821\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 2.477878 | Validation Accuracy: 0.530304\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.420019 | Validation Accuracy: 0.533757\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 2.410817 | Validation Accuracy: 0.536892\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.405838 | Validation Accuracy: 0.532248\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.336811 | Validation Accuracy: 0.531177\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.5652945637702942\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.000110 | Validation Accuracy: 0.699821\n",
      "Accuracy on Test data: 0.6759669780731201, 0.6384810209274292\n",
      "Step 2 | Training Loss: 0.000136 | Validation Accuracy: 0.701171\n",
      "Accuracy on Test data: 0.7090134620666504, 0.6297890543937683\n",
      "Step 3 | Training Loss: 0.000347 | Validation Accuracy: 0.711490\n",
      "Accuracy on Test data: 0.7019606232643127, 0.6233755350112915\n",
      "Step 4 | Training Loss: 0.000198 | Validation Accuracy: 0.730502\n",
      "Accuracy on Test data: 0.6898065805435181, 0.5832067728042603\n",
      "Step 5 | Training Loss: 0.000040 | Validation Accuracy: 0.772217\n",
      "Accuracy on Test data: 0.7408179640769958, 0.6413502097129822\n",
      "Step 6 | Training Loss: 0.000031 | Validation Accuracy: 0.780353\n",
      "Accuracy on Test data: 0.7749733924865723, 0.7088607549667358\n",
      "Step 7 | Training Loss: 1.086330 | Validation Accuracy: 0.538678\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.463382 | Validation Accuracy: 0.533042\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.691722 | Validation Accuracy: 0.536098\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.293956 | Validation Accuracy: 0.536337\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 2.297280 | Validation Accuracy: 0.530026\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.260770 | Validation Accuracy: 0.541457\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.219086 | Validation Accuracy: 0.536654\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.508036 | Validation Accuracy: 0.535463\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.848101 | Validation Accuracy: 0.532844\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.276672 | Validation Accuracy: 0.535027\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.588382 | Validation Accuracy: 0.533995\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.999855 | Validation Accuracy: 0.534471\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.462957 | Validation Accuracy: 0.536257\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.483059 | Validation Accuracy: 0.535503\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.131366 | Validation Accuracy: 0.535702\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.427725 | Validation Accuracy: 0.537011\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.029087 | Validation Accuracy: 0.537607\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.568524 | Validation Accuracy: 0.535305\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.773519 | Validation Accuracy: 0.532090\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 2.523225 | Validation Accuracy: 0.532010\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.735860 | Validation Accuracy: 0.534471\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.865220 | Validation Accuracy: 0.533161\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.138182 | Validation Accuracy: 0.534431\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.185640 | Validation Accuracy: 0.537011\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.7749733924865723\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.000227 | Validation Accuracy: 0.860528\n",
      "Accuracy on Test data: 0.7907647490501404, 0.6569620370864868\n",
      "Step 2 | Training Loss: 0.000039 | Validation Accuracy: 0.937210\n",
      "Accuracy on Test data: 0.8527324199676514, 0.7318143248558044\n",
      "Step 3 | Training Loss: 0.000080 | Validation Accuracy: 0.949712\n",
      "Accuracy on Test data: 0.8428850173950195, 0.7129113674163818\n",
      "Step 4 | Training Loss: 0.000047 | Validation Accuracy: 0.940385\n",
      "Accuracy on Test data: 0.8350780606269836, 0.6978058815002441\n",
      "Step 5 | Training Loss: 0.000057 | Validation Accuracy: 0.926890\n",
      "Accuracy on Test data: 0.8594304323196411, 0.7392405271530151\n",
      "Step 6 | Training Loss: 0.000022 | Validation Accuracy: 0.931217\n",
      "Accuracy on Test data: 0.7964868545532227, 0.6178902983665466\n",
      "Step 7 | Training Loss: 0.000013 | Validation Accuracy: 0.935305\n",
      "Accuracy on Test data: 0.8556600213050842, 0.7412658333778381\n",
      "Step 8 | Training Loss: 0.000010 | Validation Accuracy: 0.914507\n",
      "Accuracy on Test data: 0.8818311095237732, 0.7859071493148804\n",
      "Step 9 | Training Loss: 0.000015 | Validation Accuracy: 0.901766\n",
      "Accuracy on Test data: 0.8468772172927856, 0.7282700538635254\n",
      "Step 10 | Training Loss: 0.000013 | Validation Accuracy: 0.916928\n",
      "Accuracy on Test data: 0.8149396777153015, 0.654261589050293\n",
      "Step 11 | Training Loss: 0.000048 | Validation Accuracy: 0.915023\n",
      "Accuracy on Test data: 0.8591199517250061, 0.74388188123703\n",
      "Step 12 | Training Loss: 0.000078 | Validation Accuracy: 0.906013\n",
      "Accuracy on Test data: 0.8856902122497559, 0.796708881855011\n",
      "Step 13 | Training Loss: 0.000028 | Validation Accuracy: 0.926573\n",
      "Accuracy on Test data: 0.8008782863616943, 0.6591561436653137\n",
      "Step 14 | Training Loss: 0.000007 | Validation Accuracy: 0.906767\n",
      "Accuracy on Test data: 0.8501597046852112, 0.750717282295227\n",
      "Step 15 | Training Loss: 0.000037 | Validation Accuracy: 0.912880\n",
      "Accuracy on Test data: 0.8659066557884216, 0.7583122253417969\n",
      "Step 1 | Training Loss: 0.000024 | Validation Accuracy: 0.918992\n",
      "Accuracy on Test data: 0.8545067310333252, 0.7286919951438904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.000003 | Validation Accuracy: 0.919548\n",
      "Accuracy on Test data: 0.8691004514694214, 0.7659915685653687\n",
      "Step 3 | Training Loss: 0.000047 | Validation Accuracy: 0.873784\n",
      "Accuracy on Test data: 0.8163591027259827, 0.6986497640609741\n",
      "Step 4 | Training Loss: 0.000076 | Validation Accuracy: 0.894344\n",
      "Accuracy on Test data: 0.8423527479171753, 0.7253164649009705\n",
      "Step 5 | Training Loss: 0.431742 | Validation Accuracy: 0.535582\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.744948 | Validation Accuracy: 0.535622\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.141115 | Validation Accuracy: 0.532288\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 1.027838 | Validation Accuracy: 0.535305\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.055295 | Validation Accuracy: 0.536892\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 2.534327 | Validation Accuracy: 0.535741\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.274675 | Validation Accuracy: 0.533796\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.007651 | Validation Accuracy: 0.535265\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.587539 | Validation Accuracy: 0.531018\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.390128 | Validation Accuracy: 0.534630\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.066462 | Validation Accuracy: 0.528756\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.8856902122497559\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.000070 | Validation Accuracy: 0.864060\n",
      "Accuracy on Test data: 0.8336586356163025, 0.7323206663131714\n",
      "Step 2 | Training Loss: 0.000006 | Validation Accuracy: 0.903473\n",
      "Accuracy on Test data: 0.7686302065849304, 0.5948523283004761\n",
      "Step 3 | Training Loss: 0.000167 | Validation Accuracy: 0.895336\n",
      "Accuracy on Test data: 0.7985273003578186, 0.6618565320968628\n",
      "Step 4 | Training Loss: 0.000017 | Validation Accuracy: 0.862274\n",
      "Accuracy on Test data: 0.7118080258369446, 0.49282699823379517\n",
      "Step 5 | Training Loss: 0.000008 | Validation Accuracy: 0.894106\n",
      "Accuracy on Test data: 0.7937366962432861, 0.6267510652542114\n",
      "Step 6 | Training Loss: 0.000027 | Validation Accuracy: 0.866243\n",
      "Accuracy on Test data: 0.8131210207939148, 0.6902953386306763\n",
      "Step 7 | Training Loss: 0.000010 | Validation Accuracy: 0.903116\n",
      "Accuracy on Test data: 0.7984386086463928, 0.6377215385437012\n",
      "Step 8 | Training Loss: 0.000013 | Validation Accuracy: 0.912364\n",
      "Accuracy on Test data: 0.8799237012863159, 0.7912236452102661\n",
      "Step 9 | Training Loss: 0.000030 | Validation Accuracy: 0.914626\n",
      "Accuracy on Test data: 0.8757097125053406, 0.7803375720977783\n",
      "Step 10 | Training Loss: 0.000010 | Validation Accuracy: 0.914864\n",
      "Accuracy on Test data: 0.8627572655677795, 0.7563713192939758\n",
      "Step 11 | Training Loss: 0.000002 | Validation Accuracy: 0.917881\n",
      "Accuracy on Test data: 0.8733587861061096, 0.7728270292282104\n",
      "Step 12 | Training Loss: 0.000030 | Validation Accuracy: 0.919270\n",
      "Accuracy on Test data: 0.8570795059204102, 0.7411814332008362\n",
      "Step 13 | Training Loss: 0.000047 | Validation Accuracy: 0.923556\n",
      "Accuracy on Test data: 0.8618701100349426, 0.7477636933326721\n",
      "Step 14 | Training Loss: 0.000057 | Validation Accuracy: 0.916134\n",
      "Accuracy on Test data: 0.87211674451828, 0.7726582288742065\n",
      "Step 15 | Training Loss: 0.000038 | Validation Accuracy: 0.918238\n",
      "Accuracy on Test data: 0.861160397529602, 0.7481856346130371\n",
      "Step 1 | Training Loss: 0.000007 | Validation Accuracy: 0.909704\n",
      "Accuracy on Test data: 0.8639549612998962, 0.7567088603973389\n",
      "Step 2 | Training Loss: 0.000013 | Validation Accuracy: 0.915340\n",
      "Accuracy on Test data: 0.8660397529602051, 0.7621096968650818\n",
      "Step 3 | Training Loss: 0.000038 | Validation Accuracy: 0.909823\n",
      "Accuracy on Test data: 0.8595635294914246, 0.7497046589851379\n",
      "Step 4 | Training Loss: 0.000012 | Validation Accuracy: 0.920738\n",
      "Accuracy on Test data: 0.8688786625862122, 0.7633755207061768\n",
      "Step 5 | Training Loss: 0.000036 | Validation Accuracy: 0.893392\n",
      "Accuracy on Test data: 0.8675922751426697, 0.7720674872398376\n",
      "Step 6 | Training Loss: 0.000001 | Validation Accuracy: 0.894225\n",
      "Accuracy on Test data: 0.8588981628417969, 0.7493671178817749\n",
      "Step 7 | Training Loss: 0.000029 | Validation Accuracy: 0.912364\n",
      "Accuracy on Test data: 0.8685237765312195, 0.7661603093147278\n",
      "Step 8 | Training Loss: 0.000007 | Validation Accuracy: 0.918635\n",
      "Accuracy on Test data: 0.8604506850242615, 0.749113917350769\n",
      "Step 9 | Training Loss: 0.000037 | Validation Accuracy: 0.913078\n",
      "Accuracy on Test data: 0.8537526726722717, 0.7414345741271973\n",
      "Step 10 | Training Loss: 0.000039 | Validation Accuracy: 0.915975\n",
      "Accuracy on Test data: 0.8671486973762512, 0.7614346146583557\n",
      "Step 11 | Training Loss: 0.000001 | Validation Accuracy: 0.880056\n",
      "Accuracy on Test data: 0.8529098629951477, 0.7587341666221619\n",
      "Step 12 | Training Loss: 0.000025 | Validation Accuracy: 0.910419\n",
      "Accuracy on Test data: 0.868080198764801, 0.7656540274620056\n",
      "Step 13 | Training Loss: 0.000033 | Validation Accuracy: 0.915301\n",
      "Accuracy on Test data: 0.8611160516738892, 0.752489447593689\n",
      "Step 14 | Training Loss: 0.000023 | Validation Accuracy: 0.907164\n",
      "Accuracy on Test data: 0.8494055867195129, 0.7344303727149963\n",
      "Step 15 | Training Loss: 0.000023 | Validation Accuracy: 0.905100\n",
      "Accuracy on Test data: 0.8606281280517578, 0.7523206472396851\n",
      "Best Accuracy on Test data: 0.8799237012863159\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.000033 | Validation Accuracy: 0.854574\n",
      "Accuracy on Test data: 0.70737224817276, 0.5328270196914673\n",
      "Step 2 | Training Loss: 0.000131 | Validation Accuracy: 0.830522\n",
      "Accuracy on Test data: 0.6902058124542236, 0.5268354415893555\n",
      "Step 3 | Training Loss: 0.000059 | Validation Accuracy: 0.801548\n",
      "Accuracy on Test data: 0.6207416653633118, 0.5194936990737915\n",
      "Step 4 | Training Loss: 0.000129 | Validation Accuracy: 0.802897\n",
      "Accuracy on Test data: 0.5893363952636719, 0.4386498034000397\n",
      "Step 5 | Training Loss: 0.000020 | Validation Accuracy: 0.765588\n",
      "Accuracy on Test data: 0.5790454149246216, 0.41316455602645874\n",
      "Step 6 | Training Loss: 0.000010 | Validation Accuracy: 0.838897\n",
      "Accuracy on Test data: 0.7046664357185364, 0.4897046387195587\n",
      "Step 7 | Training Loss: 0.000008 | Validation Accuracy: 0.830085\n",
      "Accuracy on Test data: 0.7195262312889099, 0.5097890496253967\n",
      "Step 8 | Training Loss: 0.000032 | Validation Accuracy: 0.845009\n",
      "Accuracy on Test data: 0.7484918236732483, 0.5669198036193848\n",
      "Step 9 | Training Loss: 0.000033 | Validation Accuracy: 0.855209\n",
      "Accuracy on Test data: 0.7460521459579468, 0.5572995543479919\n",
      "Step 10 | Training Loss: 0.867616 | Validation Accuracy: 0.534431\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.678946 | Validation Accuracy: 0.535344\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.269429 | Validation Accuracy: 0.538321\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 1.313553 | Validation Accuracy: 0.531018\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.295088 | Validation Accuracy: 0.530383\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.506885 | Validation Accuracy: 0.532606\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.353613 | Validation Accuracy: 0.527367\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.180588 | Validation Accuracy: 0.530740\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.412178 | Validation Accuracy: 0.531336\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 2.251077 | Validation Accuracy: 0.534630\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.228582 | Validation Accuracy: 0.537646\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.063389 | Validation Accuracy: 0.532248\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.499909 | Validation Accuracy: 0.536495\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 2.939292 | Validation Accuracy: 0.535384\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.613026 | Validation Accuracy: 0.535622\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.745708 | Validation Accuracy: 0.534511\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 1.526883 | Validation Accuracy: 0.534749\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.543381 | Validation Accuracy: 0.532844\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.672684 | Validation Accuracy: 0.534908\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 2.188988 | Validation Accuracy: 0.539790\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.816474 | Validation Accuracy: 0.531971\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Best Accuracy on Test data: 0.7484918236732483\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.000112 | Validation Accuracy: 0.916888\n",
      "Accuracy on Test data: 0.7238289713859558, 0.5325738191604614\n",
      "Step 2 | Training Loss: 0.000172 | Validation Accuracy: 0.903274\n",
      "Accuracy on Test data: 0.8101046681404114, 0.6719831228256226\n",
      "Step 3 | Training Loss: 0.000208 | Validation Accuracy: 0.889978\n",
      "Accuracy on Test data: 0.8575674295425415, 0.7473417520523071\n",
      "Step 4 | Training Loss: 0.000098 | Validation Accuracy: 0.907164\n",
      "Accuracy on Test data: 0.8722941875457764, 0.7708860635757446\n",
      "Step 5 | Training Loss: 0.000096 | Validation Accuracy: 0.886922\n",
      "Accuracy on Test data: 0.8825408220291138, 0.803206741809845\n",
      "Step 6 | Training Loss: 0.000021 | Validation Accuracy: 0.895138\n",
      "Accuracy on Test data: 0.8794357776641846, 0.7873417735099792\n",
      "Step 7 | Training Loss: 0.000010 | Validation Accuracy: 0.898432\n",
      "Accuracy on Test data: 0.8761976361274719, 0.7797468304634094\n",
      "Step 8 | Training Loss: 0.000019 | Validation Accuracy: 0.899583\n",
      "Accuracy on Test data: 0.8282470107078552, 0.6880168914794922\n",
      "Step 9 | Training Loss: 0.000032 | Validation Accuracy: 0.901806\n",
      "Accuracy on Test data: 0.87788325548172, 0.7838818430900574\n",
      "Step 10 | Training Loss: 0.000030 | Validation Accuracy: 0.894265\n",
      "Accuracy on Test data: 0.8773509860038757, 0.7844725847244263\n",
      "Step 11 | Training Loss: 0.000008 | Validation Accuracy: 0.898154\n",
      "Accuracy on Test data: 0.891678512096405, 0.8102953433990479\n",
      "Step 12 | Training Loss: 0.000022 | Validation Accuracy: 0.896448\n",
      "Accuracy on Test data: 0.8845369219779968, 0.7934176921844482\n",
      "Step 13 | Training Loss: 0.000012 | Validation Accuracy: 0.893114\n",
      "Accuracy on Test data: 0.8878193497657776, 0.798396646976471\n",
      "Step 14 | Training Loss: 0.000014 | Validation Accuracy: 0.892717\n",
      "Accuracy on Test data: 0.8825408220291138, 0.7909704446792603\n",
      "Step 15 | Training Loss: 0.000045 | Validation Accuracy: 0.894820\n",
      "Accuracy on Test data: 0.8801011443138123, 0.7875949144363403\n",
      "Step 1 | Training Loss: 0.000011 | Validation Accuracy: 0.899504\n",
      "Accuracy on Test data: 0.8856014609336853, 0.7947679162025452\n",
      "Step 2 | Training Loss: 0.000031 | Validation Accuracy: 0.896051\n",
      "Accuracy on Test data: 0.8849804997444153, 0.7942615747451782\n",
      "Step 3 | Training Loss: 0.000007 | Validation Accuracy: 0.901290\n",
      "Accuracy on Test data: 0.8878637552261353, 0.7965400815010071\n",
      "Step 4 | Training Loss: 0.000062 | Validation Accuracy: 0.900853\n",
      "Accuracy on Test data: 0.875842809677124, 0.7763713002204895\n",
      "Step 5 | Training Loss: 0.000009 | Validation Accuracy: 0.889819\n",
      "Accuracy on Test data: 0.8692334890365601, 0.7675105333328247\n",
      "Step 6 | Training Loss: 0.000010 | Validation Accuracy: 0.891883\n",
      "Accuracy on Test data: 0.8947835564613342, 0.8172996044158936\n",
      "Step 7 | Training Loss: 0.000013 | Validation Accuracy: 0.900258\n",
      "Accuracy on Test data: 0.8918559551239014, 0.806075930595398\n",
      "Step 8 | Training Loss: 0.000023 | Validation Accuracy: 0.897638\n",
      "Accuracy on Test data: 0.8730039000511169, 0.7740084528923035\n",
      "Step 9 | Training Loss: 0.000010 | Validation Accuracy: 0.908633\n",
      "Accuracy on Test data: 0.8795244693756104, 0.7821096777915955\n",
      "Step 10 | Training Loss: 0.000031 | Validation Accuracy: 0.908712\n",
      "Accuracy on Test data: 0.8642654418945312, 0.75274258852005\n",
      "Step 11 | Training Loss: 0.000050 | Validation Accuracy: 0.849613\n",
      "Accuracy on Test data: 0.8345901370048523, 0.7156962156295776\n",
      "Step 12 | Training Loss: 0.000063 | Validation Accuracy: 0.844969\n",
      "Accuracy on Test data: 0.8437278270721436, 0.7367932200431824\n",
      "Step 13 | Training Loss: 0.000041 | Validation Accuracy: 0.852590\n",
      "Accuracy on Test data: 0.7818931937217712, 0.6157805919647217\n",
      "Step 14 | Training Loss: 0.000075 | Validation Accuracy: 0.851994\n",
      "Accuracy on Test data: 0.8068665862083435, 0.6631223559379578\n",
      "Step 15 | Training Loss: 0.000066 | Validation Accuracy: 0.863743\n",
      "Accuracy on Test data: 0.8223917484283447, 0.6722362637519836\n",
      "Best Accuracy on Test data: 0.8947835564613342\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 10\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:25:36.748882Z",
     "start_time": "2017-07-23T21:25:36.642323Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions__.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:25:37.762314Z",
     "start_time": "2017-07-23T21:25:37.692275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:25:38.996807Z",
     "start_time": "2017-07-23T21:25:38.993431Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:26:20.649059Z",
     "start_time": "2017-07-23T21:26:20.627919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891883</td>\n",
       "      <td>0.894784</td>\n",
       "      <td>0.903624</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.881402</td>\n",
       "      <td>88.621862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.898154</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>0.810295</td>\n",
       "      <td>0.881146</td>\n",
       "      <td>46.063003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906013</td>\n",
       "      <td>0.885690</td>\n",
       "      <td>0.899103</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.874276</td>\n",
       "      <td>40.399515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.886922</td>\n",
       "      <td>0.882541</td>\n",
       "      <td>0.896108</td>\n",
       "      <td>0.803207</td>\n",
       "      <td>0.876783</td>\n",
       "      <td>21.043563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.914507</td>\n",
       "      <td>0.881831</td>\n",
       "      <td>0.892892</td>\n",
       "      <td>0.785907</td>\n",
       "      <td>0.862783</td>\n",
       "      <td>26.981111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.912364</td>\n",
       "      <td>0.879924</td>\n",
       "      <td>0.890303</td>\n",
       "      <td>0.791224</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>27.275330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.907164</td>\n",
       "      <td>0.872294</td>\n",
       "      <td>0.882254</td>\n",
       "      <td>0.770886</td>\n",
       "      <td>0.850306</td>\n",
       "      <td>16.874060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926890</td>\n",
       "      <td>0.859430</td>\n",
       "      <td>0.868512</td>\n",
       "      <td>0.739241</td>\n",
       "      <td>0.825857</td>\n",
       "      <td>16.922411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.889978</td>\n",
       "      <td>0.857567</td>\n",
       "      <td>0.866069</td>\n",
       "      <td>0.747342</td>\n",
       "      <td>0.831019</td>\n",
       "      <td>12.697839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937210</td>\n",
       "      <td>0.852732</td>\n",
       "      <td>0.862332</td>\n",
       "      <td>0.731814</td>\n",
       "      <td>0.821921</td>\n",
       "      <td>6.821192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.842397</td>\n",
       "      <td>0.851158</td>\n",
       "      <td>0.710802</td>\n",
       "      <td>0.804874</td>\n",
       "      <td>1.861382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.864060</td>\n",
       "      <td>0.833659</td>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.732321</td>\n",
       "      <td>0.820059</td>\n",
       "      <td>3.465354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.903274</td>\n",
       "      <td>0.810105</td>\n",
       "      <td>0.813326</td>\n",
       "      <td>0.671983</td>\n",
       "      <td>0.764923</td>\n",
       "      <td>8.460931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.780353</td>\n",
       "      <td>0.774973</td>\n",
       "      <td>0.791757</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.805371</td>\n",
       "      <td>19.076372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860528</td>\n",
       "      <td>0.790765</td>\n",
       "      <td>0.789203</td>\n",
       "      <td>0.656962</td>\n",
       "      <td>0.755988</td>\n",
       "      <td>3.423419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.848502</td>\n",
       "      <td>0.763130</td>\n",
       "      <td>0.754911</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>0.699320</td>\n",
       "      <td>14.574448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881524</td>\n",
       "      <td>0.761799</td>\n",
       "      <td>0.740830</td>\n",
       "      <td>0.557215</td>\n",
       "      <td>0.638910</td>\n",
       "      <td>3.351328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.772217</td>\n",
       "      <td>0.740818</td>\n",
       "      <td>0.736458</td>\n",
       "      <td>0.641350</td>\n",
       "      <td>0.743822</td>\n",
       "      <td>15.911143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845009</td>\n",
       "      <td>0.748492</td>\n",
       "      <td>0.731991</td>\n",
       "      <td>0.566920</td>\n",
       "      <td>0.651453</td>\n",
       "      <td>28.795720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889303</td>\n",
       "      <td>0.751419</td>\n",
       "      <td>0.730136</td>\n",
       "      <td>0.534262</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>1.909114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.701171</td>\n",
       "      <td>0.709013</td>\n",
       "      <td>0.719945</td>\n",
       "      <td>0.629789</td>\n",
       "      <td>0.737917</td>\n",
       "      <td>6.423292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916888</td>\n",
       "      <td>0.723829</td>\n",
       "      <td>0.698206</td>\n",
       "      <td>0.532574</td>\n",
       "      <td>0.620122</td>\n",
       "      <td>4.292972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.830085</td>\n",
       "      <td>0.719526</td>\n",
       "      <td>0.694172</td>\n",
       "      <td>0.509789</td>\n",
       "      <td>0.585397</td>\n",
       "      <td>25.208755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.699821</td>\n",
       "      <td>0.675967</td>\n",
       "      <td>0.689215</td>\n",
       "      <td>0.638481</td>\n",
       "      <td>0.746629</td>\n",
       "      <td>3.246162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.779917</td>\n",
       "      <td>0.681512</td>\n",
       "      <td>0.680719</td>\n",
       "      <td>0.514684</td>\n",
       "      <td>0.621719</td>\n",
       "      <td>1.706293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.854574</td>\n",
       "      <td>0.707372</td>\n",
       "      <td>0.679586</td>\n",
       "      <td>0.532827</td>\n",
       "      <td>0.614753</td>\n",
       "      <td>3.672193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753403</td>\n",
       "      <td>0.565295</td>\n",
       "      <td>0.383027</td>\n",
       "      <td>0.232911</td>\n",
       "      <td>0.120209</td>\n",
       "      <td>2.686282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532884</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.086767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "27     14             122              3     0.891883    0.894784  0.903624   \n",
       "26     12             122              3     0.898154    0.891679  0.903600   \n",
       "15     13              12              3     0.906013    0.885690  0.899103   \n",
       "25      6             122              3     0.886922    0.882541  0.896108   \n",
       "14      9              12              3     0.914507    0.881831  0.892892   \n",
       "17      9              24              3     0.912364    0.879924  0.890303   \n",
       "24      5             122              3     0.907164    0.872294  0.882254   \n",
       "13      6              12              3     0.926890    0.859430  0.868512   \n",
       "23      4             122              3     0.889978    0.857567  0.866069   \n",
       "12      3              12              3     0.937210    0.852732  0.862332   \n",
       "3       2              12              1     0.907561    0.842397  0.851158   \n",
       "16      2              24              3     0.864060    0.833659  0.842807   \n",
       "22      3             122              3     0.903274    0.810105  0.813326   \n",
       "10      7               1              3     0.780353    0.774973  0.791757   \n",
       "11      2              12              3     0.860528    0.790765  0.789203   \n",
       "2      10               1              1     0.848502    0.763130  0.754911   \n",
       "1       3               1              1     0.881524    0.761799  0.740830   \n",
       "9       6               1              3     0.772217    0.740818  0.736458   \n",
       "20      9              48              3     0.845009    0.748492  0.731991   \n",
       "4       2              24              1     0.889303    0.751419  0.730136   \n",
       "8       3               1              3     0.701171    0.709013  0.719945   \n",
       "21      2             122              3     0.916888    0.723829  0.698206   \n",
       "19      8              48              3     0.830085    0.719526  0.694172   \n",
       "7       2               1              3     0.699821    0.675967  0.689215   \n",
       "0       2               1              1     0.779917    0.681512  0.680719   \n",
       "18      2              48              3     0.854574    0.707372  0.679586   \n",
       "6       2             122              1     0.753403    0.565295  0.383027   \n",
       "5       2              48              1     0.532884    0.430758  0.000000   \n",
       "\n",
       "    test_score_20  f1_score_20  time_taken  \n",
       "27       0.817300     0.881402   88.621862  \n",
       "26       0.810295     0.881146   46.063003  \n",
       "15       0.796709     0.874276   40.399515  \n",
       "25       0.803207     0.876783   21.043563  \n",
       "14       0.785907     0.862783   26.981111  \n",
       "17       0.791224     0.864275   27.275330  \n",
       "24       0.770886     0.850306   16.874060  \n",
       "13       0.739241     0.825857   16.922411  \n",
       "23       0.747342     0.831019   12.697839  \n",
       "12       0.731814     0.821921    6.821192  \n",
       "3        0.710802     0.804874    1.861382  \n",
       "16       0.732321     0.820059    3.465354  \n",
       "22       0.671983     0.764923    8.460931  \n",
       "10       0.708861     0.805371   19.076372  \n",
       "11       0.656962     0.755988    3.423419  \n",
       "2        0.604726     0.699320   14.574448  \n",
       "1        0.557215     0.638910    3.351328  \n",
       "9        0.641350     0.743822   15.911143  \n",
       "20       0.566920     0.651453   28.795720  \n",
       "4        0.534262     0.619615    1.909114  \n",
       "8        0.629789     0.737917    6.423292  \n",
       "21       0.532574     0.620122    4.292972  \n",
       "19       0.509789     0.585397   25.208755  \n",
       "7        0.638481     0.746629    3.246162  \n",
       "0        0.514684     0.621719    1.706293  \n",
       "18       0.532827     0.614753    3.672193  \n",
       "6        0.232911     0.120209    2.686282  \n",
       "5        0.181603     0.000000    2.086767  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:26:38.067474Z",
     "start_time": "2017-07-23T21:26:38.047595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.891883</td>\n",
       "      <td>0.894784</td>\n",
       "      <td>0.903624</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.881402</td>\n",
       "      <td>88.621862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.906013</td>\n",
       "      <td>0.885690</td>\n",
       "      <td>0.899103</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.874276</td>\n",
       "      <td>40.399515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.912364</td>\n",
       "      <td>0.879924</td>\n",
       "      <td>0.890303</td>\n",
       "      <td>0.791224</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>27.275330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.842397</td>\n",
       "      <td>0.851158</td>\n",
       "      <td>0.710802</td>\n",
       "      <td>0.804874</td>\n",
       "      <td>1.861382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.780353</td>\n",
       "      <td>0.774973</td>\n",
       "      <td>0.791757</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.805371</td>\n",
       "      <td>19.076372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.848502</td>\n",
       "      <td>0.763130</td>\n",
       "      <td>0.754911</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>0.699320</td>\n",
       "      <td>14.574448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.889303</td>\n",
       "      <td>0.751419</td>\n",
       "      <td>0.730136</td>\n",
       "      <td>0.534262</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>1.909114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.845009</td>\n",
       "      <td>0.748492</td>\n",
       "      <td>0.731991</td>\n",
       "      <td>0.566920</td>\n",
       "      <td>0.651453</td>\n",
       "      <td>28.795720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.753403</td>\n",
       "      <td>0.565295</td>\n",
       "      <td>0.383027</td>\n",
       "      <td>0.232911</td>\n",
       "      <td>0.120209</td>\n",
       "      <td>2.686282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.532884</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.086767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "122            3                 14     0.891883    0.894784  0.903624   \n",
       "12             3                 13     0.906013    0.885690  0.899103   \n",
       "24             3                  9     0.912364    0.879924  0.890303   \n",
       "12             1                  2     0.907561    0.842397  0.851158   \n",
       "1              3                  7     0.780353    0.774973  0.791757   \n",
       "               1                 10     0.848502    0.763130  0.754911   \n",
       "24             1                  2     0.889303    0.751419  0.730136   \n",
       "48             3                  9     0.845009    0.748492  0.731991   \n",
       "122            1                  2     0.753403    0.565295  0.383027   \n",
       "48             1                  2     0.532884    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "122            3                   0.817300     0.881402   88.621862  \n",
       "12             3                   0.796709     0.874276   40.399515  \n",
       "24             3                   0.791224     0.864275   27.275330  \n",
       "12             1                   0.710802     0.804874    1.861382  \n",
       "1              3                   0.708861     0.805371   19.076372  \n",
       "               1                   0.604726     0.699320   14.574448  \n",
       "24             1                   0.534262     0.619615    1.909114  \n",
       "48             3                   0.566920     0.651453   28.795720  \n",
       "122            1                   0.232911     0.120209    2.686282  \n",
       "48             1                   0.181603     0.000000    2.086767  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:26:40.917173Z",
     "start_time": "2017-07-23T21:26:40.897610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.888212</td>\n",
       "      <td>0.856791</td>\n",
       "      <td>0.866555</td>\n",
       "      <td>0.761772</td>\n",
       "      <td>0.842167</td>\n",
       "      <td>15.370342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.909030</td>\n",
       "      <td>0.854090</td>\n",
       "      <td>0.862408</td>\n",
       "      <td>0.742127</td>\n",
       "      <td>0.828165</td>\n",
       "      <td>18.909530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>6.571429</td>\n",
       "      <td>0.899181</td>\n",
       "      <td>0.847543</td>\n",
       "      <td>0.851884</td>\n",
       "      <td>0.736227</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>28.293461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.907561</td>\n",
       "      <td>0.842397</td>\n",
       "      <td>0.851158</td>\n",
       "      <td>0.710802</td>\n",
       "      <td>0.804874</td>\n",
       "      <td>1.861382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.889303</td>\n",
       "      <td>0.751419</td>\n",
       "      <td>0.730136</td>\n",
       "      <td>0.534262</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>1.909114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.836647</td>\n",
       "      <td>0.735480</td>\n",
       "      <td>0.725487</td>\n",
       "      <td>0.558875</td>\n",
       "      <td>0.653316</td>\n",
       "      <td>6.544023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.738391</td>\n",
       "      <td>0.725193</td>\n",
       "      <td>0.734344</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>0.758435</td>\n",
       "      <td>11.164242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.843223</td>\n",
       "      <td>0.725130</td>\n",
       "      <td>0.701916</td>\n",
       "      <td>0.536512</td>\n",
       "      <td>0.617201</td>\n",
       "      <td>19.225556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.753403</td>\n",
       "      <td>0.565295</td>\n",
       "      <td>0.383027</td>\n",
       "      <td>0.232911</td>\n",
       "      <td>0.120209</td>\n",
       "      <td>2.686282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.532884</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.086767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "24             3              5.500000     0.888212    0.856791  0.866555   \n",
       "12             3              6.600000     0.909030    0.854090  0.862408   \n",
       "122            3              6.571429     0.899181    0.847543  0.851884   \n",
       "12             1              2.000000     0.907561    0.842397  0.851158   \n",
       "24             1              2.000000     0.889303    0.751419  0.730136   \n",
       "1              1              5.000000     0.836647    0.735480  0.725487   \n",
       "               3              4.500000     0.738391    0.725193  0.734344   \n",
       "48             3              6.333333     0.843223    0.725130  0.701916   \n",
       "122            1              2.000000     0.753403    0.565295  0.383027   \n",
       "48             1              2.000000     0.532884    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "24             3                   0.761772     0.842167   15.370342  \n",
       "12             3                   0.742127     0.828165   18.909530  \n",
       "122            3                   0.736227     0.815100   28.293461  \n",
       "12             1                   0.710802     0.804874    1.861382  \n",
       "24             1                   0.534262     0.619615    1.909114  \n",
       "1              1                   0.558875     0.653316    6.544023  \n",
       "               3                   0.654620     0.758435   11.164242  \n",
       "48             3                   0.536512     0.617201   19.225556  \n",
       "122            1                   0.232911     0.120209    2.686282  \n",
       "48             1                   0.181603     0.000000    2.086767  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:26:42.734758Z",
     "start_time": "2017-07-23T21:26:42.708037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:38.873415Z",
     "start_time": "2017-07-23T21:31:38.844830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16476</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.718943e-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob   Normal_prob  Prediction\n",
       "16476     0.0          1.0  3.718943e-13         0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoch_nof_hidden\n",
    "Train.predictions[\"14_122_3\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:45.380054Z",
     "start_time": "2017-07-23T21:31:45.369635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.180671e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob   Normal_prob  Prediction\n",
       "11510     1.0          1.0  1.180671e-10         0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"14_122_3\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:49.028029Z",
     "start_time": "2017-07-23T21:31:49.021014Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions[\"14_122_3\"].dropna()\n",
    "df_ = Train.predictions_[\"14_122_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:50.177934Z",
     "start_time": "2017-07-23T21:31:50.171083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:53.322003Z",
     "start_time": "2017-07-23T21:31:53.276925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894784</td>\n",
       "      <td>0.903624</td>\n",
       "      <td>0.944053</td>\n",
       "      <td>0.866516</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.881402</td>\n",
       "      <td>0.940166</td>\n",
       "      <td>0.829552</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision    Recall      Scenario\n",
       "0  0.894784  0.903624   0.944053  0.866516  Train+/Test+\n",
       "1  0.817300  0.881402   0.940166  0.829552  Train+/Test-"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:55.982550Z",
     "start_time": "2017-07-23T21:31:55.976431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:57.123256Z",
     "start_time": "2017-07-23T21:31:56.847967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAGhCAYAAAAN2pFTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGuV/f6c2sLFAZboAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a1aac0898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:31:59.497355Z",
     "start_time": "2017-07-23T21:31:59.491356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:32:00.940107Z",
     "start_time": "2017-07-23T21:32:00.669570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGhCAYAAAAJL0FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noIUQQiiFCGghhBBKIQJaCCGEUvg/aTgJVsPs/fMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a1a5a3b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T21:32:04.060690Z",
     "start_time": "2017-07-23T21:32:04.040701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                  (0.64825565032, 0.80271747066)\n",
       "                3                 (0.65008285401, 0.818604779719)\n",
       "12              1                                      (nan, nan)\n",
       "                3                (0.776571777742, 0.948245001786)\n",
       "24              1                                      (nan, nan)\n",
       "                3                 (0.80073006819, 0.932379483883)\n",
       "48              1                                      (nan, nan)\n",
       "                3                (0.648904848451, 0.754927695761)\n",
       "122             1                                      (nan, nan)\n",
       "                3                 (0.70535682815, 0.998411116676)\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
