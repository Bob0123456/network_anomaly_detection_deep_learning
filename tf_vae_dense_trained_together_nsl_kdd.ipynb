{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:24.243583Z",
     "start_time": "2017-05-11T21:00:23.859256Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:24.323693Z",
     "start_time": "2017-05-11T21:00:24.245892Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:24.331213Z",
     "start_time": "2017-05-11T21:00:24.326003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:24.337088Z",
     "start_time": "2017-05-11T21:00:24.333055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:25.057332Z",
     "start_time": "2017-05-11T21:00:24.338659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:26.097508Z",
     "start_time": "2017-05-11T21:00:25.059654Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:26.566346Z",
     "start_time": "2017-05-11T21:00:26.099458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.1\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0.01, stddev=0.05, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1, 1)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "\n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            #gradients = [\n",
    "            #    None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "            #    for gradient in gradients]\n",
    "            #self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:00:26.684858Z",
     "start_time": "2017-05-11T21:00:26.568133Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e9):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "\n",
    "                valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                      feed_dict={net.x: preprocess.x_test, \n",
    "                                                                 net.y_: preprocess.y_test, \n",
    "                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual, net.y], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "            Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:15.540229Z",
     "start_time": "2017-05-11T21:00:26.686550Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:2\n",
      "Step 1 | Training Loss: 0.012653 | Validation Accuracy: 0.797330\n",
      "Step 2 | Training Loss: 0.005617 | Validation Accuracy: 0.853309\n",
      "Step 3 | Training Loss: 0.025634 | Validation Accuracy: 0.863156\n",
      "Step 4 | Training Loss: 0.011317 | Validation Accuracy: 0.853664\n",
      "Step 5 | Training Loss: 0.012561 | Validation Accuracy: 0.855704\n",
      "Step 6 | Training Loss: 0.018005 | Validation Accuracy: 0.870875\n",
      "Step 7 | Training Loss: 0.026891 | Validation Accuracy: 0.864088\n",
      "Step 8 | Training Loss: 0.081112 | Validation Accuracy: 0.833215\n",
      "Step 9 | Training Loss: 0.126772 | Validation Accuracy: 0.873314\n",
      "Step 10 | Training Loss: 0.027308 | Validation Accuracy: 0.816758\n",
      "Accuracy on Test data: 0.816536545753479\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 1585879936.000000\n",
      "Step 1 | Training Loss: 0.055691 | Validation Accuracy: 0.841421\n",
      "Step 2 | Training Loss: 0.210659 | Validation Accuracy: 0.860672\n",
      "Step 3 | Training Loss: 0.038947 | Validation Accuracy: 0.889594\n",
      "Step 4 | Training Loss: 0.051603 | Validation Accuracy: 0.874601\n",
      "Step 5 | Training Loss: 0.109639 | Validation Accuracy: 0.885202\n",
      "Step 6 | Training Loss: 0.010666 | Validation Accuracy: 0.883162\n",
      "Step 7 | Training Loss: 0.083226 | Validation Accuracy: 0.879746\n",
      "Step 8 | Training Loss: 0.067660 | Validation Accuracy: 0.880722\n",
      "Step 9 | Training Loss: 0.035487 | Validation Accuracy: 0.863378\n",
      "Step 10 | Training Loss: 0.017253 | Validation Accuracy: 0.869943\n",
      "Accuracy on Test data: 0.870652973651886\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.005905 | Validation Accuracy: 0.845502\n",
      "Step 2 | Training Loss: 0.031556 | Validation Accuracy: 0.825586\n",
      "Step 3 | Training Loss: 0.042659 | Validation Accuracy: 0.780030\n",
      "Step 4 | Training Loss: 0.075649 | Validation Accuracy: 0.878016\n",
      "Step 5 | Training Loss: 0.069987 | Validation Accuracy: 0.875754\n",
      "Step 6 | Training Loss: 0.000008 | Validation Accuracy: 0.861693\n",
      "Step 7 | Training Loss: 0.029134 | Validation Accuracy: 0.877040\n",
      "Step 8 | Training Loss: 0.079396 | Validation Accuracy: 0.877129\n",
      "Step 9 | Training Loss: 0.042524 | Validation Accuracy: 0.876242\n",
      "Step 10 | Training Loss: 0.016334 | Validation Accuracy: 0.854418\n",
      "Accuracy on Test data: 0.8544623851776123\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 3232327425589248.000000\n",
      "Step 1 | Training Loss: 21737136128.000000\n",
      "Step 1 | Training Loss: 124000034816.000000\n",
      "Step 1 | Training Loss: 10795691256760500224.000000\n",
      "Step 1 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 2 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 3 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 4 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 5 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 6 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 7 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 8 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 9 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Step 10 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 16057912057135104.000000\n",
      "Step 1 | Training Loss: 2721382400.000000\n",
      "Step 1 | Training Loss: 1.352307 | Validation Accuracy: 0.712163\n",
      "Step 2 | Training Loss: 1349169053696.000000\n",
      "Step 2 | Training Loss: 0.148294 | Validation Accuracy: 0.718284\n",
      "Step 3 | Training Loss: 0.024851 | Validation Accuracy: 0.671265\n",
      "Step 4 | Training Loss: 0.209240 | Validation Accuracy: 0.762642\n",
      "Step 5 | Training Loss: 0.064942 | Validation Accuracy: 0.875266\n",
      "Step 6 | Training Loss: 0.111736 | Validation Accuracy: 0.897622\n",
      "Step 7 | Training Loss: 0.001277 | Validation Accuracy: 0.890525\n",
      "Step 8 | Training Loss: 0.014354 | Validation Accuracy: 0.887598\n",
      "Step 9 | Training Loss: 0.021510 | Validation Accuracy: 0.865818\n",
      "Step 10 | Training Loss: 0.034004 | Validation Accuracy: 0.890836\n",
      "Accuracy on Test data: 0.8911018371582031\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:2\n",
      "Step 1 | Training Loss: 13677275136.000000\n",
      "Step 1 | Training Loss: 0.053191 | Validation Accuracy: 0.880545\n",
      "Step 2 | Training Loss: 0.051344 | Validation Accuracy: 0.798971\n",
      "Step 3 | Training Loss: 0.002829 | Validation Accuracy: 0.792007\n",
      "Step 4 | Training Loss: 0.011574 | Validation Accuracy: 0.807000\n",
      "Step 5 | Training Loss: 0.014665 | Validation Accuracy: 0.794801\n",
      "Step 6 | Training Loss: 0.045680 | Validation Accuracy: 0.769207\n",
      "Step 7 | Training Loss: 0.053114 | Validation Accuracy: 0.757496\n",
      "Step 8 | Training Loss: 0.021985 | Validation Accuracy: 0.750488\n",
      "Step 9 | Training Loss: 0.026796 | Validation Accuracy: 0.763795\n",
      "Step 10 | Training Loss: 0.011955 | Validation Accuracy: 0.764194\n",
      "Accuracy on Test data: 0.7641944885253906\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.184342 | Validation Accuracy: 0.849672\n",
      "Step 2 | Training Loss: 0.420086 | Validation Accuracy: 0.863290\n",
      "Step 3 | Training Loss: 0.035345 | Validation Accuracy: 0.775861\n",
      "Step 4 | Training Loss: 0.034662 | Validation Accuracy: 0.816625\n",
      "Step 5 | Training Loss: 0.042248 | Validation Accuracy: 0.732700\n",
      "Step 6 | Training Loss: 3235159808.000000\n",
      "Step 6 | Training Loss: 0.507525 | Validation Accuracy: 0.816537\n",
      "Step 7 | Training Loss: 0.012779 | Validation Accuracy: 0.840091\n",
      "Step 8 | Training Loss: 0.096344 | Validation Accuracy: 0.838893\n",
      "Step 9 | Training Loss: 0.047201 | Validation Accuracy: 0.843817\n",
      "Step 10 | Training Loss: 0.104673 | Validation Accuracy: 0.742237\n",
      "Accuracy on Test data: 0.742769718170166\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.047406 | Validation Accuracy: 0.798572\n",
      "Step 2 | Training Loss: 0.012462 | Validation Accuracy: 0.848164\n",
      "Step 3 | Training Loss: 0.149155 | Validation Accuracy: 0.797463\n",
      "Step 4 | Training Loss: 0.075626 | Validation Accuracy: 0.769695\n",
      "Step 5 | Training Loss: 0.041321 | Validation Accuracy: 0.868479\n",
      "Step 6 | Training Loss: 0.031836 | Validation Accuracy: 0.863689\n",
      "Step 7 | Training Loss: 0.051781 | Validation Accuracy: 0.834368\n",
      "Step 8 | Training Loss: 0.007539 | Validation Accuracy: 0.839159\n",
      "Step 9 | Training Loss: 0.010894 | Validation Accuracy: 0.850381\n",
      "Step 10 | Training Loss: 0.001768 | Validation Accuracy: 0.875266\n",
      "Accuracy on Test data: 0.8753548860549927\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.095491 | Validation Accuracy: 0.856059\n",
      "Step 2 | Training Loss: 0.074763 | Validation Accuracy: 0.814540\n",
      "Step 3 | Training Loss: 0.041340 | Validation Accuracy: 0.820662\n",
      "Step 4 | Training Loss: 0.022492 | Validation Accuracy: 0.851269\n",
      "Step 5 | Training Loss: 0.038357 | Validation Accuracy: 0.861648\n",
      "Step 6 | Training Loss: 0.047265 | Validation Accuracy: 0.694331\n",
      "Step 7 | Training Loss: 0.056560 | Validation Accuracy: 0.829267\n",
      "Step 8 | Training Loss: 0.025102 | Validation Accuracy: 0.822525\n",
      "Step 9 | Training Loss: 0.026432 | Validation Accuracy: 0.846522\n",
      "Step 10 | Training Loss: 0.016039 | Validation Accuracy: 0.841288\n",
      "Accuracy on Test data: 0.8413768410682678\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.027601 | Validation Accuracy: 0.804649\n",
      "Step 2 | Training Loss: 0.107653 | Validation Accuracy: 0.787260\n",
      "Step 3 | Training Loss: 0.081384 | Validation Accuracy: 0.854152\n",
      "Step 4 | Training Loss: 0.047903 | Validation Accuracy: 0.756299\n",
      "Step 5 | Training Loss: 0.024807 | Validation Accuracy: 0.768408\n",
      "Step 6 | Training Loss: 0.321437 | Validation Accuracy: 0.650550\n",
      "Step 7 | Training Loss: 0.091624 | Validation Accuracy: 0.656760\n",
      "Step 8 | Training Loss: 0.008126 | Validation Accuracy: 0.787837\n",
      "Step 9 | Training Loss: 0.038666 | Validation Accuracy: 0.787172\n",
      "Step 10 | Training Loss: 0.031025 | Validation Accuracy: 0.790765\n",
      "Accuracy on Test data: 0.7908534407615662\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:2\n",
      "Step 1 | Training Loss: 0.002755 | Validation Accuracy: 0.843151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.205416 | Validation Accuracy: 0.780740\n",
      "Step 3 | Training Loss: 0.184844 | Validation Accuracy: 0.818267\n",
      "Step 4 | Training Loss: 0.060980 | Validation Accuracy: 0.844571\n",
      "Step 5 | Training Loss: 0.013837 | Validation Accuracy: 0.865907\n",
      "Step 6 | Training Loss: 0.010850 | Validation Accuracy: 0.826340\n",
      "Step 7 | Training Loss: 0.023169 | Validation Accuracy: 0.845325\n",
      "Step 8 | Training Loss: 0.041191 | Validation Accuracy: 0.829622\n",
      "Step 9 | Training Loss: 0.067290 | Validation Accuracy: 0.833437\n",
      "Step 10 | Training Loss: 8009700352.000000\n",
      "Step 10 | Training Loss: inf\n",
      "Step 10 | Training Loss: 0.100000 | Validation Accuracy: 0.569242\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.033690 | Validation Accuracy: 0.735850\n",
      "Step 2 | Training Loss: 0.048714 | Validation Accuracy: 0.800435\n",
      "Step 3 | Training Loss: 0.083818 | Validation Accuracy: 0.787926\n",
      "Step 4 | Training Loss: 0.204293 | Validation Accuracy: 0.784200\n",
      "Step 5 | Training Loss: 1542876299264.000000\n",
      "Step 5 | Training Loss: 0.721702 | Validation Accuracy: 0.614532\n",
      "Step 6 | Training Loss: 0.141386 | Validation Accuracy: 0.474317\n",
      "Step 7 | Training Loss: 0.202997 | Validation Accuracy: 0.475204\n",
      "Step 8 | Training Loss: 0.116803 | Validation Accuracy: 0.475337\n",
      "Step 9 | Training Loss: 0.155709 | Validation Accuracy: 0.475337\n",
      "Step 10 | Training Loss: 0.116966 | Validation Accuracy: 0.475426\n",
      "Accuracy on Test data: 0.47524839639663696\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.119498 | Validation Accuracy: 0.790232\n",
      "Step 2 | Training Loss: 0.019913 | Validation Accuracy: 0.809794\n",
      "Step 3 | Training Loss: 0.029865 | Validation Accuracy: 0.841954\n",
      "Step 4 | Training Loss: 0.020368 | Validation Accuracy: 0.795821\n",
      "Step 5 | Training Loss: 0.055208 | Validation Accuracy: 0.841111\n",
      "Step 6 | Training Loss: 0.061824 | Validation Accuracy: 0.836231\n",
      "Step 7 | Training Loss: 0.033312 | Validation Accuracy: 0.859697\n",
      "Step 8 | Training Loss: 0.023993 | Validation Accuracy: 0.835078\n",
      "Step 9 | Training Loss: 0.090491 | Validation Accuracy: 0.835389\n",
      "Step 10 | Training Loss: 0.065764 | Validation Accuracy: 0.835832\n",
      "Accuracy on Test data: 0.8357877731323242\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.184982 | Validation Accuracy: 0.494766\n",
      "Step 2 | Training Loss: 0.011917 | Validation Accuracy: 0.789123\n",
      "Step 3 | Training Loss: 0.021253 | Validation Accuracy: 0.800213\n",
      "Step 4 | Training Loss: 0.006068 | Validation Accuracy: 0.815250\n",
      "Step 5 | Training Loss: 0.029147 | Validation Accuracy: 0.812323\n",
      "Step 6 | Training Loss: 0.038910 | Validation Accuracy: 0.806911\n",
      "Step 7 | Training Loss: 0.001900 | Validation Accuracy: 0.812633\n",
      "Step 8 | Training Loss: 0.041341 | Validation Accuracy: 0.805713\n",
      "Step 9 | Training Loss: 0.017865 | Validation Accuracy: 0.752218\n",
      "Step 10 | Training Loss: 0.032875 | Validation Accuracy: 0.741616\n",
      "Accuracy on Test data: 0.7416607737541199\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.043765 | Validation Accuracy: 0.770138\n",
      "Step 2 | Training Loss: 0.158223 | Validation Accuracy: 0.783668\n",
      "Step 3 | Training Loss: 0.000390 | Validation Accuracy: 0.817246\n",
      "Step 4 | Training Loss: 0.020177 | Validation Accuracy: 0.846655\n",
      "Step 5 | Training Loss: 0.104613 | Validation Accuracy: 0.862580\n",
      "Step 6 | Training Loss: 0.009620 | Validation Accuracy: 0.862225\n",
      "Step 7 | Training Loss: 0.071351 | Validation Accuracy: 0.863689\n",
      "Step 8 | Training Loss: 0.024599 | Validation Accuracy: 0.863644\n",
      "Step 9 | Training Loss: 0.042861 | Validation Accuracy: 0.855660\n",
      "Step 10 | Training Loss: 0.024303 | Validation Accuracy: 0.807621\n",
      "Accuracy on Test data: 0.807620644569397\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [2, 4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [10]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:15.572534Z",
     "start_time": "2017-05-11T21:09:15.541986Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:15.624110Z",
     "start_time": "2017-05-11T21:09:15.574217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.890836</td>\n",
       "      <td>0.891102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.875266</td>\n",
       "      <td>0.875355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.869943</td>\n",
       "      <td>0.870653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.854418</td>\n",
       "      <td>0.854462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.841288</td>\n",
       "      <td>0.841377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.835832</td>\n",
       "      <td>0.835788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.816758</td>\n",
       "      <td>0.816537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.807621</td>\n",
       "      <td>0.807621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.790765</td>\n",
       "      <td>0.790853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.764194</td>\n",
       "      <td>0.764194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.742237</td>\n",
       "      <td>0.742770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.741616</td>\n",
       "      <td>0.741661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475426</td>\n",
       "      <td>0.475248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "4      10              32              2     0.890836    0.891102\n",
       "7      10               8              4     0.875266    0.875355\n",
       "1      10               4              2     0.869943    0.870653\n",
       "2      10               8              2     0.854418    0.854462\n",
       "8      10              16              4     0.841288    0.841377\n",
       "12     10               8              6     0.835832    0.835788\n",
       "0      10               2              2     0.816758    0.816537\n",
       "14     10              32              6     0.807621    0.807621\n",
       "9      10              32              4     0.790765    0.790853\n",
       "5      10               2              4     0.764194    0.764194\n",
       "6      10               4              4     0.742237    0.742770\n",
       "13     10              16              6     0.741616    0.741661\n",
       "3      10              16              2     0.569242    0.569242\n",
       "10     10               2              6     0.569242    0.569242\n",
       "11     10               4              6     0.475426    0.475248"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:15.690978Z",
     "start_time": "2017-05-11T21:09:15.625730Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:15.754155Z",
     "start_time": "2017-05-11T21:09:15.692561Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T21:09:17.229003Z",
     "start_time": "2017-05-11T21:09:15.755490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8853  0.1147]\n",
      " [ 0.1012  0.8988]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f071a6425f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
