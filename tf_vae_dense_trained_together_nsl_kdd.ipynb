{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:34:59.807778Z",
     "start_time": "2017-06-02T22:34:59.388583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:34:59.905181Z",
     "start_time": "2017-06-02T22:34:59.809331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:34:59.911478Z",
     "start_time": "2017-06-02T22:34:59.906771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:34:59.917260Z",
     "start_time": "2017-06-02T22:34:59.912841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:35:00.823967Z",
     "start_time": "2017-06-02T22:34:59.918576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:35:01.909391Z",
     "start_time": "2017-06-02T22:35:00.825564Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:35:02.413658Z",
     "start_time": "2017-06-02T22:35:01.911077Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T22:35:02.689789Z",
     "start_time": "2017-06-02T22:35:02.415575Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20','time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            Train.best_acc = 0\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.2)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                                   net.regularized_loss, \n",
    "                                                                   ], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                                             net.y_: y_train[i,:], \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        count = 10\n",
    "                        \n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                            print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "                            count -= 1\n",
    "\n",
    "                    valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                    test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                      net.actual, net.y], #net.summary_op \n",
    "                                                                                      feed_dict={net.x: preprocess.x_test, \n",
    "                                                                                     net.y_: preprocess.y_test, \n",
    "                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    test_accuracy_, test_loss_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                      net.actual, net.y], #net.summary_op \n",
    "                                                                                      feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                                     net.y_: preprocess.y_test_, \n",
    "                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                    if epoch % 1 == 0:\n",
    "                        print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:.6f} | Test Accuracy: {:.6f}, {:6f}\"\n",
    "                              .format(epoch, train_loss, test_loss, test_accuracy, test_accuracy_))\n",
    "\n",
    "                    if test_accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = test_accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        \n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        \n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if test_accuracy > Train.best_acc:\n",
    "                        Train.best_acc = test_accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(epochs,h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(epochs*len(lrs),f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(epochs*len(lrs), f, h,valid_accuracy, test_accuracy, test_accuracy_, time.perf_counter() - start_time))})\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "            print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.097993Z",
     "start_time": "2017-06-02T22:35:02.691649Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:30 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.000258 | Test Loss: 0.000442 | Test Accuracy: 0.804294, 0.665316\n",
      "Step 2 | Training Loss: 0.000235 | Test Loss: 0.000494 | Test Accuracy: 0.771824, 0.611139\n",
      "Step 3 | Training Loss: 0.000182 | Test Loss: 0.000301 | Test Accuracy: 0.794225, 0.642700\n",
      "Step 4 | Training Loss: 0.000072 | Test Loss: 0.000133 | Test Accuracy: 0.810548, 0.682447\n",
      "Step 5 | Training Loss: 0.000061 | Test Loss: 0.000375 | Test Accuracy: 0.840312, 0.734684\n",
      "Step 6 | Training Loss: 0.000082 | Test Loss: 9.455018 | Test Accuracy: 0.840933, 0.738481\n",
      "Step 7 | Training Loss: 0.000089 | Test Loss: 0.000386 | Test Accuracy: 0.831840, 0.720253\n",
      "Step 8 | Training Loss: 0.000006 | Test Loss: 0.000212 | Test Accuracy: 0.846301, 0.742700\n",
      "Step 9 | Training Loss: 0.000039 | Test Loss: 0.000175 | Test Accuracy: 0.859697, 0.759072\n",
      "Step 10 | Training Loss: 0.000026 | Test Loss: 0.000199 | Test Accuracy: 0.850914, 0.749958\n",
      "Step 11 | Training Loss: 0.000064 | Test Loss: 0.000300 | Test Accuracy: 0.837562, 0.735021\n",
      "Step 12 | Training Loss: 0.000035 | Test Loss: 0.000131 | Test Accuracy: 0.847188, 0.755781\n",
      "Step 13 | Training Loss: 0.000023 | Test Loss: 0.000181 | Test Accuracy: 0.862003, 0.770127\n",
      "Step 14 | Training Loss: 0.000035 | Test Loss: 0.000190 | Test Accuracy: 0.864620, 0.805401\n",
      "Step 15 | Training Loss: 0.000081 | Test Loss: 0.000195 | Test Accuracy: 0.838449, 0.761857\n",
      "Step 16 | Training Loss: 0.000030 | Test Loss: 0.000218 | Test Accuracy: 0.831618, 0.740591\n",
      "Step 17 | Training Loss: 0.000037 | Test Loss: 0.000279 | Test Accuracy: 0.831042, 0.732827\n",
      "Step 18 | Training Loss: 0.000094 | Test Loss: 0.000261 | Test Accuracy: 0.834324, 0.730886\n",
      "Step 19 | Training Loss: 0.000078 | Test Loss: 0.000345 | Test Accuracy: 0.828957, 0.717046\n",
      "Step 20 | Training Loss: 0.000024 | Test Loss: 0.000301 | Test Accuracy: 0.839514, 0.729114\n",
      "Step 21 | Training Loss: 0.000090 | Test Loss: 0.000283 | Test Accuracy: 0.847942, 0.746414\n",
      "Step 22 | Training Loss: 0.000053 | Test Loss: 0.000250 | Test Accuracy: 0.854817, 0.748776\n",
      "Step 23 | Training Loss: 0.000061 | Test Loss: 0.000197 | Test Accuracy: 0.865995, 0.776456\n",
      "Step 24 | Training Loss: 0.000016 | Test Loss: 0.000229 | Test Accuracy: 0.869721, 0.781519\n",
      "Step 25 | Training Loss: 0.000012 | Test Loss: 0.000232 | Test Accuracy: 0.867770, 0.772321\n",
      "Step 26 | Training Loss: 0.000014 | Test Loss: 0.000237 | Test Accuracy: 0.875488, 0.796287\n",
      "Step 27 | Training Loss: 0.000037 | Test Loss: 0.000233 | Test Accuracy: 0.873226, 0.784641\n",
      "Step 28 | Training Loss: 0.000020 | Test Loss: 0.000173 | Test Accuracy: 0.878460, 0.793586\n",
      "Step 29 | Training Loss: 0.000024 | Test Loss: 0.000212 | Test Accuracy: 0.871718, 0.787511\n",
      "Step 30 | Training Loss: 0.000027 | Test Loss: 0.000187 | Test Accuracy: 0.881875, 0.799072\n",
      "Step 1 | Training Loss: 0.000018 | Test Loss: 0.000192 | Test Accuracy: 0.883783, 0.798734\n",
      "Step 2 | Training Loss: 0.000000 | Test Loss: 0.000210 | Test Accuracy: 0.881299, 0.797806\n",
      "Step 3 | Training Loss: 0.000003 | Test Loss: 0.000198 | Test Accuracy: 0.883517, 0.797300\n",
      "Step 4 | Training Loss: 0.000000 | Test Loss: 0.000186 | Test Accuracy: 0.879924, 0.795949\n",
      "Step 5 | Training Loss: 0.000006 | Test Loss: 0.000185 | Test Accuracy: 0.880145, 0.795443\n",
      "Step 6 | Training Loss: 0.000025 | Test Loss: 0.000176 | Test Accuracy: 0.883561, 0.797384\n",
      "Step 7 | Training Loss: 0.000035 | Test Loss: 0.000188 | Test Accuracy: 0.884714, 0.802278\n",
      "Step 8 | Training Loss: 0.000001 | Test Loss: 0.000187 | Test Accuracy: 0.884315, 0.794937\n",
      "Step 9 | Training Loss: 0.000010 | Test Loss: 0.000185 | Test Accuracy: 0.885956, 0.795527\n",
      "Step 10 | Training Loss: 0.000013 | Test Loss: 0.000191 | Test Accuracy: 0.884093, 0.795865\n",
      "Step 11 | Training Loss: 0.000003 | Test Loss: 0.000179 | Test Accuracy: 0.883206, 0.797637\n",
      "Step 12 | Training Loss: 0.000014 | Test Loss: 0.000174 | Test Accuracy: 0.882807, 0.794008\n",
      "Step 13 | Training Loss: 0.000006 | Test Loss: 0.000176 | Test Accuracy: 0.885735, 0.795949\n",
      "Step 14 | Training Loss: 0.000000 | Test Loss: 0.000172 | Test Accuracy: 0.882984, 0.796878\n",
      "Step 15 | Training Loss: 0.000000 | Test Loss: 0.000176 | Test Accuracy: 0.884892, 0.795527\n",
      "Step 16 | Training Loss: 0.000024 | Test Loss: 0.000161 | Test Accuracy: 0.883827, 0.800506\n",
      "Step 17 | Training Loss: 0.000002 | Test Loss: 0.000171 | Test Accuracy: 0.882142, 0.792489\n",
      "Step 18 | Training Loss: 0.000002 | Test Loss: 0.000168 | Test Accuracy: 0.881299, 0.793840\n",
      "Step 19 | Training Loss: 0.000036 | Test Loss: 0.000169 | Test Accuracy: 0.878726, 0.782700\n",
      "Step 20 | Training Loss: 0.000015 | Test Loss: 0.000169 | Test Accuracy: 0.880012, 0.781941\n",
      "Step 21 | Training Loss: 0.000007 | Test Loss: 0.000174 | Test Accuracy: 0.880057, 0.783038\n",
      "Step 22 | Training Loss: 0.000030 | Test Loss: 0.000173 | Test Accuracy: 0.885468, 0.794177\n",
      "Step 23 | Training Loss: 0.000015 | Test Loss: 0.000176 | Test Accuracy: 0.882541, 0.787848\n",
      "Step 24 | Training Loss: 0.000009 | Test Loss: 0.000162 | Test Accuracy: 0.879924, 0.783207\n",
      "Step 25 | Training Loss: 0.000015 | Test Loss: 0.000170 | Test Accuracy: 0.882319, 0.787595\n",
      "Step 26 | Training Loss: 0.000012 | Test Loss: 0.000180 | Test Accuracy: 0.887376, 0.795274\n",
      "Step 27 | Training Loss: 0.000001 | Test Loss: 0.000178 | Test Accuracy: 0.882275, 0.791308\n",
      "Step 28 | Training Loss: 0.000009 | Test Loss: 0.000176 | Test Accuracy: 0.880589, 0.779156\n",
      "Step 29 | Training Loss: 0.000005 | Test Loss: 0.000161 | Test Accuracy: 0.880633, 0.789367\n",
      "Step 30 | Training Loss: 0.000012 | Test Loss: 0.000177 | Test Accuracy: 0.881521, 0.790549\n",
      "Best Accuracy on Test data: 0.8873757719993591\n",
      "Current Layer Attributes - epochs:30 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.000116 | Test Loss: 0.000028 | Test Accuracy: 0.750665, 0.603376\n",
      "Step 2 | Training Loss: 0.000159 | Test Loss: 666.414124 | Test Accuracy: 0.815250, 0.681181\n",
      "Step 3 | High Training Loss: 7756415167948128256.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-2\n",
      "Step 3 | High Training Loss: 1271795023872.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-2\n",
      "Step 3 | High Training Loss: 90326299574272.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-2\n",
      "Step 3 | High Training Loss: 1429805203456.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-2\n",
      "Step 3 | Training Loss: 0.000009 | Test Loss: 365.262787 | Test Accuracy: 0.831973, 0.724895\n",
      "Step 4 | High Training Loss: 667321.750000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: inf ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | High Training Loss: 24026962554669004245434368.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-3\n",
      "Step 4 | Training Loss: 0.000278 | Test Loss: 0.001075 | Test Accuracy: 0.824787, 0.706160\n",
      "Step 5 | Training Loss: 0.000099 | Test Loss: 0.000503 | Test Accuracy: 0.832594, 0.733586\n",
      "Step 6 | Training Loss: 0.000021 | Test Loss: 0.000274 | Test Accuracy: 0.846345, 0.738903\n",
      "Step 7 | Training Loss: 0.000054 | Test Loss: 0.000145 | Test Accuracy: 0.856148, 0.760422\n",
      "Step 8 | Training Loss: 0.000010 | Test Loss: 0.000217 | Test Accuracy: 0.863556, 0.763376\n",
      "Step 9 | Training Loss: 0.000021 | Test Loss: 0.000230 | Test Accuracy: 0.869056, 0.773333\n",
      "Step 10 | Training Loss: 0.000044 | Test Loss: 0.000248 | Test Accuracy: 0.860140, 0.795443\n",
      "Step 11 | Training Loss: 0.000090 | Test Loss: 0.000225 | Test Accuracy: 0.871939, 0.781013\n",
      "Step 12 | Training Loss: 0.000070 | Test Loss: 0.000191 | Test Accuracy: 0.869677, 0.776709\n",
      "Step 13 | Training Loss: 0.000050 | Test Loss: 0.000151 | Test Accuracy: 0.874601, 0.781435\n",
      "Step 14 | Training Loss: 0.000005 | Test Loss: 0.000138 | Test Accuracy: 0.870254, 0.777046\n",
      "Step 15 | Training Loss: 0.000141 | Test Loss: 0.000061 | Test Accuracy: 0.866395, 0.778565\n",
      "Step 16 | Training Loss: 0.000007 | Test Loss: 0.000127 | Test Accuracy: 0.866661, 0.783882\n",
      "Step 17 | Training Loss: 0.000075 | Test Loss: 0.000087 | Test Accuracy: 0.876153, 0.786835\n",
      "Step 18 | Training Loss: 0.000004 | Test Loss: 0.000086 | Test Accuracy: 0.868568, 0.767848\n",
      "Step 19 | Training Loss: 0.000062 | Test Loss: 0.000112 | Test Accuracy: 0.870697, 0.776540\n",
      "Step 20 | Training Loss: 0.000017 | Test Loss: 0.000088 | Test Accuracy: 0.875044, 0.779240\n",
      "Step 21 | Training Loss: 0.000064 | Test Loss: 0.000171 | Test Accuracy: 0.869455, 0.790886\n",
      "Step 22 | Training Loss: 0.000055 | Test Loss: 0.000115 | Test Accuracy: 0.877262, 0.782025\n",
      "Step 23 | Training Loss: 0.000021 | Test Loss: 0.000153 | Test Accuracy: 0.871673, 0.804895\n",
      "Step 24 | Training Loss: 0.000031 | Test Loss: 0.000114 | Test Accuracy: 0.870875, 0.781519\n",
      "Step 25 | Training Loss: 0.000033 | Test Loss: 0.000127 | Test Accuracy: 0.871274, 0.781688\n",
      "Step 26 | High Training Loss: 121162106186202383122432.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-22\n",
      "Step 26 | Training Loss: 0.000109 | Test Loss: 0.000169 | Test Accuracy: 0.859697, 0.761688\n",
      "Step 27 | Training Loss: 0.000047 | Test Loss: 0.000065 | Test Accuracy: 0.866395, 0.770802\n",
      "Step 28 | Training Loss: 0.000038 | Test Loss: 0.000079 | Test Accuracy: 0.873004, 0.777046\n",
      "Step 29 | Training Loss: 0.000024 | Test Loss: 0.000161 | Test Accuracy: 0.873847, 0.777384\n",
      "Step 30 | Training Loss: 0.000019 | Test Loss: 0.000064 | Test Accuracy: 0.871939, 0.782025\n",
      "Step 1 | Training Loss: 0.000038 | Test Loss: 0.000097 | Test Accuracy: 0.873714, 0.779916\n",
      "Step 2 | Training Loss: 0.000028 | Test Loss: 0.000118 | Test Accuracy: 0.872294, 0.778903\n",
      "Step 3 | Training Loss: 0.000055 | Test Loss: 0.000116 | Test Accuracy: 0.873669, 0.780675\n",
      "Step 4 | Training Loss: 0.000021 | Test Loss: 0.000119 | Test Accuracy: 0.872250, 0.780928\n",
      "Step 5 | Training Loss: 0.000028 | Test Loss: 0.000114 | Test Accuracy: 0.871762, 0.777975\n",
      "Step 6 | Training Loss: 0.000041 | Test Loss: 0.000124 | Test Accuracy: 0.874601, 0.778312\n",
      "Step 7 | Training Loss: 0.000045 | Test Loss: 0.000136 | Test Accuracy: 0.875399, 0.779325\n",
      "Step 8 | Training Loss: 0.000018 | Test Loss: 0.000119 | Test Accuracy: 0.876597, 0.780675\n",
      "Step 9 | Training Loss: 0.000013 | Test Loss: 0.000119 | Test Accuracy: 0.876065, 0.779747\n",
      "Step 10 | Training Loss: 0.000036 | Test Loss: 0.000127 | Test Accuracy: 0.871762, 0.773755\n",
      "Step 11 | Training Loss: 0.000073 | Test Loss: 0.000111 | Test Accuracy: 0.873625, 0.776540\n",
      "Step 12 | Training Loss: 0.000013 | Test Loss: 0.000118 | Test Accuracy: 0.877174, 0.782363\n",
      "Step 13 | Training Loss: 0.000044 | Test Loss: 0.000113 | Test Accuracy: 0.877706, 0.782110\n",
      "Step 14 | Training Loss: 0.000022 | Test Loss: 0.000135 | Test Accuracy: 0.876597, 0.778312\n",
      "Step 15 | Training Loss: 0.000015 | Test Loss: 0.000109 | Test Accuracy: 0.874823, 0.777553\n",
      "Step 16 | Training Loss: 0.000024 | Test Loss: 0.000109 | Test Accuracy: 0.875044, 0.778397\n",
      "Step 17 | Training Loss: 0.000039 | Test Loss: 0.000129 | Test Accuracy: 0.875399, 0.776118\n",
      "Step 18 | Training Loss: 0.000019 | Test Loss: 0.000118 | Test Accuracy: 0.875222, 0.778987\n",
      "Step 19 | Training Loss: 0.000014 | Test Loss: 0.000112 | Test Accuracy: 0.885868, 0.795105\n",
      "Step 20 | Training Loss: 0.000021 | Test Loss: 0.000148 | Test Accuracy: 0.877883, 0.781435\n",
      "Step 21 | Training Loss: 0.000012 | Test Loss: 0.000130 | Test Accuracy: 0.879258, 0.784219\n",
      "Step 22 | Training Loss: 0.000028 | Test Loss: 0.000143 | Test Accuracy: 0.883251, 0.793080\n",
      "Step 23 | Training Loss: 0.000018 | Test Loss: 0.000133 | Test Accuracy: 0.880722, 0.784388\n",
      "Step 24 | Training Loss: 0.000026 | Test Loss: 0.000160 | Test Accuracy: 0.875355, 0.776287\n",
      "Step 25 | Training Loss: 0.000017 | Test Loss: 0.000158 | Test Accuracy: 0.875532, 0.774768\n",
      "Step 26 | Training Loss: 0.000003 | Test Loss: 0.000153 | Test Accuracy: 0.877129, 0.780422\n",
      "Step 27 | Training Loss: 0.000018 | Test Loss: 0.000154 | Test Accuracy: 0.877573, 0.777215\n",
      "Step 28 | Training Loss: 0.000002 | Test Loss: 0.000144 | Test Accuracy: 0.878016, 0.779831\n",
      "Step 29 | Training Loss: 0.000000 | Test Loss: 0.000134 | Test Accuracy: 0.883650, 0.788945\n",
      "Step 30 | Training Loss: 0.000006 | Test Loss: 0.000151 | Test Accuracy: 0.878238, 0.780675\n",
      "Best Accuracy on Test data: 0.8858676552772522\n",
      "Current Layer Attributes - epochs:30 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.000096 | Test Loss: 0.296171 | Test Accuracy: 0.794047, 0.635190\n",
      "Step 2 | Training Loss: 0.000358 | Test Loss: 11603853.000000 | Test Accuracy: 0.564452, 0.480084\n",
      "Step 3 | Training Loss: 0.000294 | Test Loss: 0.000544 | Test Accuracy: 0.645893, 0.607257\n",
      "Step 4 | Training Loss: 0.000158 | Test Loss: 0.000435 | Test Accuracy: 0.712740, 0.666245\n",
      "Step 5 | High Training Loss: 48270716.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 5 | High Training Loss: 425667744.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 5 | High Training Loss: 20571005911040.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 5 | Training Loss: 0.000049 | Test Loss: 0.000817 | Test Accuracy: 0.721168, 0.582194\n",
      "Step 6 | Training Loss: 0.000045 | Test Loss: 0.508899 | Test Accuracy: 0.723474, 0.591139\n",
      "Step 7 | High Training Loss: 148629.437500 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 7 | High Training Loss: 10734.738281 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 7 | Training Loss: 0.000092 | Test Loss: 0.000007 | Test Accuracy: 0.783889, 0.641181\n",
      "Step 8 | Training Loss: 0.000005 | Test Loss: 0.000370 | Test Accuracy: 0.752706, 0.630886\n",
      "Step 9 | Training Loss: 0.000049 | Test Loss: 0.000207 | Test Accuracy: 0.813831, 0.664135\n",
      "Step 10 | Training Loss: 0.000025 | Test Loss: 0.000339 | Test Accuracy: 0.724361, 0.601350\n",
      "Step 11 | Training Loss: 0.000014 | Test Loss: 0.000339 | Test Accuracy: 0.736515, 0.603460\n",
      "Step 12 | Training Loss: 0.000080 | Test Loss: 0.000362 | Test Accuracy: 0.712296, 0.563038\n",
      "Step 13 | Training Loss: 0.000136 | Test Loss: 0.000307 | Test Accuracy: 0.730793, 0.571055\n",
      "Step 14 | Training Loss: 0.000008 | Test Loss: 0.000354 | Test Accuracy: 0.732434, 0.587004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 | Training Loss: 0.000077 | Test Loss: 0.000337 | Test Accuracy: 0.733943, 0.584641\n",
      "Step 16 | Training Loss: 0.000045 | Test Loss: 0.000296 | Test Accuracy: 0.756077, 0.625063\n",
      "Step 17 | Training Loss: 0.000069 | Test Loss: 0.000286 | Test Accuracy: 0.788857, 0.672236\n",
      "Step 18 | Training Loss: 0.000063 | Test Loss: 0.000256 | Test Accuracy: 0.781183, 0.676709\n",
      "Step 19 | Training Loss: 0.000019 | Test Loss: 0.000199 | Test Accuracy: 0.761577, 0.625823\n",
      "Step 20 | Training Loss: 0.000075 | Test Loss: 0.000126 | Test Accuracy: 0.780075, 0.641772\n",
      "Step 21 | Training Loss: 0.000011 | Test Loss: 0.000456 | Test Accuracy: 0.792672, 0.676456\n",
      "Step 22 | High Training Loss: 11344415.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-9\n",
      "Step 22 | Training Loss: 0.000124 | Test Loss: 0.000238 | Test Accuracy: 0.811480, 0.657215\n",
      "Step 23 | Training Loss: 0.000073 | Test Loss: 0.000188 | Test Accuracy: 0.812855, 0.658397\n",
      "Step 24 | Training Loss: 0.000078 | Test Loss: 0.000270 | Test Accuracy: 0.837873, 0.711308\n",
      "Step 25 | Training Loss: 0.000039 | Test Loss: 0.000192 | Test Accuracy: 0.805270, 0.648017\n",
      "Step 26 | Training Loss: 0.000062 | Test Loss: 0.000217 | Test Accuracy: 0.821194, 0.685907\n",
      "Step 27 | Training Loss: 0.000041 | Test Loss: 0.000122 | Test Accuracy: 0.829800, 0.685316\n",
      "Step 28 | Training Loss: 0.000010 | Test Loss: 0.000223 | Test Accuracy: 0.803540, 0.644473\n",
      "Step 29 | Training Loss: 0.000017 | Test Loss: 0.000230 | Test Accuracy: 0.818976, 0.674768\n",
      "Step 30 | Training Loss: 0.000007 | Test Loss: 0.000207 | Test Accuracy: 0.817645, 0.665401\n",
      "Step 1 | Training Loss: 0.000015 | Test Loss: 0.000189 | Test Accuracy: 0.819553, 0.662278\n",
      "Step 2 | Training Loss: 0.000006 | Test Loss: 0.000203 | Test Accuracy: 0.822303, 0.671477\n",
      "Step 3 | Training Loss: 0.000003 | Test Loss: 0.000204 | Test Accuracy: 0.822525, 0.673333\n",
      "Step 4 | Training Loss: 0.000014 | Test Loss: 0.000200 | Test Accuracy: 0.820839, 0.667933\n",
      "Step 5 | Training Loss: 0.000021 | Test Loss: 0.000202 | Test Accuracy: 0.824344, 0.668861\n",
      "Step 6 | Training Loss: 0.000018 | Test Loss: 0.000204 | Test Accuracy: 0.818178, 0.660844\n",
      "Step 7 | Training Loss: 0.000010 | Test Loss: 0.000197 | Test Accuracy: 0.814762, 0.655021\n",
      "Step 8 | Training Loss: 0.000022 | Test Loss: 0.000197 | Test Accuracy: 0.822436, 0.665316\n",
      "Step 9 | Training Loss: 0.000037 | Test Loss: 0.000194 | Test Accuracy: 0.816803, 0.663376\n",
      "Step 10 | Training Loss: 0.000001 | Test Loss: 0.000200 | Test Accuracy: 0.822081, 0.665738\n",
      "Step 11 | Training Loss: 0.000005 | Test Loss: 0.000203 | Test Accuracy: 0.815782, 0.656118\n",
      "Step 12 | Training Loss: 0.000008 | Test Loss: 0.000203 | Test Accuracy: 0.820085, 0.660675\n",
      "Step 13 | Training Loss: 0.000005 | Test Loss: 0.000210 | Test Accuracy: 0.801632, 0.638734\n",
      "Step 14 | Training Loss: 0.000015 | Test Loss: 0.000200 | Test Accuracy: 0.809129, 0.647426\n",
      "Step 15 | Training Loss: 0.000008 | Test Loss: 0.000194 | Test Accuracy: 0.818267, 0.657131\n",
      "Step 16 | Training Loss: 0.000030 | Test Loss: 0.000213 | Test Accuracy: 0.814141, 0.659916\n",
      "Step 17 | Training Loss: 0.000012 | Test Loss: 0.000198 | Test Accuracy: 0.811036, 0.645401\n",
      "Step 18 | Training Loss: 0.000029 | Test Loss: 0.000221 | Test Accuracy: 0.808818, 0.641688\n",
      "Step 19 | Training Loss: 0.000022 | Test Loss: 0.000213 | Test Accuracy: 0.801721, 0.627173\n",
      "Step 20 | Training Loss: 0.000011 | Test Loss: 0.000228 | Test Accuracy: 0.792450, 0.614684\n",
      "Step 21 | Training Loss: 0.000023 | Test Loss: 0.000223 | Test Accuracy: 0.793293, 0.616624\n",
      "Step 22 | Training Loss: 0.000012 | Test Loss: 0.000230 | Test Accuracy: 0.788990, 0.607764\n",
      "Step 23 | Training Loss: 0.000000 | Test Loss: 0.000225 | Test Accuracy: 0.803673, 0.630211\n",
      "Step 24 | Training Loss: 0.000015 | Test Loss: 0.000220 | Test Accuracy: 0.803895, 0.631224\n",
      "Step 25 | Training Loss: 0.000010 | Test Loss: 0.000218 | Test Accuracy: 0.815871, 0.648523\n",
      "Step 26 | Training Loss: 0.000009 | Test Loss: 0.000221 | Test Accuracy: 0.805935, 0.633080\n",
      "Step 27 | Training Loss: 0.000010 | Test Loss: 0.000234 | Test Accuracy: 0.801632, 0.634177\n",
      "Step 28 | Training Loss: 0.000015 | Test Loss: 0.000220 | Test Accuracy: 0.798128, 0.625232\n",
      "Step 29 | Training Loss: 0.000008 | Test Loss: 0.000221 | Test Accuracy: 0.797596, 0.622954\n",
      "Step 30 | Training Loss: 0.000001 | Test Loss: 0.000220 | Test Accuracy: 0.795378, 0.612321\n",
      "Best Accuracy on Test data: 0.8378726243972778\n",
      "Current Layer Attributes - epochs:30 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.000116 | Test Loss: 0.009901 | Test Accuracy: 0.714913, 0.541688\n",
      "Step 2 | High Training Loss: 15570922.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 596158036574208.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | High Training Loss: 21301287365648580608.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-1\n",
      "Step 2 | Training Loss: 0.000108 | Test Loss: 0.000444 | Test Accuracy: 0.776969, 0.630464\n",
      "Step 3 | Training Loss: 0.000020 | Test Loss: 0.000355 | Test Accuracy: 0.788902, 0.650717\n",
      "Step 4 | Training Loss: 0.000061 | Test Loss: 0.000380 | Test Accuracy: 0.800701, 0.650127\n",
      "Step 5 | High Training Loss: 13660.256836 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13754.834961 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13736.240234 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13730.466797 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13729.579102 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13728.859375 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13736.292969 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | High Training Loss: 13729.141602 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 13742.426758 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 25792240685154304.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | High Training Loss: 110011873411639279616.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-4\n",
      "Step 5 | Training Loss: 0.000051 | Test Loss: 0.000353 | Test Accuracy: 0.842841, 0.737890\n",
      "Step 6 | High Training Loss: 948682643996672.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | High Training Loss: 23130720318783488.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_2/model-5\n",
      "Step 6 | Training Loss: 0.000160 | Test Loss: 0.000577 | Test Accuracy: 0.858100, 0.750380\n",
      "Step 7 | Training Loss: 0.000008 | Test Loss: 0.000304 | Test Accuracy: 0.877883, 0.783122\n",
      "Step 8 | Training Loss: 0.000054 | Test Loss: 0.000322 | Test Accuracy: 0.876907, 0.782025\n",
      "Step 9 | Training Loss: 0.000032 | Test Loss: 0.000343 | Test Accuracy: 0.870476, 0.767089\n",
      "Step 10 | Training Loss: 0.000004 | Test Loss: 0.000320 | Test Accuracy: 0.863156, 0.749451\n",
      "Step 11 | Training Loss: 0.000007 | Test Loss: 0.000337 | Test Accuracy: 0.860406, 0.747848\n",
      "Step 12 | Training Loss: 0.000013 | Test Loss: 0.000297 | Test Accuracy: 0.861914, 0.751983\n",
      "Step 13 | Training Loss: 0.000010 | Test Loss: 0.000282 | Test Accuracy: 0.867237, 0.760844\n",
      "Step 14 | Training Loss: 0.000102 | Test Loss: 0.000223 | Test Accuracy: 0.871008, 0.764810\n",
      "Step 15 | Training Loss: 0.000021 | Test Loss: 0.000282 | Test Accuracy: 0.856370, 0.739831\n",
      "Step 16 | Training Loss: 0.000127 | Test Loss: 0.000307 | Test Accuracy: 0.825319, 0.683966\n",
      "Step 17 | Training Loss: 0.000010 | Test Loss: 0.000264 | Test Accuracy: 0.865374, 0.759662\n",
      "Step 18 | Training Loss: 0.000018 | Test Loss: 0.000245 | Test Accuracy: 0.875754, 0.784388\n",
      "Step 19 | Training Loss: 0.000005 | Test Loss: 0.000236 | Test Accuracy: 0.879879, 0.786667\n",
      "Step 20 | Training Loss: 0.000032 | Test Loss: 0.000208 | Test Accuracy: 0.878238, 0.783882\n",
      "Step 21 | Training Loss: 0.000046 | Test Loss: 0.000213 | Test Accuracy: 0.878061, 0.784473\n",
      "Step 22 | Training Loss: 0.000026 | Test Loss: 0.000237 | Test Accuracy: 0.878903, 0.784557\n",
      "Step 23 | Training Loss: 0.000042 | Test Loss: 0.000222 | Test Accuracy: 0.878371, 0.784557\n",
      "Step 24 | Training Loss: 0.000008 | Test Loss: 0.000208 | Test Accuracy: 0.880589, 0.786498\n",
      "Step 25 | Training Loss: 0.000051 | Test Loss: 0.000203 | Test Accuracy: 0.878593, 0.784557\n",
      "Step 26 | Training Loss: 0.000043 | Test Loss: 0.000237 | Test Accuracy: 0.879391, 0.784388\n",
      "Step 27 | Training Loss: 0.000012 | Test Loss: 0.000226 | Test Accuracy: 0.878460, 0.783460\n",
      "Step 28 | Training Loss: 0.000029 | Test Loss: 0.000217 | Test Accuracy: 0.878061, 0.784895\n",
      "Step 29 | Training Loss: 0.000009 | Test Loss: 0.000221 | Test Accuracy: 0.883251, 0.794768\n",
      "Step 30 | Training Loss: 0.000007 | Test Loss: 0.000221 | Test Accuracy: 0.874024, 0.773840\n",
      "Step 1 | Training Loss: 0.000023 | Test Loss: 0.000204 | Test Accuracy: 0.874823, 0.775781\n",
      "Step 2 | Training Loss: 0.000048 | Test Loss: 0.000210 | Test Accuracy: 0.878637, 0.781772\n",
      "Step 3 | Training Loss: 0.000066 | Test Loss: 0.000209 | Test Accuracy: 0.876109, 0.778903\n",
      "Step 4 | Training Loss: 0.000033 | Test Loss: 0.000207 | Test Accuracy: 0.877174, 0.781013\n",
      "Step 5 | Training Loss: 0.000031 | Test Loss: 0.000191 | Test Accuracy: 0.877928, 0.782363\n",
      "Step 6 | Training Loss: 0.000008 | Test Loss: 0.000203 | Test Accuracy: 0.877129, 0.780675\n",
      "Step 7 | Training Loss: 0.000004 | Test Loss: 0.000197 | Test Accuracy: 0.878416, 0.782869\n",
      "Step 8 | Training Loss: 0.000001 | Test Loss: 0.000209 | Test Accuracy: 0.879258, 0.784726\n",
      "Step 9 | Training Loss: 0.000007 | Test Loss: 0.000196 | Test Accuracy: 0.878859, 0.784557\n",
      "Step 10 | Training Loss: 0.000016 | Test Loss: 0.000199 | Test Accuracy: 0.879835, 0.787173\n",
      "Step 11 | Training Loss: 0.000017 | Test Loss: 0.000204 | Test Accuracy: 0.879835, 0.786245\n",
      "Step 12 | Training Loss: 0.000034 | Test Loss: 0.000199 | Test Accuracy: 0.879968, 0.787173\n",
      "Step 13 | Training Loss: 0.000000 | Test Loss: 0.000197 | Test Accuracy: 0.879968, 0.785401\n",
      "Step 14 | Training Loss: 0.000030 | Test Loss: 0.000191 | Test Accuracy: 0.879968, 0.786245\n",
      "Step 15 | Training Loss: 0.000020 | Test Loss: 0.000194 | Test Accuracy: 0.880279, 0.786835\n",
      "Step 16 | Training Loss: 0.000045 | Test Loss: 0.000188 | Test Accuracy: 0.880545, 0.786329\n",
      "Step 17 | Training Loss: 0.000003 | Test Loss: 0.000191 | Test Accuracy: 0.880367, 0.785992\n",
      "Step 18 | Training Loss: 0.000006 | Test Loss: 0.000197 | Test Accuracy: 0.879170, 0.785232\n",
      "Step 19 | Training Loss: 0.000018 | Test Loss: 0.000200 | Test Accuracy: 0.879569, 0.784895\n",
      "Step 20 | Training Loss: 0.000026 | Test Loss: 0.000184 | Test Accuracy: 0.880500, 0.785992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21 | Training Loss: 0.000040 | Test Loss: 0.000191 | Test Accuracy: 0.880589, 0.787257\n",
      "Step 22 | Training Loss: 0.000061 | Test Loss: 0.000185 | Test Accuracy: 0.880412, 0.786582\n",
      "Step 23 | Training Loss: 0.000028 | Test Loss: 0.000185 | Test Accuracy: 0.880456, 0.787426\n",
      "Step 24 | Training Loss: 0.000019 | Test Loss: 0.000198 | Test Accuracy: 0.881654, 0.788270\n",
      "Step 25 | Training Loss: 0.000008 | Test Loss: 0.000190 | Test Accuracy: 0.881698, 0.788692\n",
      "Step 26 | Training Loss: 0.000016 | Test Loss: 0.000195 | Test Accuracy: 0.882541, 0.790717\n",
      "Step 27 | Training Loss: 0.000008 | Test Loss: 0.000183 | Test Accuracy: 0.882630, 0.791139\n",
      "Step 28 | Training Loss: 0.000003 | Test Loss: 0.000176 | Test Accuracy: 0.883605, 0.792236\n",
      "Step 29 | Training Loss: 0.000005 | Test Loss: 0.000168 | Test Accuracy: 0.883162, 0.792068\n",
      "Step 30 | Training Loss: 0.000005 | Test Loss: 0.000181 | Test Accuracy: 0.882275, 0.789620\n",
      "Best Accuracy on Test data: 0.883605420589447\n",
      "Current Layer Attributes - epochs:30 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.000383 | Test Loss: 0.000664 | Test Accuracy: 0.702803, 0.555190\n",
      "Step 2 | Training Loss: 0.000287 | Test Loss: 0.000242 | Test Accuracy: 0.765791, 0.654093\n",
      "Step 3 | Training Loss: 0.000008 | Test Loss: 0.000387 | Test Accuracy: 0.802120, 0.674852\n",
      "Step 4 | Training Loss: 0.000152 | Test Loss: 0.001493 | Test Accuracy: 0.651260, 0.528270\n",
      "Step 5 | High Training Loss: 40573.585938 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_4/model-3\n",
      "Step 5 | Training Loss: 0.000010 | Test Loss: 0.000220 | Test Accuracy: 0.834590, 0.708270\n",
      "Step 6 | Training Loss: 0.000292 | Test Loss: 0.000117 | Test Accuracy: 0.781716, 0.635949\n",
      "Step 7 | Training Loss: 0.001956 | Test Loss: 0.000140 | Test Accuracy: 0.805891, 0.694852\n",
      "Step 8 | High Training Loss: 191179344.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_4/model-5\n",
      "Step 8 | Training Loss: 0.000111 | Test Loss: 0.000292 | Test Accuracy: 0.813653, 0.676456\n",
      "Step 9 | Training Loss: 0.000118 | Test Loss: 0.000335 | Test Accuracy: 0.716865, 0.563460\n",
      "Step 10 | Training Loss: 0.000034 | Test Loss: 0.000323 | Test Accuracy: 0.742237, 0.587679\n",
      "Step 11 | Training Loss: 0.000090 | Test Loss: 0.000267 | Test Accuracy: 0.750133, 0.616118\n",
      "Step 12 | Training Loss: 0.000058 | Test Loss: 0.000209 | Test Accuracy: 0.759714, 0.629705\n",
      "Step 13 | Training Loss: 0.000036 | Test Loss: 0.000128 | Test Accuracy: 0.788103, 0.669958\n",
      "Step 14 | Training Loss: 0.000074 | Test Loss: 0.000207 | Test Accuracy: 0.797374, 0.685316\n",
      "Step 15 | Training Loss: 0.000043 | Test Loss: 0.000140 | Test Accuracy: 0.809927, 0.711224\n",
      "Step 16 | Training Loss: 0.000001 | Test Loss: 0.000177 | Test Accuracy: 0.811879, 0.699241\n",
      "Step 17 | Training Loss: 0.000035 | Test Loss: 0.000163 | Test Accuracy: 0.793781, 0.695612\n",
      "Step 18 | Training Loss: 0.000001 | Test Loss: 0.000175 | Test Accuracy: 0.781538, 0.658312\n",
      "Step 19 | Training Loss: 0.000054 | Test Loss: 0.000171 | Test Accuracy: 0.814141, 0.707595\n",
      "Step 20 | Training Loss: 0.000003 | Test Loss: 0.000144 | Test Accuracy: 0.820884, 0.718312\n",
      "Step 21 | Training Loss: 0.000018 | Test Loss: 0.000183 | Test Accuracy: 0.818488, 0.713418\n",
      "Step 22 | Training Loss: 0.000034 | Test Loss: 0.000197 | Test Accuracy: 0.816137, 0.704810\n",
      "Step 23 | Training Loss: 0.000038 | Test Loss: 0.000208 | Test Accuracy: 0.815516, 0.702785\n",
      "Step 24 | Training Loss: 0.000075 | Test Loss: 0.000204 | Test Accuracy: 0.810903, 0.697215\n",
      "Step 25 | Training Loss: 0.000077 | Test Loss: 0.000239 | Test Accuracy: 0.820484, 0.704726\n",
      "Step 26 | Training Loss: 0.000003 | Test Loss: 0.000179 | Test Accuracy: 0.811302, 0.712152\n",
      "Step 27 | Training Loss: 0.000040 | Test Loss: 0.000210 | Test Accuracy: 0.799548, 0.674515\n",
      "Step 28 | Training Loss: 0.000015 | Test Loss: 0.000171 | Test Accuracy: 0.801100, 0.669620\n",
      "Step 29 | Training Loss: 0.000049 | Test Loss: 0.000172 | Test Accuracy: 0.823101, 0.721350\n",
      "Step 30 | Training Loss: 0.000053 | Test Loss: 0.000120 | Test Accuracy: 0.814053, 0.715021\n",
      "Step 1 | Training Loss: 0.000007 | Test Loss: 0.000149 | Test Accuracy: 0.814008, 0.708439\n",
      "Step 2 | Training Loss: 0.000035 | Test Loss: 0.000158 | Test Accuracy: 0.811480, 0.707764\n",
      "Step 3 | Training Loss: 0.000009 | Test Loss: 0.000149 | Test Accuracy: 0.809750, 0.705823\n",
      "Step 4 | Training Loss: 0.000022 | Test Loss: 0.000147 | Test Accuracy: 0.808330, 0.707511\n",
      "Step 5 | Training Loss: 0.000011 | Test Loss: 0.000143 | Test Accuracy: 0.811746, 0.704557\n",
      "Step 6 | Training Loss: 0.000027 | Test Loss: 0.000130 | Test Accuracy: 0.812899, 0.706076\n",
      "Step 7 | Training Loss: 0.000055 | Test Loss: 0.000141 | Test Accuracy: 0.812456, 0.702532\n",
      "Step 8 | Training Loss: 0.000025 | Test Loss: 0.000142 | Test Accuracy: 0.810593, 0.705570\n",
      "Step 9 | Training Loss: 0.000001 | Test Loss: 0.000125 | Test Accuracy: 0.811790, 0.706751\n",
      "Step 10 | Training Loss: 0.000015 | Test Loss: 0.000140 | Test Accuracy: 0.808730, 0.705738\n",
      "Step 11 | Training Loss: 0.000042 | Test Loss: 0.000148 | Test Accuracy: 0.808641, 0.709789\n",
      "Step 12 | Training Loss: 0.000010 | Test Loss: 0.000133 | Test Accuracy: 0.809439, 0.704388\n",
      "Step 13 | Training Loss: 0.000015 | Test Loss: 0.000140 | Test Accuracy: 0.810726, 0.709114\n",
      "Step 14 | Training Loss: 0.000005 | Test Loss: 0.000142 | Test Accuracy: 0.811968, 0.706076\n",
      "Step 15 | Training Loss: 0.000026 | Test Loss: 0.000139 | Test Accuracy: 0.811036, 0.706076\n",
      "Step 16 | Training Loss: 0.000000 | Test Loss: 0.000134 | Test Accuracy: 0.810637, 0.708776\n",
      "Step 17 | Training Loss: 0.000005 | Test Loss: 0.000137 | Test Accuracy: 0.809617, 0.704051\n",
      "Step 18 | Training Loss: 0.000001 | Test Loss: 0.000147 | Test Accuracy: 0.810947, 0.706329\n",
      "Step 19 | Training Loss: 0.000020 | Test Loss: 0.000136 | Test Accuracy: 0.814807, 0.711055\n",
      "Step 20 | Training Loss: 0.000006 | Test Loss: 0.000137 | Test Accuracy: 0.814585, 0.710717\n",
      "Step 21 | Training Loss: 0.000021 | Test Loss: 0.000124 | Test Accuracy: 0.813254, 0.704895\n",
      "Step 22 | Training Loss: 0.000003 | Test Loss: 0.000119 | Test Accuracy: 0.811879, 0.713587\n",
      "Step 23 | Training Loss: 0.000009 | Test Loss: 0.000126 | Test Accuracy: 0.813742, 0.709705\n",
      "Step 24 | Training Loss: 0.000013 | Test Loss: 0.000137 | Test Accuracy: 0.810637, 0.705992\n",
      "Step 25 | Training Loss: 0.000000 | Test Loss: 0.000141 | Test Accuracy: 0.811746, 0.710211\n",
      "Step 26 | Training Loss: 0.000036 | Test Loss: 0.000150 | Test Accuracy: 0.811125, 0.706329\n",
      "Step 27 | Training Loss: 0.000004 | Test Loss: 0.000150 | Test Accuracy: 0.813077, 0.701013\n",
      "Step 28 | Training Loss: 0.000002 | Test Loss: 0.000168 | Test Accuracy: 0.808597, 0.705232\n",
      "Step 29 | Training Loss: 0.000025 | Test Loss: 0.000155 | Test Accuracy: 0.807754, 0.702278\n",
      "Step 30 | Training Loss: 0.000016 | Test Loss: 0.000143 | Test Accuracy: 0.809351, 0.706498\n",
      "Best Accuracy on Test data: 0.8345901370048523\n",
      "Current Layer Attributes - epochs:30 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.000209 | Test Loss: 0.000194 | Test Accuracy: 0.607922, 0.517468\n",
      "Step 2 | Training Loss: 0.000272 | Test Loss: 0.000058 | Test Accuracy: 0.740641, 0.637384\n",
      "Step 3 | Training Loss: 0.000170 | Test Loss: 0.000321 | Test Accuracy: 0.698146, 0.558397\n",
      "Step 4 | Training Loss: 0.000058 | Test Loss: 0.000308 | Test Accuracy: 0.772534, 0.634430\n",
      "Step 5 | Training Loss: 0.000020 | Test Loss: 0.000252 | Test Accuracy: 0.753948, 0.579747\n",
      "Step 6 | Training Loss: 0.000078 | Test Loss: 0.000410 | Test Accuracy: 0.736116, 0.565823\n",
      "Step 7 | Training Loss: 0.000092 | Test Loss: 0.000309 | Test Accuracy: 0.773953, 0.620675\n",
      "Step 8 | Training Loss: 0.000025 | Test Loss: 0.000274 | Test Accuracy: 0.762598, 0.604051\n",
      "Step 9 | Training Loss: 0.000012 | Test Loss: 0.000254 | Test Accuracy: 0.821771, 0.702110\n",
      "Step 10 | Training Loss: 0.000015 | Test Loss: 0.000257 | Test Accuracy: 0.790321, 0.645738\n",
      "Step 11 | Training Loss: 0.000010 | Test Loss: 0.000333 | Test Accuracy: 0.761932, 0.587004\n",
      "Step 12 | Training Loss: 0.000088 | Test Loss: 0.000360 | Test Accuracy: 0.772667, 0.633502\n",
      "Step 13 | Training Loss: 0.000013 | Test Loss: 0.000270 | Test Accuracy: 0.746718, 0.580338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14 | Training Loss: 0.000013 | Test Loss: 0.000318 | Test Accuracy: 0.712828, 0.534937\n",
      "Step 15 | Training Loss: 0.000013 | Test Loss: 0.000329 | Test Accuracy: 0.710921, 0.507089\n",
      "Step 16 | Training Loss: 0.000068 | Test Loss: 0.000339 | Test Accuracy: 0.701739, 0.488186\n",
      "Step 17 | Training Loss: 0.000035 | Test Loss: 0.000295 | Test Accuracy: 0.716865, 0.522363\n",
      "Step 18 | Training Loss: 0.000046 | Test Loss: 0.000281 | Test Accuracy: 0.689585, 0.485485\n",
      "Step 19 | Training Loss: 0.000005 | Test Loss: 0.000312 | Test Accuracy: 0.702049, 0.495865\n",
      "Step 20 | Training Loss: 0.000003 | Test Loss: 0.000312 | Test Accuracy: 0.702537, 0.497975\n",
      "Step 21 | Training Loss: 0.000011 | Test Loss: 0.000267 | Test Accuracy: 0.722942, 0.526245\n",
      "Step 22 | Training Loss: 0.000023 | Test Loss: 0.000329 | Test Accuracy: 0.714824, 0.502110\n",
      "Step 23 | Training Loss: 0.000024 | Test Loss: 0.000292 | Test Accuracy: 0.729817, 0.547004\n",
      "Step 24 | Training Loss: 0.000016 | Test Loss: 0.000259 | Test Accuracy: 0.656627, 0.461688\n",
      "Step 25 | Training Loss: 0.000004 | Test Loss: 0.000241 | Test Accuracy: 0.707372, 0.512321\n",
      "Step 26 | Training Loss: 0.000031 | Test Loss: 0.000287 | Test Accuracy: 0.719792, 0.530717\n",
      "Step 27 | Training Loss: 0.000047 | Test Loss: 0.000302 | Test Accuracy: 0.668515, 0.484473\n",
      "Step 28 | Training Loss: 0.000060 | Test Loss: 0.000164 | Test Accuracy: 0.698501, 0.523797\n",
      "Step 29 | Training Loss: 0.000024 | Test Loss: 0.000251 | Test Accuracy: 0.704489, 0.540506\n",
      "Step 30 | Training Loss: 0.000001 | Test Loss: 0.000192 | Test Accuracy: 0.727023, 0.582869\n",
      "Step 1 | Training Loss: 0.000075 | Test Loss: 0.000219 | Test Accuracy: 0.719571, 0.574177\n",
      "Step 2 | Training Loss: 0.000004 | Test Loss: 0.000222 | Test Accuracy: 0.719216, 0.570886\n",
      "Step 3 | Training Loss: 0.000021 | Test Loss: 0.000213 | Test Accuracy: 0.719038, 0.573840\n",
      "Step 4 | Training Loss: 0.000015 | Test Loss: 0.000204 | Test Accuracy: 0.719615, 0.575696\n",
      "Step 5 | Training Loss: 0.000074 | Test Loss: 0.000208 | Test Accuracy: 0.720946, 0.571392\n",
      "Step 6 | Training Loss: 0.000047 | Test Loss: 0.000202 | Test Accuracy: 0.720280, 0.572236\n",
      "Step 7 | Training Loss: 0.000041 | Test Loss: 0.000209 | Test Accuracy: 0.718550, 0.569283\n",
      "Step 8 | Training Loss: 0.000003 | Test Loss: 0.000190 | Test Accuracy: 0.718683, 0.569451\n",
      "Step 9 | Training Loss: 0.000022 | Test Loss: 0.000197 | Test Accuracy: 0.716909, 0.564641\n",
      "Step 10 | Training Loss: 0.000003 | Test Loss: 0.000189 | Test Accuracy: 0.712917, 0.557637\n",
      "Step 11 | Training Loss: 0.000011 | Test Loss: 0.000197 | Test Accuracy: 0.713405, 0.565401\n",
      "Step 12 | Training Loss: 0.000021 | Test Loss: 0.000201 | Test Accuracy: 0.711231, 0.555949\n",
      "Step 13 | Training Loss: 0.000010 | Test Loss: 0.000193 | Test Accuracy: 0.709413, 0.558987\n",
      "Step 14 | Training Loss: 0.000012 | Test Loss: 0.000174 | Test Accuracy: 0.710256, 0.559578\n",
      "Step 15 | Training Loss: 0.000039 | Test Loss: 0.000179 | Test Accuracy: 0.706618, 0.558481\n",
      "Step 16 | Training Loss: 0.000017 | Test Loss: 0.000196 | Test Accuracy: 0.706308, 0.551477\n",
      "Step 17 | Training Loss: 0.000010 | Test Loss: 0.000185 | Test Accuracy: 0.707727, 0.553418\n",
      "Step 18 | Training Loss: 0.000021 | Test Loss: 0.000200 | Test Accuracy: 0.707328, 0.552743\n",
      "Step 19 | Training Loss: 0.000024 | Test Loss: 0.000203 | Test Accuracy: 0.702094, 0.551055\n",
      "Step 20 | Training Loss: 0.000057 | Test Loss: 0.000187 | Test Accuracy: 0.700807, 0.549620\n",
      "Step 21 | Training Loss: 0.000009 | Test Loss: 0.000198 | Test Accuracy: 0.695529, 0.543966\n",
      "Step 22 | Training Loss: 0.000027 | Test Loss: 0.000189 | Test Accuracy: 0.701340, 0.550042\n",
      "Step 23 | Training Loss: 0.000050 | Test Loss: 0.000169 | Test Accuracy: 0.699787, 0.543122\n",
      "Step 24 | Training Loss: 0.000002 | Test Loss: 0.000180 | Test Accuracy: 0.690871, 0.535612\n",
      "Step 25 | Training Loss: 0.000020 | Test Loss: 0.000181 | Test Accuracy: 0.684883, 0.530295\n",
      "Step 26 | Training Loss: 0.000031 | Test Loss: 0.000176 | Test Accuracy: 0.686081, 0.530464\n",
      "Step 27 | Training Loss: 0.000009 | Test Loss: 0.000193 | Test Accuracy: 0.683242, 0.522869\n",
      "Step 28 | Training Loss: 0.000004 | Test Loss: 0.000184 | Test Accuracy: 0.682044, 0.514177\n",
      "Step 29 | Training Loss: 0.000014 | Test Loss: 0.000175 | Test Accuracy: 0.684617, 0.521772\n",
      "Step 30 | Training Loss: 0.000004 | Test Loss: 0.000179 | Test Accuracy: 0.676765, 0.515612\n",
      "Best Accuracy on Test data: 0.8217707872390747\n",
      "Current Layer Attributes - epochs:30 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.000301 | Test Loss: 0.000348 | Test Accuracy: 0.820263, 0.749620\n",
      "Step 2 | High Training Loss: 287511445504.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_4/model-1\n",
      "Step 2 | Training Loss: 0.000084 | Test Loss: 0.000273 | Test Accuracy: 0.795023, 0.677131\n",
      "Step 3 | Training Loss: 0.000125 | Test Loss: 0.000226 | Test Accuracy: 0.820662, 0.691392\n",
      "Step 4 | Training Loss: 0.000030 | Test Loss: 0.020158 | Test Accuracy: 0.818577, 0.708692\n",
      "Step 5 | Training Loss: 0.000067 | Test Loss: 0.000353 | Test Accuracy: 0.831485, 0.686329\n",
      "Step 6 | Training Loss: 0.000065 | Test Loss: 0.000339 | Test Accuracy: 0.813343, 0.671646\n",
      "Step 7 | Training Loss: 0.000014 | Test Loss: 0.000245 | Test Accuracy: 0.844970, 0.725570\n",
      "Step 8 | Training Loss: 0.000058 | Test Loss: 0.000240 | Test Accuracy: 0.878460, 0.787173\n",
      "Step 9 | Training Loss: 0.000022 | Test Loss: 0.000226 | Test Accuracy: 0.854817, 0.738987\n",
      "Step 10 | Training Loss: 0.000014 | Test Loss: 0.000219 | Test Accuracy: 0.861515, 0.765485\n",
      "Step 11 | Training Loss: 0.000009 | Test Loss: 0.000132 | Test Accuracy: 0.865374, 0.760759\n",
      "Step 12 | Training Loss: 0.000050 | Test Loss: 0.000201 | Test Accuracy: 0.867104, 0.762194\n",
      "Step 13 | Training Loss: 0.000082 | Test Loss: 0.000098 | Test Accuracy: 0.871762, 0.772658\n",
      "Step 14 | Training Loss: 0.000062 | Test Loss: 976506.125000 | Test Accuracy: 0.844615, 0.739662\n",
      "Step 15 | Training Loss: 0.000048 | Test Loss: 0.000176 | Test Accuracy: 0.861914, 0.746751\n",
      "Step 16 | Training Loss: 0.000022 | Test Loss: 0.000168 | Test Accuracy: 0.851002, 0.732152\n",
      "Step 17 | Training Loss: 0.000019 | Test Loss: 0.000113 | Test Accuracy: 0.861560, 0.749198\n",
      "Step 18 | Training Loss: 0.000043 | Test Loss: 0.000163 | Test Accuracy: 0.862314, 0.745148\n",
      "Step 19 | Training Loss: 0.000026 | Test Loss: 0.000202 | Test Accuracy: 0.861604, 0.751055\n",
      "Step 20 | Training Loss: 0.000003 | Test Loss: 0.000186 | Test Accuracy: 0.861027, 0.749620\n",
      "Step 21 | Training Loss: 0.000009 | Test Loss: 0.000170 | Test Accuracy: 0.858854, 0.744388\n",
      "Step 22 | Training Loss: 0.000040 | Test Loss: 0.000175 | Test Accuracy: 0.877661, 0.782025\n",
      "Step 23 | Training Loss: 0.000002 | Test Loss: 0.000158 | Test Accuracy: 0.877085, 0.782110\n",
      "Step 24 | Training Loss: 0.000022 | Test Loss: 0.000151 | Test Accuracy: 0.878416, 0.782954\n",
      "Step 25 | Training Loss: 0.000007 | Test Loss: 0.000127 | Test Accuracy: 0.869322, 0.775527\n",
      "Step 26 | Training Loss: 0.000031 | Test Loss: 0.000209 | Test Accuracy: 0.782204, 0.591477\n",
      "Step 27 | Training Loss: 0.000023 | Test Loss: 0.000290 | Test Accuracy: 0.773909, 0.577046\n",
      "Step 28 | Training Loss: 0.000039 | Test Loss: 0.000178 | Test Accuracy: 0.813032, 0.651139\n",
      "Step 29 | Training Loss: 0.000074 | Test Loss: 0.000221 | Test Accuracy: 0.815472, 0.657637\n",
      "Step 30 | Training Loss: 0.000005 | Test Loss: 0.000219 | Test Accuracy: 0.805713, 0.637300\n",
      "Step 1 | Training Loss: 0.000013 | Test Loss: 0.000201 | Test Accuracy: 0.805314, 0.635781\n",
      "Step 2 | Training Loss: 0.000014 | Test Loss: 0.000192 | Test Accuracy: 0.806024, 0.637890\n",
      "Step 3 | Training Loss: 0.000014 | Test Loss: 0.000200 | Test Accuracy: 0.805181, 0.636793\n",
      "Step 4 | Training Loss: 0.000046 | Test Loss: 0.000191 | Test Accuracy: 0.805492, 0.637384\n",
      "Step 5 | Training Loss: 0.000002 | Test Loss: 0.000182 | Test Accuracy: 0.805802, 0.638059\n",
      "Step 6 | Training Loss: 0.000002 | Test Loss: 0.000178 | Test Accuracy: 0.805137, 0.637553\n",
      "Step 7 | Training Loss: 0.000010 | Test Loss: 0.000178 | Test Accuracy: 0.806290, 0.637975\n",
      "Step 8 | Training Loss: 0.000011 | Test Loss: 0.000171 | Test Accuracy: 0.805758, 0.637553\n",
      "Step 9 | Training Loss: 0.000019 | Test Loss: 0.000166 | Test Accuracy: 0.804693, 0.636203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.000040 | Test Loss: 0.000161 | Test Accuracy: 0.805625, 0.637722\n",
      "Step 11 | Training Loss: 0.000016 | Test Loss: 0.000158 | Test Accuracy: 0.806423, 0.638397\n",
      "Step 12 | Training Loss: 0.000006 | Test Loss: 0.000161 | Test Accuracy: 0.806556, 0.638481\n",
      "Step 13 | Training Loss: 0.000018 | Test Loss: 0.000159 | Test Accuracy: 0.806512, 0.638397\n",
      "Step 14 | Training Loss: 0.000005 | Test Loss: 0.000155 | Test Accuracy: 0.806689, 0.638819\n",
      "Step 15 | Training Loss: 0.000002 | Test Loss: 0.000157 | Test Accuracy: 0.806467, 0.638734\n",
      "Step 16 | Training Loss: 0.000013 | Test Loss: 0.000147 | Test Accuracy: 0.806423, 0.638481\n",
      "Step 17 | Training Loss: 0.000009 | Test Loss: 0.000156 | Test Accuracy: 0.806645, 0.638481\n",
      "Step 18 | Training Loss: 0.000007 | Test Loss: 0.000159 | Test Accuracy: 0.806556, 0.639241\n",
      "Step 19 | Training Loss: 0.000009 | Test Loss: 0.000159 | Test Accuracy: 0.807044, 0.638565\n",
      "Step 20 | Training Loss: 0.000006 | Test Loss: 0.000155 | Test Accuracy: 0.806689, 0.638987\n",
      "Step 21 | Training Loss: 0.000015 | Test Loss: 0.000145 | Test Accuracy: 0.806467, 0.638481\n",
      "Step 22 | Training Loss: 0.000024 | Test Loss: 0.000146 | Test Accuracy: 0.806512, 0.638143\n",
      "Step 23 | Training Loss: 0.000007 | Test Loss: 0.000152 | Test Accuracy: 0.806556, 0.638565\n",
      "Step 24 | Training Loss: 0.000015 | Test Loss: 0.000156 | Test Accuracy: 0.806467, 0.638565\n",
      "Step 25 | Training Loss: 0.000008 | Test Loss: 0.000152 | Test Accuracy: 0.806068, 0.638143\n",
      "Step 26 | Training Loss: 0.000006 | Test Loss: 0.000161 | Test Accuracy: 0.806911, 0.638734\n",
      "Step 27 | Training Loss: 0.000036 | Test Loss: 0.000154 | Test Accuracy: 0.807000, 0.639072\n",
      "Step 28 | Training Loss: 0.000012 | Test Loss: 0.000146 | Test Accuracy: 0.806822, 0.638565\n",
      "Step 29 | Training Loss: 0.000010 | Test Loss: 0.000139 | Test Accuracy: 0.806955, 0.638312\n",
      "Step 30 | Training Loss: 0.000010 | Test Loss: 0.000143 | Test Accuracy: 0.806733, 0.638565\n",
      "Best Accuracy on Test data: 0.8784599304199219\n",
      "Current Layer Attributes - epochs:30 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.000088 | Test Loss: 0.000345 | Test Accuracy: 0.782425, 0.630464\n",
      "Step 2 | Training Loss: 0.000058 | Test Loss: 0.000180 | Test Accuracy: 0.825142, 0.726245\n",
      "Step 3 | Training Loss: 0.000285 | Test Loss: 1105964160.000000 | Test Accuracy: 0.662438, 0.462278\n",
      "Step 4 | Training Loss: 0.000090 | Test Loss: 0.000256 | Test Accuracy: 0.792761, 0.701603\n",
      "Step 5 | Training Loss: 0.000047 | Test Loss: 0.000222 | Test Accuracy: 0.855882, 0.765232\n",
      "Step 6 | Training Loss: 0.000113 | Test Loss: 0.000206 | Test Accuracy: 0.840312, 0.718819\n",
      "Step 7 | Training Loss: 0.000091 | Test Loss: 0.000316 | Test Accuracy: 0.825364, 0.696118\n",
      "Step 8 | Training Loss: 0.000003 | Test Loss: 0.000222 | Test Accuracy: 0.835566, 0.712743\n",
      "Step 9 | Training Loss: 0.000055 | Test Loss: 0.000251 | Test Accuracy: 0.846301, 0.730295\n",
      "Step 10 | Training Loss: 0.000040 | Test Loss: 0.000253 | Test Accuracy: 0.865862, 0.763882\n",
      "Step 11 | Training Loss: 0.000007 | Test Loss: 0.000139 | Test Accuracy: 0.861338, 0.800000\n",
      "Step 12 | Training Loss: 0.000047 | Test Loss: 0.000182 | Test Accuracy: 0.872782, 0.809198\n",
      "Step 13 | Training Loss: 0.000014 | Test Loss: 0.000215 | Test Accuracy: 0.862713, 0.775527\n",
      "Step 14 | Training Loss: 0.000011 | Test Loss: 0.000146 | Test Accuracy: 0.873492, 0.777300\n",
      "Step 15 | Training Loss: 0.000027 | Test Loss: 0.000154 | Test Accuracy: 0.881033, 0.823376\n",
      "Step 16 | Training Loss: 0.000045 | Test Loss: 0.000174 | Test Accuracy: 0.887198, 0.821097\n",
      "Step 17 | Training Loss: 0.000058 | Test Loss: 0.000199 | Test Accuracy: 0.875577, 0.781350\n",
      "Step 18 | Training Loss: 0.000030 | Test Loss: 0.000167 | Test Accuracy: 0.884404, 0.823207\n",
      "Step 19 | Training Loss: 0.000022 | Test Loss: 0.000186 | Test Accuracy: 0.875266, 0.831899\n",
      "Step 20 | Training Loss: 0.000019 | Test Loss: 0.000109 | Test Accuracy: 0.893808, 0.834768\n",
      "Step 21 | Training Loss: 0.000015 | Test Loss: 0.000130 | Test Accuracy: 0.880323, 0.794684\n",
      "Step 22 | Training Loss: 0.000016 | Test Loss: 0.000088 | Test Accuracy: 0.882896, 0.820506\n",
      "Step 23 | Training Loss: 0.000054 | Test Loss: 0.000174 | Test Accuracy: 0.877972, 0.803207\n",
      "Step 24 | Training Loss: 0.000024 | Test Loss: 0.000145 | Test Accuracy: 0.889061, 0.808354\n",
      "Step 25 | Training Loss: 0.000029 | Test Loss: 0.000107 | Test Accuracy: 0.893630, 0.841266\n",
      "Step 26 | Training Loss: 0.000031 | Test Loss: 0.000122 | Test Accuracy: 0.901082, 0.831983\n",
      "Step 27 | Training Loss: 0.000063 | Test Loss: 0.000111 | Test Accuracy: 0.900151, 0.837215\n",
      "Step 28 | Training Loss: 0.000023 | Test Loss: 0.000144 | Test Accuracy: 0.899973, 0.838228\n",
      "Step 29 | Training Loss: 0.000016 | Test Loss: 0.000104 | Test Accuracy: 0.888174, 0.841941\n",
      "Step 30 | Training Loss: 0.000009 | Test Loss: 0.000144 | Test Accuracy: 0.892166, 0.811308\n",
      "Step 1 | Training Loss: 0.000015 | Test Loss: 0.000126 | Test Accuracy: 0.891324, 0.809198\n",
      "Step 2 | Training Loss: 0.000005 | Test Loss: 0.000117 | Test Accuracy: 0.891235, 0.810464\n",
      "Step 3 | Training Loss: 0.000025 | Test Loss: 0.000116 | Test Accuracy: 0.890348, 0.807595\n",
      "Step 4 | Training Loss: 0.000024 | Test Loss: 0.000111 | Test Accuracy: 0.888573, 0.805485\n",
      "Step 5 | Training Loss: 0.000023 | Test Loss: 0.000115 | Test Accuracy: 0.890570, 0.807426\n",
      "Step 6 | Training Loss: 0.000000 | Test Loss: 0.000104 | Test Accuracy: 0.890791, 0.809030\n",
      "Step 7 | Training Loss: 0.000020 | Test Loss: 0.000109 | Test Accuracy: 0.890570, 0.807679\n",
      "Step 8 | Training Loss: 0.000002 | Test Loss: 0.000108 | Test Accuracy: 0.890436, 0.808017\n",
      "Step 9 | Training Loss: 0.000003 | Test Loss: 0.000106 | Test Accuracy: 0.888086, 0.802954\n",
      "Step 10 | Training Loss: 0.000030 | Test Loss: 0.000097 | Test Accuracy: 0.889682, 0.806414\n",
      "Step 11 | Training Loss: 0.000002 | Test Loss: 0.000108 | Test Accuracy: 0.890082, 0.805823\n",
      "Step 12 | Training Loss: 0.000040 | Test Loss: 0.000099 | Test Accuracy: 0.889549, 0.804895\n",
      "Step 13 | Training Loss: 0.000008 | Test Loss: 0.000090 | Test Accuracy: 0.889594, 0.806751\n",
      "Step 14 | Training Loss: 0.000004 | Test Loss: 0.000102 | Test Accuracy: 0.890348, 0.806160\n",
      "Step 15 | Training Loss: 0.000024 | Test Loss: 0.000101 | Test Accuracy: 0.890215, 0.805992\n",
      "Step 16 | Training Loss: 0.000026 | Test Loss: 0.000089 | Test Accuracy: 0.888529, 0.803460\n",
      "Step 17 | Training Loss: 0.000030 | Test Loss: 0.000076 | Test Accuracy: 0.891324, 0.808270\n",
      "Step 18 | Training Loss: 0.000020 | Test Loss: 0.000099 | Test Accuracy: 0.892033, 0.809705\n",
      "Step 19 | Training Loss: 0.000061 | Test Loss: 0.000084 | Test Accuracy: 0.891013, 0.807764\n",
      "Step 20 | Training Loss: 0.000004 | Test Loss: 0.000089 | Test Accuracy: 0.890436, 0.808186\n",
      "Step 21 | Training Loss: 0.000028 | Test Loss: 0.000097 | Test Accuracy: 0.890037, 0.808017\n",
      "Step 22 | Training Loss: 0.000038 | Test Loss: 0.000101 | Test Accuracy: 0.887642, 0.803629\n",
      "Step 23 | Training Loss: 0.000020 | Test Loss: 0.000103 | Test Accuracy: 0.887243, 0.803122\n",
      "Step 24 | Training Loss: 0.000033 | Test Loss: 0.000107 | Test Accuracy: 0.885779, 0.799325\n",
      "Step 25 | Training Loss: 0.000008 | Test Loss: 0.000106 | Test Accuracy: 0.884803, 0.798565\n",
      "Step 26 | Training Loss: 0.000001 | Test Loss: 0.000109 | Test Accuracy: 0.885912, 0.800169\n",
      "Step 27 | Training Loss: 0.000004 | Test Loss: 0.000101 | Test Accuracy: 0.885956, 0.801350\n",
      "Step 28 | Training Loss: 0.000016 | Test Loss: 0.000107 | Test Accuracy: 0.885069, 0.798143\n",
      "Step 29 | Training Loss: 0.000018 | Test Loss: 0.000114 | Test Accuracy: 0.883783, 0.797131\n",
      "Step 30 | Training Loss: 0.000007 | Test Loss: 0.000111 | Test Accuracy: 0.882408, 0.796624\n",
      "Best Accuracy on Test data: 0.9010823369026184\n",
      "Current Layer Attributes - epochs:30 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.000217 | Test Loss: 0.000396 | Test Accuracy: 0.751286, 0.663122\n",
      "Step 2 | Training Loss: 0.000015 | Test Loss: 0.000155 | Test Accuracy: 0.802741, 0.710549\n",
      "Step 3 | Training Loss: 0.000010 | Test Loss: 0.000192 | Test Accuracy: 0.755944, 0.599831\n",
      "Step 4 | High Training Loss: 4141680718512128.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_6/model-2\n",
      "Step 4 | Training Loss: 0.000018 | Test Loss: 0.000377 | Test Accuracy: 0.657425, 0.546329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.000054 | Test Loss: 0.000279 | Test Accuracy: 0.681867, 0.591646\n",
      "Step 6 | Training Loss: 0.000064 | Test Loss: 0.000376 | Test Accuracy: 0.649086, 0.503966\n",
      "Step 7 | Training Loss: 0.000222 | Test Loss: 0.000494 | Test Accuracy: 0.725559, 0.529873\n",
      "Step 8 | Training Loss: 0.000148 | Test Loss: 0.000111 | Test Accuracy: 0.766102, 0.641772\n",
      "Step 9 | Training Loss: 0.000057 | Test Loss: 0.000127 | Test Accuracy: 0.591244, 0.392405\n",
      "Step 10 | Training Loss: 0.000208 | Test Loss: 0.000220 | Test Accuracy: 0.738911, 0.611730\n",
      "Step 11 | Training Loss: 0.000001 | Test Loss: 0.000245 | Test Accuracy: 0.752307, 0.621350\n",
      "Step 12 | Training Loss: 0.000037 | Test Loss: 0.000190 | Test Accuracy: 0.762775, 0.646920\n",
      "Step 13 | Training Loss: 0.000048 | Test Loss: 0.000212 | Test Accuracy: 0.780562, 0.666413\n",
      "Step 14 | Training Loss: 0.000018 | Test Loss: 0.000288 | Test Accuracy: 0.787660, 0.675359\n",
      "Step 15 | Training Loss: 0.000063 | Test Loss: 0.000158 | Test Accuracy: 0.791607, 0.676118\n",
      "Step 16 | Training Loss: 0.000008 | Test Loss: 0.000222 | Test Accuracy: 0.796709, 0.690886\n",
      "Step 17 | Training Loss: 0.000082 | Test Loss: 0.000195 | Test Accuracy: 0.792938, 0.682025\n",
      "Step 18 | Training Loss: 0.000003 | Test Loss: 0.000197 | Test Accuracy: 0.789744, 0.674093\n",
      "Step 19 | Training Loss: 0.000034 | Test Loss: 0.000254 | Test Accuracy: 0.817335, 0.708608\n",
      "Step 20 | Training Loss: 0.000073 | Test Loss: 0.000297 | Test Accuracy: 0.818843, 0.707764\n",
      "Step 21 | Training Loss: 0.000041 | Test Loss: 0.000234 | Test Accuracy: 0.809750, 0.693418\n",
      "Step 22 | Training Loss: 0.000005 | Test Loss: 0.000274 | Test Accuracy: 0.831574, 0.724473\n",
      "Step 23 | Training Loss: 0.000008 | Test Loss: 0.000196 | Test Accuracy: 0.838272, 0.733165\n",
      "Step 24 | Training Loss: 0.000086 | Test Loss: 0.000193 | Test Accuracy: 0.834058, 0.734177\n",
      "Step 25 | Training Loss: 0.000013 | Test Loss: 0.000233 | Test Accuracy: 0.831663, 0.722110\n",
      "Step 26 | Training Loss: 0.000029 | Test Loss: 0.000269 | Test Accuracy: 0.805979, 0.672321\n",
      "Step 27 | Training Loss: 0.000108 | Test Loss: 0.000137 | Test Accuracy: 0.834280, 0.730549\n",
      "Step 28 | Training Loss: 0.000001 | Test Loss: 0.000231 | Test Accuracy: 0.828735, 0.720000\n",
      "Step 29 | Training Loss: 0.000014 | Test Loss: 0.000177 | Test Accuracy: 0.825985, 0.722616\n",
      "Step 30 | Training Loss: 0.000056 | Test Loss: 0.000142 | Test Accuracy: 0.836010, 0.739662\n",
      "Step 1 | Training Loss: 0.000058 | Test Loss: 0.000144 | Test Accuracy: 0.834723, 0.743376\n",
      "Step 2 | Training Loss: 0.000028 | Test Loss: 0.000165 | Test Accuracy: 0.833925, 0.734430\n",
      "Step 3 | Training Loss: 0.000082 | Test Loss: 0.000153 | Test Accuracy: 0.830687, 0.737131\n",
      "Step 4 | Training Loss: 0.000027 | Test Loss: 0.000142 | Test Accuracy: 0.832062, 0.740844\n",
      "Step 5 | Training Loss: 0.000021 | Test Loss: 0.000163 | Test Accuracy: 0.830509, 0.739916\n",
      "Step 6 | Training Loss: 0.000036 | Test Loss: 0.000143 | Test Accuracy: 0.831130, 0.737806\n",
      "Step 7 | Training Loss: 0.000031 | Test Loss: 0.000153 | Test Accuracy: 0.831840, 0.738397\n",
      "Step 8 | Training Loss: 0.000012 | Test Loss: 0.000116 | Test Accuracy: 0.832195, 0.738903\n",
      "Step 9 | Training Loss: 0.000035 | Test Loss: 0.000130 | Test Accuracy: 0.833348, 0.737131\n",
      "Step 10 | Training Loss: 0.000003 | Test Loss: 0.000125 | Test Accuracy: 0.831485, 0.733165\n",
      "Step 11 | Training Loss: 0.000009 | Test Loss: 0.000125 | Test Accuracy: 0.826828, 0.732827\n",
      "Step 12 | Training Loss: 0.000054 | Test Loss: 0.000143 | Test Accuracy: 0.826340, 0.725401\n",
      "Step 13 | Training Loss: 0.000005 | Test Loss: 0.000145 | Test Accuracy: 0.826251, 0.729705\n",
      "Step 14 | Training Loss: 0.000005 | Test Loss: 0.000156 | Test Accuracy: 0.827759, 0.729114\n",
      "Step 15 | Training Loss: 0.000014 | Test Loss: 0.000150 | Test Accuracy: 0.829223, 0.731899\n",
      "Step 16 | Training Loss: 0.000026 | Test Loss: 0.000142 | Test Accuracy: 0.825408, 0.730127\n",
      "Step 17 | Training Loss: 0.000038 | Test Loss: 0.000165 | Test Accuracy: 0.826783, 0.727933\n",
      "Step 18 | Training Loss: 0.000020 | Test Loss: 0.000169 | Test Accuracy: 0.826073, 0.722532\n",
      "Step 19 | Training Loss: 0.000032 | Test Loss: 0.000153 | Test Accuracy: 0.829400, 0.729873\n",
      "Step 20 | Training Loss: 0.000001 | Test Loss: 0.000158 | Test Accuracy: 0.827626, 0.730380\n",
      "Step 21 | Training Loss: 0.000010 | Test Loss: 0.000170 | Test Accuracy: 0.821726, 0.724641\n",
      "Step 22 | Training Loss: 0.000022 | Test Loss: 0.000176 | Test Accuracy: 0.818444, 0.719578\n",
      "Step 23 | Training Loss: 0.000025 | Test Loss: 0.000174 | Test Accuracy: 0.818888, 0.716540\n",
      "Step 24 | Training Loss: 0.000025 | Test Loss: 0.000164 | Test Accuracy: 0.817645, 0.717722\n",
      "Step 25 | Training Loss: 0.000009 | Test Loss: 0.000168 | Test Accuracy: 0.818888, 0.721350\n",
      "Step 26 | Training Loss: 0.000046 | Test Loss: 0.000182 | Test Accuracy: 0.816936, 0.718059\n",
      "Step 27 | Training Loss: 0.000028 | Test Loss: 0.000162 | Test Accuracy: 0.818222, 0.719072\n",
      "Step 28 | Training Loss: 0.000017 | Test Loss: 0.000177 | Test Accuracy: 0.815649, 0.716878\n",
      "Step 29 | Training Loss: 0.000022 | Test Loss: 0.000180 | Test Accuracy: 0.814452, 0.715190\n",
      "Step 30 | Training Loss: 0.000011 | Test Loss: 0.000180 | Test Accuracy: 0.816537, 0.720084\n",
      "Best Accuracy on Test data: 0.8382717967033386\n",
      "Current Layer Attributes - epochs:30 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.000212 | Test Loss: 0.000101 | Test Accuracy: 0.763307, 0.682278\n",
      "Step 2 | High Training Loss: 1105368629460139507712.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_6/model-1\n",
      "Step 2 | Training Loss: 0.000360 | Test Loss: 0.000284 | Test Accuracy: 0.798927, 0.679747\n",
      "Step 3 | High Training Loss: 150824.812500 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_6/model-2\n",
      "Step 3 | Training Loss: 0.000035 | Test Loss: 0.000264 | Test Accuracy: 0.828291, 0.711055\n",
      "Step 4 | Training Loss: 0.000039 | Test Loss: 0.000088 | Test Accuracy: 0.754037, 0.576709\n",
      "Step 5 | Training Loss: 0.000004 | Test Loss: 0.000328 | Test Accuracy: 0.787172, 0.660169\n",
      "Step 6 | Training Loss: 0.000085 | Test Loss: 0.000193 | Test Accuracy: 0.802919, 0.682025\n",
      "Step 7 | Training Loss: 0.000183 | Test Loss: 0.000135 | Test Accuracy: 0.751508, 0.676118\n",
      "Step 8 | Training Loss: 0.000000 | Test Loss: 0.000313 | Test Accuracy: 0.803850, 0.681181\n",
      "Step 9 | Training Loss: 0.000154 | Test Loss: 0.000320 | Test Accuracy: 0.747738, 0.604051\n",
      "Step 10 | Training Loss: 0.000003 | Test Loss: 0.000192 | Test Accuracy: 0.772534, 0.625823\n",
      "Step 11 | Training Loss: 0.000061 | Test Loss: 0.000205 | Test Accuracy: 0.803274, 0.685232\n",
      "Step 12 | Training Loss: 0.000006 | Test Loss: 0.000184 | Test Accuracy: 0.804959, 0.649705\n",
      "Step 13 | Training Loss: 0.000035 | Test Loss: 0.000213 | Test Accuracy: 0.839913, 0.724979\n",
      "Step 14 | Training Loss: 0.000050 | Test Loss: 0.000227 | Test Accuracy: 0.830332, 0.694430\n",
      "Step 15 | Training Loss: 0.000056 | Test Loss: 0.000220 | Test Accuracy: 0.790011, 0.633333\n",
      "Step 16 | Training Loss: 0.000054 | Test Loss: 0.000275 | Test Accuracy: 0.815161, 0.665738\n",
      "Step 17 | Training Loss: 0.000038 | Test Loss: 0.000342 | Test Accuracy: 0.805492, 0.664135\n",
      "Step 18 | Training Loss: 0.000006 | Test Loss: 0.000282 | Test Accuracy: 0.822614, 0.676456\n",
      "Step 19 | Training Loss: 0.000049 | Test Loss: 0.000271 | Test Accuracy: 0.815250, 0.664979\n",
      "Step 20 | Training Loss: 0.000015 | Test Loss: 0.000212 | Test Accuracy: 0.838848, 0.765485\n",
      "Step 21 | Training Loss: 0.000091 | Test Loss: 0.000201 | Test Accuracy: 0.844837, 0.752405\n",
      "Step 22 | Training Loss: 0.000054 | Test Loss: 0.000142 | Test Accuracy: 0.843329, 0.761688\n",
      "Step 23 | Training Loss: 0.000009 | Test Loss: 0.000253 | Test Accuracy: 0.800080, 0.699409\n",
      "Step 24 | Training Loss: 0.000024 | Test Loss: 0.000259 | Test Accuracy: 0.864532, 0.778312\n",
      "Step 25 | Training Loss: 0.000052 | Test Loss: 0.000143 | Test Accuracy: 0.589869, 0.661603\n",
      "Step 26 | Training Loss: 0.000207 | Test Loss: 0.000793 | Test Accuracy: 0.527857, 0.494937\n",
      "Step 27 | Training Loss: 0.000253 | Test Loss: 0.000507 | Test Accuracy: 0.496451, 0.330295\n",
      "Step 28 | Training Loss: 0.000046 | Test Loss: 0.000561 | Test Accuracy: 0.522445, 0.348608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29 | Training Loss: 0.000153 | Test Loss: 0.000381 | Test Accuracy: 0.517433, 0.319072\n",
      "Step 30 | Training Loss: 0.000049 | Test Loss: 0.000356 | Test Accuracy: 0.519429, 0.309705\n",
      "Step 1 | Training Loss: 0.000086 | Test Loss: 0.000279 | Test Accuracy: 0.519163, 0.309705\n",
      "Step 2 | Training Loss: 0.000005 | Test Loss: 0.000241 | Test Accuracy: 0.517654, 0.307679\n",
      "Step 3 | Training Loss: 0.000042 | Test Loss: 0.000234 | Test Accuracy: 0.519251, 0.308608\n",
      "Step 4 | Training Loss: 0.000047 | Test Loss: 0.000228 | Test Accuracy: 0.522312, 0.307511\n",
      "Step 5 | Training Loss: 0.000022 | Test Loss: 0.000227 | Test Accuracy: 0.520227, 0.304051\n",
      "Step 6 | Training Loss: 0.000046 | Test Loss: 0.000224 | Test Accuracy: 0.525417, 0.299578\n",
      "Step 7 | Training Loss: 0.000018 | Test Loss: 0.000241 | Test Accuracy: 0.519917, 0.300591\n",
      "Step 8 | Training Loss: 0.000017 | Test Loss: 0.000225 | Test Accuracy: 0.518852, 0.292996\n",
      "Step 9 | Training Loss: 0.000026 | Test Loss: 0.000228 | Test Accuracy: 0.520981, 0.292827\n",
      "Step 10 | Training Loss: 0.000004 | Test Loss: 0.000231 | Test Accuracy: 0.518275, 0.295527\n",
      "Step 11 | Training Loss: 0.000013 | Test Loss: 0.000218 | Test Accuracy: 0.521868, 0.297890\n",
      "Step 12 | Training Loss: 0.000016 | Test Loss: 0.000237 | Test Accuracy: 0.519961, 0.294852\n",
      "Step 13 | Training Loss: 0.000017 | Test Loss: 0.000227 | Test Accuracy: 0.521292, 0.290042\n",
      "Step 14 | Training Loss: 0.000011 | Test Loss: 0.000228 | Test Accuracy: 0.519828, 0.286329\n",
      "Step 15 | Training Loss: 0.000006 | Test Loss: 0.000202 | Test Accuracy: 0.519163, 0.289705\n",
      "Step 16 | Training Loss: 0.000005 | Test Loss: 0.000217 | Test Accuracy: 0.520138, 0.285992\n",
      "Step 17 | Training Loss: 0.000008 | Test Loss: 0.000220 | Test Accuracy: 0.521247, 0.286920\n",
      "Step 18 | Training Loss: 0.000035 | Test Loss: 0.000225 | Test Accuracy: 0.521203, 0.283797\n",
      "Step 19 | Training Loss: 0.000021 | Test Loss: 0.000216 | Test Accuracy: 0.520138, 0.290127\n",
      "Step 20 | Training Loss: 0.000000 | Test Loss: 0.000218 | Test Accuracy: 0.518320, 0.279578\n",
      "Step 21 | Training Loss: 0.000030 | Test Loss: 0.000249 | Test Accuracy: 0.512376, 0.269283\n",
      "Step 22 | Training Loss: 0.000040 | Test Loss: 0.000229 | Test Accuracy: 0.509005, 0.263797\n",
      "Step 23 | Training Loss: 0.000003 | Test Loss: 0.000232 | Test Accuracy: 0.508472, 0.269536\n",
      "Step 24 | Training Loss: 0.000010 | Test Loss: 0.000232 | Test Accuracy: 0.507363, 0.264895\n",
      "Step 25 | Training Loss: 0.000071 | Test Loss: 0.000226 | Test Accuracy: 0.510513, 0.265570\n",
      "Step 26 | Training Loss: 0.000010 | Test Loss: 0.000225 | Test Accuracy: 0.510823, 0.272152\n",
      "Step 27 | Training Loss: 0.000036 | Test Loss: 0.000234 | Test Accuracy: 0.513086, 0.272321\n",
      "Step 28 | Training Loss: 0.000055 | Test Loss: 0.000217 | Test Accuracy: 0.513307, 0.274599\n",
      "Step 29 | Training Loss: 0.000001 | Test Loss: 0.000212 | Test Accuracy: 0.512110, 0.275696\n",
      "Step 30 | Training Loss: 0.000043 | Test Loss: 0.000202 | Test Accuracy: 0.512420, 0.270295\n",
      "Best Accuracy on Test data: 0.8645315766334534\n",
      "Current Layer Attributes - epochs:30 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.000070 | Test Loss: 0.000157 | Test Accuracy: 0.786506, 0.669789\n",
      "Step 2 | Training Loss: 0.000013 | Test Loss: 0.000198 | Test Accuracy: 0.839070, 0.715190\n",
      "Step 3 | Training Loss: 0.000027 | Test Loss: 0.000243 | Test Accuracy: 0.832905, 0.707173\n",
      "Step 4 | Training Loss: 0.000031 | Test Loss: 0.000231 | Test Accuracy: 0.848918, 0.734008\n",
      "Step 5 | Training Loss: 0.000064 | Test Loss: 0.000214 | Test Accuracy: 0.843062, 0.715696\n",
      "Step 6 | Training Loss: 0.000050 | Test Loss: 0.005123 | Test Accuracy: 0.775861, 0.599325\n",
      "Step 7 | Training Loss: 0.000040 | Test Loss: 0.000351 | Test Accuracy: 0.782780, 0.627426\n",
      "Step 8 | Training Loss: 0.000111 | Test Loss: 0.000167 | Test Accuracy: 0.788059, 0.666160\n",
      "Step 9 | High Training Loss: inf ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_6/model-4\n",
      "Step 9 | Training Loss: 0.000036 | Test Loss: 0.000244 | Test Accuracy: 0.844438, 0.740422\n",
      "Step 10 | Training Loss: 0.000009 | Test Loss: 0.000230 | Test Accuracy: 0.787127, 0.605316\n",
      "Step 11 | Training Loss: 0.000032 | Test Loss: 0.000364 | Test Accuracy: 0.743036, 0.525232\n",
      "Step 12 | Training Loss: 0.000155 | Test Loss: 0.000214 | Test Accuracy: 0.800080, 0.666160\n",
      "Step 13 | Training Loss: 0.000035 | Test Loss: 0.000238 | Test Accuracy: 0.799459, 0.631983\n",
      "Step 14 | Training Loss: 0.000042 | Test Loss: 0.000911 | Test Accuracy: 0.806334, 0.671392\n",
      "Step 15 | High Training Loss: 28829.203125 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_30_features count_6/model-4\n",
      "Step 15 | Training Loss: 0.000070 | Test Loss: 0.000199 | Test Accuracy: 0.866128, 0.760422\n",
      "Step 16 | Training Loss: 0.000069 | Test Loss: 0.000379 | Test Accuracy: 0.803229, 0.661857\n",
      "Step 17 | Training Loss: 0.000082 | Test Loss: 0.000398 | Test Accuracy: 0.741838, 0.540506\n",
      "Step 18 | Training Loss: 0.000051 | Test Loss: 0.000326 | Test Accuracy: 0.783180, 0.622110\n",
      "Step 19 | Training Loss: 0.000068 | Test Loss: 0.000261 | Test Accuracy: 0.777191, 0.603966\n",
      "Step 20 | Training Loss: 0.000013 | Test Loss: 0.000276 | Test Accuracy: 0.766989, 0.573671\n",
      "Step 21 | Training Loss: 0.000003 | Test Loss: 0.000370 | Test Accuracy: 0.744588, 0.527173\n",
      "Step 22 | Training Loss: 0.000012 | Test Loss: 0.000319 | Test Accuracy: 0.755367, 0.542869\n",
      "Step 23 | Training Loss: 0.000102 | Test Loss: 0.000374 | Test Accuracy: 0.746762, 0.524641\n",
      "Step 24 | Training Loss: 0.000050 | Test Loss: 0.000271 | Test Accuracy: 0.751109, 0.535359\n",
      "Step 25 | Training Loss: 0.000009 | Test Loss: 0.000277 | Test Accuracy: 0.754037, 0.541857\n",
      "Step 26 | Training Loss: 0.000048 | Test Loss: 0.000236 | Test Accuracy: 0.760202, 0.549114\n",
      "Step 27 | Training Loss: 0.000056 | Test Loss: 0.000219 | Test Accuracy: 0.800390, 0.629705\n",
      "Step 28 | Training Loss: 0.000012 | Test Loss: 0.000214 | Test Accuracy: 0.791741, 0.614599\n",
      "Step 29 | Training Loss: 0.000045 | Test Loss: 0.000085 | Test Accuracy: 0.715179, 0.508608\n",
      "Step 30 | Training Loss: 0.000021 | Test Loss: 0.000060 | Test Accuracy: 0.794047, 0.650717\n",
      "Step 1 | Training Loss: 0.000009 | Test Loss: 0.000150 | Test Accuracy: 0.749468, 0.585232\n",
      "Step 2 | Training Loss: 0.000035 | Test Loss: 0.000113 | Test Accuracy: 0.739842, 0.568439\n",
      "Step 3 | Training Loss: 0.000042 | Test Loss: 0.000142 | Test Accuracy: 0.734963, 0.564219\n",
      "Step 4 | Training Loss: 0.000009 | Test Loss: 0.000136 | Test Accuracy: 0.745254, 0.581266\n",
      "Step 5 | Training Loss: 0.000006 | Test Loss: 0.000126 | Test Accuracy: 0.739842, 0.558481\n",
      "Step 6 | Training Loss: 0.000069 | Test Loss: 0.000145 | Test Accuracy: 0.747871, 0.588017\n",
      "Step 7 | Training Loss: 0.000045 | Test Loss: 0.000149 | Test Accuracy: 0.757008, 0.596034\n",
      "Step 8 | Training Loss: 0.000049 | Test Loss: 0.000150 | Test Accuracy: 0.738112, 0.564304\n",
      "Step 9 | Training Loss: 0.000003 | Test Loss: 0.000126 | Test Accuracy: 0.740774, 0.558481\n",
      "Step 10 | Training Loss: 0.000044 | Test Loss: 0.000136 | Test Accuracy: 0.743657, 0.558734\n",
      "Step 11 | Training Loss: 0.000043 | Test Loss: 0.000165 | Test Accuracy: 0.736160, 0.549958\n",
      "Step 12 | Training Loss: 0.000023 | Test Loss: 0.000173 | Test Accuracy: 0.742104, 0.552574\n",
      "Step 13 | Training Loss: 0.000023 | Test Loss: 0.000156 | Test Accuracy: 0.738467, 0.550211\n",
      "Step 14 | Training Loss: 0.000003 | Test Loss: 0.000163 | Test Accuracy: 0.746584, 0.563291\n",
      "Step 15 | Training Loss: 0.000037 | Test Loss: 0.000173 | Test Accuracy: 0.747693, 0.564641\n",
      "Step 16 | Training Loss: 0.000022 | Test Loss: 0.000152 | Test Accuracy: 0.755589, 0.567679\n",
      "Step 17 | Training Loss: 0.000013 | Test Loss: 0.000160 | Test Accuracy: 0.754081, 0.569620\n",
      "Step 18 | Training Loss: 0.000004 | Test Loss: 0.000136 | Test Accuracy: 0.751952, 0.576203\n",
      "Step 19 | Training Loss: 0.000019 | Test Loss: 0.000134 | Test Accuracy: 0.757718, 0.576287\n",
      "Step 20 | Training Loss: 0.000026 | Test Loss: 0.000149 | Test Accuracy: 0.760069, 0.580000\n",
      "Step 21 | Training Loss: 0.000015 | Test Loss: 0.000141 | Test Accuracy: 0.760868, 0.580928\n",
      "Step 22 | Training Loss: 0.000007 | Test Loss: 0.000146 | Test Accuracy: 0.760735, 0.580591\n",
      "Step 23 | Training Loss: 0.000013 | Test Loss: 0.000144 | Test Accuracy: 0.758916, 0.581519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24 | Training Loss: 0.000012 | Test Loss: 0.000152 | Test Accuracy: 0.760735, 0.583797\n",
      "Step 25 | Training Loss: 0.000018 | Test Loss: 0.000149 | Test Accuracy: 0.768808, 0.593333\n",
      "Step 26 | Training Loss: 0.000001 | Test Loss: 0.000178 | Test Accuracy: 0.775062, 0.605232\n",
      "Step 27 | Training Loss: 0.000015 | Test Loss: 0.000162 | Test Accuracy: 0.782470, 0.611899\n",
      "Step 28 | Training Loss: 0.000011 | Test Loss: 0.000173 | Test Accuracy: 0.781893, 0.611646\n",
      "Step 29 | Training Loss: 0.000015 | Test Loss: 0.000147 | Test Accuracy: 0.785131, 0.614515\n",
      "Step 30 | Training Loss: 0.000006 | Test Loss: 0.000150 | Test Accuracy: 0.782425, 0.608861\n",
      "Best Accuracy on Test data: 0.8661284446716309\n",
      "Current Layer Attributes - epochs:30 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.000115 | Test Loss: 0.000277 | Test Accuracy: 0.805891, 0.647004\n",
      "Step 2 | Training Loss: 0.000080 | Test Loss: 0.000391 | Test Accuracy: 0.793914, 0.629451\n",
      "Step 3 | Training Loss: 0.000095 | Test Loss: 0.000168 | Test Accuracy: 0.819996, 0.693249\n",
      "Step 4 | Training Loss: 0.000180 | Test Loss: 0.000161 | Test Accuracy: 0.861116, 0.750633\n",
      "Step 5 | Training Loss: 0.000058 | Test Loss: 0.000231 | Test Accuracy: 0.811435, 0.659325\n",
      "Step 6 | Training Loss: 0.000061 | Test Loss: 0.000352 | Test Accuracy: 0.740951, 0.557131\n",
      "Step 7 | Training Loss: 0.000081 | Test Loss: 0.000193 | Test Accuracy: 0.731458, 0.535949\n",
      "Step 8 | Training Loss: 0.000001 | Test Loss: 0.000229 | Test Accuracy: 0.774131, 0.597046\n",
      "Step 9 | Training Loss: 0.000030 | Test Loss: 0.000209 | Test Accuracy: 0.847543, 0.736878\n",
      "Step 10 | Training Loss: 0.000036 | Test Loss: 0.000219 | Test Accuracy: 0.853043, 0.749958\n",
      "Step 11 | Training Loss: 0.000039 | Test Loss: 0.000230 | Test Accuracy: 0.855350, 0.751477\n",
      "Step 12 | Training Loss: 0.000226 | Test Loss: 0.000401 | Test Accuracy: 0.831352, 0.742278\n",
      "Step 13 | Training Loss: 0.000253 | Test Loss: 0.000427 | Test Accuracy: 0.809262, 0.686667\n",
      "Step 14 | Training Loss: 0.000014 | Test Loss: 0.000111 | Test Accuracy: 0.836054, 0.718903\n",
      "Step 15 | Training Loss: 0.000041 | Test Loss: 0.000110 | Test Accuracy: 0.828779, 0.715612\n",
      "Step 16 | Training Loss: 0.000123 | Test Loss: 0.000406 | Test Accuracy: 0.785575, 0.602363\n",
      "Step 17 | Training Loss: 0.000002 | Test Loss: 0.000308 | Test Accuracy: 0.771292, 0.576287\n",
      "Step 18 | Training Loss: 0.000009 | Test Loss: 0.000245 | Test Accuracy: 0.795777, 0.626329\n",
      "Step 19 | Training Loss: 0.000135 | Test Loss: 0.000344 | Test Accuracy: 0.789611, 0.620000\n",
      "Step 20 | Training Loss: 0.000187 | Test Loss: 0.000313 | Test Accuracy: 0.747250, 0.650886\n",
      "Step 21 | Training Loss: 0.000194 | Test Loss: 0.000666 | Test Accuracy: 0.617237, 0.470211\n",
      "Step 22 | Training Loss: 0.000005 | Test Loss: 0.000240 | Test Accuracy: 0.612535, 0.455443\n",
      "Step 23 | Training Loss: 0.000205 | Test Loss: 0.000611 | Test Accuracy: 0.597676, 0.428270\n",
      "Step 24 | Training Loss: 0.000094 | Test Loss: 0.000311 | Test Accuracy: 0.583082, 0.394599\n",
      "Step 25 | Training Loss: 0.000008 | Test Loss: 0.000633 | Test Accuracy: 0.532115, 0.290886\n",
      "Step 26 | Training Loss: 0.000112 | Test Loss: 0.000564 | Test Accuracy: 0.547906, 0.345823\n",
      "Step 27 | Training Loss: 0.000034 | Test Loss: 0.000513 | Test Accuracy: 0.546753, 0.320253\n",
      "Step 28 | Training Loss: 0.000074 | Test Loss: 0.000483 | Test Accuracy: 0.530562, 0.289198\n",
      "Step 29 | Training Loss: 0.000067 | Test Loss: 0.000499 | Test Accuracy: 0.518098, 0.266582\n",
      "Step 30 | Training Loss: 0.000018 | Test Loss: 0.000443 | Test Accuracy: 0.519650, 0.266751\n",
      "Step 1 | Training Loss: 0.000023 | Test Loss: 0.000430 | Test Accuracy: 0.520626, 0.269030\n",
      "Step 2 | Training Loss: 0.000021 | Test Loss: 0.000427 | Test Accuracy: 0.520271, 0.268776\n",
      "Step 3 | Training Loss: 0.000007 | Test Loss: 0.000425 | Test Accuracy: 0.520449, 0.268861\n",
      "Step 4 | Training Loss: 0.000014 | Test Loss: 0.000414 | Test Accuracy: 0.520582, 0.268523\n",
      "Step 5 | Training Loss: 0.000009 | Test Loss: 0.000416 | Test Accuracy: 0.520715, 0.268861\n",
      "Step 6 | Training Loss: 0.000034 | Test Loss: 0.000423 | Test Accuracy: 0.520449, 0.268608\n",
      "Step 7 | Training Loss: 0.000032 | Test Loss: 0.000401 | Test Accuracy: 0.520759, 0.268945\n",
      "Step 8 | Training Loss: 0.000004 | Test Loss: 0.000401 | Test Accuracy: 0.520449, 0.268608\n",
      "Step 9 | Training Loss: 0.000009 | Test Loss: 0.000397 | Test Accuracy: 0.520405, 0.268439\n",
      "Step 10 | Training Loss: 0.000027 | Test Loss: 0.000395 | Test Accuracy: 0.520582, 0.268608\n",
      "Step 11 | Training Loss: 0.000015 | Test Loss: 0.000380 | Test Accuracy: 0.520405, 0.268692\n",
      "Step 12 | Training Loss: 0.000009 | Test Loss: 0.000382 | Test Accuracy: 0.520493, 0.268861\n",
      "Step 13 | Training Loss: 0.000038 | Test Loss: 0.000375 | Test Accuracy: 0.520271, 0.268692\n",
      "Step 14 | Training Loss: 0.000019 | Test Loss: 0.000368 | Test Accuracy: 0.520671, 0.268523\n",
      "Step 15 | Training Loss: 0.000034 | Test Loss: 0.000356 | Test Accuracy: 0.520405, 0.269367\n",
      "Step 16 | Training Loss: 0.000019 | Test Loss: 0.000350 | Test Accuracy: 0.520449, 0.268608\n",
      "Step 17 | Training Loss: 0.000017 | Test Loss: 0.000349 | Test Accuracy: 0.520538, 0.268945\n",
      "Step 18 | Training Loss: 0.000020 | Test Loss: 0.000365 | Test Accuracy: 0.520449, 0.268523\n",
      "Step 19 | Training Loss: 0.000029 | Test Loss: 0.000367 | Test Accuracy: 0.520405, 0.268776\n",
      "Step 20 | Training Loss: 0.000002 | Test Loss: 0.000350 | Test Accuracy: 0.520538, 0.268608\n",
      "Step 21 | Training Loss: 0.000020 | Test Loss: 0.000344 | Test Accuracy: 0.520538, 0.268608\n",
      "Step 22 | Training Loss: 0.000011 | Test Loss: 0.000345 | Test Accuracy: 0.520538, 0.268608\n",
      "Step 23 | Training Loss: 0.000009 | Test Loss: 0.000347 | Test Accuracy: 0.520493, 0.268608\n",
      "Step 24 | Training Loss: 0.000012 | Test Loss: 0.000336 | Test Accuracy: 0.520493, 0.268439\n",
      "Step 25 | Training Loss: 0.000014 | Test Loss: 0.000304 | Test Accuracy: 0.520405, 0.268861\n",
      "Step 26 | Training Loss: 0.000006 | Test Loss: 0.000314 | Test Accuracy: 0.520538, 0.268186\n",
      "Step 27 | Training Loss: 0.000020 | Test Loss: 0.000324 | Test Accuracy: 0.520493, 0.268523\n",
      "Step 28 | Training Loss: 0.000010 | Test Loss: 0.000303 | Test Accuracy: 0.520449, 0.268523\n",
      "Step 29 | Training Loss: 0.000008 | Test Loss: 0.000300 | Test Accuracy: 0.520759, 0.269114\n",
      "Step 30 | Training Loss: 0.000004 | Test Loss: 0.000302 | Test Accuracy: 0.520227, 0.270127\n",
      "Best Accuracy on Test data: 0.8611160516738892\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [30]\n",
    "    lrs = [1e-2, 1e-3]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f, lrs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.104683Z",
     "start_time": "2017-06-02T23:51:50.099752Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.135861Z",
     "start_time": "2017-06-02T23:51:50.106860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.143439Z",
     "start_time": "2017-06-02T23:51:50.137389Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.161819Z",
     "start_time": "2017-06-02T23:51:50.144893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.901082</td>\n",
       "      <td>0.831983</td>\n",
       "      <td>171.832848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938321</td>\n",
       "      <td>0.887376</td>\n",
       "      <td>0.795274</td>\n",
       "      <td>206.089209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916452</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.795105</td>\n",
       "      <td>188.690874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.921492</td>\n",
       "      <td>0.883605</td>\n",
       "      <td>0.792236</td>\n",
       "      <td>233.175519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.787173</td>\n",
       "      <td>49.377280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.899742</td>\n",
       "      <td>0.866128</td>\n",
       "      <td>0.760422</td>\n",
       "      <td>136.489631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.854297</td>\n",
       "      <td>0.864532</td>\n",
       "      <td>0.778312</td>\n",
       "      <td>213.884803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.850010</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.750633</td>\n",
       "      <td>37.115757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.809962</td>\n",
       "      <td>0.838272</td>\n",
       "      <td>0.733165</td>\n",
       "      <td>197.642441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.920063</td>\n",
       "      <td>0.837873</td>\n",
       "      <td>0.711308</td>\n",
       "      <td>95.061680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.883271</td>\n",
       "      <td>0.834590</td>\n",
       "      <td>0.708270</td>\n",
       "      <td>29.312675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.865370</td>\n",
       "      <td>0.821771</td>\n",
       "      <td>0.702110</td>\n",
       "      <td>55.951580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "7      60              32              4     0.891010    0.901082   \n",
       "0      60               4              2     0.938321    0.887376   \n",
       "1      60               8              2     0.916452    0.885868   \n",
       "3      60              32              2     0.921492    0.883605   \n",
       "6      60              16              4     0.916174    0.878460   \n",
       "10     60              16              6     0.899742    0.866128   \n",
       "9      60               8              6     0.854297    0.864532   \n",
       "11     60              32              6     0.850010    0.861116   \n",
       "8      60               4              6     0.809962    0.838272   \n",
       "2      60              16              2     0.920063    0.837873   \n",
       "4      60               4              4     0.883271    0.834590   \n",
       "5      60               8              4     0.865370    0.821771   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "7        0.831983  171.832848  \n",
       "0        0.795274  206.089209  \n",
       "1        0.795105  188.690874  \n",
       "3        0.792236  233.175519  \n",
       "6        0.787173   49.377280  \n",
       "10       0.760422  136.489631  \n",
       "9        0.778312  213.884803  \n",
       "11       0.750633   37.115757  \n",
       "8        0.733165  197.642441  \n",
       "2        0.711308   95.061680  \n",
       "4        0.708270   29.312675  \n",
       "5        0.702110   55.951580  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.177559Z",
     "start_time": "2017-06-02T23:51:50.163456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.241321Z",
     "start_time": "2017-06-02T23:51:50.179309Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.639367Z",
     "start_time": "2017-06-02T23:51:50.242960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8578  0.1422]\n",
      " [ 0.0662  0.9338]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlLbQWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66ec154b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T23:51:50.927577Z",
     "start_time": "2017-06-02T23:51:50.641487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.4633  0.5367]\n",
      " [ 0.0862  0.9138]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66ec606a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
