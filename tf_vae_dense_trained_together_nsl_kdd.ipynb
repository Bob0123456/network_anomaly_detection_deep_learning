{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:55.560909Z",
     "start_time": "2017-05-31T00:31:55.067682Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:55.771437Z",
     "start_time": "2017-05-31T00:31:55.562572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:55.778082Z",
     "start_time": "2017-05-31T00:31:55.773233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:55.787479Z",
     "start_time": "2017-05-31T00:31:55.779794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:56.648060Z",
     "start_time": "2017-05-31T00:31:55.788975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:57.803520Z",
     "start_time": "2017-05-31T00:31:56.649645Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:58.330323Z",
     "start_time": "2017-05-31T00:31:57.805331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:31:58.541758Z",
     "start_time": "2017-05-31T00:31:58.332153Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            Train.best_acc = 0\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.2)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    \n",
    "                    def train_batch():\n",
    "                        nonlocal train_loss\n",
    "                        _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:1})\n",
    "                    \n",
    "                    train_batch()\n",
    "                    count = 10\n",
    "                    while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count < 1):\n",
    "                        print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                        net.saver.restore(sess, \n",
    "                                          tf.train.latest_checkpoint('dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                     .format(epochs,h,f)))\n",
    "                        train_batch()\n",
    "                        count -= 1\n",
    "                    \n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                \n",
    "                test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                  net.actual, net.y], #net.summary_op \n",
    "                                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                                 net.y_: preprocess.y_test, \n",
    "                                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:.6f} | Test Accuracy: {:.6f}\"\n",
    "                          .format(epoch, train_loss, test_loss, test_accuracy))\n",
    "\n",
    "                if test_accuracy > Train.best_acc_global:\n",
    "                    Train.best_acc_global = test_accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    \n",
    "                if test_accuracy > Train.best_acc:\n",
    "                    Train.best_acc = test_accuracy\n",
    "                    \n",
    "                    if not (np.isnan(train_loss)):\n",
    "                        net.saver.save(sess, \n",
    "                                   \"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                   .format(epochs,h,f), \n",
    "                                   global_step = epoch, \n",
    "                                   write_meta_graph=False)\n",
    "                    \n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):\n",
    "                                              (curr_pred, \n",
    "                                               Train.result(epochs, f, h,valid_accuracy, test_accuracy, time.perf_counter() - start_time))})\n",
    "                    #Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "            print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.665758Z",
     "start_time": "2017-05-31T00:31:58.543636Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.000001 | Test Loss: 0.000228 | Test Accuracy: 0.779099\n",
      "Step 2 | Training Loss: 0.000078 | Test Loss: 0.000192 | Test Accuracy: 0.801544\n",
      "Step 3 | Training Loss: 0.000005 | Test Loss: 0.000283 | Test Accuracy: 0.795600\n",
      "Step 4 | Training Loss: 0.000094 | Test Loss: 0.000254 | Test Accuracy: 0.795112\n",
      "Step 5 | Training Loss: 0.000070 | Test Loss: 0.000167 | Test Accuracy: 0.810504\n",
      "Step 6 | Training Loss: 0.000050 | Test Loss: 0.000249 | Test Accuracy: 0.785752\n",
      "Step 7 | Training Loss: 0.000088 | Test Loss: 0.000243 | Test Accuracy: 0.804604\n",
      "Step 8 | Training Loss: 0.000053 | Test Loss: 0.000157 | Test Accuracy: 0.792273\n",
      "Step 9 | Training Loss: 0.000020 | Test Loss: 0.000287 | Test Accuracy: 0.739310\n",
      "Step 10 | Training Loss: 0.000001 | Test Loss: 0.000256 | Test Accuracy: 0.738112\n",
      "Step 11 | Training Loss: 0.000022 | Test Loss: 0.000222 | Test Accuracy: 0.770981\n",
      "Step 12 | Training Loss: 0.000006 | Test Loss: 0.000231 | Test Accuracy: 0.741483\n",
      "Step 13 | Training Loss: 0.000059 | Test Loss: 0.000146 | Test Accuracy: 0.719837\n",
      "Step 14 | Training Loss: 0.000011 | Test Loss: 0.000227 | Test Accuracy: 0.722498\n",
      "Step 15 | Training Loss: 0.000077 | Test Loss: 0.000308 | Test Accuracy: 0.736293\n",
      "Step 16 | Training Loss: 0.000024 | Test Loss: 0.000280 | Test Accuracy: 0.728265\n",
      "Step 17 | Training Loss: 0.000099 | Test Loss: 0.000297 | Test Accuracy: 0.719260\n",
      "Step 18 | Training Loss: 0.000016 | Test Loss: 0.000232 | Test Accuracy: 0.737757\n",
      "Step 19 | Training Loss: 0.000052 | Test Loss: 0.000177 | Test Accuracy: 0.748536\n",
      "Step 20 | Training Loss: 0.000037 | Test Loss: 0.000261 | Test Accuracy: 0.754436\n",
      "Best Accuracy on Test data: 0.8105039000511169\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.000287 | Test Loss: 0.000533 | Test Accuracy: 0.826650\n",
      "Step 2 | Training Loss: 0.000051 | Test Loss: 0.000301 | Test Accuracy: 0.835877\n",
      "Step 3 | Training Loss: 0.000156 | Test Loss: 0.000395 | Test Accuracy: 0.759182\n",
      "Step 4 | Training Loss: 0.000044 | Test Loss: 0.000378 | Test Accuracy: 0.716022\n",
      "Step 5 | Training Loss: 0.000178 | Test Loss: 0.000463 | Test Accuracy: 0.751641\n",
      "Step 6 | Training Loss: 0.000089 | Test Loss: 0.000360 | Test Accuracy: 0.768275\n",
      "Step 7 | Training Loss: 0.000076 | Test Loss: 0.000334 | Test Accuracy: 0.777768\n",
      "Step 8 | Training Loss: 0.000063 | Test Loss: 0.000280 | Test Accuracy: 0.780962\n",
      "Step 9 | Training Loss: 0.000028 | Test Loss: 0.000215 | Test Accuracy: 0.784289\n",
      "Step 10 | Training Loss: 0.000116 | Test Loss: 0.000314 | Test Accuracy: 0.781583\n",
      "Step 11 | Training Loss: 0.000039 | Test Loss: 0.000269 | Test Accuracy: 0.800390\n",
      "Step 12 | Training Loss: 0.000012 | Test Loss: 0.000259 | Test Accuracy: 0.781095\n",
      "Step 13 | Training Loss: 0.000052 | Test Loss: 0.000190 | Test Accuracy: 0.784067\n",
      "Step 14 | Training Loss: 0.000058 | Test Loss: 0.000301 | Test Accuracy: 0.799902\n",
      "Step 15 | Training Loss: 0.000042 | Test Loss: 0.000251 | Test Accuracy: 0.802608\n",
      "Step 16 | Training Loss: 0.000057 | Test Loss: 0.000296 | Test Accuracy: 0.841466\n",
      "Step 17 | Training Loss: 0.000016 | Test Loss: 0.000207 | Test Accuracy: 0.821327\n",
      "Step 18 | Training Loss: 0.000027 | Test Loss: 0.000212 | Test Accuracy: 0.833038\n",
      "Step 19 | Training Loss: 0.000001 | Test Loss: 0.000202 | Test Accuracy: 0.818577\n",
      "Step 20 | Training Loss: 0.000000 | Test Loss: 0.000159 | Test Accuracy: 0.822037\n",
      "Best Accuracy on Test data: 0.8414655923843384\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.000138 | Test Loss: 0.000152 | Test Accuracy: 0.759226\n",
      "Step 2 | Training Loss: 0.000116 | Test Loss: 0.000342 | Test Accuracy: 0.775106\n",
      "Step 3 | Training Loss: 0.000057 | Test Loss: 0.000447 | Test Accuracy: 0.795999\n",
      "Step 4 | Training Loss: 0.000024 | Test Loss: 0.000430 | Test Accuracy: 0.757452\n",
      "Step 5 | Training Loss: 0.000066 | Test Loss: 0.000306 | Test Accuracy: 0.752528\n",
      "Step 6 | Training Loss: 0.000004 | Test Loss: 0.000313 | Test Accuracy: 0.774441\n",
      "Step 7 | Training Loss: 0.007141 | Test Loss: 0.000916 | Test Accuracy: 0.787881\n",
      "Step 8 | Training Loss: 0.000048 | Test Loss: 0.000502 | Test Accuracy: 0.793426\n",
      "Step 9 | Training Loss: 0.000007 | Test Loss: 0.000295 | Test Accuracy: 0.829312\n",
      "Step 10 | Training Loss: 0.000002 | Test Loss: 0.000281 | Test Accuracy: 0.837961\n",
      "Step 11 | Training Loss: 0.000025 | Test Loss: 0.000220 | Test Accuracy: 0.853930\n",
      "Step 12 | Training Loss: 0.000084 | Test Loss: 0.000162 | Test Accuracy: 0.860495\n",
      "Step 13 | Training Loss: 0.000011 | Test Loss: 0.000234 | Test Accuracy: 0.846123\n",
      "Step 14 | Training Loss: 0.000021 | Test Loss: 0.000263 | Test Accuracy: 0.844083\n",
      "Step 15 | Training Loss: 0.000043 | Test Loss: 0.000197 | Test Accuracy: 0.844260\n",
      "Step 16 | Training Loss: 0.000050 | Test Loss: 0.000168 | Test Accuracy: 0.861160\n",
      "Step 17 | Training Loss: 0.000027 | Test Loss: 0.000147 | Test Accuracy: 0.884315\n",
      "Step 18 | Training Loss: 0.000054 | Test Loss: 0.000152 | Test Accuracy: 0.861781\n",
      "Step 19 | Training Loss: 0.000040 | Test Loss: 0.000182 | Test Accuracy: 0.861027\n",
      "Step 20 | Training Loss: 0.000044 | Test Loss: 0.000138 | Test Accuracy: 0.865020\n",
      "Best Accuracy on Test data: 0.8843151330947876\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.000067 | Test Loss: 0.034272 | Test Accuracy: 0.773554\n",
      "Step 2 | Training Loss: 0.000238 | Test Loss: 0.000648 | Test Accuracy: 0.570928\n",
      "Step 3 | Training Loss: 0.000058 | Test Loss: 0.000408 | Test Accuracy: 0.704622\n",
      "Step 4 | Training Loss: 0.000003 | Test Loss: 0.000466 | Test Accuracy: 0.680935\n",
      "Step 5 | Training Loss: 0.000021 | Test Loss: 0.000364 | Test Accuracy: 0.686968\n",
      "Step 6 | Training Loss: 0.000007 | Test Loss: 0.000445 | Test Accuracy: 0.704755\n",
      "Step 7 | Training Loss: 0.000051 | Test Loss: 0.000448 | Test Accuracy: 0.719970\n",
      "Step 8 | Training Loss: 0.000030 | Test Loss: 0.000224 | Test Accuracy: 0.746318\n",
      "Step 9 | Training Loss: 0.000034 | Test Loss: 0.003258 | Test Accuracy: 0.752040\n",
      "Step 10 | Training Loss: 0.000042 | Test Loss: 0.000343 | Test Accuracy: 0.736471\n",
      "Step 11 | Training Loss: 0.000022 | Test Loss: 0.000327 | Test Accuracy: 0.730704\n",
      "Step 12 | Training Loss: 0.000015 | Test Loss: 0.000255 | Test Accuracy: 0.733632\n",
      "Step 13 | Training Loss: 0.000002 | Test Loss: 0.000293 | Test Accuracy: 0.734830\n",
      "Step 14 | Training Loss: 0.000023 | Test Loss: 0.000375 | Test Accuracy: 0.735051\n",
      "Step 15 | Training Loss: 0.000003 | Test Loss: inf | Test Accuracy: 0.741306\n",
      "Step 16 | Training Loss: 0.000061 | Test Loss: inf | Test Accuracy: 0.737447\n",
      "Step 17 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Best Accuracy on Test data: 0.7735539674758911\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.000204 | Test Loss: 0.000097 | Test Accuracy: 0.695263\n",
      "Step 2 | Training Loss: 0.000153 | Test Loss: 0.000369 | Test Accuracy: 0.773687\n",
      "Step 3 | Training Loss: 0.000135 | Test Loss: 0.000238 | Test Accuracy: 0.807266\n",
      "Step 4 | Training Loss: 0.000103 | Test Loss: 0.317446 | Test Accuracy: 0.817335\n",
      "Step 5 | Training Loss: 0.000153 | Test Loss: 0.001388 | Test Accuracy: 0.778300\n",
      "Step 6 | Training Loss: 0.000039 | Test Loss: 0.000238 | Test Accuracy: 0.857967\n",
      "Step 7 | Training Loss: 0.000094 | Test Loss: 0.000262 | Test Accuracy: 0.773155\n",
      "Step 8 | Training Loss: 0.000035 | Test Loss: 0.000176 | Test Accuracy: 0.809351\n",
      "Step 9 | Training Loss: 0.000017 | Test Loss: 0.000193 | Test Accuracy: 0.805758\n",
      "Step 10 | Training Loss: 0.000019 | Test Loss: 0.000184 | Test Accuracy: 0.805048\n",
      "Step 11 | Training Loss: 0.000000 | Test Loss: 0.000273 | Test Accuracy: 0.780252\n",
      "Step 12 | Training Loss: 0.000004 | Test Loss: 0.000149 | Test Accuracy: 0.785575\n",
      "Step 13 | Training Loss: 0.000026 | Test Loss: 0.000148 | Test Accuracy: 0.823545\n",
      "Step 14 | Training Loss: 0.000024 | Test Loss: 0.000151 | Test Accuracy: 0.784909\n",
      "Step 15 | Training Loss: 0.000027 | Test Loss: 0.000126 | Test Accuracy: 0.784289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.000021 | Test Loss: 0.000106 | Test Accuracy: 0.784555\n",
      "Step 17 | Training Loss: 0.000004 | Test Loss: 0.000108 | Test Accuracy: 0.821061\n",
      "Step 18 | Training Loss: 0.000032 | Test Loss: 0.000072 | Test Accuracy: 0.818355\n",
      "Step 19 | Training Loss: 0.000036 | Test Loss: 0.000124 | Test Accuracy: 0.816448\n",
      "Step 20 | Training Loss: 0.000009 | Test Loss: 0.000091 | Test Accuracy: 0.810415\n",
      "Best Accuracy on Test data: 0.8579666614532471\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.000017 | Test Loss: 0.000369 | Test Accuracy: 0.718240\n",
      "Step 2 | Training Loss: 0.000187 | Test Loss: 0.000338 | Test Accuracy: 0.668604\n",
      "Step 3 | Training Loss: 0.000278 | Test Loss: 0.000564 | Test Accuracy: 0.659111\n",
      "Step 4 | Training Loss: 0.136439 | Test Loss: 0.000335 | Test Accuracy: 0.633783\n",
      "Step 5 | Training Loss: 0.000005 | Test Loss: 0.000100 | Test Accuracy: 0.675923\n",
      "Step 6 | Training Loss: 0.000089 | Test Loss: 0.000379 | Test Accuracy: 0.662349\n",
      "Step 7 | Training Loss: 0.000262 | Test Loss: 0.000112 | Test Accuracy: 0.644917\n",
      "Step 8 | Training Loss: 0.000100 | Test Loss: 0.000189 | Test Accuracy: 0.649264\n",
      "Step 9 | Training Loss: 0.000018 | Test Loss: 0.000397 | Test Accuracy: 0.643009\n",
      "Step 10 | Training Loss: 0.000001 | Test Loss: 0.000254 | Test Accuracy: 0.679427\n",
      "Step 11 | Training Loss: 0.000108 | Test Loss: 0.000275 | Test Accuracy: 0.724539\n",
      "Step 12 | Training Loss: 0.000092 | Test Loss: 0.000214 | Test Accuracy: 0.733144\n",
      "Step 13 | Training Loss: 0.000069 | Test Loss: 0.000124 | Test Accuracy: 0.723430\n",
      "Step 14 | Training Loss: 0.000074 | Test Loss: 0.000316 | Test Accuracy: 0.758206\n",
      "Step 15 | Training Loss: 0.000090 | Test Loss: 0.000329 | Test Accuracy: 0.734342\n",
      "Step 16 | Training Loss: 0.000032 | Test Loss: 0.000289 | Test Accuracy: 0.735185\n",
      "Step 17 | Training Loss: 0.000088 | Test Loss: 0.000392 | Test Accuracy: 0.710788\n",
      "Step 18 | Training Loss: 0.000080 | Test Loss: 0.123107 | Test Accuracy: 0.700452\n",
      "Step 19 | Training Loss: 0.000054 | Test Loss: 0.000401 | Test Accuracy: 0.771735\n",
      "Step 20 | Training Loss: 0.000043 | Test Loss: 0.000370 | Test Accuracy: 0.767876\n",
      "Best Accuracy on Test data: 0.7717352509498596\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.000128 | Test Loss: 0.000803 | Test Accuracy: 0.836985\n",
      "Step 2 | Training Loss: 0.000193 | Test Loss: 0.000375 | Test Accuracy: 0.750310\n",
      "Step 3 | Training Loss: 0.000038 | Test Loss: 0.000235 | Test Accuracy: 0.769163\n",
      "Step 4 | Training Loss: 0.000111 | Test Loss: 0.000212 | Test Accuracy: 0.792362\n",
      "Step 5 | Training Loss: 0.000038 | Test Loss: 0.000158 | Test Accuracy: 0.793781\n",
      "Step 6 | Training Loss: 0.000031 | Test Loss: 0.000162 | Test Accuracy: 0.818976\n",
      "Step 7 | Training Loss: 0.000045 | Test Loss: 0.000192 | Test Accuracy: 0.804205\n",
      "Step 8 | Training Loss: 0.000013 | Test Loss: 0.000125 | Test Accuracy: 0.815161\n",
      "Step 9 | Training Loss: 0.000014 | Test Loss: 0.000190 | Test Accuracy: 0.797152\n",
      "Step 10 | Training Loss: 0.000033 | Test Loss: 0.000144 | Test Accuracy: 0.803762\n",
      "Step 11 | Training Loss: 0.000036 | Test Loss: 0.000148 | Test Accuracy: 0.821549\n",
      "Step 12 | Training Loss: 0.000081 | Test Loss: 0.000109 | Test Accuracy: 0.835300\n",
      "Step 13 | Training Loss: 0.000026 | Test Loss: 0.000130 | Test Accuracy: 0.826207\n",
      "Step 14 | Training Loss: 0.000109 | Test Loss: 0.000071 | Test Accuracy: 0.808508\n",
      "Step 15 | Training Loss: 0.000068 | Test Loss: 0.000199 | Test Accuracy: 0.777280\n",
      "Step 16 | Training Loss: 0.000020 | Test Loss: 0.000302 | Test Accuracy: 0.736427\n",
      "Step 17 | Training Loss: 0.000004 | Test Loss: 0.000181 | Test Accuracy: 0.799769\n",
      "Step 18 | Training Loss: 0.000010 | Test Loss: 0.000220 | Test Accuracy: 0.803185\n",
      "Step 19 | Training Loss: 0.000021 | Test Loss: 0.000083 | Test Accuracy: 0.815694\n",
      "Step 20 | Training Loss: 0.000028 | Test Loss: 0.000100 | Test Accuracy: 0.812589\n",
      "Best Accuracy on Test data: 0.8369854688644409\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.000052 | Test Loss: 0.000073 | Test Accuracy: 0.806512\n",
      "Step 2 | Training Loss: 0.000117 | Test Loss: 0.000295 | Test Accuracy: 0.799015\n",
      "Step 3 | Training Loss: 0.000054 | Test Loss: 1.334467 | Test Accuracy: 0.784155\n",
      "Step 4 | Training Loss: 0.000013 | Test Loss: 0.000218 | Test Accuracy: 0.812988\n",
      "Step 5 | Training Loss: 0.000005 | Test Loss: 0.000263 | Test Accuracy: 0.807177\n",
      "Step 6 | Training Loss: 0.000037 | Test Loss: 0.000378 | Test Accuracy: 0.782692\n",
      "Step 7 | Training Loss: 0.000115 | Test Loss: 0.000436 | Test Accuracy: 0.761577\n",
      "Step 8 | Training Loss: 0.000001 | Test Loss: 0.000288 | Test Accuracy: 0.779232\n",
      "Step 9 | Training Loss: 0.000030 | Test Loss: 0.000246 | Test Accuracy: 0.825586\n",
      "Step 10 | Training Loss: 0.000002 | Test Loss: 0.000217 | Test Accuracy: 0.812278\n",
      "Step 11 | Training Loss: 0.000029 | Test Loss: 0.000246 | Test Accuracy: 0.820440\n",
      "Step 12 | Training Loss: 0.000041 | Test Loss: 0.000292 | Test Accuracy: 0.803318\n",
      "Step 13 | Training Loss: 0.000029 | Test Loss: 0.000207 | Test Accuracy: 0.844792\n",
      "Step 14 | Training Loss: 0.000074 | Test Loss: 0.000159 | Test Accuracy: 0.877395\n",
      "Step 15 | Training Loss: 0.000050 | Test Loss: 0.000264 | Test Accuracy: 0.818754\n",
      "Step 16 | Training Loss: 0.000046 | Test Loss: 0.000301 | Test Accuracy: 0.827049\n",
      "Step 17 | Training Loss: 0.000038 | Test Loss: 0.000257 | Test Accuracy: 0.828646\n",
      "Step 18 | Training Loss: 0.000036 | Test Loss: 0.000177 | Test Accuracy: 0.828380\n",
      "Step 19 | Training Loss: 0.000031 | Test Loss: 0.000210 | Test Accuracy: 0.823589\n",
      "Step 20 | Training Loss: 0.000007 | Test Loss: 0.000245 | Test Accuracy: 0.823545\n",
      "Best Accuracy on Test data: 0.8773953318595886\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.000208 | Test Loss: 0.000474 | Test Accuracy: 0.584723\n",
      "Step 2 | Training Loss: 0.000169 | Test Loss: 0.000275 | Test Accuracy: 0.729906\n",
      "Step 3 | Training Loss: 0.000039 | Test Loss: 0.000435 | Test Accuracy: 0.721744\n",
      "Step 4 | Training Loss: 0.000046 | Test Loss: 0.000396 | Test Accuracy: 0.732080\n",
      "Step 5 | Training Loss: 0.000023 | Test Loss: 0.000299 | Test Accuracy: 0.804028\n",
      "Step 6 | Training Loss: 0.000004 | Test Loss: 0.000266 | Test Accuracy: 0.779986\n",
      "Step 7 | Training Loss: 0.000005 | Test Loss: 0.000332 | Test Accuracy: 0.849006\n",
      "Step 8 | Training Loss: 0.000027 | Test Loss: 0.000154 | Test Accuracy: 0.817867\n",
      "Step 9 | Training Loss: 0.000048 | Test Loss: 0.000210 | Test Accuracy: 0.883960\n",
      "Step 10 | Training Loss: 0.000003 | Test Loss: 0.000227 | Test Accuracy: 0.777901\n",
      "Step 11 | Training Loss: 0.000008 | Test Loss: 0.000215 | Test Accuracy: 0.773643\n",
      "Step 12 | Training Loss: 0.000085 | Test Loss: 0.000268 | Test Accuracy: 0.753371\n",
      "Step 13 | Training Loss: 0.000030 | Test Loss: 0.000258 | Test Accuracy: 0.784688\n",
      "Step 14 | Training Loss: 0.000116 | Test Loss: 0.000298 | Test Accuracy: 0.772134\n",
      "Step 15 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss: nan | Test Accuracy: 0.569242\n",
      "Best Accuracy on Test data: 0.8839602470397949\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.000258 | Test Loss: 0.000017 | Test Accuracy: 0.806157\n",
      "Step 2 | Training Loss: 0.000233 | Test Loss: 0.000074 | Test Accuracy: 0.892832\n",
      "Step 3 | Training Loss: 0.000045 | Test Loss: 0.000079 | Test Accuracy: 0.817113\n",
      "Step 4 | Training Loss: 0.000083 | Test Loss: 0.000151 | Test Accuracy: 0.813387\n",
      "Step 5 | Training Loss: 0.000040 | Test Loss: 0.000181 | Test Accuracy: 0.835522\n",
      "Step 6 | Training Loss: 0.000032 | Test Loss: 0.000322 | Test Accuracy: 0.802475\n",
      "Step 7 | Training Loss: 0.000002 | Test Loss: 0.000251 | Test Accuracy: 0.831485\n",
      "Step 8 | Training Loss: 0.000008 | Test Loss: 0.000229 | Test Accuracy: 0.817512\n",
      "Step 9 | Training Loss: 0.000033 | Test Loss: 0.000250 | Test Accuracy: 0.806290\n",
      "Step 10 | Training Loss: 0.000042 | Test Loss: 0.000249 | Test Accuracy: 0.786506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 | Training Loss: 0.000047 | Test Loss: 0.000243 | Test Accuracy: 0.801588\n",
      "Step 12 | Training Loss: 0.000007 | Test Loss: 0.000303 | Test Accuracy: 0.802741\n",
      "Step 13 | Training Loss: 0.000005 | Test Loss: 0.000253 | Test Accuracy: 0.848075\n",
      "Step 14 | Training Loss: 0.000089 | Test Loss: 0.000228 | Test Accuracy: 0.835477\n",
      "Step 15 | Training Loss: 0.000098 | Test Loss: 0.000138 | Test Accuracy: 0.863334\n",
      "Step 16 | Training Loss: 0.000069 | Test Loss: 0.000220 | Test Accuracy: 0.828469\n",
      "Step 17 | Training Loss: 0.000027 | Test Loss: 0.000222 | Test Accuracy: 0.821771\n",
      "Step 18 | Training Loss: 0.000075 | Test Loss: 0.000232 | Test Accuracy: 0.864221\n",
      "Step 19 | Training Loss: 0.000001 | Test Loss: 0.000175 | Test Accuracy: 0.854107\n",
      "Step 20 | Training Loss: 0.000000 | Test Loss: 0.000173 | Test Accuracy: 0.853043\n",
      "Best Accuracy on Test data: 0.8928318023681641\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.000009 | Test Loss: 0.000301 | Test Accuracy: 0.821327\n",
      "Step 2 | Training Loss: 0.000066 | Test Loss: 0.000341 | Test Accuracy: 0.737535\n",
      "Step 3 | Training Loss: 0.000038 | Test Loss: 0.000185 | Test Accuracy: 0.790277\n",
      "Step 4 | Training Loss: 0.000047 | Test Loss: 0.000253 | Test Accuracy: 0.795600\n",
      "Step 5 | Training Loss: 0.000040 | Test Loss: 0.000025 | Test Accuracy: 0.806068\n",
      "Step 6 | Training Loss: 0.000001 | Test Loss: 0.000131 | Test Accuracy: 0.793958\n",
      "Step 7 | Training Loss: 0.000056 | Test Loss: 0.000202 | Test Accuracy: 0.771247\n",
      "Step 8 | Training Loss: 0.000021 | Test Loss: 0.000331 | Test Accuracy: 0.696638\n",
      "Step 9 | Training Loss: 0.000015 | Test Loss: 0.000255 | Test Accuracy: 0.754924\n",
      "Step 10 | Training Loss: 0.000068 | Test Loss: 0.000276 | Test Accuracy: 0.801189\n",
      "Step 11 | Training Loss: 0.000010 | Test Loss: 0.000183 | Test Accuracy: 0.805004\n",
      "Step 12 | Training Loss: 0.000067 | Test Loss: 0.000161 | Test Accuracy: 0.781228\n",
      "Step 13 | Training Loss: 0.000009 | Test Loss: 0.000211 | Test Accuracy: 0.757452\n",
      "Step 14 | Training Loss: 0.000005 | Test Loss: 0.000191 | Test Accuracy: 0.755767\n",
      "Step 15 | Training Loss: 0.000023 | Test Loss: 0.000134 | Test Accuracy: 0.752528\n",
      "Step 16 | Training Loss: 0.000007 | Test Loss: 0.000171 | Test Accuracy: 0.744322\n",
      "Step 17 | Training Loss: 0.000016 | Test Loss: 0.000224 | Test Accuracy: 0.737890\n",
      "Step 18 | Training Loss: 0.000011 | Test Loss: 0.000208 | Test Accuracy: 0.752883\n",
      "Step 19 | Training Loss: 0.000004 | Test Loss: 0.000124 | Test Accuracy: 0.744322\n",
      "Step 20 | Training Loss: 0.000024 | Test Loss: 0.000144 | Test Accuracy: 0.744455\n",
      "Best Accuracy on Test data: 0.8213272094726562\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.000042 | Test Loss: 0.000177 | Test Accuracy: 0.809883\n",
      "Step 2 | Training Loss: 0.000116 | Test Loss: 0.000359 | Test Accuracy: 0.842308\n",
      "Step 3 | Training Loss: 0.000044 | Test Loss: 0.000222 | Test Accuracy: 0.830199\n",
      "Step 4 | Training Loss: 0.000072 | Test Loss: 0.000305 | Test Accuracy: 0.820706\n",
      "Step 5 | Training Loss: 0.000007 | Test Loss: 0.000132 | Test Accuracy: 0.835078\n",
      "Step 6 | Training Loss: 0.000008 | Test Loss: 0.000178 | Test Accuracy: 0.858321\n",
      "Step 7 | Training Loss: 0.000070 | Test Loss: 0.000270 | Test Accuracy: 0.811657\n",
      "Step 8 | Training Loss: 0.000023 | Test Loss: 0.000261 | Test Accuracy: 0.838760\n",
      "Step 9 | Training Loss: 0.000080 | Test Loss: 0.000179 | Test Accuracy: 0.789789\n",
      "Step 10 | Training Loss: 0.000054 | Test Loss: 0.000124 | Test Accuracy: 0.793604\n",
      "Step 11 | Training Loss: 0.000020 | Test Loss: 0.000146 | Test Accuracy: 0.737669\n",
      "Step 12 | Training Loss: 0.000043 | Test Loss: 0.000121 | Test Accuracy: 0.747782\n",
      "Step 13 | Training Loss: 0.000096 | Test Loss: 0.000132 | Test Accuracy: 0.776260\n",
      "Step 14 | Training Loss: 0.000082 | Test Loss: 0.000093 | Test Accuracy: 0.781405\n",
      "Step 15 | Training Loss: 0.000002 | Test Loss: 0.000137 | Test Accuracy: 0.768098\n",
      "Step 16 | Training Loss: 0.000020 | Test Loss: 0.000041 | Test Accuracy: 0.768719\n",
      "Step 17 | Training Loss: 0.000030 | Test Loss: 0.000053 | Test Accuracy: 0.763795\n",
      "Step 18 | Training Loss: 0.000040 | Test Loss: 0.000030 | Test Accuracy: 0.775284\n",
      "Step 19 | Training Loss: 0.000848 | Test Loss: 0.001020 | Test Accuracy: 0.543116\n",
      "Step 20 | Training Loss: 0.000648 | Test Loss: 0.000921 | Test Accuracy: 0.468772\n",
      "Best Accuracy on Test data: 0.858321487903595\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [20]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.674961Z",
     "start_time": "2017-05-31T00:59:35.668347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.682057Z",
     "start_time": "2017-05-31T00:59:35.677384Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.690100Z",
     "start_time": "2017-05-31T00:59:35.684505Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.711529Z",
     "start_time": "2017-05-31T00:59:35.692514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.896606</td>\n",
       "      <td>0.892832</td>\n",
       "      <td>18.282973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.910498</td>\n",
       "      <td>0.884315</td>\n",
       "      <td>52.052245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.900099</td>\n",
       "      <td>0.883960</td>\n",
       "      <td>79.896244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876999</td>\n",
       "      <td>0.877395</td>\n",
       "      <td>109.307383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.858321</td>\n",
       "      <td>56.983235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.878309</td>\n",
       "      <td>0.857967</td>\n",
       "      <td>36.212009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.831117</td>\n",
       "      <td>0.841466</td>\n",
       "      <td>46.666873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.930304</td>\n",
       "      <td>0.836985</td>\n",
       "      <td>8.436142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.882993</td>\n",
       "      <td>0.821327</td>\n",
       "      <td>9.489717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.902123</td>\n",
       "      <td>0.810504</td>\n",
       "      <td>14.201883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.925581</td>\n",
       "      <td>0.773554</td>\n",
       "      <td>4.045676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.877555</td>\n",
       "      <td>0.771735</td>\n",
       "      <td>156.046256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  time_taken\n",
       "9      20               8              6     0.896606    0.892832   18.282973\n",
       "2      20              16              2     0.910498    0.884315   52.052245\n",
       "8      20               4              6     0.900099    0.883960   79.896244\n",
       "7      20              32              4     0.876999    0.877395  109.307383\n",
       "11     20              32              6     0.872832    0.858321   56.983235\n",
       "4      20               4              4     0.878309    0.857967   36.212009\n",
       "1      20               8              2     0.831117    0.841466   46.666873\n",
       "6      20              16              4     0.930304    0.836985    8.436142\n",
       "10     20              16              6     0.882993    0.821327    9.489717\n",
       "0      20               4              2     0.902123    0.810504   14.201883\n",
       "3      20              32              2     0.925581    0.773554    4.045676\n",
       "5      20               8              4     0.877555    0.771735  156.046256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.733150Z",
     "start_time": "2017-05-31T00:59:35.713831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:35.823485Z",
     "start_time": "2017-05-31T00:59:35.735182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:59:36.283561Z",
     "start_time": "2017-05-31T00:59:35.825516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8587  0.1413]\n",
      " [ 0.0621  0.9379]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcXfP9x/HXe7LIHokQWRBELFFEgvzQigaJ2tXW2imq\ni1LUVqralNa+FKUU1Yqtak1iKa0tyGbfYs2GRBZiyTL5/P443xk3k8xkMrkzkzn3/fS4j7n3rN9z\nc93P/XzO93yPIgIzM7M8KWvsBpiZmRWbg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZ\nmeWOg5uZmeWOg5uZmeVO88ZugJmZFVezDutELPyqaNuLr6aPioihRdtgA3BwMzPLmVj4FatseEDR\ntvf1hD93KdrGGoiDm5lZ7ghU2medSvvozcwsl5y5mZnljQCpsVvRqBzczMzyyGVJMzOzfHHmZmaW\nRy5LmplZvri3ZGkfvZmZ5ZIzNzOzPHJZ0szMckW4LNnYDTAzMys2Z25mZrkjlyUbuwFmZlYPXJY0\nMzPLF2duZmZ55LKkmZnliy/iLu2jNzOzXHLmZmaWN77ljYObmVkuuSxpZmaWL87czMxyxx1KHNzM\nzPKorLTPuZV2aDczs1xy5mZmlje+K4CDm5lZLpX4pQClHdrNzCyXnLmZmeWOe0uW9tGbmVkuOXMz\nM8ujEj/n5uBmZpZHLkuamZnlizM3M7O8kVyWbOwGmJlZPXBZ0szMrO4k3SjpE0mvFEzrLOkRSW+n\nv50K5p0haaKkNyUNKZjeX9LLad4VUpZ+SlpF0u1p+nOSei2rTQ5uZmZ5VFGaLMZj2W4ChlaZdjrw\nWERsADyWXiNpE+AgoG9a52pJzdI61wDHABukR8U2jwZmRURv4FLgj8tqkIObmVnupIu4i/VYhoj4\nHzCzyuS9gJvT85uBvQumD4+IeRHxHjAR2FpSN6BDRIyOiABuqbJOxbbuAgZXZHXVcXArEZJelTSo\nmnmDJE2uYd2bJP2+3hpnZiu7LpLGFDyOrcU6XSNiWnr+EdA1Pe8BTCpYbnKa1iM9rzp9sXUiYiEw\nB1itpp07uOWApPcl7VRl2hGSnqp4HRF9I+KJBm9cDaq2sSmQtLqkf0qaI2mWpH/Ucr1ekkLS3ILH\ni0Voz7mSbl3R7RSLpD6S7pQ0I71HL0n6ZUHZqb72u8wfYOnf4KH07/aRpKsk5bdTXXHLkjMiYkDB\n47rlaUrKxKJejrMaDm5WspRZ3v8H/kX2K3RtYA3gouVcf9WIaJcemy/nukVXzC93SesDz5H9wv5W\nRHQE9gf6A+2LtZ8VcDUwHegGbAHsAPykUVtUXypuedNAZclqfJxKjaS/n6TpU4C1CpbrmaZNSc+r\nTl9snfSZ7Qh8WtPOHdxKRGF2J6l1+qU7S9JrwFZVlu0naZykzyXdDrSqMn93SRMkzZb0jKTNquzn\nlPSLfU7q4bTY+rVs75GSXk9teFfScQXzXpG0R8HrFilT6JdeD0ztmi3pxcJyrKQnJA2T9DTwJbBe\nyiDfTft6T9LB1bRpF7L/wU6NiDkRsSAixi/vsVWz7aPS8c6SNErSOgXzLpc0SdJnksZK+naaPhQ4\nEziwMBOsmskXZncFGeTRkj4E/lOL96xW7w/wW+CZiPhlRTkqIt6MiIMjYnba1p7KSuSz07/FxgX7\nCUm9C15XZmNKpXNJJyvrlTdN0pFp3rHAwcCv0vtwfzXtWxe4PSK+joiPgJFknRqsftwHHJ6eHw7c\nWzD9IGU9INcl6zjyfPrMfJY+iwIOq7JOxbb2A/6TssFqObiVpt8A66fHEL750CCpJfBv4O9AZ+BO\n4PsF8/sBNwLHkdW8/wLcJ2mVgu0fQNbLaV1gM+CIOrTxE2B3oANwJHCppC3TvFuAQwqW/R4wLSLG\nS+oBPAj8PrX/FOBuSasXLH8ocCxZNjEduALYNSLaA9sCE9Kxrp2+hNdO6w0E3gRulvSppBck7VCH\nY1uMpL3IgtS+wOrAk8BtBYu8QJZpdAb+CdwpqVVEjAT+QPaFvbyZ4A7AxsCQmt4zSW2p5v1Zip3I\nTvZXd5x90nGdmI7zIeD+9JmrjTXJfrH3IOs992dJnVKJ7B/An9L7sEfa39WSri5Y/zKyHwJt0jHv\nShbgcqhhO5RIug14Ftgw/Qg5GrgA2FnS22SfjQsAIuJV4A7gNbL3/6cRUZ429RPgr2SdTN4BRqTp\nNwCrSZoI/JLU87ImDm758e/0RTxb0myyEkx1DgCGRcTMiJhE9uVVYSDQArgsZSZ3kX25VjgW+EtE\nPBcR5RFxMzAvrVfhioiYGhEzgfvJvpiXS0Q8GBHvROa/wMPAt9PsW4HvSeqQXh9KFowhC3oPRcRD\nEbEoIh4BxpAFwAo3RcSr6cT0QmARsKmk1hExLf3PR0R8GBGrRsSHab2ewC7A42RftBcD90rqshyH\nNqPg3+mUNO3HwPkR8Xpq0x+ALSqyt4i4NSI+jYiFEXExsAqw4XLsc2nOjYgvIuIrlv2eLfX9WYrV\ngGnVzAM4EHgwIh6JiAVkJd3WZAGzNhYA56XP5UPAXGp4HyLiJxFRWHb8H7Ap8BlZZ4UxZD/k8qkB\nLwWIiB9ERLeIaBERPSPihvSZHRwRG0TETun7oGL5YRGxfkRsGBEjCqaPiYhN07yfVWRnKdvePyJ6\nR8TWEfHustrk4JYfe6cv4lUjYlVqPpfQncV7K31QZd6UKil/4fx1gJOrBNK10noVPip4/iXQbnkO\nBEDSrpJGS5qZ9vE9oAtAREwFnga+L2lVsl/gFR071gH2r9K+7cnOs1SoPPaI+ILsS/fHwDRJD0ra\nqJpmfQW8n/7HXRARw9O2tluOQ+tS8O9Ucb5uHeDygvbOJDtr0iO9F6ekkuWcNL9jxXuxAgr//at9\nz5bz/fmUxd/nqrpT8FmKiEWpHT2qXaPK9lPwr1Drz5ayc6sjyc6ZtiV7/zpRi+ulrGlycCtN01j8\nhO7aVeb1SDXvpc2fRJb1rVrwaBMRhWW0FZJKnHeT/bLvmoL1Q2Rf+BVuJss49geejYiKE8+TgL9X\naV/biLigYN3FavURMSoidib7Yn4DuL6apr1Udd2lvK6LScBxVdrcOiKeSefXfkWWbXdK78Ucvnkv\nlrb/L4A2Ba/XXMoyhevV+J4tx/vzKAUl7KWYShZIgaxDD9nnsOLf7statLs6y/p36Ez2Ob4qsuur\nPgX+xuIZfb40foeSRtU0W20r6g7gDEmdJPUEfl4w71myUt0Jyjpq7AtsXTD/euDHkrZRpq2k3STV\ntTecJLUqfAAtyUpv04GFknYlKwcW+jewJfALsnNwFW4F9pA0RFKztM1B6TiXtvOukvZK55bmkZW6\nFlXT1nuATpIOT9vej6xU+XTa1rmSnqjDe3At2b9H37SdjpL2T/Pak/17TAeaSzqH7DxkhY+BXlq8\n1+cEshP2LSQNIDsBX5Nq37PlfH9+A2wr6UJJa6Zj6S3p1pRh3wHsJmmwpBbAyWmbzxS0+4epDUPJ\nzgvW1sfAetXNjIgZwHtkn93mqT2Hk/1gyaeGHaFkpePgVpp+S1Yeeo/sXFbF+SoiYj5Zx4YjyMpj\nB5KVcirmjyEbHucqYBbZid8jVqAt25KV+6o+TiD7MpwF/JCst1SldK7obrJOK4Xtm0Q2msGZZAFh\nEnAq1X/Wy8hOUE8lO94dgOOhskPJ3IoOJemcwZ5kHS7mkJ3U3it9cUKWhTy9vG9ARNxDVh4bLukz\n4BWyUivAKLJy2ltk/2Zfs3hJ8c7091NJ49Lzs8k6C80i+7f+5zL2X9N7Vu37s5TtvAP8H9ALeFXS\nHLJ/ozHA5xHxJlm2fSUwA9gD2CN95iD7obIHMJus9+PynA+7AdgklVX/DSDpWknXFiyzL9n7Op3s\nc7sAOGk59mFNiKLm3pRmK62UxfSJiEOWuXADkDQBGJxKXmaNpqxTr1hl0K+Ltr2v/33M2IgYULQN\nNoD8Xp1vuSapM1l38EMbuy0VImK5e4Wa1ZsmWk4sFpclrcmRdAxZ6WxEZAO2mpktxpmbNTkRcT3V\n99gzM0Alnrk5uJmZ5YxwcHNZ0szMcseZWx2peetQy5VhoHNrCjbts9ayFzIDJk/6gJmfzlixtEss\nPuRBCXJwqyO1bM8qGx7Q2M2wJuL+R5f3zjhWqvYYvDyjuVVHLks2dgPMzMyKzZmbmVkOlXrm5uBm\nZpZDpR7cXJY0M7PcceZmZpZDpZ65ObiZmeWNLwVwWdLMzPLHmZuZWc7I17k5uJmZ5VGpBzeXJc3M\nLHecuZmZ5VCpZ24ObmZmOVTqwc1lSTMzyx1nbmZmeePr3BzczMzyyGVJMzOznHHmZmaWM76I28HN\nzCyXSj24uSxpZma548zNzCyPSjtxc3AzM8sduSzpsqSZmeWOMzczsxwq9czNwc3MLIdKPbi5LGlm\nZrnjzM3MLGd8EbeDm5lZPpV2bHNZ0szM8seZm5lZ3vg6Nwc3M7M8KvXg5rKkmZnljjM3M7McKvXM\nzcHNzCyPSju2uSxpZmb548zNzCyHXJY0M7NckTxCicuSZmaWO87czMxyqNQzNwc3M7McKvXg5rKk\nmZnljjM3M7M8Ku3EzcHNzCyPXJY0MzPLGWduZmZ541veOLiZmeWNgBKPbS5LmplZ/jhzMzPLHQ+/\n5eBmZpZDJR7bXJY0M7P8ceZmZpZDLkuamVm+yGVJlyXNzCx3nLmZmeWMgLKy0k7dnLmZmdkKkXSS\npFclvSLpNkmtJHWW9Iikt9PfTgXLnyFpoqQ3JQ0pmN5f0stp3hVagROHDm5mZjkkFe9R837UAzgB\nGBARmwLNgIOA04HHImID4LH0GkmbpPl9gaHA1ZKapc1dAxwDbJAeQ+t6/A5uZmY5JKloj1poDrSW\n1BxoA0wF9gJuTvNvBvZOz/cChkfEvIh4D5gIbC2pG9AhIkZHRAC3FKyz3BzczMxsWbpIGlPwOLZi\nRkRMAS4CPgSmAXMi4mGga0RMS4t9BHRNz3sAkwq2PTlN65GeV51eJ+5QYmaWN8W/FGBGRAxY6q6y\nc2l7AesCs4E7JR1SuExEhKQoaouWwcHNzCxnsrsCNFhvyZ2A9yJiOtl+/wVsC3wsqVtETEslx0/S\n8lOAtQrW75mmTUnPq06vE5clzcxsRXwIDJTUJvVuHAy8DtwHHJ6WORy4Nz2/DzhI0iqS1iXrOPJ8\nKmF+Jmlg2s5hBessNwc3q7Tzthvz4j1n88q9v+GUI3deYn6Hdq2467LjeO720xl711kcuufAynlv\nPPhbXrjjTEYPP52n/vGryumb9enBf28+uXL6gL7rAHDQrgMYPfz0yscXY69gsz51Lq9bI3jisYf5\n7jabscNWfbn68guXmD/x7TfZZ+gO9OnekeuuunSJ+eXl5Xxvx4Ec9YN9K6c9eO/d7Lzdlqy7ehte\nGj+2cvqEcS+w66Bt2HXQNgzdYWtGPljn77wSUbzOJMvKACPiOeAuYBzwMllcuQ64ANhZ0ttk2d0F\naflXgTuA14CRwE8jojxt7ifAX8k6mbwDjKjrO+CypAHZBZ+XnX4Aux1/FVM+ns1T/ziVB/77Mm+8\n+1HlMscd8B3eePcj9jvxL3Tp1I4X7zmb4Q+9wIKF2edy6LGX8+nsLxbb7rAT92bYdSN4+OnXGLL9\nJgw7cW+GHHM5w0eMYfiIMQD07d2dOy45hpfeqnMFwhpYeXk555x2Irfe9SBrdu/Bnjtvz85Dd2eD\nDTeuXGbVVTtx7h8u5uER9y91G3/7y1X03mBD5n7+eeW0DTfuy7U3DefMk3+22LIbbtSX+x99mubN\nm/PJR9PYddA27DRkN5o391dYdRpy+K2I+A3wmyqT55FlcUtbfhgwbCnTxwCbFqNNztwMgK027cU7\nk2bw/pRPWbCwnDtHjWP3QZsttkwA7dquAkDb1qswa86XLCxfVON2I6BD21YAdGzXmmnT5yyxzAFD\n+3PnqHHFORBrEBPGvcA6667P2r3WpWXLluyxz/48POKBxZbpsvoabL7lAJo3b7HE+tOmTuY/j4zk\noEOOXGx67z4bsf4GfZZYvnWbNpWBbN68eSU/KLAtm3/2GADd1+jI5I9nVb6e8vEstt6012LLXDv8\nv9x12XG8+/Aw2rdtxaGn3Uh2OQpEBA9e+3PKyxdxw91Pc+O/ngbg1Ivu4v4//5TzT9qHsjKx4xEX\nL7Hv/XbZkv1Puq7+Ds6K7uNpU+ne/Ztz/92692DC2Odrvf55Z53KGb8Zxty5c2u9zvixz/OrE37M\nlMkfcsnVNzhrW4ZS/wHgzM1qbedtN+alNyez3i5nsc1B53Pp6fvTPmVlg4+8lIEHXcDeP7ua4w78\nNtttuT4Ax+7/bX518b/YYNez+dVFd3PNbw5ebJtbbboOX369gNfembbE/iyfHhv1EKt1WYNvbbHl\ncq3Xr//WPPL0OO575CmuuexCvv7663pqYQ4UcXSSphojGyy4SXqmjuttISkkDS2YtqqknxS87iXp\nhyvQtickLfUajlIx9ZM59OxaOfQbPbp2YkqVEuKhew7k3v+8CMC7qYS5Ya/susypadnps+Zy339e\nYqu+vQA4ePdt+PdjEwC4+5HxlR1KKuw/pD93jBxTL8dk9adrt+5MnfrN9bbTpk6ha7fadQga8/yz\nPDryAbbrtyE/P/YwnnnqCU788ZHLXjHp3Wcj2rRtx1uvv7rc7bbS0WDBLSK2reOqPwCeSn8rrErW\nq6ZCL6DOwc1gzKsf0Hvt1Vmn+2q0aN6M/YdsyYNPvLTYMpM+msWgrTcEYI3O7enTqyvvTZlBm1Yt\nadcmOxfXplVLdvq/jXj1nakATJs+h2/33wCAQVv3YeKH0yu3J4nv77Ild44aizUtm/cbwPvvTmTS\nB+8zf/587r/nTnYeulut1j3t7N8x+uV3eHr8m1x53S1su/0gLrv2bzWuM+mD91m4cCEAkyd9wDtv\nv0nPtdepcZ1SVnGdWwMOv7XSabCitaS5EdEuXcx3O9Ah7f/4iHiymnUE7A/sDDwpqVVEfE3WpXR9\nSROAR4BvAxun1zcD9wB/B9qmTf0sIp5J2zwNOARYBIyIiNML9lcG3AhMjohfF/cdWLmVly/ipD/e\nwf1X/5RmZeLme0fz+rsf8aP9tgfgr3c9xQXXj+S63x7CC3eciQRnXX4vn87+gl49VuP2S44BoHmz\nZtw+YgyPPPM6AD/93T+58NT9aN68jHnzFvKz399Wuc/tt+zN5I9m8f6UTxv+gG2FNG/enPMuuJTD\n9t+D8kXlHPDDw+mz0Sbc+rfrATjkyGP45OOP2HOn7Zj7+eeorIwb/3IVjzwznvbtO1S73ZEP3su5\np/+SmZ/O4Kgf7svGm27G3++8nxeee4ZrLr+I5i1aUKYyfnfh5XRerUtDHW6T1ERjUtGookNAve/o\nm+B2MtAqIoalkaDbRMTn1ayzHXBeRAyW9E/g7oi4W1Iv4IE0AjWSBgGnRMTu6XUbYFFEfC1pA+C2\niBggaVfgbGCniPhSUueImCnpCbIRq38BvJK6qS6tPccC2ZhqLdr1b9X38KUtZraENx69qLGbYE3E\nHoO346UJY1coNLXtsWFsfPy1xWoSY8/+7tjqht9aWTVGd6MXgBsltQD+HRETalj2B8Dw9Hw42RXr\nd9diHy2AqyRtAZQDFX2LdwL+FhFfAkTEzIJ1/gLcUV1gS8tfR3ZxImVt1mjQcdLMzJZHUy0nFkuD\n95aMiP8B3yEbM+wmSYctbbmU1X0fOEfS+8CVwFBJ7Wuxm5OAj4HNgQFAy1qs8wywo6RWtVjWzGyl\n5t6SDUzSOsDHEXE92TAr1fUHHgy8FBFrRUSviFiHLGvbB/gcKAxyVV93BKZFxCLgULKb50F2fu7I\nVLZEUueCdW4AHgLuSPckMjOzJqoxrnMbBLwoaTxwIHB5Ncv9gKxjSKG7gR9ExKfA08puaX4h8BJQ\nLulFSScBVwOHS3oR2Aj4AiAiRpIN2jkmdT45pXDjEXEJMB74e+pcYmbW9Mi9JRssQ4mIdunvzXxz\nd9aall/iwpeIuI8sOBERVbv+f7fK68Kxo04r2MYFpAE8C6YNKnhedXw0M7MmJbsUoLFb0bicnZiZ\nWe6sFOeWJD0HrFJl8qER8XJjtMfMrGlruuXEYlkpgltEbNPYbTAzy5MSj20uS5qZWf6sFJmbmZkV\nl8uSZmaWL0344uticVnSzMxyx5mbmVnOVNzyppQ5uJmZ5VCpBzeXJc3MLHecuZmZ5VCJJ24ObmZm\neeSypJmZWc44czMzyxtf5+bgZmaWN/LAyS5LmplZ/jhzMzPLoRJP3BzczMzyqKzEo5vLkmZmljvO\n3MzMcqjEEzcHNzOzvJF8EbfLkmZmljvO3MzMcqistBM3BzczszxyWdLMzCxnnLmZmeVQiSduDm5m\nZnkjsvElS5nLkmZmljvO3MzMcsi9Jc3MLF/kW964LGlmZrnjzM3MLIdKPHFzcDMzyxvhW964LGlm\nZrnjzM3MLIdKPHFzcDMzyyP3ljQzM8sZZ25mZjmT3ay0sVvRuBzczMxyyL0lzczMcqbazE1Sh5pW\njIjPit8cMzMrhtLO22ouS74KBIu/RxWvA1i7HttlZmYroNR7S1Yb3CJirYZsiJmZWbHU6pybpIMk\nnZme95TUv36bZWZmdZUNv1W8R1O0zOAm6SpgR+DQNOlL4Nr6bJSZma2AdMubYj2aotpcCrBtRGwp\naTxARMyU1LKe22VmZlZntQluCySVkXUiQdJqwKJ6bZWZma2QJppwFU1tgtufgbuB1SX9FjgA+G29\ntsrMzFZIUy0nFssyg1tE3CJpLLBTmrR/RLxSv80yMzOru9oOv9UMWEBWmvSoJmZmK7GK3pKlrDa9\nJc8CbgO6Az2Bf0o6o74bZmZmdefekst2GNAvIr4EkDQMGA+cX58NMzMzq6vaBLdpVZZrnqaZmdlK\nqmnmW8VTbVlS0qWSLgFmAq9K+quk64GXgRkN1UAzM1s+UnbLm2I9lr0/rSrpLklvSHpd0v9J6izp\nEUlvp7+dCpY/Q9JESW9KGlIwvb+kl9O8K7QCNdGaMreKHpGvAg8WTB9d152ZmVkuXQ6MjIj90iAf\nbYAzgcci4gJJpwOnA6dJ2gQ4COhL1pfjUUl9IqIcuAY4BngOeAgYCoyoS4NqGjj5hrps0MzMGl9D\n9QOR1BH4DnAEQETMB+ZL2gsYlBa7GXgCOA3YCxgeEfOA9yRNBLaW9D7QISJGp+3eAuxNsYNbQcPX\nB4YBmwCtKqZHRJ+67NDMzHJlXWA68DdJmwNjgV8AXSOion/GR0DX9LwHi1cAJ6dpC9LzqtPrpDbX\nrN0E/I3s/OSuwB3A7XXdoZmZ1b8iXwrQRdKYgsexBbtqDmwJXBMR/YAvyEqQlSIiSEM4NpTaBLc2\nETEKICLeiYhfkwU5MzNbSUnFewAzImJAweO6gl1NBiZHxHPp9V1kwe5jSd2ytqgb8EmaPwUovF9o\nzzRtSnpedXqd1Ca4zUsDJ78j6ceS9gDa13WHZmaWHxHxETBJ0oZp0mDgNeA+4PA07XDg3vT8PuAg\nSatIWhfYAHg+lTA/kzQw9ZI8rGCd5Vab69xOAtoCJ5Cde+sIHFXXHZqZWf0StevCX0Q/B/6Rekq+\nCxxJljzdIelo4AOyQfeJiFcl3UEWABcCP009JQF+QnYqrDVZR5I6dSaB2g2cXJFqfs43Nyw1M7OV\nlRqutyRAREwABixl1uBqlh9GlixVnT4G2LQYbao2uEm6hxpOAEbEvsVogJmZWbHVlLld1WCtaIL6\nbbw2Tz/nt8hqp+ePhjd2E6yJmD15VlG201QHPC6Wmi7ifqwhG2JmZsVT6vcmK/XjNzOzHKrtzUrN\nzKyJEC5L1jq4SVoljQVmZmYrOd+JexkkbS3pZeDt9HpzSVfWe8vMzMzqqDbn3K4Adgc+BYiIF4Ed\n67NRZma2YspUvEdTVJuyZFlEfFClflte3cJmZta4sjEhm2hUKpLaBLdJkrYGQlIzsmFW3qrfZpmZ\nmdVdbYLb8WSlybWBj4FH0zQzM1tJNdVyYrHUZmzJT8huCW5mZk1EiVcla3Un7utZyhiTEXHsUhY3\nMzNrdLUpSz5a8LwVsA8wqX6aY2ZmK0rQ0Le8WenUpix5e+FrSX8Hnqq3FpmZ2Qor9bEV63L86wJd\ni90QMzOzYqnNObdZfHPOrQyYCZxen40yM7MVU+JVyZqDm7KrADcHpqRJiyKi2huYmplZ45NU8ufc\naixLpkD2UESUp4cDm5mZrfRqc85tgqR+9d4SMzMrmmwIruI8mqJqy5KSmkfEQqAf8IKkd4AvyHqZ\nRkRs2UBtNDOz5eQRSqr3PLAlsGcDtcXMzKwoagpuAoiIdxqoLWZmVgS+iLvm4La6pF9WNzMiLqmH\n9piZWRGUeGyrMbg1A9qRMjgzM7OmoqbgNi0izmuwlpiZWXE04TtoF8syz7mZmVnToxL/Cq/pOrfB\nDdYKMzOzIqo2c4uImQ3ZEDMzK46st2Rjt6Jx1eZ+bmZm1sSUenAr9Vv+mJlZDjlzMzPLIZX4hW4O\nbmZmOeNzbi5LmplZDjlzMzPLmyZ8q5picXAzM8uhUh842WVJMzPLHWduZmY54w4lDm5mZrlU4lVJ\nlyXNzCx/nLmZmeWOKCvxuwI4uJmZ5YxwWdJlSTMzyx1nbmZmeeM7cTu4mZnlkS/iNjMzyxlnbmZm\nOeMOJQ5uZma55LKkmZlZzjhzMzPLoRJP3BzczMzyRrgsV+rHb2ZmOeTMzcwsbwQq8bqkg5uZWQ6V\ndmhzWdLMzHLImZuZWc5kd+Iu7dzNwc3MLIdKO7S5LGlmZjnkzM3MLIdKvCrp4GZmlj8q+UsBXJY0\nM7PcceZmZpYzHn7Lwc3MLJdcljQzM8sZBzer9PCokWzWd0P6btSbC/90wRLzI4JfnngCfTfqzVb9\nNmP8uHGV82bPns0PDtyPzTfdiC2+tTGjn30WgDNOO5XNN92IrfptxgH77cPs2bMB+PTTTxmy0450\nWbUdJ57ws4Y5QCuq735rTUaf/z2e/+NunLDbxkvM79imBTf/fHv++7uhPHzOzmzUoyMAq7Qo4+Fz\nduaJ84Yt2vRfAAAYyUlEQVTw1LBdOW3vTSvX+evx2/L4eUN4/LwhjLtoDx4/bwgALZqVccXRW/O/\n3w3lifOGsN1GazTMQTZhKuKjKXJwMwDKy8s58YSfcu/9Ixj/0mvcOfw2Xn/ttcWWGTVyBO9MfJtX\nXn+bq665jhN+dnzlvFNO+gW77DKUF195g+fHvshGG2dfdoN32pmxE17hhfEvscEGfbjwj+cD0KpV\nK84593ec/8eLGu4grWjKJP546AAOvOS/bHfmCPbdZm36dO+w2DIn7bEJr3w4ix3OHslPrh/NHw7e\nEoB5Cxaxzx8fZ9A5oxh0zki++61u9F9/NQB+dM0z7HjOKHY8ZxQPjJnEg2MmA3DooPUA+M7ZI9nv\nwic476AtSr6re43SwMnFetRql1IzSeMlPZBed5b0iKS3099OBcueIWmipDclDSmY3l/Sy2neFVqB\n2qqDmwHwwvPPs/76vVl3vfVo2bIl+x94EA/cf+9iyzxw37388JDDkMQ2AwcyZ85spk2bxpw5c3jq\nqf9xxFFHA9CyZUtWXXVVAHbaeReaN89O7W69zUCmTM6+rNq2bct2229Pq1atGvAorVi2XK8z7338\nOR9M/4IF5Yu457kP2bVfj8WW2bB7R558/RMAJk77nLW6tGX1DqsA8MW8hUCWkbVoJiJiiX3stdXa\n/Ou5D5bY1ozP5zHnywVs0atzvR2f1ckvgNcLXp8OPBYRGwCPpddI2gQ4COgLDAWultQsrXMNcAyw\nQXoMrWtjHNwMgKlTp9Cz51qVr3v06MmUKVOWuczUKVN4/7336NJldY49+kgGDujH8cf+iC+++GKJ\nfdxy040MGbpr/R2ENZhunVozdeaXla+nzvqKbp1aL7bMKx/OZvf+PQHot25n1lqtDd07tQGyzO/x\n84bw+hV788SrHzPu3ZmLrft/fVZn+mdf8+7HcwF49cPZDO3XnWZlYu0ubdm8Vyd6rNamPg+xSavo\nLVmsxzL3J/UEdgP+WjB5L+Dm9PxmYO+C6cMjYl5EvAdMBLaW1A3oEBGjI/u1c0vBOsvNwc1W2MKF\nC5kwfhzHHHc8o8eMp03btlxU5ZzdH88fRrPmzTnohwc3UiutoV3+4Gt0aNOCx88bwjE79+HlD2ZR\nnjK0RRHseM4oNvvlfWy5XufK83EV9h34TdYG8I8n32XazK949NxdGPbDfjz/9gzKFy2Z7dk3Grgs\neRnwK2BRwbSuETEtPf8I6Jqe9wAmFSw3OU3rkZ5XnV4n9XYpgKRnImLb5VznfWBsRHw/vd4P2D0i\njih+C6ttw7nA3IgoqZNB3bv3YPLkbz5vU6ZMpkePHstcpnuPHkiiR8+ebL3NNgDs8/39uLgguP39\n5pt46MEHGPHwYyXfPTkvps36iu6dv8mcundqzbRZXy22zNyvF3LCDc9Xvh530R68/8ncxZb57MsF\nPPX6Jwz+1pq8MWUOAM3KxG7912LwuaMqlytfFPz6tvGVrx86ayfe+ejzoh6T1aiLpDEFr6+LiOsA\nJO0OfBIRYyUNWtrKERGSGvTXSL1lbssb2Ar0TzXZ5SbJ1+3V0YCttmLixLd5/733mD9/PnfePpzd\ndt9zsWV222NP/nnrLUQEz40eTYcOHenWrRtrrrkmPXuuxVtvvgnAE/95jI02zv4JHx41kksu/hN3\n3XMfbdq4jJQX49+byXpd27N2l7a0aFbGPtuszcjxi5exO7RpQYtm2VfMoTusx7NvfsLcrxeyWvtV\n6NCmBQCtWjRjh75r8va0bwLVDn27MnHaZ4sFy9Ytm9GmZbPK+eWLFvHW1M/q+zCbtCL3lpwREQMK\nHtcV7Go7YM+UnAwHvivpVuDjVGok/f0kLT8FWKtg/Z5p2pT0vOr0OqnPzG1uRLRLB3U70CHt7/iI\neLKGVS8GzgIWq19J6gzcCKwHfAkcGxEvpUxr/TT9Q0mjyOq0bclOSF4EtAQOBeYB34uImZKOAY5N\n8yYCh0bEl9RA0rFpHdZae+3avhVNQvPmzbn08qvYY7chlJeXc/gRR7FJ375c/5drATjmuB8zdNfv\nMWrEQ/TdqDdtWrfhL3/9W+X6l1x2JUcedjDz58+n13rrcV2ad9Ivfsa8efPYfejOQNap5Mqrs21u\n2LsXn3/2GfPnz+f++/7NAw89zMab1Ol3jTWw8kXB6beO5c5TdqCsrIx/Pvkub079jCN2XB+Amx5/\nhz7dOvDnY7YhAt6YModf3JhlcV07tuKqYwbSrEyUCe59fhIPvzi1ctv7bLPOYiVJgC4dWnHnyTuw\nKIJps77i+OtGN9zBNlENVSSJiDOAM7J9ahBwSkQcIulC4HDggvS3oofafcA/JV0CdCf7nn4+Isol\nfSZpIPAccBhwZV3bpaX1UiqGguB2MtAqIoalHjFtImKp9YQU+bcBngD2ALYglSUlXUn26+G3kr4L\nXBIRW6TgtgewfUR8JekI4NdAP6AVWeA6LSKulXQp8EFEXCZptYj4NO3398DHEXFlbcuS/fsPiKef\nG1PTImaVev5oeGM3wZqI2Q+cycIZ765QaOrdd/O4ePioZS9YS3tv1m1sRAxY1nIFwW13SasBdwBr\nAx8AB0TEzLTcWcBRwELgxIgYkaYPAG4CWgMjgJ9HHYNUQ5TxXgBulNQC+HdETFjG8uXAhWS/BEYU\nTN8e+D5ARPxH0mqSKi6suS8iCgv+j6cA+rmkOcD9afrLwGbp+aYpqK0KtAOK90kwM2tEWW/Jhj+/\nHRFPkCUnpORhcDXLDQOGLWX6GGDTJddYfvXeWzIi/gd8h6x2epOkw2qx2t/TOmsta8Gkar/zeQXP\nFxW8XsQ3Af0m4GcR8S3gt2RZnpmZ5UC9BzdJ65CV/K4nuwZiy2WtExELgEuBkwomP0k6D5dS3xkR\nsSJnlNsD01JG6f7pZpYrUvEeTVFDlCUHAadKWgDMJTtJWBs3kJ07q3AuWXnzJbIOJYevYLvOJjtp\nOT39bb+C2zMzW0kINdlRIYuj3oJbRLRLf2/mm6vUl7VOr4Ln88h60lS8nslSrlaPiHOrvL6JrOS4\ntG1WzouIa8iGeqlxe2Zm1vT4ujAzsxxqquXEYmmU4CbpOWCVKpMPjYiXG6M9ZmZ50li9JVcmjRLc\nImKbxtivmZmVBpclzczypgn3ciwWBzczsxwq9eDmW96YmVnuOHMzM8shX+dmZma5IqCstGOby5Jm\nZpY/ztzMzHLIZUkzM8sd95Y0MzPLGWduZmY55LKkmZnlintLuixpZmY55MzNzCx3fLNSBzczs7zx\nwMkuS5qZWf44czMzy6EST9wc3MzM8ibrLVna4c1lSTMzyx1nbmZmOVTaeZuDm5lZPpV4dHNZ0szM\ncseZm5lZDvkibjMzy50S7yzpsqSZmeWPMzczsxwq8cTNwc3MLJdKPLq5LGlmZrnjzM3MLGeEe0s6\nuJmZ5Y1veeOypJmZ5Y8zNzOzHCrxxM3Bzcwsl0o8urksaWZmuePMzcwsd+Teko3dADMzKz73ljQz\nM8sZZ25mZjkjSr4/iYObmVkulXh0c1nSzMxyx5mbmVkOubekmZnljntLmpmZ5YwzNzOzHCrxxM3B\nzcwsd3wtgMuSZmaWP87czMxyyL0lzcwsV4R7S7osaWZmuePMzcwsh0o8cXNwMzPLpRKPbi5LmplZ\n7jhzMzPLIfeWNDOz3HFvSTMzs5xx5mZmlkMlnrg5uJmZ5VKJRzeXJc3MLHecuZmZ5Ux2U4DSTt2c\nuZmZ5Y2y3pLFetS4K2ktSY9Lek3Sq5J+kaZ3lvSIpLfT304F65whaaKkNyUNKZjeX9LLad4VUt37\nfDq4mZnZilgInBwRmwADgZ9K2gQ4HXgsIjYAHkuvSfMOAvoCQ4GrJTVL27oGOAbYID2G1rVRLkvW\n0bhxY2e0bqEPGrsdK5kuwIzGboQ1Gf68LN06xdhIQxUlI2IaMC09/1zS60APYC9gUFrsZuAJ4LQ0\nfXhEzAPekzQR2FrS+0CHiBgNIOkWYG9gRF3a5eBWRxGxemO3YWUjaUxEDGjsdljT4M9LPWuEU26S\negH9gOeArinwAXwEdE3PewCjC1abnKYtSM+rTq8TBzczM1uWLpLGFLy+LiKuK1xAUjvgbuDEiPis\n8HRZRISkaJimZhzczMxyR8XuLTmjpixbUguywPaPiPhXmvyxpG4RMU1SN+CTNH0KsFbB6j3TtCnp\nedXpdeIOJVZM1y17EbNK/rzUowbsLSngBuD1iLikYNZ9wOHp+eHAvQXTD5K0iqR1yTqOPJ9KmJ9J\nGpi2eVjBOsvNmZsVTdUyhVlN/HnJje2AQ4GXJU1I084ELgDukHQ08AFwAEBEvCrpDuA1sp6WP42I\n8rTeT4CbgNZkHUnq1JkEHNzMzHJHNGhvyadq2N3gatYZBgxbyvQxwKbFaJeDm5lZHpX2ACU+52Zm\nZvnjzM1WCpIUEQ3aVdiaFkmdgS4R8VZjt6Up8NiSZo1I0lqQXQfT2G2xlZekVsAJwFGSNm7s9jQF\nDdVbcmXl4GYNSlI7SS3T842BP0lq38jNspVcRHwNPJpe7p/GJzSrloObNRhJbYF/APunSV+mx9x0\nEWjFNTNmlSo+E6lX3n1AB2A/B7iaqYiPpsjBzRpMRHwB3A4cKelAoBfwVWQWpGVcnrRKFediJa0r\nqXlEPAP8DehIFuBcorSlcocSaxCSmkVEeUT8U9J0stHBxwLrSrqcbJDUeUDzKqMcWAlLgW034Gzg\nSUlzgcvIRjc5GjhE0j8i4rXGbOdKpwmfKysWZ25W79Kv73JJO0v6U0Q8AlxOdoHnfODD9Lcd2Wji\nZgBIGgj8ATiQ7Mf43sCfgOlkt1FpS/bZsSWUdmHSmZvVu/TrezBwNXBcmna/pIXAL4G3IuL+xmyj\nrVwklQFBds+3w4CNgO+Q3fDyWOAisuz/rFTuNluMMzerV8o0J7uj7tkR8Z+K3pIRMQK4FjhNUp3v\n22T5UdChqF06F/tARLxIlrH9KCJGkY0u35zsfmEObEshfCmAg5vVq/QFtRD4GhgoqVVEzAeQtBXw\nELBnRNT51haWHwXn2B6TdK6kfdOsNYBjJW0DbA1cFBGvNFpDm4DSLko6uFk9qPj1LWltSRX3ZxoB\ntAB2SPM2By4F+kTEzEZpqK100n2/DiYrO84EhqRgdxTZPcDOAc6PiJcar5XWFPicmxVdwa/v84Fn\nJHWOiANSt+1DJZ1G1pX796nkZIakAcDmwJSIuF3S6sAQYB+gRUTsLqlNRHzp4dqWramWE4vFwc2K\npuCapIFkPdp2J8vUbpT0aETsJOkmsi+wORHxjr+kDEDSILLej6PIuvffFhHjJI0AWgJ7SXo+IqaC\nr4esjVIfW9LBzVZYGvdvQeru3xX4lOzGhBuQ9Y7sCDwh6ZmI2BYYV7Guv6Qs3Y35TODQiPifpInA\nrZIOjojxku4FRlYENrPa8Dk3WyGpy/a2wImSdic7J/I52V12dwNujIjPyX6Vr506kViJKzgvuxVZ\ndt+RrEckEfEn4AbgPkn9I+JTB7Y6KPEeJQ5uVgwvAbsAfwfuioiPyP6XmAasL+kYshLlzhHxQuM1\n01YWqXz9HbLy9ctkF2q3kfSzNP9i4M9kF/ZbHZR4bHNws7qR1FZSz4hYBKyTJj8O7Jq6+y8iG8X9\nS7LAdm1EvN5IzbWVjKQNgeOBmyJiLPAE8BiwkaSTASLigoj4rwfTtrrwOTerq17A7yWNATYFTgZm\nkY0BeAnwE+BdsoD3h4hY6M4jVuBbQFdgJ0kPRcR0SSPJLhcZJGmdiPgAfF62LpryxdfF4szN6iQi\nXgUmknUEeC5dUDudbIitVSQ9RvZrfEG6iNtfUiWs4BxbT0kdI+Iush9Cn5GN7r9aOjd7P3BORWCz\nulMR/2uKHNys1iStKqlNwaRXgIuBwyQNjoj56eLas4CbgJMiYnQjNNVWIpLK0jm2Xcku5r9B0v+A\n14EHgIrrH1eLiM/TOVuzFeKypNWKpM7AW8Cjkp6MiD9HxM1p3iTgEkmHA7OBfStuW+NSZOmS1Doi\nvoqIRZJ6A78DjouIZyRdAfyb7CLtFulvW7LLSKwYmmbCVTQOblZbs4CHyXpAHixpa+Ap4M6IuF7S\nfOBuYCFwYsVKDmylSVJH4AJJ90TEw2Q/et4g+4FERJwg6Tbg9Ij4jaQXImJaIzY5d0o8trksabWT\ngtQ4sk4A3yErO34H+K+kHck6jmwDfD+N9m+lrQPZOdkfptsdfQasBuxUsMxDpHuxObBZsTlzs1qL\niIskPUT2BfUKsAXZr/GDgN7AgR6pvbRJap/Om02SdAvZZ+Moss5GZwI3SdoImJOm/6rxWptvpd5b\n0sHNakVSs4goJ8vY9iEb0f+GFPDWIBvYdkZjttEal6RewF2SxgJ3AG8DfwPmkV0q8kdgf2BXoDtZ\nh6NHfV62PjTdXo7F4uBmtZICG8BzwLnAsxFxUZo23V9OBrQCugF7Ae+TjTByLdAJeIas6/+wiLi8\ncCV/dqw++Jyb1Vr6hf0B8EugXcXds/3lZKm7/xtkJes5wIfAgcBUsrEj90uv/5QuKfF3Tz3ynbid\nuVkVBbetKUtDaFUqCGKTgUVLrm2lKnX3L4uI1yUdAgwnG5nmBkl3kd0hYi9gQkTMbtTGWklwcLNK\nBYFtMFlmNioivq66XES8Ium0iJjSCM20lVRBgHtB0kHAbWmc0T8Db5INkuxrH61BuDRgQGWHkZA0\nFLgGmLW0wKZMWUR8IKmNpNUavrW2sioMcGRlyLMl/bTKMg5sDaDUy5IObiVOUu/UfbtcUieyk/4/\nTjeN/Lakw9MF2xXK0hfYqmTXtnVulIZboyoYK3KJ75CCADcW2AN4taHbZx5b0mVJ6wqsIWl0RMyS\n9DhwdLoHWxmwgOx8yfOSmqfR/TsCdwKnRsTbjdd0awy1KV9XyeBcirQG58ytxEXE02Q3i3xXUgey\n69ieB66MiAPJrlfqK6llCmydgHuA8yLif43VbmsctS1fVyye1mlNdjmANZQiliRdlrQmK91q5Bdk\n1yLNiIjL0+C23yYb7PavETE/Lf4D4PcR8WQjNdcawfKWrysu+k/l6yfIht6yBlLMu3A30djmsqRl\nIuJeSQuAsZL6A1+TXZv064h4sKKsFBFXN25LrZG4fG1NioObVYqIhyQtIrvP1obAaRHxdcE5Fp83\nKVER8bSk9mTl683Iyte7AS+kLH9P4MhUvp6fsru7gd84y28kTTXlKhKXJW0xETES+BHQr+JcSkVA\nc2ArbS5fNy3uLWlWRUQ8CO7hZkty+dqaCgc3q5YDmy2Ny9dNQ1Pt5VgsLkua2XJz+Xrl596SZmZ1\n4PK1rcwc3MxshTiwraSaaspVJA5uZmY51FR7ORaLz7mZmVnuOHMzM8uZijtxlzK5XG55I6mcbDDo\n5mTd1Q+PiC/ruK1BwCkRsXsahWOTiLigmmVXBX64vNd4SToXmBsRF9VmepVlbgIeiIi7armvXmn5\nTZenjda0SBoJdCniJmdExNAibq/eOXOzPPoqIrYAkPQP4MfAJRUz073IFBGLlmejEXEfcF8Ni6wK\n/ATwBczWqJpaIKoPPudmefck0FtSL0lvSroFeAVYS9Iukp6VNE7SnZLaAUgaKukNSeOAfSs2JOkI\nSVel510l3SPpxfTYFrgAWF/SBEkXpuVOlfSCpJck/bZgW2dJekvSU2QXQtdI0jFpOy9KultSm4LZ\nO0kak7a3e1q+maQLC/Z93Iq+kWZNiYOb5Zak5sCuZCVKyEatvzoi+gJfAL8GdoqILYExwC8ltQKu\nJ7uDdH9gzWo2fwXw34jYHNiS7G7TpwPvRMQWEXGqpF3SPrcGtgD6S/pOGrbqoDTte8BWtTicf0XE\nVml/rwNHF8zrlfaxG3BtOoajgTkRsVXa/jGS1q3FfsxywWVJy6PWkiak508CNwDdgQ8iYnSaPhDY\nBHg6q1LSEngW2Ah4r+IWLZJuBY5dyj6+CxwGEBHlwJw0En6hXdJjfHrdjizYtQfuqTgPKKmmUmeF\nTSX9nqz02Q4YVTDvjlRifVvSu+kYdgE2k7RfWqZj2vdbtdiXWZPn4GZ5VHnOrUIKYF8UTgIeiYgf\nVFlusfVWkIDzI+IvVfZxYh22dROwd0S8KOkIYFDBvKq9wiLt++cRURgEKzqUmOWey5JWqkYD20nq\nDSCpraQ+wBtAL0nrp+V+UM36jwHHp3WbpRtzfk6WlVUYBRxVcC6vh6Q1gP8Be0tqne6Rtkct2tse\nmCapBXBwlXn7SypLbV4PeDPt+/i0PJL6SGpbi/2Y5YIzNytJETE9ZUC3SVolTf51RLwl6VjgQUlf\nkpU12y9lE78ArpN0NFAOHB8Rz0p6WtIrwIh03m1j4NmUOc4FDomIcZJuB14EPgFeqEWTzwaeA6an\nv4Vt+hB4HugA/DiN0P9XsnNx41Lv0OnA3rV7d8yaPl/nZmZmueOypJmZ5Y6Dm5mZ5Y6Dm5mZ5Y6D\nm5mZ5Y6Dm5mZ5Y6Dm5mZ5Y6Dm5mZ5Y6Dm5mZ5c7/AwHDdfw2jOgEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1d0ce38d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
