{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:26.940895Z",
     "start_time": "2017-05-13T16:06:26.547929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:27.022150Z",
     "start_time": "2017-05-13T16:06:26.942374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:27.028383Z",
     "start_time": "2017-05-13T16:06:27.023871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:27.046217Z",
     "start_time": "2017-05-13T16:06:27.029831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:27.807128Z",
     "start_time": "2017-05-13T16:06:27.047568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:28.864833Z",
     "start_time": "2017-05-13T16:06:27.808671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:29.361327Z",
     "start_time": "2017-05-13T16:06:28.866549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:06:29.484893Z",
     "start_time": "2017-05-13T16:06:29.362919Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e9):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "\n",
    "                valid_accuracy,valid_loss = sess.run([net.tf_accuracy, net.regularized_loss], #net.summary_op \n",
    "                                                      feed_dict={net.x: preprocess.x_test, \n",
    "                                                                 net.y_: preprocess.y_test, \n",
    "                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:.6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, valid_loss, valid_accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual, net.y], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "            Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:21.917198Z",
     "start_time": "2017-05-13T16:06:29.486502Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.015910 | Test Loss: 0.069591 | Test Accuracy: 0.483100\n",
      "Step 2 | Training Loss: 0.015930 | Test Loss: 0.017508 | Test Accuracy: 0.501331\n",
      "Step 3 | Training Loss: 0.015206 | Test Loss: 0.017145 | Test Accuracy: 0.492282\n",
      "Step 4 | Training Loss: 0.015924 | Test Loss: 0.016964 | Test Accuracy: 0.505367\n",
      "Step 5 | Training Loss: 0.014840 | Test Loss: 0.016877 | Test Accuracy: 0.509980\n",
      "Step 6 | Training Loss: 0.014739 | Test Loss: 0.016727 | Test Accuracy: 0.517965\n",
      "Step 7 | Training Loss: 0.014880 | Test Loss: 0.016639 | Test Accuracy: 0.526703\n",
      "Step 8 | Training Loss: 0.014955 | Test Loss: 0.016585 | Test Accuracy: 0.527857\n",
      "Step 9 | Training Loss: 0.015183 | Test Loss: 0.016489 | Test Accuracy: 0.540099\n",
      "Step 10 | Training Loss: 0.014547 | Test Loss: 0.016403 | Test Accuracy: 0.544092\n",
      "Step 11 | Training Loss: 0.014583 | Test Loss: 0.016372 | Test Accuracy: 0.544003\n",
      "Step 12 | Training Loss: 0.014613 | Test Loss: 0.016314 | Test Accuracy: 0.547951\n",
      "Step 13 | Training Loss: 0.014768 | Test Loss: 0.016243 | Test Accuracy: 0.556379\n",
      "Step 14 | Training Loss: 0.014521 | Test Loss: 0.016224 | Test Accuracy: 0.554471\n",
      "Step 15 | Training Loss: 0.014499 | Test Loss: 0.016178 | Test Accuracy: 0.557709\n",
      "Step 16 | Training Loss: 0.014778 | Test Loss: 0.016153 | Test Accuracy: 0.558508\n",
      "Step 17 | Training Loss: 0.014442 | Test Loss: 0.016119 | Test Accuracy: 0.563698\n",
      "Step 18 | Training Loss: 0.014670 | Test Loss: 0.016091 | Test Accuracy: 0.562367\n",
      "Step 19 | Training Loss: 0.014389 | Test Loss: 0.016028 | Test Accuracy: 0.569109\n",
      "Step 20 | Training Loss: 0.014694 | Test Loss: 0.015954 | Test Accuracy: 0.579223\n",
      "Accuracy on Test data: 0.5770493149757385\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.024334 | Test Loss: 2.641380 | Test Accuracy: 0.614665\n",
      "Step 2 | Training Loss: 0.015270 | Test Loss: 0.073586 | Test Accuracy: 0.613644\n",
      "Step 3 | Training Loss: 0.015879 | Test Loss: 0.016656 | Test Accuracy: 0.617149\n",
      "Step 4 | Training Loss: 0.015526 | Test Loss: 0.016441 | Test Accuracy: 0.619278\n",
      "Step 5 | Training Loss: 0.014994 | Test Loss: 0.016251 | Test Accuracy: 0.626553\n",
      "Step 6 | Training Loss: 0.015474 | Test Loss: 0.016151 | Test Accuracy: 0.622028\n",
      "Step 7 | Training Loss: 0.015200 | Test Loss: 0.016019 | Test Accuracy: 0.627528\n",
      "Step 8 | Training Loss: 0.015302 | Test Loss: 0.015925 | Test Accuracy: 0.623492\n",
      "Step 9 | Training Loss: 0.014892 | Test Loss: 0.015872 | Test Accuracy: 0.623403\n",
      "Step 10 | Training Loss: 0.015382 | Test Loss: 0.015829 | Test Accuracy: 0.620919\n",
      "Step 11 | Training Loss: 0.014967 | Test Loss: 0.015781 | Test Accuracy: 0.618479\n",
      "Step 12 | Training Loss: 0.014756 | Test Loss: 0.015727 | Test Accuracy: 0.622427\n",
      "Step 13 | Training Loss: 0.014641 | Test Loss: 0.015697 | Test Accuracy: 0.620209\n",
      "Step 14 | Training Loss: 0.014810 | Test Loss: 0.015639 | Test Accuracy: 0.627928\n",
      "Step 15 | Training Loss: 0.014872 | Test Loss: 0.015620 | Test Accuracy: 0.622960\n",
      "Step 16 | Training Loss: 0.014578 | Test Loss: 0.015586 | Test Accuracy: 0.624468\n",
      "Step 17 | Training Loss: 0.014627 | Test Loss: 0.015548 | Test Accuracy: 0.625266\n",
      "Step 18 | Training Loss: 0.014678 | Test Loss: 0.015527 | Test Accuracy: 0.623226\n",
      "Step 19 | Training Loss: 0.015014 | Test Loss: 0.015521 | Test Accuracy: 0.624778\n",
      "Step 20 | Training Loss: 0.014545 | Test Loss: 0.015503 | Test Accuracy: 0.625044\n",
      "Accuracy on Test data: 0.6207416653633118\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.017435 | Test Loss: 0.149632 | Test Accuracy: 0.409732\n",
      "Step 2 | Training Loss: 0.017110 | Test Loss: 0.653539 | Test Accuracy: 0.410930\n",
      "Step 3 | Training Loss: 0.019363 | Test Loss: 0.528303 | Test Accuracy: 0.411994\n",
      "Step 4 | Training Loss: 0.016683 | Test Loss: 0.918609 | Test Accuracy: 0.424902\n",
      "Step 5 | Training Loss: 0.016602 | Test Loss: 0.653087 | Test Accuracy: 0.423882\n",
      "Step 6 | Training Loss: 0.016973 | Test Loss: 0.473210 | Test Accuracy: 0.434572\n",
      "Step 7 | Training Loss: 0.016348 | Test Loss: 1.117840 | Test Accuracy: 0.442557\n",
      "Step 8 | Training Loss: 0.016480 | Test Loss: 0.627990 | Test Accuracy: 0.447791\n",
      "Step 9 | Training Loss: 0.016755 | Test Loss: 0.741387 | Test Accuracy: 0.454533\n",
      "Step 10 | Training Loss: 0.016142 | Test Loss: 0.021907 | Test Accuracy: 0.465889\n",
      "Step 11 | Training Loss: 0.015656 | Test Loss: 0.178045 | Test Accuracy: 0.474938\n",
      "Step 12 | Training Loss: 0.015762 | Test Loss: 0.237936 | Test Accuracy: 0.480704\n",
      "Step 13 | Training Loss: 0.015300 | Test Loss: 0.676284 | Test Accuracy: 0.485140\n",
      "Step 14 | Training Loss: 0.015385 | Test Loss: 0.920164 | Test Accuracy: 0.492016\n",
      "Step 15 | Training Loss: 0.015554 | Test Loss: 0.895451 | Test Accuracy: 0.496984\n",
      "Step 16 | Training Loss: 0.015415 | Test Loss: 0.031240 | Test Accuracy: 0.498359\n",
      "Step 17 | Training Loss: 0.015318 | Test Loss: 0.140074 | Test Accuracy: 0.511089\n",
      "Step 18 | Training Loss: 0.014974 | Test Loss: 1.147642 | Test Accuracy: 0.515259\n",
      "Step 19 | Training Loss: 0.015008 | Test Loss: 0.231231 | Test Accuracy: 0.523421\n",
      "Step 20 | Training Loss: 0.015219 | Test Loss: 0.624064 | Test Accuracy: 0.531982\n",
      "Accuracy on Test data: 0.5314052700996399\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.015856 | Test Loss: 0.506806 | Test Accuracy: 0.576473\n",
      "Step 2 | Training Loss: 0.017353 | Test Loss: 0.214579 | Test Accuracy: 0.578602\n",
      "Step 3 | Training Loss: 0.017528 | Test Loss: 1.250336 | Test Accuracy: 0.581308\n",
      "Step 4 | Training Loss: 0.015706 | Test Loss: 0.892542 | Test Accuracy: 0.583880\n",
      "Step 5 | Training Loss: 0.016198 | Test Loss: 1.496530 | Test Accuracy: 0.587163\n",
      "Step 6 | Training Loss: 0.015747 | Test Loss: 0.256251 | Test Accuracy: 0.592353\n",
      "Step 7 | Training Loss: 0.015572 | Test Loss: 0.479072 | Test Accuracy: 0.590534\n",
      "Step 8 | Training Loss: 0.015533 | Test Loss: 0.689994 | Test Accuracy: 0.590091\n",
      "Step 9 | Training Loss: 0.015191 | Test Loss: 1.076999 | Test Accuracy: 0.595591\n",
      "Step 10 | Training Loss: 0.015387 | Test Loss: 0.558672 | Test Accuracy: 0.595236\n",
      "Step 11 | Training Loss: 0.015207 | Test Loss: 1.385179 | Test Accuracy: 0.598785\n",
      "Step 12 | Training Loss: 0.015376 | Test Loss: 1.460062 | Test Accuracy: 0.599450\n",
      "Step 13 | Training Loss: 0.014873 | Test Loss: 0.610620 | Test Accuracy: 0.602555\n",
      "Step 14 | Training Loss: 0.015352 | Test Loss: 0.935827 | Test Accuracy: 0.601269\n",
      "Step 15 | Training Loss: 0.015241 | Test Loss: 0.091647 | Test Accuracy: 0.604729\n",
      "Step 16 | Training Loss: 0.014947 | Test Loss: 0.408951 | Test Accuracy: 0.610318\n",
      "Step 17 | Training Loss: 0.015065 | Test Loss: 0.187540 | Test Accuracy: 0.607390\n",
      "Step 18 | Training Loss: 0.015056 | Test Loss: 1.188640 | Test Accuracy: 0.613156\n",
      "Step 19 | Training Loss: 0.014919 | Test Loss: 0.699090 | Test Accuracy: 0.610229\n",
      "Step 20 | Training Loss: 0.015081 | Test Loss: 1.157652 | Test Accuracy: 0.618834\n",
      "Accuracy on Test data: 0.6171486973762512\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.014921 | Test Loss: 0.015251 | Test Accuracy: 0.507541\n",
      "Step 2 | Training Loss: 0.014711 | Test Loss: 0.015133 | Test Accuracy: 0.504879\n",
      "Step 3 | Training Loss: 0.014846 | Test Loss: 0.015018 | Test Accuracy: 0.508605\n",
      "Step 4 | Training Loss: 0.014431 | Test Loss: 0.014916 | Test Accuracy: 0.512686\n",
      "Step 5 | Training Loss: 0.014490 | Test Loss: 0.014811 | Test Accuracy: 0.521780\n",
      "Step 6 | Training Loss: 0.014738 | Test Loss: 0.014765 | Test Accuracy: 0.526038\n",
      "Step 7 | Training Loss: 0.014662 | Test Loss: 0.014702 | Test Accuracy: 0.529853\n",
      "Step 8 | Training Loss: 0.014927 | Test Loss: 0.014898 | Test Accuracy: 0.531405\n",
      "Step 9 | Training Loss: 0.014555 | Test Loss: 0.014860 | Test Accuracy: 0.534821\n",
      "Step 10 | Training Loss: 0.014552 | Test Loss: 0.014779 | Test Accuracy: 0.541075\n",
      "Step 11 | Training Loss: 0.014633 | Test Loss: 0.014797 | Test Accuracy: 0.537482\n",
      "Step 12 | Training Loss: 0.014519 | Test Loss: 0.014730 | Test Accuracy: 0.542628\n",
      "Step 13 | Training Loss: 0.014587 | Test Loss: 0.014700 | Test Accuracy: 0.547729\n",
      "Step 14 | Training Loss: 0.014626 | Test Loss: 0.014664 | Test Accuracy: 0.552253\n",
      "Step 15 | Training Loss: 0.014491 | Test Loss: 0.014633 | Test Accuracy: 0.551899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.014458 | Test Loss: 0.014624 | Test Accuracy: 0.551011\n",
      "Step 17 | Training Loss: 0.014227 | Test Loss: 0.014577 | Test Accuracy: 0.558064\n",
      "Step 18 | Training Loss: 0.014332 | Test Loss: 0.014578 | Test Accuracy: 0.556600\n",
      "Step 19 | Training Loss: 0.014140 | Test Loss: 0.014557 | Test Accuracy: 0.558330\n",
      "Step 20 | Training Loss: 0.014318 | Test Loss: 0.014509 | Test Accuracy: 0.561125\n",
      "Accuracy on Test data: 0.5601934194564819\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.014720 | Test Loss: 0.018512 | Test Accuracy: 0.500798\n",
      "Step 2 | Training Loss: 0.014568 | Test Loss: 0.014949 | Test Accuracy: 0.502928\n",
      "Step 3 | Training Loss: 0.014541 | Test Loss: 0.014891 | Test Accuracy: 0.504879\n",
      "Step 4 | Training Loss: 0.014559 | Test Loss: 0.014846 | Test Accuracy: 0.506565\n",
      "Step 5 | Training Loss: 0.014419 | Test Loss: 0.014829 | Test Accuracy: 0.506432\n",
      "Step 6 | Training Loss: 0.015025 | Test Loss: 0.014763 | Test Accuracy: 0.514815\n",
      "Step 7 | Training Loss: 0.014368 | Test Loss: 0.014733 | Test Accuracy: 0.519251\n",
      "Step 8 | Training Loss: 0.014375 | Test Loss: 0.014674 | Test Accuracy: 0.522622\n",
      "Step 9 | Training Loss: 0.014351 | Test Loss: 0.014666 | Test Accuracy: 0.525373\n",
      "Step 10 | Training Loss: 0.014431 | Test Loss: 0.014636 | Test Accuracy: 0.529764\n",
      "Step 11 | Training Loss: 0.014300 | Test Loss: 0.014611 | Test Accuracy: 0.529542\n",
      "Step 12 | Training Loss: 0.014156 | Test Loss: 0.014575 | Test Accuracy: 0.534466\n",
      "Step 13 | Training Loss: 0.014370 | Test Loss: 0.014579 | Test Accuracy: 0.531538\n",
      "Step 14 | Training Loss: 0.014284 | Test Loss: 0.014532 | Test Accuracy: 0.536728\n",
      "Step 15 | Training Loss: 0.014419 | Test Loss: 0.014536 | Test Accuracy: 0.533224\n",
      "Step 16 | Training Loss: 0.014184 | Test Loss: 0.014493 | Test Accuracy: 0.542273\n",
      "Step 17 | Training Loss: 0.014328 | Test Loss: 0.014491 | Test Accuracy: 0.546221\n",
      "Step 18 | Training Loss: 0.014065 | Test Loss: 0.014462 | Test Accuracy: 0.541164\n",
      "Step 19 | Training Loss: 0.014113 | Test Loss: 0.014479 | Test Accuracy: 0.542229\n",
      "Step 20 | Training Loss: 0.014183 | Test Loss: 0.014410 | Test Accuracy: 0.553096\n",
      "Accuracy on Test data: 0.5498136878013611\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.014695 | Test Loss: 0.016043 | Test Accuracy: 0.522933\n",
      "Step 2 | Training Loss: 0.014850 | Test Loss: 0.015708 | Test Accuracy: 0.512509\n",
      "Step 3 | Training Loss: 0.014628 | Test Loss: 0.015546 | Test Accuracy: 0.517078\n",
      "Step 4 | Training Loss: 0.014935 | Test Loss: 0.015429 | Test Accuracy: 0.522001\n",
      "Step 5 | Training Loss: 0.014797 | Test Loss: 0.015302 | Test Accuracy: 0.527413\n",
      "Step 6 | Training Loss: 0.014799 | Test Loss: 0.015195 | Test Accuracy: 0.537349\n",
      "Step 7 | Training Loss: 0.014409 | Test Loss: 0.015134 | Test Accuracy: 0.532825\n",
      "Step 8 | Training Loss: 0.014390 | Test Loss: 0.015141 | Test Accuracy: 0.527990\n",
      "Step 9 | Training Loss: 0.014651 | Test Loss: 0.014996 | Test Accuracy: 0.540321\n",
      "Step 10 | Training Loss: 0.014541 | Test Loss: 0.014876 | Test Accuracy: 0.548527\n",
      "Step 11 | Training Loss: 0.014833 | Test Loss: 0.014874 | Test Accuracy: 0.542983\n",
      "Step 12 | Training Loss: 0.014428 | Test Loss: 0.014829 | Test Accuracy: 0.547551\n",
      "Step 13 | Training Loss: 0.014526 | Test Loss: 0.014759 | Test Accuracy: 0.555137\n",
      "Step 14 | Training Loss: 0.014341 | Test Loss: 0.014728 | Test Accuracy: 0.555802\n",
      "Step 15 | Training Loss: 0.014607 | Test Loss: 0.014694 | Test Accuracy: 0.553274\n",
      "Step 16 | Training Loss: 0.014204 | Test Loss: 0.014688 | Test Accuracy: 0.549548\n",
      "Step 17 | Training Loss: 0.014386 | Test Loss: 0.014650 | Test Accuracy: 0.555758\n",
      "Step 18 | Training Loss: 0.014312 | Test Loss: 0.014679 | Test Accuracy: 0.550257\n",
      "Step 19 | Training Loss: 0.014465 | Test Loss: 0.014599 | Test Accuracy: 0.558552\n",
      "Step 20 | Training Loss: 0.014296 | Test Loss: 0.014557 | Test Accuracy: 0.563742\n",
      "Accuracy on Test data: 0.5617902874946594\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.014696 | Test Loss: 0.491759 | Test Accuracy: 0.476890\n",
      "Step 2 | Training Loss: 0.014714 | Test Loss: 0.026986 | Test Accuracy: 0.477555\n",
      "Step 3 | Training Loss: 0.014922 | Test Loss: 0.024881 | Test Accuracy: 0.489265\n",
      "Step 4 | Training Loss: 0.014655 | Test Loss: 0.015644 | Test Accuracy: 0.495076\n",
      "Step 5 | Training Loss: 0.014534 | Test Loss: 0.015462 | Test Accuracy: 0.498625\n",
      "Step 6 | Training Loss: 0.014790 | Test Loss: 0.015313 | Test Accuracy: 0.498048\n",
      "Step 7 | Training Loss: 0.014430 | Test Loss: 0.015233 | Test Accuracy: 0.499024\n",
      "Step 8 | Training Loss: 0.014658 | Test Loss: 0.015147 | Test Accuracy: 0.506033\n",
      "Step 9 | Training Loss: 0.014409 | Test Loss: 0.015083 | Test Accuracy: 0.505678\n",
      "Step 10 | Training Loss: 0.014465 | Test Loss: 0.015006 | Test Accuracy: 0.508428\n",
      "Step 11 | Training Loss: 0.014543 | Test Loss: 0.014922 | Test Accuracy: 0.512509\n",
      "Step 12 | Training Loss: 0.014548 | Test Loss: 0.014868 | Test Accuracy: 0.517388\n",
      "Step 13 | Training Loss: 0.014143 | Test Loss: 0.014843 | Test Accuracy: 0.515259\n",
      "Step 14 | Training Loss: 0.014182 | Test Loss: 0.014777 | Test Accuracy: 0.521558\n",
      "Step 15 | Training Loss: 0.014094 | Test Loss: 0.014705 | Test Accuracy: 0.525594\n",
      "Step 16 | Training Loss: 0.014229 | Test Loss: 0.014682 | Test Accuracy: 0.528034\n",
      "Step 17 | Training Loss: 0.014225 | Test Loss: 0.014679 | Test Accuracy: 0.525373\n",
      "Step 18 | Training Loss: 0.014330 | Test Loss: 0.014633 | Test Accuracy: 0.532337\n",
      "Step 19 | Training Loss: 0.014170 | Test Loss: 0.014620 | Test Accuracy: 0.526348\n",
      "Step 20 | Training Loss: 0.014287 | Test Loss: 0.014572 | Test Accuracy: 0.536773\n",
      "Accuracy on Test data: 0.5420954823493958\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.014387 | Test Loss: 0.014815 | Test Accuracy: 0.474184\n",
      "Step 2 | Training Loss: 0.014659 | Test Loss: 0.014750 | Test Accuracy: 0.479906\n",
      "Step 3 | Training Loss: 0.014764 | Test Loss: 0.014749 | Test Accuracy: 0.481148\n",
      "Step 4 | Training Loss: 0.014498 | Test Loss: 0.014748 | Test Accuracy: 0.479108\n",
      "Step 5 | Training Loss: 0.014737 | Test Loss: 0.014702 | Test Accuracy: 0.481636\n",
      "Step 6 | Training Loss: 0.014448 | Test Loss: 0.014716 | Test Accuracy: 0.480261\n",
      "Step 7 | Training Loss: 0.014376 | Test Loss: 0.014649 | Test Accuracy: 0.487935\n",
      "Step 8 | Training Loss: 0.014459 | Test Loss: 0.014645 | Test Accuracy: 0.486427\n",
      "Step 9 | Training Loss: 0.014749 | Test Loss: 0.014607 | Test Accuracy: 0.492858\n",
      "Step 10 | Training Loss: 0.014461 | Test Loss: 0.014583 | Test Accuracy: 0.494588\n",
      "Step 11 | Training Loss: 0.014484 | Test Loss: 0.014598 | Test Accuracy: 0.490507\n",
      "Step 12 | Training Loss: 0.014465 | Test Loss: 0.014585 | Test Accuracy: 0.491971\n",
      "Step 13 | Training Loss: 0.014339 | Test Loss: 0.014571 | Test Accuracy: 0.492903\n",
      "Step 14 | Training Loss: 0.014486 | Test Loss: 0.014500 | Test Accuracy: 0.501774\n",
      "Step 15 | Training Loss: 0.014643 | Test Loss: 0.014543 | Test Accuracy: 0.499379\n",
      "Step 16 | Training Loss: 0.014540 | Test Loss: 0.014489 | Test Accuracy: 0.501331\n",
      "Step 17 | Training Loss: 0.014681 | Test Loss: 0.014506 | Test Accuracy: 0.498093\n",
      "Step 18 | Training Loss: 0.014308 | Test Loss: 0.014452 | Test Accuracy: 0.505944\n",
      "Step 19 | Training Loss: 0.014405 | Test Loss: 0.014493 | Test Accuracy: 0.501375\n",
      "Step 20 | Training Loss: 0.014311 | Test Loss: 0.014457 | Test Accuracy: 0.504081\n",
      "Accuracy on Test data: 0.4980926215648651\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.014097 | Test Loss: 0.014410 | Test Accuracy: 0.532381\n",
      "Step 2 | Training Loss: 0.014160 | Test Loss: 0.014375 | Test Accuracy: 0.535309\n",
      "Step 3 | Training Loss: 0.014204 | Test Loss: 0.014387 | Test Accuracy: 0.532159\n",
      "Step 4 | Training Loss: 0.014295 | Test Loss: 0.014378 | Test Accuracy: 0.532603\n",
      "Step 5 | Training Loss: 0.014385 | Test Loss: 0.014315 | Test Accuracy: 0.538502\n",
      "Step 6 | Training Loss: 0.014372 | Test Loss: 0.014364 | Test Accuracy: 0.531982\n",
      "Step 7 | Training Loss: 0.014452 | Test Loss: 0.014309 | Test Accuracy: 0.538591\n",
      "Step 8 | Training Loss: 0.014301 | Test Loss: 0.014297 | Test Accuracy: 0.541297\n",
      "Step 9 | Training Loss: 0.014455 | Test Loss: 0.014303 | Test Accuracy: 0.535131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.014507 | Test Loss: 0.014271 | Test Accuracy: 0.542716\n",
      "Step 11 | Training Loss: 0.014063 | Test Loss: 0.014264 | Test Accuracy: 0.540410\n",
      "Step 12 | Training Loss: 0.014128 | Test Loss: 0.014275 | Test Accuracy: 0.537837\n",
      "Step 13 | Training Loss: 0.014180 | Test Loss: 0.014285 | Test Accuracy: 0.538369\n",
      "Step 14 | Training Loss: 0.014377 | Test Loss: 0.014223 | Test Accuracy: 0.544313\n",
      "Step 15 | Training Loss: 0.014399 | Test Loss: 0.014226 | Test Accuracy: 0.540543\n",
      "Step 16 | Training Loss: 0.014073 | Test Loss: 0.014225 | Test Accuracy: 0.540410\n",
      "Step 17 | Training Loss: 0.014099 | Test Loss: 0.014230 | Test Accuracy: 0.540055\n",
      "Step 18 | Training Loss: 0.014035 | Test Loss: 0.014191 | Test Accuracy: 0.547596\n",
      "Step 19 | Training Loss: 0.014355 | Test Loss: 0.014198 | Test Accuracy: 0.547374\n",
      "Step 20 | Training Loss: 0.014269 | Test Loss: 0.014210 | Test Accuracy: 0.542850\n",
      "Accuracy on Test data: 0.542317271232605\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.014500 | Test Loss: 0.014828 | Test Accuracy: 0.481946\n",
      "Step 2 | Training Loss: 0.014370 | Test Loss: 0.014788 | Test Accuracy: 0.484475\n",
      "Step 3 | Training Loss: 0.014491 | Test Loss: 0.014820 | Test Accuracy: 0.479374\n",
      "Step 4 | Training Loss: 0.014485 | Test Loss: 0.014751 | Test Accuracy: 0.486781\n",
      "Step 5 | Training Loss: 0.014393 | Test Loss: 0.014743 | Test Accuracy: 0.490596\n",
      "Step 6 | Training Loss: 0.014568 | Test Loss: 0.014676 | Test Accuracy: 0.492149\n",
      "Step 7 | Training Loss: 0.015115 | Test Loss: 0.014696 | Test Accuracy: 0.489532\n",
      "Step 8 | Training Loss: 0.014292 | Test Loss: 0.014675 | Test Accuracy: 0.493391\n",
      "Step 9 | Training Loss: 0.014281 | Test Loss: 0.014691 | Test Accuracy: 0.491395\n",
      "Step 10 | Training Loss: 0.014351 | Test Loss: 0.014659 | Test Accuracy: 0.494766\n",
      "Step 11 | Training Loss: 0.014544 | Test Loss: 0.014657 | Test Accuracy: 0.490951\n",
      "Step 12 | Training Loss: 0.014633 | Test Loss: 0.014618 | Test Accuracy: 0.494189\n",
      "Step 13 | Training Loss: 0.014576 | Test Loss: 0.014602 | Test Accuracy: 0.497782\n",
      "Step 14 | Training Loss: 0.014347 | Test Loss: 0.014601 | Test Accuracy: 0.496895\n",
      "Step 15 | Training Loss: 0.014272 | Test Loss: 0.014610 | Test Accuracy: 0.495742\n",
      "Step 16 | Training Loss: 0.014719 | Test Loss: 0.014563 | Test Accuracy: 0.499202\n",
      "Step 17 | Training Loss: 0.014316 | Test Loss: 0.014512 | Test Accuracy: 0.506254\n",
      "Step 18 | Training Loss: 0.014689 | Test Loss: 0.014573 | Test Accuracy: 0.497738\n",
      "Step 19 | Training Loss: 0.014475 | Test Loss: 0.014536 | Test Accuracy: 0.502129\n",
      "Step 20 | Training Loss: 0.014458 | Test Loss: 0.014476 | Test Accuracy: 0.513618\n",
      "Accuracy on Test data: 0.506076991558075\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.015074 | Test Loss: 0.015371 | Test Accuracy: 0.473696\n",
      "Step 2 | Training Loss: 0.014964 | Test Loss: 0.015130 | Test Accuracy: 0.477954\n",
      "Step 3 | Training Loss: 0.014700 | Test Loss: 0.015061 | Test Accuracy: 0.479684\n",
      "Step 4 | Training Loss: 0.014639 | Test Loss: 0.015040 | Test Accuracy: 0.474184\n",
      "Step 5 | Training Loss: 0.014620 | Test Loss: 0.014966 | Test Accuracy: 0.479950\n",
      "Step 6 | Training Loss: 0.014677 | Test Loss: 0.014929 | Test Accuracy: 0.483987\n",
      "Step 7 | Training Loss: 0.014544 | Test Loss: 0.014870 | Test Accuracy: 0.485983\n",
      "Step 8 | Training Loss: 0.014664 | Test Loss: 0.014855 | Test Accuracy: 0.487092\n",
      "Step 9 | Training Loss: 0.014748 | Test Loss: 0.014837 | Test Accuracy: 0.488511\n",
      "Step 10 | Training Loss: 0.014655 | Test Loss: 0.014784 | Test Accuracy: 0.491794\n",
      "Step 11 | Training Loss: 0.014464 | Test Loss: 0.014746 | Test Accuracy: 0.495476\n",
      "Step 12 | Training Loss: 0.014774 | Test Loss: 0.014746 | Test Accuracy: 0.496052\n",
      "Step 13 | Training Loss: 0.014559 | Test Loss: 0.014728 | Test Accuracy: 0.495121\n",
      "Step 14 | Training Loss: 0.014424 | Test Loss: 0.014754 | Test Accuracy: 0.489709\n",
      "Step 15 | Training Loss: 0.014400 | Test Loss: 0.014696 | Test Accuracy: 0.494855\n",
      "Step 16 | Training Loss: 0.014339 | Test Loss: 0.014667 | Test Accuracy: 0.498758\n",
      "Step 17 | Training Loss: 0.014325 | Test Loss: 0.014651 | Test Accuracy: 0.499601\n",
      "Step 18 | Training Loss: 0.014106 | Test Loss: 0.014643 | Test Accuracy: 0.498581\n",
      "Step 19 | Training Loss: 0.014619 | Test Loss: 0.014603 | Test Accuracy: 0.505190\n",
      "Step 20 | Training Loss: 0.014333 | Test Loss: 0.014609 | Test Accuracy: 0.503016\n",
      "Accuracy on Test data: 0.5027945637702942\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [20]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:21.924154Z",
     "start_time": "2017-05-13T16:30:21.919248Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:21.963984Z",
     "start_time": "2017-05-13T16:30:21.925985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625044</td>\n",
       "      <td>0.620742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.618834</td>\n",
       "      <td>0.617149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.579223</td>\n",
       "      <td>0.577049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.563742</td>\n",
       "      <td>0.561790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.561125</td>\n",
       "      <td>0.560193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.553096</td>\n",
       "      <td>0.549814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.542850</td>\n",
       "      <td>0.542317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.536773</td>\n",
       "      <td>0.542095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.531982</td>\n",
       "      <td>0.531405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.513618</td>\n",
       "      <td>0.506077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.503016</td>\n",
       "      <td>0.502795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.504081</td>\n",
       "      <td>0.498093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "1      20               8              2     0.625044    0.620742\n",
       "3      20              32              2     0.618834    0.617149\n",
       "0      20               4              2     0.579223    0.577049\n",
       "6      20              16              4     0.563742    0.561790\n",
       "4      20               4              4     0.561125    0.560193\n",
       "5      20               8              4     0.553096    0.549814\n",
       "9      20               8              6     0.542850    0.542317\n",
       "7      20              32              4     0.536773    0.542095\n",
       "2      20              16              2     0.531982    0.531405\n",
       "10     20              16              6     0.513618    0.506077\n",
       "11     20              32              6     0.503016    0.502795\n",
       "8      20               4              6     0.504081    0.498093"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:21.983726Z",
     "start_time": "2017-05-13T16:30:21.965722Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:22.076876Z",
     "start_time": "2017-05-13T16:30:21.985893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T16:30:22.478364Z",
     "start_time": "2017-05-13T16:30:22.079554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.6309  0.3691]\n",
      " [ 0.3926  0.6074]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedc22c1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
