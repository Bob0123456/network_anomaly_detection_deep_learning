{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:08.004990Z",
     "start_time": "2017-06-01T00:49:07.484155Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:08.095032Z",
     "start_time": "2017-06-01T00:49:08.007008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:08.107456Z",
     "start_time": "2017-06-01T00:49:08.100316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:08.116667Z",
     "start_time": "2017-06-01T00:49:08.111596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:08.953232Z",
     "start_time": "2017-06-01T00:49:08.119362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_output = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    #y_test_output = 1 - dataset.kdd_test_2labels_y \n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test_output.values\n",
    "\n",
    "    x_train_all = x_train #[y_output.is_Normal == 1]\n",
    "    y_train_all = 1 - np.argmax(y_output.values, axis = 1) # Inverting because here H0 = Normal and H1 is Attack. \n",
    "    #y_output.is_Normal.values #[y_output.is_Normal == 1]\n",
    "    \n",
    "    x_test_all = x_test\n",
    "    y_test_all = 1- np.argmax(y_test_output.values, axis = 1) # Inverting because here H0 = Normal and H1 is Attack.\n",
    "    \n",
    "preprocess.x_train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Reduction by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:10.470657Z",
     "start_time": "2017-06-01T00:49:08.957951Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:10.477386Z",
     "start_time": "2017-06-01T00:49:10.472596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 1\n",
    "    hidden_dim = 122\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:10.863041Z",
     "start_time": "2017-06-01T00:49:10.481006Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class discriminator(network):\n",
    "    \n",
    "    def __init__(self, hidden_layers, f):\n",
    "        \n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_dim = self.hidden_dim #f\n",
    "\n",
    "        self.x_real = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "        self.y_real_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "        self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "\n",
    "        self.x_random = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "        self.y_fake_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "        self.learning_rate = tf.placeholder(\"float\")\n",
    "\n",
    "        def discriminator_network(x, reuse=False):\n",
    "            with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "                hidden = tf.layers.dense(x, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "                for h in range(hidden_layers - 1):\n",
    "                    hidden = tf.layers.dense(hidden, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                    hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "            y = tf.layers.dense(hidden, classes, activation=tf.nn.sigmoid)\n",
    "            return y\n",
    "        \n",
    "        def generator_network(x, reuse=False):\n",
    "            with tf.variable_scope('generator', reuse=reuse):\n",
    "                hidden = tf.layers.dense(x, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "                for h in range(hidden_layers - 2):\n",
    "                    hidden = tf.layers.dense(hidden, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                    hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "            y = tf.layers.dense(hidden, input_dim, activation=tf.nn.tanh)\n",
    "            return y\n",
    "\n",
    "        x_fake = generator_network(self.x_random)\n",
    "\n",
    "        self.y = discriminator_network(self.x_real)\n",
    "        y_fake = discriminator_network(x_fake, reuse=True)\n",
    "\n",
    "        \n",
    "        loss_real = tf.losses.sigmoid_cross_entropy(self.y_real_, self.y)\n",
    "        loss_fake = tf.losses.sigmoid_cross_entropy(self.y_fake_, y_fake)\n",
    "\n",
    "        self.loss = loss_real + loss_fake\n",
    "\n",
    "        #correct_prediction = tf.equal(self.y_, self.y)\n",
    "        #self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        d_optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        g_optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        #gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "        #gradients = [\n",
    "        #    None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "        #    for gradient in gradients]\n",
    "        #self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.d_train_op = d_optimizer.minimize(self.loss)\n",
    "        self.g_train_op = g_optimizer.minimize(loss_fake)\n",
    "\n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        #self.pred = tf.argmax(self.y, axis = 1)\n",
    "        #self.actual = tf.argmax(self.y_real_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:10.921627Z",
     "start_time": "2017-06-01T00:49:10.865320Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generator1(network):\n",
    "    \n",
    "    def __init__(self, hidden_layers):\n",
    "        \n",
    "        input_dim = self.input_dim\n",
    "        hidden_dim = self.hidden_dim\n",
    "            \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "            self.learning_rate = tf.placeholder(\"float\")\n",
    "            self.loss = tf.placeholder(\"float\")\n",
    "\n",
    "            hidden = tf.layers.dense(self.x, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden = tf.layers.dense(hidden, hidden_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden = tf.nn.dropout(hidden, self.keep_prob)\n",
    "\n",
    "            self.y = tf.layers.dense(hidden, input_dim, activation=tf.nn.tanh)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            #gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "            #gradients = [\n",
    "            #    None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "            #    for gradient in gradients]\n",
    "            #self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            self.train_op = optimizer.minimize(hidden)\n",
    "\n",
    "            # add Saver ops\n",
    "            self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:49:11.696098Z",
     "start_time": "2017-06-01T00:49:10.924023Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    alpha = 0.8\n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, h, f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        \n",
    "        d = discriminator(h, f)\n",
    "        #g = generator(h)\n",
    "        \n",
    "        os.makedirs(\"dataset/tf_GAN_with_Dense_nsl_kdd/hidden_layers_{}_features_count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        \n",
    "        d.sess = tf.Session()\n",
    "        \n",
    "        #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "        d.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for lr in lrs:\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train_all, \n",
    "                                                                          preprocess.y_train_all, \n",
    "                                                                          test_size=0.1)\n",
    "                \n",
    "                x_train = x_train[y_train == 0]\n",
    "                y_train = y_train[y_train == 0]\n",
    "                \n",
    "                y_train = np.reshape(y_train, (-1,1))\n",
    "                y_train_normal = np.zeros_like(y_train)\n",
    "                y_train_attack = np.ones_like(y_train)\n",
    "                x_train_attack_rnd = np.random.normal(size=x_train.shape)\n",
    "\n",
    "                y_valid = np.reshape(y_valid, (-1,1))\n",
    "                y_valid_normal = np.zeros_like(y_valid)\n",
    "                y_valid_attack = np.ones_like(y_valid)\n",
    "                x_valid_attack_rnd = np.random.normal(size=x_valid.shape)\n",
    "\n",
    "                y_test_all = np.reshape(preprocess.y_test_all, (-1,1))\n",
    "                y_test_normal = np.zeros_like(y_test_all)\n",
    "                y_test_attack = np.ones_like(y_test_all)\n",
    "                x_test_attack_rnd = np.random.normal(size=preprocess.x_test_all.shape)\n",
    "\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "                for i in batch_indices:\n",
    "\n",
    "                    def train_batch():\n",
    "                        nonlocal train_loss\n",
    "\n",
    "                        # Passing Normal (real) and Attack (fake) Traffic and training together\n",
    "                        d_loss, _ = d.sess.run([d.loss, d.d_train_op], \n",
    "                                              feed_dict={d.x_real: x_train[i,:],\n",
    "                                                         d.y_:y_train[i,:],\n",
    "                                                         d.y_real_: y_train_normal[i,:],\n",
    "                                                         d.x_random: x_train_attack_rnd[i,:],\n",
    "                                                         d.y_fake_: y_train_attack[i,:],\n",
    "                                                         d.keep_prob:1,\n",
    "                                                         d.learning_rate:lr})\n",
    "\n",
    "                        #Train Generator\n",
    "                        #g.sess.run([g.train_op], feed_dict={g.loss:d_loss_attack_fake})\n",
    "\n",
    "                        train_loss = d_loss\n",
    "\n",
    "                    train_batch()\n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                        d.saver.restore(sess, \n",
    "                                          tf.train.latest_checkpoint('dataset/tf_GAN_with_Dense_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                     .format(epochs,h,f)))\n",
    "                        train_batch()\n",
    "\n",
    "\n",
    "                y_valid_pred = d.sess.run(d.y, #net.summary_op \n",
    "                                                      feed_dict={d.x_real: x_valid, \n",
    "                                                                 d.y_real_: y_valid_normal,\n",
    "                                                                 d.y_:y_valid,\n",
    "                                                                 d.x_random: x_valid_attack_rnd,\n",
    "                                                                 d.y_fake_: y_valid_attack,\n",
    "                                                                 d.keep_prob:1,\n",
    "                                                                 d.learning_rate:lr})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                y_test_pred = d.sess.run(d.y, \n",
    "                                                feed_dict={d.x_real: preprocess.x_test_all, \n",
    "                                                         d.y_real_: y_test_normal,\n",
    "                                                         d.y_: y_test_all,\n",
    "                                                         d.x_random: x_test_attack_rnd,\n",
    "                                                         d.y_fake_: y_test_attack,\n",
    "                                                         d.keep_prob:1,\n",
    "                                                         d.learning_rate:lr})\n",
    "\n",
    "                def get_accuracy(y_pred, y_actual):\n",
    "                    y_attack = np.logical_or(y_pred > Train.alpha, y_pred < -Train.alpha)\n",
    "                    y_normal = np.logical_or(y_pred < Train.alpha, y_pred > -Train.alpha)\n",
    "                    \n",
    "                    pred_value = np.argmax([y_normal, y_attack], axis = 0)\n",
    "                    accuracy = np.mean(np.equal(y_actual, pred_value))\n",
    "                    \n",
    "                    pred_value = np.squeeze(pred_value)\n",
    "                    y_attack = np.squeeze(y_attack)\n",
    "                    y_normal = np.squeeze(y_normal)\n",
    "                    \n",
    "                    return accuracy, pred_value, y_attack, y_normal\n",
    "                    \n",
    "                valid_accuracy, _,_,_ = get_accuracy(y_valid_pred, y_valid)\n",
    "                accuracy, pred_value, y_attack, y_normal = get_accuracy(y_test_pred, y_test_all)\n",
    "                \n",
    "                print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "                \n",
    "                if accuracy > Train.best_acc:\n",
    "                    \n",
    "                    actual_value = y_test_normal\n",
    "                    \n",
    "                    Train.best_acc = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                                   \n",
    "                    if not (np.isnan(train_loss)):\n",
    "                        d.saver.save(d.sess, \n",
    "                                   \"dataset/tf_GAN_with_Dense_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                    global_step = epochs)\n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_attack, \"Normal_prob\":y_normal, \"Prediction\":pred_value})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs*len(lrs),f,h):(curr_pred, \n",
    "                                               Train.result(epochs*len(lrs), f, h,valid_accuracy, accuracy))})\n",
    "\n",
    "                    #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.771687Z",
     "start_time": "2017-06-01T00:49:11.698245Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 1.006458 | Validation Accuracy: 0.532862\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006459 | Validation Accuracy: 0.529290\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 1.006452 | Validation Accuracy: 0.529290\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006450 | Validation Accuracy: 0.540324\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 1.006454 | Validation Accuracy: 0.531672\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006439 | Validation Accuracy: 0.540244\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 1.006455 | Validation Accuracy: 0.535799\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006448 | Validation Accuracy: 0.536037\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:2 features count:122\n",
      "Step 1 | Training Loss: 1.006439 | Validation Accuracy: 0.536117\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006435 | Validation Accuracy: 0.531751\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.526592\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.535244\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.537943\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.538339\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.535879\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.541356\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.534529\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.533736\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:4 features count:122\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.531195\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.527703\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.537863\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.530640\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.536831\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.542229\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.536355\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.529449\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.537625\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.540006\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Current Layer Attributes - epochs:1 hidden layers:6 features count:122\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.532307\n",
      "Accuracy on Test data: 0.43075762952448543\n",
      "Step 1 | Training Loss: 1.006408 | Validation Accuracy: 0.531434\n",
      "Accuracy on Test data: 0.43075762952448543\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [10]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        \n",
    "        Train.train(e, h, f, [0.001, 0.0001])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.781630Z",
     "start_time": "2017-06-01T00:54:25.774398Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.788499Z",
     "start_time": "2017-06-01T00:54:25.783687Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.796162Z",
     "start_time": "2017-06-01T00:54:25.790871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.817768Z",
     "start_time": "2017-06-01T00:54:25.798485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.532862</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.529290</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.531672</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.535799</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>0.536117</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.526592</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537943</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.535879</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.534529</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>0.531195</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.537863</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.536831</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.536355</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.537625</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>6</td>\n",
       "      <td>0.532307</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0       2               4              2     0.532862    0.430758\n",
       "1       2               8              2     0.529290    0.430758\n",
       "2       2              16              2     0.531672    0.430758\n",
       "3       2              32              2     0.535799    0.430758\n",
       "4       2             122              2     0.536117    0.430758\n",
       "5       2               4              4     0.526592    0.430758\n",
       "6       2               8              4     0.537943    0.430758\n",
       "7       2              16              4     0.535879    0.430758\n",
       "8       2              32              4     0.534529    0.430758\n",
       "9       2             122              4     0.531195    0.430758\n",
       "10      2               4              6     0.537863    0.430758\n",
       "11      2               8              6     0.536831    0.430758\n",
       "12      2              16              6     0.536355    0.430758\n",
       "13      2              32              6     0.537625    0.430758\n",
       "14      2             122              6     0.532307    0.430758"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:25.902363Z",
     "start_time": "2017-06-01T00:54:25.820715Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_GAN_with_Dense_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_GAN_with_Dense_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:26.009423Z",
     "start_time": "2017-06-01T00:54:25.905371Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:54:26.414630Z",
     "start_time": "2017-06-01T00:54:26.014727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc033f526d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
