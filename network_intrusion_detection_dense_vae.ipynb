{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"x\"]\n",
    "\n",
    "kdd_train = pd.read_csv(\"dataset/KDDTrain+.txt\",names = col_names,)\n",
    "kdd_test = pd.read_csv(\"dataset/KDDTest+.txt\",names = col_names,)\n",
    "\n",
    "kdd_train = kdd_train.drop(\"x\", axis = 1)\n",
    "kdd_test = kdd_test.drop(\"x\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"protocol_type\",\"service\",\"flag\"]\n",
    "for cv in category_variables:\n",
    "    kdd_train[cv] = kdd_train[cv].astype(\"category\")\n",
    "kdd_train[\"label\"] = kdd_train[\"label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125966</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125967</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>udp</td>\n",
       "      <td>private</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>klogin</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0              0           tcp  ftp_data   SF        491          0     0   \n",
       "1              0           udp     other   SF        146          0     0   \n",
       "2              0           tcp   private   S0          0          0     0   \n",
       "3              0           tcp      http   SF        232       8153     0   \n",
       "4              0           tcp      http   SF        199        420     0   \n",
       "5              0           tcp   private  REJ          0          0     0   \n",
       "6              0           tcp   private   S0          0          0     0   \n",
       "...          ...           ...       ...  ...        ...        ...   ...   \n",
       "125966         0           tcp   private   S0          0          0     0   \n",
       "125967         0           tcp      http   SF        359        375     0   \n",
       "125968         0           tcp   private   S0          0          0     0   \n",
       "125969         8           udp   private   SF        105        145     0   \n",
       "125970         0           tcp      smtp   SF       2231        384     0   \n",
       "125971         0           tcp    klogin   S0          0          0     0   \n",
       "125972         0           tcp  ftp_data   SF        151          0     0   \n",
       "\n",
       "        wrong_fragment  urgent  hot   ...     dst_host_srv_count  \\\n",
       "0                    0       0    0   ...                     25   \n",
       "1                    0       0    0   ...                      1   \n",
       "2                    0       0    0   ...                     26   \n",
       "3                    0       0    0   ...                    255   \n",
       "4                    0       0    0   ...                    255   \n",
       "5                    0       0    0   ...                     19   \n",
       "6                    0       0    0   ...                      9   \n",
       "...                ...     ...  ...   ...                    ...   \n",
       "125966               0       0    0   ...                     13   \n",
       "125967               0       0    0   ...                    255   \n",
       "125968               0       0    0   ...                     25   \n",
       "125969               0       0    0   ...                    244   \n",
       "125970               0       0    0   ...                     30   \n",
       "125971               0       0    0   ...                      8   \n",
       "125972               0       0    0   ...                     77   \n",
       "\n",
       "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                         0.17                    0.03   \n",
       "1                         0.00                    0.60   \n",
       "2                         0.10                    0.05   \n",
       "3                         1.00                    0.00   \n",
       "4                         1.00                    0.00   \n",
       "5                         0.07                    0.07   \n",
       "6                         0.04                    0.05   \n",
       "...                        ...                     ...   \n",
       "125966                    0.05                    0.07   \n",
       "125967                    1.00                    0.00   \n",
       "125968                    0.10                    0.06   \n",
       "125969                    0.96                    0.01   \n",
       "125970                    0.12                    0.06   \n",
       "125971                    0.03                    0.05   \n",
       "125972                    0.30                    0.03   \n",
       "\n",
       "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                              0.17                         0.00   \n",
       "1                              0.88                         0.00   \n",
       "2                              0.00                         0.00   \n",
       "3                              0.03                         0.04   \n",
       "4                              0.00                         0.00   \n",
       "5                              0.00                         0.00   \n",
       "6                              0.00                         0.00   \n",
       "...                             ...                          ...   \n",
       "125966                         0.00                         0.00   \n",
       "125967                         0.33                         0.04   \n",
       "125968                         0.00                         0.00   \n",
       "125969                         0.01                         0.00   \n",
       "125970                         0.00                         0.00   \n",
       "125971                         0.00                         0.00   \n",
       "125972                         0.30                         0.00   \n",
       "\n",
       "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                       0.00                      0.00                  0.05   \n",
       "1                       0.00                      0.00                  0.00   \n",
       "2                       1.00                      1.00                  0.00   \n",
       "3                       0.03                      0.01                  0.00   \n",
       "4                       0.00                      0.00                  0.00   \n",
       "5                       0.00                      0.00                  1.00   \n",
       "6                       1.00                      1.00                  0.00   \n",
       "...                      ...                       ...                   ...   \n",
       "125966                  1.00                      1.00                  0.00   \n",
       "125967                  0.33                      0.00                  0.00   \n",
       "125968                  1.00                      1.00                  0.00   \n",
       "125969                  0.00                      0.00                  0.00   \n",
       "125970                  0.72                      0.00                  0.01   \n",
       "125971                  1.00                      1.00                  0.00   \n",
       "125972                  0.00                      0.00                  0.00   \n",
       "\n",
       "        dst_host_srv_rerror_rate    label  \n",
       "0                           0.00   normal  \n",
       "1                           0.00   normal  \n",
       "2                           0.00  neptune  \n",
       "3                           0.01   normal  \n",
       "4                           0.00   normal  \n",
       "5                           1.00  neptune  \n",
       "6                           0.00  neptune  \n",
       "...                          ...      ...  \n",
       "125966                      0.00  neptune  \n",
       "125967                      0.00   normal  \n",
       "125968                      0.00  neptune  \n",
       "125969                      0.00   normal  \n",
       "125970                      0.00   normal  \n",
       "125971                      0.00  neptune  \n",
       "125972                      0.00   normal  \n",
       "\n",
       "[125973 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125973.00000</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.14465</td>\n",
       "      <td>4.556674e+04</td>\n",
       "      <td>1.977911e+04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.279250</td>\n",
       "      <td>...</td>\n",
       "      <td>182.148945</td>\n",
       "      <td>115.653005</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.284452</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.120240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2604.51531</td>\n",
       "      <td>5.870331e+06</td>\n",
       "      <td>4.021269e+06</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>23.942042</td>\n",
       "      <td>...</td>\n",
       "      <td>99.206213</td>\n",
       "      <td>110.702741</td>\n",
       "      <td>0.448949</td>\n",
       "      <td>0.188922</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.306557</td>\n",
       "      <td>0.319459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42908.00000</td>\n",
       "      <td>1.379964e+09</td>\n",
       "      <td>1.309937e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7479.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes           land  \\\n",
       "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
       "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
       "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
       "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
       "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
       "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
       "mean         0.022687       0.000111       0.204409           0.001222   \n",
       "std          0.253530       0.014366       2.149968           0.045239   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      77.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  125973.000000    125973.000000            ...              \n",
       "mean        0.395736         0.279250            ...              \n",
       "std         0.489010        23.942042            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         1.000000         0.000000            ...              \n",
       "max         1.000000      7479.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   125973.000000       125973.000000           125973.000000   \n",
       "mean       182.148945          115.653005                0.521242   \n",
       "std         99.206213          110.702741                0.448949   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%         82.000000           10.000000                0.050000   \n",
       "50%        255.000000           63.000000                0.510000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           125973.000000                125973.000000   \n",
       "mean                 0.082951                     0.148379   \n",
       "std                  0.188922                     0.308997   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.020000                     0.000000   \n",
       "75%                  0.070000                     0.060000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                125973.000000         125973.000000   \n",
       "mean                      0.032542              0.284452   \n",
       "std                       0.112564              0.444784   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.020000              1.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             125973.000000         125973.000000   \n",
       "mean                   0.278485              0.118832   \n",
       "std                    0.445669              0.306557   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    1.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             125973.000000  \n",
       "mean                   0.120240  \n",
       "std                    0.319459  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22544.000000</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.859076</td>\n",
       "      <td>1.039545e+04</td>\n",
       "      <td>2.056019e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.105394</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>193.869411</td>\n",
       "      <td>140.750532</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.226683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1407.176612</td>\n",
       "      <td>4.727864e+05</td>\n",
       "      <td>2.121930e+04</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.928428</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>7.269597</td>\n",
       "      <td>...</td>\n",
       "      <td>94.035663</td>\n",
       "      <td>111.783972</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.273139</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.400875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870000e+02</td>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57715.000000</td>\n",
       "      <td>6.282565e+07</td>\n",
       "      <td>1.345927e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
       "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
       "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
       "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
       "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
       "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
       "mean       0.000710      0.105394           0.021647      0.442202   \n",
       "std        0.036473      0.928428           0.150328      0.496659   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      1.000000   \n",
       "max        3.000000    101.000000           4.000000      1.000000   \n",
       "\n",
       "       num_compromised            ...             dst_host_count  \\\n",
       "count     22544.000000            ...               22544.000000   \n",
       "mean          0.119899            ...                 193.869411   \n",
       "std           7.269597            ...                  94.035663   \n",
       "min           0.000000            ...                   0.000000   \n",
       "25%           0.000000            ...                 121.000000   \n",
       "50%           0.000000            ...                 255.000000   \n",
       "75%           0.000000            ...                 255.000000   \n",
       "max         796.000000            ...                 255.000000   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        22544.000000            22544.000000            22544.000000   \n",
       "mean           140.750532                0.608722                0.090540   \n",
       "std            111.783972                0.435688                0.220717   \n",
       "min              0.000000                0.000000                0.000000   \n",
       "25%             15.000000                0.070000                0.000000   \n",
       "50%            168.000000                0.920000                0.010000   \n",
       "75%            255.000000                1.000000                0.060000   \n",
       "max            255.000000                1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 22544.000000                 22544.000000   \n",
       "mean                      0.132261                     0.019638   \n",
       "std                       0.306268                     0.085394   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.030000                     0.010000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          22544.000000              22544.000000          22544.000000   \n",
       "mean               0.097814                  0.099426              0.233385   \n",
       "std                0.273139                  0.281866              0.387229   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.360000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              22544.000000  \n",
       "mean                   0.226683  \n",
       "std                    0.400875  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.170000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          normal\n",
       "1          normal\n",
       "2         neptune\n",
       "3          normal\n",
       "4          normal\n",
       "5         neptune\n",
       "6         neptune\n",
       "           ...   \n",
       "125966    neptune\n",
       "125967     normal\n",
       "125968    neptune\n",
       "125969     normal\n",
       "125970     normal\n",
       "125971    neptune\n",
       "125972     normal\n",
       "Name: label, dtype: category\n",
       "Categories (23, object): [back, buffer_overflow, ftp_write, guess_passwd, ..., spy, teardrop, warezclient, warezmaster]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "#scatter_matrix(kdd_train, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_types = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "is_sess = {\n",
    "    \"DoS\":\"Attack\",\n",
    "    \"R2L\":\"Attack\",\n",
    "    \"U2R\":\"Attack\",\n",
    "    \"Probe\":\"Attack\",\n",
    "    \"normal\":\"Normal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_train[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])\n",
    "kdd_test[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_test[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_sess_type_group = kdd_train.groupby(\"sess_type\")\n",
    "kdd_is_sess_group = kdd_train.groupby(\"is_sess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_type\n",
       "DoS       45927\n",
       "Probe     11656\n",
       "R2L         995\n",
       "U2R          52\n",
       "normal    67343\n",
       "Name: is_sess, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.is_sess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sess\n",
       "Attack    58630\n",
       "Normal    67343\n",
       "Name: sess_type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.sess_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sess_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">DoS</th>\n",
       "      <th>count</th>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.090034</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>1.692015e+02</td>\n",
       "      <td>244.600475</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.747922</td>\n",
       "      <td>26.524005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.748494</td>\n",
       "      <td>1.176321e+03</td>\n",
       "      <td>32.656346</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.445748</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>1.168004e+03</td>\n",
       "      <td>41.324475</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.188947</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>48.303117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299931</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>7.686120e+03</td>\n",
       "      <td>94.667526</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "sess_type                                                                    \n",
       "DoS       count  45927.000000   45927.000000  4.592700e+04    45927.000000   \n",
       "          mean     178.090034       0.065403  1.692015e+02      244.600475   \n",
       "          std      104.445748       0.064023  1.168004e+03       41.324475   \n",
       "          min        1.000000       0.000000  0.000000e+00        1.000000   \n",
       "          25%      109.000000       0.050000  0.000000e+00      255.000000   \n",
       "          50%      172.000000       0.060000  0.000000e+00      255.000000   \n",
       "          75%      249.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                       ...            ...           ...             ...   \n",
       "normal    mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "          std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "          min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "          25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "          50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "          75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "          max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "                 dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "sess_type                                                       \n",
       "DoS       count            45927.000000          45927.000000   \n",
       "          mean                 0.066333              0.157569   \n",
       "          std                  0.058079              0.358934   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.050000              0.000000   \n",
       "          50%                  0.070000              0.000000   \n",
       "          75%                  0.070000              0.000000   \n",
       "...                                 ...                   ...   \n",
       "normal    mean                 0.040134              0.046589   \n",
       "          std                  0.128529              0.195306   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.000000              0.000000   \n",
       "          50%                  0.000000              0.000000   \n",
       "          75%                  0.020000              0.000000   \n",
       "          max                  1.000000              1.000000   \n",
       "\n",
       "                 dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "sess_type                                                              \n",
       "DoS       count                 45927.000000            45927.000000   \n",
       "          mean                      0.049492                0.123423   \n",
       "          std                       0.188947                0.228287   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.020000   \n",
       "          50%                       0.000000                0.050000   \n",
       "          75%                       0.000000                0.080000   \n",
       "...                                      ...                     ...   \n",
       "normal    mean                      0.121726                0.811875   \n",
       "          std                       0.254382                0.324091   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.750000   \n",
       "          50%                       0.010000                1.000000   \n",
       "          75%                       0.080000                1.000000   \n",
       "          max                       1.000000                1.000000   \n",
       "\n",
       "                 dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "sess_type                                                       ...         \n",
       "DoS       count          45927.000000        45927.000000       ...         \n",
       "          mean               0.747922           26.524005       ...         \n",
       "          std                0.431707           48.303117       ...         \n",
       "          min                0.000000            1.000000       ...         \n",
       "          25%                0.180000            6.000000       ...         \n",
       "          50%                1.000000           13.000000       ...         \n",
       "          75%                1.000000           20.000000       ...         \n",
       "...                               ...                 ...       ...         \n",
       "normal    mean               0.013930          190.285761       ...         \n",
       "          std                0.092006           92.608377       ...         \n",
       "          min                0.000000            0.000000       ...         \n",
       "          25%                0.000000          121.000000       ...         \n",
       "          50%                0.000000          255.000000       ...         \n",
       "          75%                0.000000          255.000000       ...         \n",
       "          max                1.000000          255.000000       ...         \n",
       "\n",
       "                 same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "sess_type                                                                  \n",
       "DoS       count   45927.000000  45927.000000  4.592700e+04  45927.000000   \n",
       "          mean        0.191887      0.748494  1.176321e+03     32.656346   \n",
       "          std         0.299931      0.432559  7.686120e+03     94.667526   \n",
       "          min         0.000000      0.000000  0.000000e+00      1.000000   \n",
       "          25%         0.040000      0.290000  0.000000e+00      5.000000   \n",
       "          50%         0.070000      1.000000  0.000000e+00     11.000000   \n",
       "          75%         0.150000      1.000000  0.000000e+00     18.000000   \n",
       "...                        ...           ...           ...           ...   \n",
       "normal    mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "          std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "          min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "          25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "          50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "          75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "          max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "                 srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "sess_type                                                               \n",
       "DoS       count        45927.000000     45927.000000     45927.000000   \n",
       "          mean             0.005317         0.153000         0.746678   \n",
       "          std              0.056390         0.357561         0.434050   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         1.000000   \n",
       "          75%              0.000000         0.000000         1.000000   \n",
       "...                             ...              ...              ...   \n",
       "normal    mean             0.126263         0.044629         0.012083   \n",
       "          std              0.271621         0.202264         0.086426   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         0.000000   \n",
       "          75%              0.110000         0.000000         0.000000   \n",
       "          max              1.000000         1.000000         1.000000   \n",
       "\n",
       "                 su_attempted        urgent  wrong_fragment  \n",
       "sess_type                                                    \n",
       "DoS       count  45927.000000  45927.000000    45927.000000  \n",
       "          mean       0.000000      0.000000        0.062229  \n",
       "          std        0.000000      0.000000        0.416951  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "...                       ...           ...             ...  \n",
       "normal    mean       0.002049      0.000148        0.000000  \n",
       "          std        0.061622      0.017233        0.000000  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "          max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[40 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Attack</th>\n",
       "      <th>count</th>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.849991</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>3.752448e+04</td>\n",
       "      <td>222.025260</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.595177</td>\n",
       "      <td>29.929081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>8.282014e+04</td>\n",
       "      <td>27.797885</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0.209114</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.048746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.334483</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>5.893991e+06</td>\n",
       "      <td>79.196259</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>52.289254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>8.593025e+06</td>\n",
       "      <td>84.710761</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.369916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "is_sess                                                                    \n",
       "Attack  count  58630.000000   58630.000000  5.863000e+04    58630.000000   \n",
       "        mean     154.849991       0.102410  3.752448e+04      222.025260   \n",
       "        std      124.334483       0.206408  5.893991e+06       79.196259   \n",
       "        min        0.000000       0.000000  0.000000e+00        1.000000   \n",
       "        25%       40.000000       0.050000  0.000000e+00      255.000000   \n",
       "        50%      138.000000       0.060000  0.000000e+00      255.000000   \n",
       "        75%      241.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                     ...            ...           ...             ...   \n",
       "Normal  mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "        std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "        min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "        25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "        50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "        75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "        max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "               dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "is_sess                                                       \n",
       "Attack  count            58630.000000          58630.000000   \n",
       "        mean                 0.132131              0.201810   \n",
       "        std                  0.230626              0.381090   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.050000              0.000000   \n",
       "        50%                  0.070000              0.000000   \n",
       "        75%                  0.080000              0.020000   \n",
       "...                               ...                   ...   \n",
       "Normal  mean                 0.040134              0.046589   \n",
       "        std                  0.128529              0.195306   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.000000              0.000000   \n",
       "        50%                  0.000000              0.000000   \n",
       "        75%                  0.020000              0.000000   \n",
       "        max                  1.000000              1.000000   \n",
       "\n",
       "               dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "is_sess                                                              \n",
       "Attack  count                 58630.000000            58630.000000   \n",
       "        mean                      0.178993                0.187417   \n",
       "        std                       0.359262                0.322430   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.020000   \n",
       "        50%                       0.000000                0.050000   \n",
       "        75%                       0.020000                0.090000   \n",
       "...                                    ...                     ...   \n",
       "Normal  mean                      0.121726                0.811875   \n",
       "        std                       0.254382                0.324091   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.750000   \n",
       "        50%                       0.010000                1.000000   \n",
       "        75%                       0.080000                1.000000   \n",
       "        max                       1.000000                1.000000   \n",
       "\n",
       "               dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "is_sess                                                       ...         \n",
       "Attack  count          58630.000000        58630.000000       ...         \n",
       "        mean               0.595177           29.929081       ...         \n",
       "        std                0.484495           52.289254       ...         \n",
       "        min                0.000000            1.000000       ...         \n",
       "        25%                0.000000            4.000000       ...         \n",
       "        50%                1.000000           12.000000       ...         \n",
       "        75%                1.000000           21.000000       ...         \n",
       "...                             ...                 ...       ...         \n",
       "Normal  mean               0.013930          190.285761       ...         \n",
       "        std                0.092006           92.608377       ...         \n",
       "        min                0.000000            0.000000       ...         \n",
       "        25%                0.000000          121.000000       ...         \n",
       "        50%                0.000000          255.000000       ...         \n",
       "        75%                0.000000          255.000000       ...         \n",
       "        max                1.000000          255.000000       ...         \n",
       "\n",
       "               same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "is_sess                                                                  \n",
       "Attack  count   58630.000000  58630.000000  5.863000e+04  58630.000000   \n",
       "        mean        0.306659      0.595808  8.282014e+04     27.797885   \n",
       "        std         0.395655      0.486588  8.593025e+06     84.710761   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         0.040000      0.000000  0.000000e+00      3.000000   \n",
       "        50%         0.080000      1.000000  0.000000e+00     10.000000   \n",
       "        75%         0.500000      1.000000  0.000000e+00     18.000000   \n",
       "...                      ...           ...           ...           ...   \n",
       "Normal  mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "        std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "        50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "        75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "        max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "               srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "is_sess                                                               \n",
       "Attack  count        58630.000000     58630.000000     58630.000000   \n",
       "        mean             0.064079         0.209114         0.593072   \n",
       "        std              0.241348         0.404487         0.490234   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         1.000000   \n",
       "        75%              0.000000         0.000000         1.000000   \n",
       "...                           ...              ...              ...   \n",
       "Normal  mean             0.126263         0.044629         0.012083   \n",
       "        std              0.271621         0.202264         0.086426   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         0.000000   \n",
       "        75%              0.110000         0.000000         0.000000   \n",
       "        max              1.000000         1.000000         1.000000   \n",
       "\n",
       "               su_attempted        urgent  wrong_fragment  \n",
       "is_sess                                                    \n",
       "Attack  count  58630.000000  58630.000000    58630.000000  \n",
       "        mean       0.000017      0.000068        0.048746  \n",
       "        std        0.004130      0.010116        0.369916  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "...                     ...           ...             ...  \n",
       "Normal  mean       0.002049      0.000148        0.000000  \n",
       "        std        0.061622      0.017233        0.000000  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "        max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[16 rows x 38 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kdd_is_sess_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kdd_sess_type_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_dummy(train, test):\n",
    "    dummy_variables_2labels = [*category_variables, \"is_sess\"]\n",
    "    dummy_variables_5labels = [*category_variables, \"sess_type\"]\n",
    "\n",
    "    drop_variables = [*category_variables, \"is_sess\", \"sess_type\", \"duration\", \"label\"]\n",
    "    \n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    def dummy(kdd):\n",
    "        kdd_one_hot_2labels = pd.get_dummies(kdd[dummy_variables_2labels], prefix=dummy_variables_2labels, drop_first=False)\n",
    "        kdd_one_hot_5labels = pd.get_dummies(kdd[dummy_variables_5labels], prefix=dummy_variables_5labels, drop_first=False)\n",
    "\n",
    "        kdd_2labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                       , kdd_one_hot_2labels]\n",
    "                                      , axis = 1)\n",
    "        kdd_5labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                      , kdd_one_hot_5labels]\n",
    "                                      , axis = 1)\n",
    "\n",
    "        return kdd_2labels, kdd_5labels\n",
    "    \n",
    "    kdd_2labels, kdd_5labels = dummy(pd.concat([train, test], axis = 0))\n",
    "    \n",
    "    kdd_2labels_train, kdd_2labels_test = kdd_2labels.iloc[:train_size,:], kdd_2labels.iloc[train_size:,:]\n",
    "    kdd_5labels_train, kdd_5labels_test = kdd_5labels.iloc[:train_size,:], kdd_5labels.iloc[train_size:,:]\n",
    "    \n",
    "    return kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_columns_2labels = ['is_sess_Normal', 'is_sess_Attack']\n",
    "output_columns_5labels = ['sess_type_normal', 'sess_type_DoS', 'sess_type_Probe', 'sess_type_R2L', 'sess_type_U2R']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test = encode_dummy(kdd_train, kdd_test)\n",
    "\n",
    "x_kdd_train = kdd_2labels_train.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_train = kdd_2labels_train.loc[:,output_columns_2labels].values \n",
    "y_5labels_train = kdd_5labels_train.loc[:,output_columns_5labels].values\n",
    "\n",
    "x_kdd_test = kdd_2labels_test.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_test = kdd_2labels_test.loc[:,output_columns_2labels].values \n",
    "y_5labels_test = kdd_5labels_test.loc[:,output_columns_5labels].values\n",
    "\n",
    "        \n",
    "ss = pp.StandardScaler()\n",
    "x_kdd_train = ss.fit_transform(x_kdd_train)\n",
    "x_test = ss.transform(x_kdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sanity before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: mean:-0.0000, std:0.9959, shape:(125973, 121)\n",
      "Testing  data: mean:0.0170, std:1.4175, shape:(22544, 121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_kdd_train.mean(), x_kdd_train.std(), x_kdd_train.shape))\n",
    "print(\"Testing  data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_test.mean(), x_test.std(), x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "                      input_dim = 121, classes = 2, hidden_layers = 8, epochs = 10, hidden_units = 5):\n",
    "   \n",
    "    model_dense = Sequential()\n",
    "    model_dense.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        model_dense.add(Dense(hidden_units, kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    model_dense.add(Dense(classes, kernel_initializer='uniform' ,activation = 'softmax'))\n",
    "\n",
    "    model_dense.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'],\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                  activity_regularizer=keras.regularizers.l1(0.01)) \n",
    "\n",
    "    model_dense.fit(x_train, y_train, \n",
    "                  epochs = epochs, batch_size=500, \n",
    "                  validation_data = (x_valid, y_valid),\n",
    "                  verbose=1)\n",
    "    \n",
    "    scores_train = model_dense.evaluate(x_train, y_train)\n",
    "    scores_valid = model_dense.evaluate(x_valid, y_valid)\n",
    "    scores_test = model_dense.evaluate(x_test, y_test)\n",
    "    train_loss = scores_train[0]\n",
    "    valid_accuracy = scores_valid[1] \n",
    "    test_accuracy = scores_test[1]\n",
    "    \n",
    "    print(\"\\n Test loss: {}, accuracy: {}\".format(scores_test[0], scores_test[1]))\n",
    "    return train_loss, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_vae(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "              input_dim = 121, classes = 2, hidden_units = 8, hidden_layers = 4, epochs = 5):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    # tf.reset_default_graph()\n",
    "    with graph.as_default():\n",
    "        latent_dim = hidden_units\n",
    "        dense_hidden_units = hidden_units\n",
    "        hidden_encoder_dim = latent_dim #60\n",
    "        hidden_decoder_dim = latent_dim #60\n",
    "\n",
    "        lam = 0.01\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            initial = tf.constant(0.01, shape=shape)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        l2_loss = tf.constant(0.001)\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "                b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "\n",
    "                # Hidden layer encoder\n",
    "                hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "                tf.summary.histogram(\"Weights_Encoder\", W_encoder_input_hidden)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "\n",
    "            # Mu encoder\n",
    "            mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "            tf.summary.histogram(\"Weights_Mean\", W_encoder_hidden_mu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "\n",
    "            # Sigma encoder\n",
    "            logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "            tf.summary.histogram(\"Weights_Variance\", W_encoder_hidden_logvar)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "                b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "\n",
    "                # Hidden layer decoder\n",
    "                hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "                tf.summary.histogram(\"Weights_Decoder\", W_decoder_z_hidden)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "            b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "            l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "\n",
    "            x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "            tf.summary.histogram(\"Weights_Reconstruction\", W_decoder_hidden_reconstruction)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):    \n",
    "            z = tf.layers.dense(z,dense_hidden_units, activation=tf.nn.relu)\n",
    "            z = tf.nn.dropout(z, keep_prob = keep_prob)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), \n",
    "                                 reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), \n",
    "                                        reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "            reconst_loss = tf.reduce_mean(BCE + KLD)\n",
    "            loss = tf.reduce_mean( reconst_loss + softmax_loss)\n",
    "            regularized_loss = tf.abs(loss + lam * l2_loss, name = \"Regularized_loss\")\n",
    "\n",
    "            pred = tf.argmax(y, 1)\n",
    "            actual = tf.argmax(y_, 1)\n",
    "            \n",
    "            correct_prediction = tf.equal(actual, pred)\n",
    "            tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "            tf.summary.scalar(\"BCE\", tf.reduce_mean(BCE))\n",
    "            tf.summary.scalar(\"KLD\", tf.reduce_mean(KLD))\n",
    "            tf.summary.scalar(\"Softmax_loss\", softmax_loss)\n",
    "\n",
    "            tf.summary.scalar(\"Total_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "\n",
    "    batch_iterations = 10\n",
    "\n",
    "    batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                sess.run(optimizer, feed_dict={x: x_train[i,:], y_: y_train[i,:], keep_prob:0.6})\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train, train_reduction_loss = sess.run([regularized_loss, summary_op, reconst_loss], feed_dict={x: x_train, y_: y_train, keep_prob:1})\n",
    "            valid_accuracy, summary_str_valid, valid_reduction_loss = sess.run([tf_accuracy, summary_op, reconst_loss], feed_dict={x: x_valid, y_:y_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}, Reduction Loss: {:.4f} | \"\n",
    "                      \"Validation - Acc: {:.4f}, Reduction Loss: {:.4f}\"\n",
    "                      .format(epoch, train_loss, train_reduction_loss, valid_accuracy, valid_reduction_loss))\n",
    "\n",
    "        test_accuracy, y_pred, y_actual, test_reduction_loss = sess.run([tf_accuracy, pred, actual, reconst_loss], feed_dict={x: x_test, y_:y_test, keep_prob:1})\n",
    "        print(\"Test - Accuracy: {:.4f}, Feature reduction loss:{:.4f}\".format(test_accuracy, test_reduction_loss))\n",
    "        \n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "        \n",
    "        outputs = np.hstack((y_pred, y_actual))\n",
    "        \n",
    "    return train_loss, valid_accuracy, test_accuracy, train_reduction_loss, valid_reduction_loss, test_reduction_loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_scenario_vae(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"])\n",
    "    \n",
    "    print(\"Training for {} labels on VAE Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.hidden_units, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a, t_r_l, v_r_l, te_r_l, op = train_vae(x_train, x_valid, \n",
    "                                                           y_train, y_valid, \n",
    "                                                           x_kdd_test, y_test_labels, \n",
    "                                                           classes = classes, \n",
    "                                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                                           epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a, t_r_l, v_r_l, te_r_l]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    \n",
    "    return df_results, op\n",
    "        \n",
    "def run_scenario_dense(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"])\n",
    "    \n",
    "    print(\"Training for {} labels on Dense Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.hidden_units, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a = train_dense(x_train, x_valid, y_train, y_valid, x_kdd_test, y_test_labels, \n",
    "                 classes = classes, \n",
    "                 hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                 hidden_units = Hyper_parameters.reduced_features,\n",
    "                 epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2094: UserWarning: Expected no kwargs, you passed 2\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.4370 - acc: 0.8134 - val_loss: 0.2661 - val_acc: 0.9773\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1982 - acc: 0.9792 - val_loss: 0.1391 - val_acc: 0.9852\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1100 - acc: 0.9866 - val_loss: 0.0834 - val_acc: 0.9896\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0736 - acc: 0.9886 - val_loss: 0.0609 - val_acc: 0.9902\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0586 - acc: 0.9891 - val_loss: 0.0519 - val_acc: 0.9896\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0520 - acc: 0.9895 - val_loss: 0.0475 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0489 - acc: 0.9895 - val_loss: 0.0457 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0474 - acc: 0.9898 - val_loss: 0.0449 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0465 - acc: 0.9895 - val_loss: 0.0437 - val_acc: 0.9903\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0460 - acc: 0.9897 - val_loss: 0.0464 - val_acc: 0.9894\n",
      "21184/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 3.395341613260544, accuracy: 0.47325230660042583\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6911 - acc: 0.5348 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6908 - acc: 0.5347 - val_loss: 0.6908 - val_acc: 0.5342\n",
      "21472/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 0.690812740874172, accuracy: 0.5341554293825408\n",
      "Training for 2 labels on VAE Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Step  0 | Training - Loss: 1.3298, Reduction Loss: 0.6442 | Validation - Acc: 0.5644, Reduction Loss: 0.6441\n",
      "Step  1 | Training - Loss: 1.2662, Reduction Loss: 0.5608 | Validation - Acc: 0.5600, Reduction Loss: 0.5607\n",
      "Step  2 | Training - Loss: 1.1941, Reduction Loss: 0.4710 | Validation - Acc: 0.5900, Reduction Loss: 0.4709\n",
      "Step  3 | Training - Loss: 1.1249, Reduction Loss: 0.3964 | Validation - Acc: 0.6282, Reduction Loss: 0.3962\n",
      "Step  4 | Training - Loss: 1.0535, Reduction Loss: 0.3498 | Validation - Acc: 0.6913, Reduction Loss: 0.3493\n",
      "Step  5 | Training - Loss: 0.9948, Reduction Loss: 0.3132 | Validation - Acc: 0.7268, Reduction Loss: 0.3123\n",
      "Step  6 | Training - Loss: 0.9485, Reduction Loss: 0.2842 | Validation - Acc: 0.7444, Reduction Loss: 0.2821\n",
      "Step  7 | Training - Loss: 0.9123, Reduction Loss: 0.2598 | Validation - Acc: 0.7662, Reduction Loss: 0.2585\n",
      "Step  8 | Training - Loss: 0.8798, Reduction Loss: 0.2340 | Validation - Acc: 0.7706, Reduction Loss: 0.2323\n",
      "Step  9 | Training - Loss: 0.8528, Reduction Loss: 0.2136 | Validation - Acc: 0.7869, Reduction Loss: 0.2109\n",
      "Test - Accuracy: 0.4663, Feature reduction loss:652420.4375\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Step  0 | Training - Loss: 1.3366, Reduction Loss: 0.6438 | Validation - Acc: 0.5558, Reduction Loss: 0.6438\n",
      "Step  1 | Training - Loss: 1.2702, Reduction Loss: 0.5638 | Validation - Acc: 0.5540, Reduction Loss: 0.5636\n",
      "Step  2 | Training - Loss: 1.2058, Reduction Loss: 0.4750 | Validation - Acc: 0.5557, Reduction Loss: 0.4747\n",
      "Step  3 | Training - Loss: 1.1436, Reduction Loss: 0.3957 | Validation - Acc: 0.5732, Reduction Loss: 0.3959\n",
      "Step  4 | Training - Loss: 1.0929, Reduction Loss: 0.3389 | Validation - Acc: 0.5931, Reduction Loss: 0.3386\n",
      "Step  5 | Training - Loss: 1.0442, Reduction Loss: 0.3011 | Validation - Acc: 0.6248, Reduction Loss: 0.3016\n",
      "Step  6 | Training - Loss: 1.0003, Reduction Loss: 0.2765 | Validation - Acc: 0.6560, Reduction Loss: 0.2761\n",
      "Step  7 | Training - Loss: 0.9555, Reduction Loss: 0.2597 | Validation - Acc: 0.6976, Reduction Loss: 0.2595\n",
      "Step  8 | Training - Loss: 0.9224, Reduction Loss: 0.2390 | Validation - Acc: 0.7154, Reduction Loss: 0.2376\n",
      "Step  9 | Training - Loss: 0.8905, Reduction Loss: 0.2197 | Validation - Acc: 0.7209, Reduction Loss: 0.2194\n",
      "Test - Accuracy: 0.4984, Feature reduction loss:1044.6680\n",
      "Training for 5 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.1299 - acc: 0.6919 - val_loss: 0.7951 - val_acc: 0.9623\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.5612 - acc: 0.9608 - val_loss: 0.1478 - val_acc: 0.9715\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1002 - acc: 0.9761 - val_loss: 0.0821 - val_acc: 0.9777\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0722 - acc: 0.9783 - val_loss: 0.0673 - val_acc: 0.9783\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0606 - acc: 0.9792 - val_loss: 0.0581 - val_acc: 0.9784\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0533 - acc: 0.9796 - val_loss: 0.0528 - val_acc: 0.9797\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0497 - acc: 0.9800 - val_loss: 0.0503 - val_acc: 0.9802\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0477 - acc: 0.9803 - val_loss: 0.0482 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0462 - acc: 0.9805 - val_loss: 0.0472 - val_acc: 0.9806\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0447 - acc: 0.9807 - val_loss: 0.0464 - val_acc: 0.9806\n",
      "22368/22544 [============================>.] - ETA: 0s\n",
      " Test loss: 8.768838155176557, accuracy: 0.4026348474095103\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.5187 - acc: 0.5205 - val_loss: 1.4345 - val_acc: 0.5355\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.3698 - acc: 0.5343 - val_loss: 1.3084 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.2640 - acc: 0.5343 - val_loss: 1.2194 - val_acc: 0.5355\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.1895 - acc: 0.5343 - val_loss: 1.1567 - val_acc: 0.5355\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.1370 - acc: 0.5343 - val_loss: 1.1122 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.0993 - acc: 0.5343 - val_loss: 1.0800 - val_acc: 0.5355\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.0718 - acc: 0.5343 - val_loss: 1.0562 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.0513 - acc: 0.5343 - val_loss: 1.0381 - val_acc: 0.5355\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100778/100778 [==============================] - 1s - loss: 1.0355 - acc: 0.5343 - val_loss: 1.0242 - val_acc: 0.5355\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.0232 - acc: 0.5343 - val_loss: 1.0132 - val_acc: 0.5355\n",
      "21248/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 1.0147234612614489, accuracy: 0.5341554293825408\n",
      "Training for 5 labels on VAE Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Step  0 | Training - Loss: 2.1789, Reduction Loss: 0.6455 | Validation - Acc: 0.3845, Reduction Loss: 0.6456\n",
      "Step  1 | Training - Loss: 2.0869, Reduction Loss: 0.5677 | Validation - Acc: 0.3901, Reduction Loss: 0.5679\n",
      "Step  2 | Training - Loss: 2.0001, Reduction Loss: 0.4793 | Validation - Acc: 0.4115, Reduction Loss: 0.4790\n",
      "Step  3 | Training - Loss: 1.9152, Reduction Loss: 0.4011 | Validation - Acc: 0.4835, Reduction Loss: 0.4017\n",
      "Step  4 | Training - Loss: 1.8349, Reduction Loss: 0.3457 | Validation - Acc: 0.5548, Reduction Loss: 0.3460\n",
      "Step  5 | Training - Loss: 1.7563, Reduction Loss: 0.3071 | Validation - Acc: 0.5838, Reduction Loss: 0.3085\n",
      "Step  6 | Training - Loss: 1.7018, Reduction Loss: 0.2766 | Validation - Acc: 0.5856, Reduction Loss: 0.2775\n",
      "Step  7 | Training - Loss: 1.6539, Reduction Loss: 0.2622 | Validation - Acc: 0.6099, Reduction Loss: 0.2617\n",
      "Step  8 | Training - Loss: 1.6029, Reduction Loss: 0.2486 | Validation - Acc: 0.6547, Reduction Loss: 0.2489\n",
      "Step  9 | Training - Loss: 1.5657, Reduction Loss: 0.2314 | Validation - Acc: 0.6687, Reduction Loss: 0.2318\n",
      "Test - Accuracy: 0.4382, Feature reduction loss:2679747328.0000\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Step  0 | Training - Loss: 2.2018, Reduction Loss: 0.6497 | Validation - Acc: 0.4268, Reduction Loss: 0.6497\n",
      "Step  1 | Training - Loss: 2.0672, Reduction Loss: 0.5790 | Validation - Acc: 0.5408, Reduction Loss: 0.5793\n",
      "Step  2 | Training - Loss: 1.9532, Reduction Loss: 0.4905 | Validation - Acc: 0.5649, Reduction Loss: 0.4904\n",
      "Step  3 | Training - Loss: 1.8467, Reduction Loss: 0.4167 | Validation - Acc: 0.6187, Reduction Loss: 0.4172\n",
      "Step  4 | Training - Loss: 1.7466, Reduction Loss: 0.3528 | Validation - Acc: 0.6539, Reduction Loss: 0.3545\n",
      "Step  5 | Training - Loss: 1.6576, Reduction Loss: 0.3056 | Validation - Acc: 0.6911, Reduction Loss: 0.3058\n",
      "Step  6 | Training - Loss: 1.6026, Reduction Loss: 0.2528 | Validation - Acc: 0.6929, Reduction Loss: 0.2537\n",
      "Step  7 | Training - Loss: 1.5493, Reduction Loss: 0.2185 | Validation - Acc: 0.7163, Reduction Loss: 0.2201\n",
      "Step  8 | Training - Loss: 1.5129, Reduction Loss: 0.1788 | Validation - Acc: 0.7224, Reduction Loss: 0.1815\n",
      "Step  9 | Training - Loss: 1.4838, Reduction Loss: 0.1476 | Validation - Acc: 0.7379, Reduction Loss: 0.1458\n",
      "Test - Accuracy: 0.3842, Feature reduction loss:2041496.7500\n"
     ]
    }
   ],
   "source": [
    "# Scenario for classes = 2\n",
    "df_results_2label_dense = run_scenario_dense(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "df_results_2label_vae, labels2_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "\n",
    "# Scenario for classes = 5\n",
    "df_results_5label_dense = run_scenario_dense(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)\n",
    "df_results_5label_vae, labels5_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.989442</td>\n",
       "      <td>0.473252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.690738</td>\n",
       "      <td>0.534154</td>\n",
       "      <td>0.534155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_units  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10             4              2    0.048020             0.989442   \n",
       "1      10             4              4    0.690738             0.534154   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.473252  \n",
       "1       0.534155  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852779</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.466332</td>\n",
       "      <td>0.213570</td>\n",
       "      <td>0.210913</td>\n",
       "      <td>652420.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.890489</td>\n",
       "      <td>0.720937</td>\n",
       "      <td>0.498447</td>\n",
       "      <td>0.219737</td>\n",
       "      <td>0.219393</td>\n",
       "      <td>1044.667969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_units  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10             4              2    0.852779             0.786862   \n",
       "1      10             4              4    0.890489             0.720937   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.466332                      0.213570                      0.210913   \n",
       "1       0.498447                      0.219737                      0.219393   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                652420.437500  \n",
       "1                  1044.667969  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>0.980631</td>\n",
       "      <td>0.402635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.017929</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.534155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_units  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10             4              2    0.043760             0.980631   \n",
       "1      10             4              4    1.017929             0.535543   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.402635  \n",
       "1       0.534155  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.565730</td>\n",
       "      <td>0.668704</td>\n",
       "      <td>0.438165</td>\n",
       "      <td>0.231430</td>\n",
       "      <td>0.231765</td>\n",
       "      <td>2.679747e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.483811</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>0.384226</td>\n",
       "      <td>0.147648</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>2.041497e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_units  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10             4              2    1.565730             0.668704   \n",
       "1      10             4              4    1.483811             0.737885   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.438165                      0.231430                      0.231765   \n",
       "1       0.384226                      0.147648                      0.145793   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                 2.679747e+09  \n",
       "1                 2.041497e+06  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[6775 5267]\n",
      " [6040 4462]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAGfCAYAAAAtY8c9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB0Z6MRmNwhk3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3315e2358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = labels2_pred_vae_arr[:,0], y_true = labels2_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, output_columns_2labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 1279 10763     0     0     0]\n",
      " [  888  7383     0     0     0]\n",
      " [  222  1815     0     0     0]\n",
      " [   17   166     0     0     0]\n",
      " [    0    11     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAIkCAYAAADyCkRHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMwqy4GQmZmZVdb/Aye5j0ZSPaPzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3316af080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_5labels = confusion_matrix(y_pred = labels5_pred_vae_arr[:,0], y_true = labels5_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[8,8])\n",
    "plot_confusion_matrix(cm_5labels, output_columns_5labels, normalize = False)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Implemented Grad clip, getting good accuracy!",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
