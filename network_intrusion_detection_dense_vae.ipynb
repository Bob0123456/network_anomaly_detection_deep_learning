{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"x\"]\n",
    "\n",
    "kdd_train = pd.read_csv(\"dataset/KDDTrain+.txt\",names = col_names,)\n",
    "kdd_test = pd.read_csv(\"dataset/KDDTest+.txt\",names = col_names,)\n",
    "\n",
    "kdd_train = kdd_train.drop(\"x\", axis = 1)\n",
    "kdd_test = kdd_test.drop(\"x\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"protocol_type\",\"service\",\"flag\"]\n",
    "for cv in category_variables:\n",
    "    kdd_train[cv] = kdd_train[cv].astype(\"category\")\n",
    "kdd_train[\"label\"] = kdd_train[\"label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125966</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125967</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>udp</td>\n",
       "      <td>private</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>klogin</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0              0           tcp  ftp_data   SF        491          0     0   \n",
       "1              0           udp     other   SF        146          0     0   \n",
       "2              0           tcp   private   S0          0          0     0   \n",
       "3              0           tcp      http   SF        232       8153     0   \n",
       "4              0           tcp      http   SF        199        420     0   \n",
       "5              0           tcp   private  REJ          0          0     0   \n",
       "6              0           tcp   private   S0          0          0     0   \n",
       "...          ...           ...       ...  ...        ...        ...   ...   \n",
       "125966         0           tcp   private   S0          0          0     0   \n",
       "125967         0           tcp      http   SF        359        375     0   \n",
       "125968         0           tcp   private   S0          0          0     0   \n",
       "125969         8           udp   private   SF        105        145     0   \n",
       "125970         0           tcp      smtp   SF       2231        384     0   \n",
       "125971         0           tcp    klogin   S0          0          0     0   \n",
       "125972         0           tcp  ftp_data   SF        151          0     0   \n",
       "\n",
       "        wrong_fragment  urgent  hot   ...     dst_host_srv_count  \\\n",
       "0                    0       0    0   ...                     25   \n",
       "1                    0       0    0   ...                      1   \n",
       "2                    0       0    0   ...                     26   \n",
       "3                    0       0    0   ...                    255   \n",
       "4                    0       0    0   ...                    255   \n",
       "5                    0       0    0   ...                     19   \n",
       "6                    0       0    0   ...                      9   \n",
       "...                ...     ...  ...   ...                    ...   \n",
       "125966               0       0    0   ...                     13   \n",
       "125967               0       0    0   ...                    255   \n",
       "125968               0       0    0   ...                     25   \n",
       "125969               0       0    0   ...                    244   \n",
       "125970               0       0    0   ...                     30   \n",
       "125971               0       0    0   ...                      8   \n",
       "125972               0       0    0   ...                     77   \n",
       "\n",
       "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                         0.17                    0.03   \n",
       "1                         0.00                    0.60   \n",
       "2                         0.10                    0.05   \n",
       "3                         1.00                    0.00   \n",
       "4                         1.00                    0.00   \n",
       "5                         0.07                    0.07   \n",
       "6                         0.04                    0.05   \n",
       "...                        ...                     ...   \n",
       "125966                    0.05                    0.07   \n",
       "125967                    1.00                    0.00   \n",
       "125968                    0.10                    0.06   \n",
       "125969                    0.96                    0.01   \n",
       "125970                    0.12                    0.06   \n",
       "125971                    0.03                    0.05   \n",
       "125972                    0.30                    0.03   \n",
       "\n",
       "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                              0.17                         0.00   \n",
       "1                              0.88                         0.00   \n",
       "2                              0.00                         0.00   \n",
       "3                              0.03                         0.04   \n",
       "4                              0.00                         0.00   \n",
       "5                              0.00                         0.00   \n",
       "6                              0.00                         0.00   \n",
       "...                             ...                          ...   \n",
       "125966                         0.00                         0.00   \n",
       "125967                         0.33                         0.04   \n",
       "125968                         0.00                         0.00   \n",
       "125969                         0.01                         0.00   \n",
       "125970                         0.00                         0.00   \n",
       "125971                         0.00                         0.00   \n",
       "125972                         0.30                         0.00   \n",
       "\n",
       "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                       0.00                      0.00                  0.05   \n",
       "1                       0.00                      0.00                  0.00   \n",
       "2                       1.00                      1.00                  0.00   \n",
       "3                       0.03                      0.01                  0.00   \n",
       "4                       0.00                      0.00                  0.00   \n",
       "5                       0.00                      0.00                  1.00   \n",
       "6                       1.00                      1.00                  0.00   \n",
       "...                      ...                       ...                   ...   \n",
       "125966                  1.00                      1.00                  0.00   \n",
       "125967                  0.33                      0.00                  0.00   \n",
       "125968                  1.00                      1.00                  0.00   \n",
       "125969                  0.00                      0.00                  0.00   \n",
       "125970                  0.72                      0.00                  0.01   \n",
       "125971                  1.00                      1.00                  0.00   \n",
       "125972                  0.00                      0.00                  0.00   \n",
       "\n",
       "        dst_host_srv_rerror_rate    label  \n",
       "0                           0.00   normal  \n",
       "1                           0.00   normal  \n",
       "2                           0.00  neptune  \n",
       "3                           0.01   normal  \n",
       "4                           0.00   normal  \n",
       "5                           1.00  neptune  \n",
       "6                           0.00  neptune  \n",
       "...                          ...      ...  \n",
       "125966                      0.00  neptune  \n",
       "125967                      0.00   normal  \n",
       "125968                      0.00  neptune  \n",
       "125969                      0.00   normal  \n",
       "125970                      0.00   normal  \n",
       "125971                      0.00  neptune  \n",
       "125972                      0.00   normal  \n",
       "\n",
       "[125973 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125973.00000</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.14465</td>\n",
       "      <td>4.556674e+04</td>\n",
       "      <td>1.977911e+04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.279250</td>\n",
       "      <td>...</td>\n",
       "      <td>182.148945</td>\n",
       "      <td>115.653005</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.284452</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.120240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2604.51531</td>\n",
       "      <td>5.870331e+06</td>\n",
       "      <td>4.021269e+06</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>23.942042</td>\n",
       "      <td>...</td>\n",
       "      <td>99.206213</td>\n",
       "      <td>110.702741</td>\n",
       "      <td>0.448949</td>\n",
       "      <td>0.188922</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.306557</td>\n",
       "      <td>0.319459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42908.00000</td>\n",
       "      <td>1.379964e+09</td>\n",
       "      <td>1.309937e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7479.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes           land  \\\n",
       "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
       "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
       "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
       "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
       "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
       "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
       "mean         0.022687       0.000111       0.204409           0.001222   \n",
       "std          0.253530       0.014366       2.149968           0.045239   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      77.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  125973.000000    125973.000000            ...              \n",
       "mean        0.395736         0.279250            ...              \n",
       "std         0.489010        23.942042            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         1.000000         0.000000            ...              \n",
       "max         1.000000      7479.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   125973.000000       125973.000000           125973.000000   \n",
       "mean       182.148945          115.653005                0.521242   \n",
       "std         99.206213          110.702741                0.448949   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%         82.000000           10.000000                0.050000   \n",
       "50%        255.000000           63.000000                0.510000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           125973.000000                125973.000000   \n",
       "mean                 0.082951                     0.148379   \n",
       "std                  0.188922                     0.308997   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.020000                     0.000000   \n",
       "75%                  0.070000                     0.060000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                125973.000000         125973.000000   \n",
       "mean                      0.032542              0.284452   \n",
       "std                       0.112564              0.444784   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.020000              1.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             125973.000000         125973.000000   \n",
       "mean                   0.278485              0.118832   \n",
       "std                    0.445669              0.306557   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    1.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             125973.000000  \n",
       "mean                   0.120240  \n",
       "std                    0.319459  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22544.000000</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.859076</td>\n",
       "      <td>1.039545e+04</td>\n",
       "      <td>2.056019e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.105394</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>193.869411</td>\n",
       "      <td>140.750532</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.226683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1407.176612</td>\n",
       "      <td>4.727864e+05</td>\n",
       "      <td>2.121930e+04</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.928428</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>7.269597</td>\n",
       "      <td>...</td>\n",
       "      <td>94.035663</td>\n",
       "      <td>111.783972</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.273139</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.400875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870000e+02</td>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57715.000000</td>\n",
       "      <td>6.282565e+07</td>\n",
       "      <td>1.345927e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
       "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
       "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
       "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
       "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
       "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
       "mean       0.000710      0.105394           0.021647      0.442202   \n",
       "std        0.036473      0.928428           0.150328      0.496659   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      1.000000   \n",
       "max        3.000000    101.000000           4.000000      1.000000   \n",
       "\n",
       "       num_compromised            ...             dst_host_count  \\\n",
       "count     22544.000000            ...               22544.000000   \n",
       "mean          0.119899            ...                 193.869411   \n",
       "std           7.269597            ...                  94.035663   \n",
       "min           0.000000            ...                   0.000000   \n",
       "25%           0.000000            ...                 121.000000   \n",
       "50%           0.000000            ...                 255.000000   \n",
       "75%           0.000000            ...                 255.000000   \n",
       "max         796.000000            ...                 255.000000   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        22544.000000            22544.000000            22544.000000   \n",
       "mean           140.750532                0.608722                0.090540   \n",
       "std            111.783972                0.435688                0.220717   \n",
       "min              0.000000                0.000000                0.000000   \n",
       "25%             15.000000                0.070000                0.000000   \n",
       "50%            168.000000                0.920000                0.010000   \n",
       "75%            255.000000                1.000000                0.060000   \n",
       "max            255.000000                1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 22544.000000                 22544.000000   \n",
       "mean                      0.132261                     0.019638   \n",
       "std                       0.306268                     0.085394   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.030000                     0.010000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          22544.000000              22544.000000          22544.000000   \n",
       "mean               0.097814                  0.099426              0.233385   \n",
       "std                0.273139                  0.281866              0.387229   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.360000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              22544.000000  \n",
       "mean                   0.226683  \n",
       "std                    0.400875  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.170000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          normal\n",
       "1          normal\n",
       "2         neptune\n",
       "3          normal\n",
       "4          normal\n",
       "5         neptune\n",
       "6         neptune\n",
       "           ...   \n",
       "125966    neptune\n",
       "125967     normal\n",
       "125968    neptune\n",
       "125969     normal\n",
       "125970     normal\n",
       "125971    neptune\n",
       "125972     normal\n",
       "Name: label, dtype: category\n",
       "Categories (23, object): [back, buffer_overflow, ftp_write, guess_passwd, ..., spy, teardrop, warezclient, warezmaster]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "#scatter_matrix(kdd_train, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_types = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "is_sess = {\n",
    "    \"DoS\":\"Attack\",\n",
    "    \"R2L\":\"Attack\",\n",
    "    \"U2R\":\"Attack\",\n",
    "    \"Probe\":\"Attack\",\n",
    "    \"normal\":\"Normal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_train[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])\n",
    "kdd_test[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_test[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_sess_type_group = kdd_train.groupby(\"sess_type\")\n",
    "kdd_is_sess_group = kdd_train.groupby(\"is_sess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_type\n",
       "DoS       45927\n",
       "Probe     11656\n",
       "R2L         995\n",
       "U2R          52\n",
       "normal    67343\n",
       "Name: is_sess, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.is_sess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sess\n",
       "Attack    58630\n",
       "Normal    67343\n",
       "Name: sess_type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.sess_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sess_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">DoS</th>\n",
       "      <th>count</th>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.090034</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>1.692015e+02</td>\n",
       "      <td>244.600475</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.747922</td>\n",
       "      <td>26.524005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.748494</td>\n",
       "      <td>1.176321e+03</td>\n",
       "      <td>32.656346</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.445748</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>1.168004e+03</td>\n",
       "      <td>41.324475</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.188947</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>48.303117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299931</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>7.686120e+03</td>\n",
       "      <td>94.667526</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "sess_type                                                                    \n",
       "DoS       count  45927.000000   45927.000000  4.592700e+04    45927.000000   \n",
       "          mean     178.090034       0.065403  1.692015e+02      244.600475   \n",
       "          std      104.445748       0.064023  1.168004e+03       41.324475   \n",
       "          min        1.000000       0.000000  0.000000e+00        1.000000   \n",
       "          25%      109.000000       0.050000  0.000000e+00      255.000000   \n",
       "          50%      172.000000       0.060000  0.000000e+00      255.000000   \n",
       "          75%      249.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                       ...            ...           ...             ...   \n",
       "normal    mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "          std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "          min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "          25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "          50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "          75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "          max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "                 dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "sess_type                                                       \n",
       "DoS       count            45927.000000          45927.000000   \n",
       "          mean                 0.066333              0.157569   \n",
       "          std                  0.058079              0.358934   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.050000              0.000000   \n",
       "          50%                  0.070000              0.000000   \n",
       "          75%                  0.070000              0.000000   \n",
       "...                                 ...                   ...   \n",
       "normal    mean                 0.040134              0.046589   \n",
       "          std                  0.128529              0.195306   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.000000              0.000000   \n",
       "          50%                  0.000000              0.000000   \n",
       "          75%                  0.020000              0.000000   \n",
       "          max                  1.000000              1.000000   \n",
       "\n",
       "                 dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "sess_type                                                              \n",
       "DoS       count                 45927.000000            45927.000000   \n",
       "          mean                      0.049492                0.123423   \n",
       "          std                       0.188947                0.228287   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.020000   \n",
       "          50%                       0.000000                0.050000   \n",
       "          75%                       0.000000                0.080000   \n",
       "...                                      ...                     ...   \n",
       "normal    mean                      0.121726                0.811875   \n",
       "          std                       0.254382                0.324091   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.750000   \n",
       "          50%                       0.010000                1.000000   \n",
       "          75%                       0.080000                1.000000   \n",
       "          max                       1.000000                1.000000   \n",
       "\n",
       "                 dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "sess_type                                                       ...         \n",
       "DoS       count          45927.000000        45927.000000       ...         \n",
       "          mean               0.747922           26.524005       ...         \n",
       "          std                0.431707           48.303117       ...         \n",
       "          min                0.000000            1.000000       ...         \n",
       "          25%                0.180000            6.000000       ...         \n",
       "          50%                1.000000           13.000000       ...         \n",
       "          75%                1.000000           20.000000       ...         \n",
       "...                               ...                 ...       ...         \n",
       "normal    mean               0.013930          190.285761       ...         \n",
       "          std                0.092006           92.608377       ...         \n",
       "          min                0.000000            0.000000       ...         \n",
       "          25%                0.000000          121.000000       ...         \n",
       "          50%                0.000000          255.000000       ...         \n",
       "          75%                0.000000          255.000000       ...         \n",
       "          max                1.000000          255.000000       ...         \n",
       "\n",
       "                 same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "sess_type                                                                  \n",
       "DoS       count   45927.000000  45927.000000  4.592700e+04  45927.000000   \n",
       "          mean        0.191887      0.748494  1.176321e+03     32.656346   \n",
       "          std         0.299931      0.432559  7.686120e+03     94.667526   \n",
       "          min         0.000000      0.000000  0.000000e+00      1.000000   \n",
       "          25%         0.040000      0.290000  0.000000e+00      5.000000   \n",
       "          50%         0.070000      1.000000  0.000000e+00     11.000000   \n",
       "          75%         0.150000      1.000000  0.000000e+00     18.000000   \n",
       "...                        ...           ...           ...           ...   \n",
       "normal    mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "          std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "          min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "          25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "          50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "          75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "          max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "                 srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "sess_type                                                               \n",
       "DoS       count        45927.000000     45927.000000     45927.000000   \n",
       "          mean             0.005317         0.153000         0.746678   \n",
       "          std              0.056390         0.357561         0.434050   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         1.000000   \n",
       "          75%              0.000000         0.000000         1.000000   \n",
       "...                             ...              ...              ...   \n",
       "normal    mean             0.126263         0.044629         0.012083   \n",
       "          std              0.271621         0.202264         0.086426   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         0.000000   \n",
       "          75%              0.110000         0.000000         0.000000   \n",
       "          max              1.000000         1.000000         1.000000   \n",
       "\n",
       "                 su_attempted        urgent  wrong_fragment  \n",
       "sess_type                                                    \n",
       "DoS       count  45927.000000  45927.000000    45927.000000  \n",
       "          mean       0.000000      0.000000        0.062229  \n",
       "          std        0.000000      0.000000        0.416951  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "...                       ...           ...             ...  \n",
       "normal    mean       0.002049      0.000148        0.000000  \n",
       "          std        0.061622      0.017233        0.000000  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "          max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[40 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Attack</th>\n",
       "      <th>count</th>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.849991</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>3.752448e+04</td>\n",
       "      <td>222.025260</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.595177</td>\n",
       "      <td>29.929081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>8.282014e+04</td>\n",
       "      <td>27.797885</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0.209114</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.048746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.334483</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>5.893991e+06</td>\n",
       "      <td>79.196259</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>52.289254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>8.593025e+06</td>\n",
       "      <td>84.710761</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.369916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "is_sess                                                                    \n",
       "Attack  count  58630.000000   58630.000000  5.863000e+04    58630.000000   \n",
       "        mean     154.849991       0.102410  3.752448e+04      222.025260   \n",
       "        std      124.334483       0.206408  5.893991e+06       79.196259   \n",
       "        min        0.000000       0.000000  0.000000e+00        1.000000   \n",
       "        25%       40.000000       0.050000  0.000000e+00      255.000000   \n",
       "        50%      138.000000       0.060000  0.000000e+00      255.000000   \n",
       "        75%      241.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                     ...            ...           ...             ...   \n",
       "Normal  mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "        std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "        min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "        25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "        50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "        75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "        max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "               dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "is_sess                                                       \n",
       "Attack  count            58630.000000          58630.000000   \n",
       "        mean                 0.132131              0.201810   \n",
       "        std                  0.230626              0.381090   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.050000              0.000000   \n",
       "        50%                  0.070000              0.000000   \n",
       "        75%                  0.080000              0.020000   \n",
       "...                               ...                   ...   \n",
       "Normal  mean                 0.040134              0.046589   \n",
       "        std                  0.128529              0.195306   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.000000              0.000000   \n",
       "        50%                  0.000000              0.000000   \n",
       "        75%                  0.020000              0.000000   \n",
       "        max                  1.000000              1.000000   \n",
       "\n",
       "               dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "is_sess                                                              \n",
       "Attack  count                 58630.000000            58630.000000   \n",
       "        mean                      0.178993                0.187417   \n",
       "        std                       0.359262                0.322430   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.020000   \n",
       "        50%                       0.000000                0.050000   \n",
       "        75%                       0.020000                0.090000   \n",
       "...                                    ...                     ...   \n",
       "Normal  mean                      0.121726                0.811875   \n",
       "        std                       0.254382                0.324091   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.750000   \n",
       "        50%                       0.010000                1.000000   \n",
       "        75%                       0.080000                1.000000   \n",
       "        max                       1.000000                1.000000   \n",
       "\n",
       "               dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "is_sess                                                       ...         \n",
       "Attack  count          58630.000000        58630.000000       ...         \n",
       "        mean               0.595177           29.929081       ...         \n",
       "        std                0.484495           52.289254       ...         \n",
       "        min                0.000000            1.000000       ...         \n",
       "        25%                0.000000            4.000000       ...         \n",
       "        50%                1.000000           12.000000       ...         \n",
       "        75%                1.000000           21.000000       ...         \n",
       "...                             ...                 ...       ...         \n",
       "Normal  mean               0.013930          190.285761       ...         \n",
       "        std                0.092006           92.608377       ...         \n",
       "        min                0.000000            0.000000       ...         \n",
       "        25%                0.000000          121.000000       ...         \n",
       "        50%                0.000000          255.000000       ...         \n",
       "        75%                0.000000          255.000000       ...         \n",
       "        max                1.000000          255.000000       ...         \n",
       "\n",
       "               same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "is_sess                                                                  \n",
       "Attack  count   58630.000000  58630.000000  5.863000e+04  58630.000000   \n",
       "        mean        0.306659      0.595808  8.282014e+04     27.797885   \n",
       "        std         0.395655      0.486588  8.593025e+06     84.710761   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         0.040000      0.000000  0.000000e+00      3.000000   \n",
       "        50%         0.080000      1.000000  0.000000e+00     10.000000   \n",
       "        75%         0.500000      1.000000  0.000000e+00     18.000000   \n",
       "...                      ...           ...           ...           ...   \n",
       "Normal  mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "        std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "        50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "        75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "        max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "               srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "is_sess                                                               \n",
       "Attack  count        58630.000000     58630.000000     58630.000000   \n",
       "        mean             0.064079         0.209114         0.593072   \n",
       "        std              0.241348         0.404487         0.490234   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         1.000000   \n",
       "        75%              0.000000         0.000000         1.000000   \n",
       "...                           ...              ...              ...   \n",
       "Normal  mean             0.126263         0.044629         0.012083   \n",
       "        std              0.271621         0.202264         0.086426   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         0.000000   \n",
       "        75%              0.110000         0.000000         0.000000   \n",
       "        max              1.000000         1.000000         1.000000   \n",
       "\n",
       "               su_attempted        urgent  wrong_fragment  \n",
       "is_sess                                                    \n",
       "Attack  count  58630.000000  58630.000000    58630.000000  \n",
       "        mean       0.000017      0.000068        0.048746  \n",
       "        std        0.004130      0.010116        0.369916  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "...                     ...           ...             ...  \n",
       "Normal  mean       0.002049      0.000148        0.000000  \n",
       "        std        0.061622      0.017233        0.000000  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "        max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[16 rows x 38 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kdd_is_sess_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kdd_sess_type_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_dummy(train, test):\n",
    "    dummy_variables_2labels = [*category_variables, \"is_sess\"]\n",
    "    dummy_variables_5labels = [*category_variables, \"sess_type\"]\n",
    "\n",
    "    drop_variables = [*category_variables, \"is_sess\", \"sess_type\", \"duration\", \"label\"]\n",
    "    \n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    def dummy(kdd):\n",
    "        kdd_one_hot_2labels = pd.get_dummies(kdd[dummy_variables_2labels], prefix=dummy_variables_2labels, drop_first=False)\n",
    "        kdd_one_hot_5labels = pd.get_dummies(kdd[dummy_variables_5labels], prefix=dummy_variables_5labels, drop_first=False)\n",
    "\n",
    "        kdd_2labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                       , kdd_one_hot_2labels]\n",
    "                                      , axis = 1)\n",
    "        kdd_5labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                      , kdd_one_hot_5labels]\n",
    "                                      , axis = 1)\n",
    "\n",
    "        return kdd_2labels, kdd_5labels\n",
    "    \n",
    "    kdd_2labels, kdd_5labels = dummy(pd.concat([train, test], axis = 0))\n",
    "    \n",
    "    kdd_2labels_train, kdd_2labels_test = kdd_2labels.iloc[:train_size,:], kdd_2labels.iloc[train_size:,:]\n",
    "    kdd_5labels_train, kdd_5labels_test = kdd_5labels.iloc[:train_size,:], kdd_5labels.iloc[train_size:,:]\n",
    "    \n",
    "    return kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_columns_2labels = ['is_sess_Normal', 'is_sess_Attack']\n",
    "output_columns_5labels = ['sess_type_normal', 'sess_type_DoS', 'sess_type_Probe', 'sess_type_R2L', 'sess_type_U2R']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test = encode_dummy(kdd_train, kdd_test)\n",
    "\n",
    "x_kdd_train = kdd_2labels_train.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_train = kdd_2labels_train.loc[:,output_columns_2labels].values \n",
    "y_5labels_train = kdd_5labels_train.loc[:,output_columns_5labels].values\n",
    "\n",
    "x_kdd_test = kdd_2labels_test.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_test = kdd_2labels_test.loc[:,output_columns_2labels].values \n",
    "y_5labels_test = kdd_5labels_test.loc[:,output_columns_5labels].values\n",
    "\n",
    "        \n",
    "ss = pp.StandardScaler()\n",
    "x_kdd_train = ss.fit_transform(x_kdd_train)\n",
    "x_test = ss.transform(x_kdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sanity before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: mean:-0.0000, std:0.9959, shape:(125973, 121)\n",
      "Testing  data: mean:0.0170, std:1.4175, shape:(22544, 121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_kdd_train.mean(), x_kdd_train.std(), x_kdd_train.shape))\n",
    "print(\"Testing  data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_test.mean(), x_test.std(), x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "                      input_dim = 121, classes = 2, hidden_layers = 8, epochs = 10, hidden_units = 5):\n",
    "   \n",
    "    model_dense = Sequential()\n",
    "    model_dense.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        model_dense.add(Dense(hidden_units, kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    model_dense.add(Dense(classes, kernel_initializer='uniform' ,activation = 'softmax'))\n",
    "\n",
    "    model_dense.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'],\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                  activity_regularizer=keras.regularizers.l1(0.01)) \n",
    "\n",
    "    model_dense.fit(x_train, y_train, \n",
    "                  epochs = epochs, batch_size=500, \n",
    "                  validation_data = (x_valid, y_valid),\n",
    "                  verbose=1)\n",
    "    \n",
    "    scores_train = model_dense.evaluate(x_train, y_train)\n",
    "    scores_valid = model_dense.evaluate(x_valid, y_valid)\n",
    "    scores_test = model_dense.evaluate(x_test, y_test)\n",
    "    train_loss = scores_train[0]\n",
    "    valid_accuracy = scores_valid[1] \n",
    "    test_accuracy = scores_test[1]\n",
    "    \n",
    "    print(\"\\n Test loss: {}, accuracy: {}\".format(scores_test[0], scores_test[1]))\n",
    "    return train_loss, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_vae(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5):\n",
    "    \n",
    "    graph_vae = tf.Graph()\n",
    "    with graph_vae.as_default():\n",
    "        latent_dim = hidden_units\n",
    "        dense_hidden_units = hidden_units\n",
    "        hidden_encoder_dim = 60\n",
    "        hidden_decoder_dim = 60\n",
    "\n",
    "        lam = 0.01\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            initial = tf.constant(0.01, shape=shape)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        l2_loss = tf.constant(0.001)\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "                b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "\n",
    "                # Hidden layer encoder\n",
    "                hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "                tf.summary.histogram(\"Weights_Encoder\", W_encoder_input_hidden)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "\n",
    "            # Mu encoder\n",
    "            mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "            tf.summary.histogram(\"Weights_Mean\", W_encoder_hidden_mu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "\n",
    "            # Sigma encoder\n",
    "            logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "            tf.summary.histogram(\"Weights_Variance\", W_encoder_hidden_logvar)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "                b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "\n",
    "                # Hidden layer decoder\n",
    "                hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "                tf.summary.histogram(\"Weights_Decoder\", W_decoder_z_hidden)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "            b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "            l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "\n",
    "            x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "            tf.summary.histogram(\"Weights_Reconstruction\", W_decoder_hidden_reconstruction)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), \n",
    "                                 reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), \n",
    "                                        reduction_indices=1)\n",
    "            \n",
    "            reconst_loss = tf.abs(tf.reduce_mean(BCE + KLD))\n",
    "            regularized_loss = reconst_loss + lam * l2_loss\n",
    "\n",
    "            tf.summary.scalar(\"BCE\", tf.reduce_mean(BCE))\n",
    "            tf.summary.scalar(\"KLD\", tf.reduce_mean(KLD))\n",
    "\n",
    "            tf.summary.scalar(\"Total_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph_vae) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                sess.run(optimizer, feed_dict={x: x_train[i,:], keep_prob:0.6})\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train, train_reduction_loss, train_z = sess.run([regularized_loss, summary_op, reconst_loss, z], feed_dict={x: x_train, keep_prob:1})\n",
    "            summary_str_valid, valid_reduction_loss, valid_z = sess.run([summary_op, reconst_loss, z], feed_dict={x: x_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}, Reduction Loss: {:.4f} | \"\n",
    "                      \"Validation - Reduction Loss: {:.4f}\"\n",
    "                      .format(epoch, train_loss, train_reduction_loss, valid_reduction_loss))\n",
    "\n",
    "        test_reduction_loss, test_z = sess.run([reconst_loss, z], feed_dict={x: x_test, keep_prob:1})\n",
    "        print(\"Test - Feature reduction loss:{:.4f}\".format(test_reduction_loss))\n",
    "    return train_reduction_loss, valid_reduction_loss, test_reduction_loss, train_z, valid_z, test_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_softmax(z_train, z_valid, y_train, y_valid, z_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5,\n",
    "              only_vae = False):\n",
    "\n",
    "    latent_dim = hidden_units\n",
    "    dense_hidden_units = hidden_units\n",
    "    \n",
    "    graph_softmax = tf.Graph()\n",
    "    with graph_softmax.as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"Sampled_Distribution\"):\n",
    "            z = tf.placeholder(\"float32\", shape=[None, latent_dim], name=\"Z\")\n",
    "            y_ = tf.placeholder(\"float\", shape=[None, classes], name = \"y_\")\n",
    "            keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "\n",
    "        z_h = z\n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):    \n",
    "            z_h = tf.layers.dense(z_h,dense_hidden_units, activation=tf.nn.relu)\n",
    "            z_h = tf.nn.dropout(z_h, keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z_h, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            regularized_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "            pred = tf.argmax(y, 1)\n",
    "            actual = tf.argmax(y_, 1)\n",
    "            \n",
    "            correct_prediction = tf.equal(actual, pred)\n",
    "            tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "            tf.summary.scalar(\"Softmax_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "\n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(z_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    with tf.Session(graph=graph_softmax) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/softmax/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/softmax/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                feed_dict={z: z_train[i,:], y_: y_train[i,:], keep_prob:0.6}\n",
    "                sess.run(optimizer, feed_dict = feed_dict)\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train = sess.run([regularized_loss, summary_op], feed_dict={z: z_train, y_: y_train, keep_prob:1})\n",
    "            valid_accuracy, summary_str_valid = sess.run([tf_accuracy, summary_op], feed_dict={z: z_valid, y_:y_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}| \"\n",
    "                      \"Validation - Acc: {:.4f}\"\n",
    "                      .format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "        test_accuracy, y_pred, y_actual = sess.run([tf_accuracy, pred, actual], feed_dict={z: z_test, y_:y_test, keep_prob:1})\n",
    "        print(\"Test - Accuracy: {:.4f}\".format(test_accuracy))\n",
    "\n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "\n",
    "        outputs = np.hstack((y_pred, y_actual))\n",
    "        \n",
    "    return train_loss, valid_accuracy, test_accuracy, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_scenario_vae(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"])\n",
    "    \n",
    "    print(\"Training for {} labels on VAE Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"*********** Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{} **************\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        print(\"VAE for Feature reduction:\")\n",
    "        t_r_l, v_r_l, te_r_l, t_z, v_z, te_z = train_vae(x_train, x_valid, \n",
    "                                           y_train, y_valid, \n",
    "                                           x_kdd_test, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        print(\"Softmax for Prediction:\")\n",
    "        t_l, v_a, te_a, op = train_softmax(t_z, v_z, \n",
    "                                           y_train, y_valid, \n",
    "                                           te_z, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a, t_r_l, v_r_l, te_r_l]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    \n",
    "    return df_results, op\n",
    "        \n",
    "def run_scenario_dense(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"])\n",
    "    \n",
    "    print(\"Training for {} labels on Dense Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a = train_dense(x_train, x_valid, y_train, y_valid, x_kdd_test, y_test_labels, \n",
    "                 classes = classes, \n",
    "                 hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                 hidden_units = Hyper_parameters.reduced_features,\n",
    "                 epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2094: UserWarning: Expected no kwargs, you passed 2\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.3213 - acc: 0.8814 - val_loss: 0.0811 - val_acc: 0.9741\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0684 - acc: 0.9762 - val_loss: 0.0570 - val_acc: 0.9813\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0482 - acc: 0.9828 - val_loss: 0.0422 - val_acc: 0.9825\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0353 - acc: 0.9844 - val_loss: 0.0331 - val_acc: 0.9842\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0285 - acc: 0.9859 - val_loss: 0.0290 - val_acc: 0.9888\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0244 - acc: 0.9923 - val_loss: 0.0252 - val_acc: 0.9910\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0210 - acc: 0.9937 - val_loss: 0.0216 - val_acc: 0.9931\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0185 - acc: 0.9943 - val_loss: 0.0198 - val_acc: 0.9937\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0174 - acc: 0.9943 - val_loss: 0.0192 - val_acc: 0.9933\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0182 - val_acc: 0.9945\n",
      "22208/22544 [============================>.] - ETA: 0s\n",
      " Test loss: 8.567797148456126, accuracy: 0.4659776437189496\n",
      "Training for 2 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5854, Reduction Loss: 0.5073 | Validation - Reduction Loss: 0.5072\n",
      "Step  1 | Training - Loss: 0.4481, Reduction Loss: 0.2917 | Validation - Reduction Loss: 0.2915\n",
      "Step  2 | Training - Loss: 0.3825, Reduction Loss: 0.2004 | Validation - Reduction Loss: 0.2003\n",
      "Step  3 | Training - Loss: 0.3276, Reduction Loss: 0.1670 | Validation - Reduction Loss: 0.1670\n",
      "Step  4 | Training - Loss: 0.2664, Reduction Loss: 0.1434 | Validation - Reduction Loss: 0.1431\n",
      "Step  5 | Training - Loss: 0.2112, Reduction Loss: 0.1166 | Validation - Reduction Loss: 0.1164\n",
      "Step  6 | Training - Loss: 0.1803, Reduction Loss: 0.0972 | Validation - Reduction Loss: 0.0969\n",
      "Step  7 | Training - Loss: 0.1616, Reduction Loss: 0.0841 | Validation - Reduction Loss: 0.0838\n",
      "Step  8 | Training - Loss: 0.1466, Reduction Loss: 0.0745 | Validation - Reduction Loss: 0.0742\n",
      "Step  9 | Training - Loss: 0.1348, Reduction Loss: 0.0672 | Validation - Reduction Loss: 0.0670\n",
      "Test - Feature reduction loss:inf\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.7091| Validation - Acc: 0.5067\n",
      "Step  1 | Training - Loss: 0.6963| Validation - Acc: 0.5175\n",
      "Step  2 | Training - Loss: 0.6919| Validation - Acc: 0.5339\n",
      "Step  3 | Training - Loss: 0.6912| Validation - Acc: 0.5365\n",
      "Step  4 | Training - Loss: 0.6909| Validation - Acc: 0.5367\n",
      "Step  5 | Training - Loss: 0.6908| Validation - Acc: 0.5367\n",
      "Step  6 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  7 | Training - Loss: 0.6908| Validation - Acc: 0.5365\n",
      "Step  8 | Training - Loss: 0.6909| Validation - Acc: 0.5364\n",
      "Step  9 | Training - Loss: 0.6908| Validation - Acc: 0.5364\n",
      "Test - Accuracy: 0.5341\n",
      "Training for 5 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.8709 - acc: 0.6499 - val_loss: 0.2821 - val_acc: 0.8857\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.2376 - acc: 0.9296 - val_loss: 0.2131 - val_acc: 0.9616\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1874 - acc: 0.9522 - val_loss: 0.1488 - val_acc: 0.9467\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0860 - acc: 0.9788 - val_loss: 0.0560 - val_acc: 0.9850\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0459 - acc: 0.9850 - val_loss: 0.0458 - val_acc: 0.9873\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0390 - acc: 0.9865 - val_loss: 0.0429 - val_acc: 0.9877\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0359 - acc: 0.9868 - val_loss: 0.0407 - val_acc: 0.9877\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0335 - acc: 0.9873 - val_loss: 0.0389 - val_acc: 0.9881\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0320 - acc: 0.9875 - val_loss: 0.0379 - val_acc: 0.9885\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0306 - acc: 0.9881 - val_loss: 0.0376 - val_acc: 0.9886\n",
      "21216/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 10.694165438942404, accuracy: 0.29790631653655075\n",
      "Training for 5 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5857, Reduction Loss: 0.5072 | Validation - Reduction Loss: 0.5075\n",
      "Step  1 | Training - Loss: 0.4485, Reduction Loss: 0.2917 | Validation - Reduction Loss: 0.2914\n",
      "Step  2 | Training - Loss: 0.3834, Reduction Loss: 0.2005 | Validation - Reduction Loss: 0.2015\n",
      "Step  3 | Training - Loss: 0.3283, Reduction Loss: 0.1680 | Validation - Reduction Loss: 0.1693\n",
      "Step  4 | Training - Loss: 0.2673, Reduction Loss: 0.1437 | Validation - Reduction Loss: 0.1443\n",
      "Step  5 | Training - Loss: 0.2116, Reduction Loss: 0.1165 | Validation - Reduction Loss: 0.1170\n",
      "Step  6 | Training - Loss: 0.1805, Reduction Loss: 0.0970 | Validation - Reduction Loss: 0.0976\n",
      "Step  7 | Training - Loss: 0.1616, Reduction Loss: 0.0840 | Validation - Reduction Loss: 0.0847\n",
      "Step  8 | Training - Loss: 0.1465, Reduction Loss: 0.0746 | Validation - Reduction Loss: 0.0754\n",
      "Step  9 | Training - Loss: 0.1348, Reduction Loss: 0.0675 | Validation - Reduction Loss: 0.0683\n",
      "Test - Feature reduction loss:inf\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.5687| Validation - Acc: 0.3939\n",
      "Step  1 | Training - Loss: 1.5150| Validation - Acc: 0.4670\n",
      "Step  2 | Training - Loss: 1.4792| Validation - Acc: 0.4781\n",
      "Step  3 | Training - Loss: 1.4538| Validation - Acc: 0.4876\n",
      "Step  4 | Training - Loss: 1.4333| Validation - Acc: 0.4999\n",
      "Step  5 | Training - Loss: 1.4147| Validation - Acc: 0.5314\n",
      "Step  6 | Training - Loss: 1.3977| Validation - Acc: 0.5315\n",
      "Step  7 | Training - Loss: 1.3845| Validation - Acc: 0.5316\n",
      "Step  8 | Training - Loss: 1.3761| Validation - Acc: 0.5316\n",
      "Step  9 | Training - Loss: 1.3715| Validation - Acc: 0.5316\n",
      "Test - Accuracy: 0.5342\n"
     ]
    }
   ],
   "source": [
    "# Scenario for classes = 2\n",
    "df_results_2label_dense = run_scenario_dense(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "df_results_2label_vae, labels2_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "\n",
    "# Scenario for classes = 5\n",
    "df_results_5label_dense = run_scenario_dense(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)\n",
    "df_results_5label_vae, labels5_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.994483</td>\n",
       "      <td>0.465978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.015505             0.994483   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.465978  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.690775</td>\n",
       "      <td>0.536416</td>\n",
       "      <td>0.534111</td>\n",
       "      <td>0.067246</td>\n",
       "      <td>0.066963</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.690775             0.536416   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534111                      0.067246                      0.066963   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                          inf  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029831</td>\n",
       "      <td>0.988569</td>\n",
       "      <td>0.297906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.029831             0.988569   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.297906  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.371524</td>\n",
       "      <td>0.531574</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.067456</td>\n",
       "      <td>0.068294</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    1.371524             0.531574   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534155                      0.067456                      0.068294   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                          inf  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12040     2]\n",
      " [10501     1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAGdCAYAAABkXrYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4245c86080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = labels2_pred_vae_arr[:,0], y_true = labels2_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, output_columns_2labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12041     1     0     0     0]\n",
      " [ 8270     1     0     0     0]\n",
      " [ 2037     0     0     0     0]\n",
      " [  183     0     0     0     0]\n",
      " [   11     0     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAIkCAYAAADyCkRHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xm8VWXd/vHPBQcQxRkhOWAgqMigKKBiapoZ5oSZA6UJ\nYQ5l+dhopqU9NqCWpj9LU3tyTBBTcUYf01IeEVAQBCcMTQ4q4IQD4+H7+2Otg5vjPudsjuyz99nr\nevfaL/e+13SvW3vx5Vr3WksRgZmZmVkWtSl1B8zMzMxKxYWQmZmZZZYLITMzM8ssF0JmZmaWWS6E\nzMzMLLNcCJmZmVlmuRAyMzOzzHIhZGZmZi1C0v9IWiTp2Zy2iyU9L2mWpDskbZGz7GxJ8yS9IGl4\nTvtgSbPTZZdLUtreQdL4tP1JST2b6pMLITMzM2sp1wEH12t7CBgQEbsALwJnA0jqB4wE+qfb/ElS\n23SbK4GTgR3ST90+TwLeiYg+wKXAhU11qOpTnIyZmZm1Am03+2zE6mVFPUYsWzwpIuoXOeuuE/Gv\n+ilNRDyY83MKcHT6fQQwLiJWAPMlzQP2kPQKsFlETAGQdANwJHB/us356fa3AVdIUjTyGg0XQmZm\nZhUuVi+jw07HFvUYy2f+sa+k6TlNV0fE1eu5mzHA+PR7NUlhVGdB2rYq/V6/vW6b1wAiYrWk94Ct\ngSUNHdCFkJmZWcUTqOizYZZExJDmbizpHGA1cPOG61LTPEfIzMzMSkrSaOAw4Picy1g1QI+c1bqn\nbTXp9/rt62wjqQrYHHirsWO7EDIzM6t0AqTifprbNelg4CfAERHxUc6iu4CR6Z1gvUgmRU+NiNeB\npZL2Su8WOxGYmLPNqPT70cA/GpsfBL40ZmZmZi1E0i3A/kBnSQuA80juEusAPJTeBT8lIk6LiDmS\nbgXmklwyOz0iatNdfYfkDrSOJJOk70/b/wLcmE6sfpvkrrPG+9REoWRmZmatXJtNukaHnb9e1GMs\nf+oPT32aOUKl4ktjZmZmllm+NGZmZpYFn2IeTyVzImRmZmaZ5UTIzMys4rXIc4RaJY+KmZmZZZYT\nITMzsyzwHKG8nAiZmZlZZjkRMjMzq3TCc4Qa4FExMzOzzHIiZGZmVvE+3fvAKpkTITMzM8ssJ0Jm\nZmZZ4DlCeXlUzMzMLLOcCJmZmWWB5wjl5UTIzMzMMsuJkJmZWcXzu8Ya4lExMzOzzHIiZGZmVumE\n5wg1wImQmZmZZZYTITMzsyzwHKG8PCpmZmaWWU6EzMzMKp7vGmuIR8XMzMwyy4mQmZlZFrTxXWP5\nOBEyMzOzzHIiZGZmVumE5wg1wKNiZmZmmeVEyMzMLAv8ZOm8nAiZmZlZZjkRMjMzq3h+jlBDPCpm\nZmaWWU6EzMzMssBzhPJyImRmZmaZ5UTIzMwsCzxHKC+PipmZmWWWEyEzM7NKJ3mOUAOcCJmZmVlm\nOREyMzPLAs8RysujYmZmZpnlRMjMzCwLPEcoLydCZmZmlllOhMzMzCqe3zXWEI+KmZmZZZYTITMz\nsyzwHKG8nAiZmZlZZjkRMjMzq3TCc4Qa4FExMzOzzHIiZGZmVvF811hDPCpmZmaWWU6EzMzMssB3\njeXlRMjMPhVJHSXdLek9SRM+xX6Ol/TghuxbqUjaV9ILpe6HmTXNhZBZRkj6uqTpkj6Q9Lqk+yXt\nswF2fTTQFdg6Io5p7k4i4uaI+NIG6E9RSQpJfRpbJyIei4idWqpPZgVRm+J+WqnW23MzK5ikHwB/\nAH5DUrRsB/wROGID7P6zwIsRsXoD7KvVk+QpB2atiAshswonaXPgv4HTI+L2iPgwIlZFxD0R8ZN0\nnQ6S/iBpYfr5g6QO6bL9JS2Q9ENJi9I06Zvpsl8CvwCOS5OmkySdL+mmnOP3TFOUqvT3aEn/lvS+\npPmSjs9pfzxnu70lTUsvuU2TtHfOskclXSBpcrqfByV1buD86/r/k5z+HynpEEkvSnpb0s9y1t9D\n0hOS3k3XvUJS+3TZv9LVnknP97ic/Z8l6Q3gr3Vt6Ta902Psnv7uJmmxpP0/1b9Ys/UlFffTSrkQ\nMqt8w4CNgDsaWeccYC9gELArsAdwbs7yzwCbA9XAScAfJW0ZEeeRpEzjI6JTRPylsY5I2gS4HPhy\nRGwK7A3MzLPeVsC96bpbA5cA90raOme1rwPfBLoA7YEfNXLoz5CMQTVJ4XYNcAIwGNgX+LmkXum6\ntcD3gc4kY3cg8B2AiNgvXWfX9HzH5+x/K5J07JTcA0fEy8BZwE2SNgb+ClwfEY820l8zayEuhMwq\n39bAkiYuXR0P/HdELIqIxcAvgW/kLF+VLl8VEfcBHwDNnQOzBhggqWNEvB4Rc/KscyjwUkTcGBGr\nI+IW4Hng8Jx1/hoRL0bEMuBWkiKuIauAX0fEKmAcSZFzWUS8nx5/LkkBSEQ8FRFT0uO+AvwZ+HwB\n53ReRKxI+7OOiLgGmAc8CWxLUniatRzJc4Qa0Hp7bmaFegvo3MTclW7Aqzm/X03b1u6jXiH1EdBp\nfTsSER8CxwGnAa9LuldS3wL6U9en6pzfb6xHf96KiNr0e12h8mbO8mV120vaUdI9kt6QtJQk8cp7\n2S3H4ohY3sQ61wADgP8XESuaWNfMWogLIbPK9wSwAjiykXUWklzWqbNd2tYcHwIb5/z+TO7CiJgU\nEQeRJCPPkxQITfWnrk81zezT+riSpF87RMRmwM9I3tTUmGhsoaROJJPV/wKcn176M2tZniOUlwsh\nswoXEe+RzIv5YzpJeGNJ7SR9WdJF6Wq3AOdK2iaddPwL4KaG9tmEmcB+krZLJ2qfXbdAUldJI9K5\nQitILrGtybOP+4Ad01v+qyQdB/QD7mlmn9bHpsBS4IM0rfp2veVvAtuv5z4vA6ZHxLdI5j5d9al7\naWYbhAshswyIiN8DPyCZAL0YeA34LnBnusqvgOnALGA28HTa1pxjPQSMT/f1FOsWL23SfiwE3iaZ\ne1O/0CAi3gIOA35IcmnvJ8BhEbGkOX1aTz8imYj9PklaNb7e8vOB69O7yo5tameSRgAH8/F5/gDY\nve5uObOWIqmon9ZKEY0mumZmZtbKtdmyZ2z0hV8U9RjLbj/pqYgYUtSDFIEf/GVmZlbhBK06tSkm\nXxozMzOzzHIiZGZmVulE0/c+ZpQTITMzM8ssJ0IVSFUdQ+03LXU3ytpuO29X6i6Yma3j6aefWhIR\n2xRn7637zq5iciFUgdR+Uzrs1ORdvZk2+ckrSt0FM7N1dGyn+k9TtxbgQsjMzCwDnAjl5zlCZmZm\nlllOhMzMzDLAiVB+ToTMzMwss5wImZmZZYATofycCJmZmVlmOREyMzOrdH6ydIOcCJmZmVlmOREy\nMzOrcPKTpRvkRMjMzMxahKT/kbRI0rM5bVtJekjSS+k/t8xZdrakeZJekDQ8p32wpNnpssuVVnmS\nOkgan7Y/KalnU31yIWRmZpYBkor6KdB1wMH12n4KPBwROwAPp7+R1A8YCfRPt/mTpLbpNlcCJwM7\npJ+6fZ4EvBMRfYBLgQub6pALITMzM2sREfEv4O16zSOA69Pv1wNH5rSPi4gVETEfmAfsIWlbYLOI\nmBIRAdxQb5u6fd0GHKgmqjTPETIzM8uAMp4j1DUiXk+/vwF0Tb9XA1Ny1luQtq1Kv9dvr9vmNYCI\nWC3pPWBrYElDB3chZGZmZhtCZ0nTc35fHRFXr88OIiIkxQbuV6NcCJmZmWVACyRCSyJiSDO2e1PS\nthHxenrZa1HaXgP0yFmve9pWk36v3567zQJJVcDmwFuNHdxzhMzMzKyU7gJGpd9HARNz2kemd4L1\nIpkUPTW9jLZU0l7p/J8T621Tt6+jgX+k84ga5ETIzMys0pXJk6Ul3QLsT3IZbQFwHjAWuFXSScCr\nwLEAETFH0q3AXGA1cHpE1Ka7+g7JHWgdgfvTD8BfgBslzSOZlD2yqT65EDIzM7MWERFfa2DRgQ2s\n/2vg13napwMD8rQvB45Znz65EDIzM8uAMr5rrKQ8R8jMzMwyy4mQmZlZhfO7xhrmRMjMzMwyy4mQ\nmZlZBjgRys+JkJmZmWWWEyEzM7MscCCUlxMhMzMzyywnQmZmZpVOniPUECdCZmZmllkuhKxgV513\nPK8+/FumT/jZ2rbfnHkkM28/l6njz2b8709m804d1y770Zgv8ezE83jmjp/zxWE7f2J/E/5w6jr7\n+tzuvfm/v53F+9Mu4ytfHFTckykjp35rDNt168LgQZ94WrzleHDSA+zSfyf69+3DxReNLXV3ypLH\nqDBZHSdJRf20Vi6ErGA33j2FEaf/cZ22h6c8z+BjfsMex/2Wl15dxI/HfAmAvtt/hmOG787uR/+a\nI07/E5edfSxt2nz8f5QRX9iVDz9asc6+Xnv9HU4570bGPzC9+CdTRr4xajQT73mg1N0oa7W1tZx5\nxulMvPt+Zsyay4Rxt/Dc3Lml7lZZ8RgVxuNk9bkQsoJNfvpl3n7vo3XaHp7yPLW1awCYOns+1V23\nAOCw/XdhwqSnWblqNa8ufIuXX1vC0AE9AdikY3vOOOELjL123T/8//P62zz70kLWrInin0wZ2Wff\n/dhqq61K3Y2yNm3qVHr37kOv7benffv2HHPcSO65e2Kpu1VWPEaFyfI4ORHKz4WQbTAnjhjGpMnJ\n36yqt9mcBW+8s3ZZzaJ36NZlcwDO+85hXHbjw3y0bGVJ+mmtz8KFNXTv3mPt7+rq7tTU1JSwR+XH\nY1QYj5PV50LINoifnDSc2to1jLtvWqPr7bJjNb16bMNdj8xqoZ6ZmVndu8acCH1Sqy+EJPWU9PVS\n96OlSHpU0pBS9yPXCYfvySH7DWD0OdetbatZ/B7dP7Pl2t/VXbZk4aL32HPXXgzutx3P3/tL/vHX\n77PDZ7sw6Zr/KkGvrTXp1q2aBQteW/u7pmYB1dXVJexR+fEYFcbjZPW1+kII6Am0ikJIUsU9t+mg\nvXfmB6O/yNFn/plly1etbb/30VkcM3x32rer4rPdtqbPdtsw7dlXuGbC42z/pXPoe+h5fOGbl/LS\nq4sYfvJlJTwDaw2GDB3KvHkv8cr8+axcuZIJ48dx6GFHlLpbZcVjVJhMj5OK/GmlivYHs6RNgFuB\n7kBb4AJgHnAJ0AlYAoyOiNclnQGcBqwG5kbESEmfB+r+hAxgv4h4P8+hxgI7S5oJXA98BTgjImam\n/XgcOD1t7w30AToDF0XENek6PwaOBToAd0TEeQ2cU0/gfuBxYG+gBhgREcskDQKuAjYGXgbGRMQ7\nkh4FZgL7ALdIGggsA3YDugBjgBOBYcCTETE6PdaVwFCgI3BbQ33K6dspwCkAtOvU2KrNdv1vR7Pv\n4B3ovEUn5j1wARdcdR8//uaX6NC+inuu/C4AU2e/whm/Hsdz/36Dvz84gxl/P4fVtWs4c+ytTU6C\nHtxvO8ZfcjJbbLYxh+w3kHNPO5TBR/+6KOdSTk484Ws89s9HWbJkCb17dufnv/glo8ecVOpulZWq\nqiouvewKDj90OLW1tYwaPYZ+/fuXultlxWNUGI+T1aeI4tyhI+mrwMERcXL6e3OSImJERCyWdBww\nPCLGSFoI9IqIFZK2iIh3Jd0NjI2IyZI6AcsjYnWe4+wP/CgiDkt/jwJ2i4gzJe0I/C0ihkg6n6QY\n2gvYBJgB7AkMAI4GTiWpae8iKZL+ledYPUmKuSERMVPSrcBdEXGTpFnA9yLin5L+G9gs7cOjJMXd\nd9J9XAdsBHwNOAK4EfgcMAeYBpyU7nuriHhbUlvgYZLibla6vx9FRIP3mLfZuEt02OnYRv/9ZN07\n064odRfMzNbRsZ2eioiiTH1o36VPbPPVi4ux67UWXnVU0fpfTMW8NDYbOEjShZL2BXqQFB0PpenN\nuSRpEcAs4GZJJ5CkQgCTgUvStGiLfEVQAyYAh0lqR5K2XJezbGJELIuIJcAjwB7Al9LPDOBpoC+w\nQyP7n1+XNgFPAT3TIm+LiPhn2n49sF/ONuPr7ePuSCrQ2cCbETE7ItaQFEM903WOlfR02q/+QL9C\nTt7MzMwKV7RLYxHxoqTdgUOAXwH/AOZExLA8qx9KUjgcDpwjaWBEjJV0b7r9ZEnDI+L5Ao77kaSH\ngBEkl7sG5y6uvzpJCvTbiPhzgaeW+xTAWpJLV035sIF9rKm3vzVAlaRewI+AoenltetIUiQzM7Nm\nac13dhVT0RIhSd2AjyLiJuBikstQ20gali5vJ6m/pDZAj4h4BDgL2BzoJKl3mpRcSHLJqG8Dh3of\n2LRe27XA5cC0iHgnp32EpI0kbQ3sn+53EjAmvfyGpGpJXdbnXCPiPeCdNPkC+Abwz0Y2acpmJMXT\ne5K6Al/+FPsyMzOzBhTzLqaBwMWS1gCrgG+TXPa6PL2UVAX8AXgRuCltE3B5OkfoAkkHkKQkc0jm\nF+UzC6iV9AxwXURcGhFPSVoK/DXPuo+QTJa+ICIWAgsl7Qw8kVbLHwAnAIvW83xHAVdJ2hj4N/DN\n9dx+rYh4RtIM4HngNZLLhGZmZs3mRCi/ok2WLqU0jXoU6JvOvSGdLP1BRPyuhF1rEZ4s3TRPljaz\nclPsydJdj/l9MXa91oI/HenJ0uVA0onAk8A5dUWQmZlZ5vk5Qnm1mgf8pc/fubFe84qI2DO3ISJu\nAG6ov31EnL8ex9qa5Jb1+g6MiLcK3Y+ZmZmVt1ZTCEXEbGBQCx3rrZY6lpmZWUvwHKH8Ku7SmJmZ\nmVmhWk0iZGZmZs3T2t8QX0xOhMzMzCyznAiZmZllgBOh/JwImZmZWWY5ETIzM8sAJ0L5OREyMzOz\nzHIiZGZmlgUOhPJyImRmZmaZ5UTIzMwsAzxHKD8nQmZmZpZZToTMzMwqnZwINcSJkJmZmWWWEyEz\nM7MKJ8CBUH5OhMzMzCyznAiZmZlVPL99viFOhMzMzCyznAiZmZllgAOh/JwImZmZWWY5ETIzM8sA\nzxHKz4mQmZmZZZYTITMzs0onzxFqiBMhMzMzyywnQmZmZhVOQJs2joTycSJkZmZmmeVEyMzMLAM8\nRyg/F0IVaKMtt6TvV44qdTfMzMzKngshMzOzDPBzhPLzHCEzMzPLLCdCZmZmlc7PEWqQEyEzMzPL\nLCdCZmZmFU54jlBDnAiZmZlZZjkRMjMzq3hyItQAJ0JmZmaWWU6EzMzMMsCBUH5OhMzMzCyznAiZ\nmZllgOcI5edEyMzMzDLLiZCZmVml85OlG+REyMzMzDLLiZCZmVmF85OlG+ZEyMzMzDLLiZCZmVkG\nOBDKz4mQmZmZZZYTITMzswzwHKH8nAiZmZlZZjkRMjMzywAHQvk5ETIzM7PMciJkZmZW6eQ5Qg1x\nImRmZmYtQtL3Jc2R9KykWyRtJGkrSQ9Jein955Y5658taZ6kFyQNz2kfLGl2uuxyfYoqz4WQmZlZ\nhUueLF3cT5N9kKqBM4AhETEAaAuMBH4KPBwROwAPp7+R1C9d3h84GPiTpLbp7q4ETgZ2SD8HN3ds\nXAiZmZlZS6kCOkqqAjYGFgIjgOvT5dcDR6bfRwDjImJFRMwH5gF7SNoW2CwipkREADfkbNOsDpmZ\nmVlFU0vMEeosaXrO76sj4uq6HxFRI+l3wH+AZcCDEfGgpK4R8Xq62htA1/R7NTAlZ38L0rZV6ff6\n7c3iQsjMzMw2hCURMaShhencnxFAL+BdYIKkE3LXiYiQFMXt5rpcCJmZmWVAGdw09kVgfkQsBpB0\nO7A38KakbSPi9fSy16J0/RqgR8723dO2mvR7/fZm8Rwha7bj9+zBhNP24NbT9uA3R/Wnfds2nPnF\n3vz9O3sy/tQ9+N2xA+nUIam1vzygK7ecMnTtZ/rPD2DHrp0A2HnbTRl/6h5M/O5e/Hj4DqU8pZI4\n9Vtj2K5bFwYPGlDqrpS1Byc9wC79d6J/3z5cfNHYUnenLHmMCuNxKpn/AHtJ2ji9y+tA4DngLmBU\nus4oYGL6/S5gpKQOknqRTIqeml5GWyppr3Q/J+Zss95cCFmzbLNpe0bu0Z0Trp3OsVdNpY1g+IAu\nTPn3Oxx75VSO+/NU/vPWR4zZ57MA3P/sm3zt6ml87epp/PzOudS8s5wX3/wAgLMP2Ylf3fM8I66Y\nwnZbb8zefbYq5am1uG+MGs3Eex4odTfKWm1tLWeecToT776fGbPmMmHcLTw3d26pu1VWPEaFyfI4\nSSrqpykR8SRwG/A0MJukBrkaGAscJOklktRobLr+HOBWYC7wAHB6RNSmu/sOcC3JBOqXgfubOy4u\nhKzZ2rYRHara0FaiY7u2LH5/JVP+/Ta1kVzenb3gPbps1uET2x08oCsPznkTgM6d2rNJh7bMrlkK\nwD3PvMEBO23TcidRBvbZdz+22ipbxd/6mjZ1Kr1796HX9tvTvn17jjluJPfc3ey/AFYkj1FhPE6l\nFRHnRUTfiBgQEd9I7wh7KyIOjIgdIuKLEfF2zvq/jojeEbFTRNyf0z493UfviPhuevdYs7gQsmZZ\n/P5KbnziP9x35t48+IPP8f6K1Uz599vrrDNit27837y3PrHtQf268sCzSSG0zaYdWLR0xdpli95f\nTpdNP1k8WbYtXFhD9+4fTxWoru5OTU2zpwRUJI9RYTI7TkV+hlAZzD9qNhdC1iybblTF/jttw2GX\nP8HwSyfTsV1bDhnYde3yk/b5LKvXBPfNfnOd7QZUb8byVbW8vPjDlu6ymZnZJ2S6EJLUU9LXW/B4\ntZJmpo8Xf0bSDyU1+u8gnVR2c/oo8WclPS6pU0v1uSF79tqSmneX8e5Hq1i9JvjH84vZpfvmABy+\n62fYd8fOnHv7nE9sN7x/FybN+bg4Wvz+inUun3XZdCMWvb/iE9tZtnXrVs2CBa+t/V1Ts4Dq6mY/\nNqQieYwKk9VxSp4sXdo5QuUq04UQ0BNosUIIWBYRgyKiP3AQ8GXgvCa2+S/gzYgYmD6S/CSSh0mV\n1BtLVzCwejM2qkr+E9qj15bMX/IRe/feilF7f5Yzx81i+eo162wjkstik579uBBa8sFKPlxRy8Dq\nzQA4bNfP8OgLS1rsPKx1GDJ0KPPmvcQr8+ezcuVKJowfx6GHHVHqbpUVj1FhPE5WX1k+R0jSJiQz\nxbuTvIvkApKZ4ZcAnYAlwOj0mQNnAKcBq4G5ETFS0ueBy9LdBbBfRLyf51BjgZ0lzSR5rPdXgDMi\nYmbaj8eB09P23kAfoDNwUURck67zY+BYoANwR0Q0VdgknYpYJOkUYJqk89PtrwSGpOfyg4h4BNgW\neDVnuxcaGLNTgFMA2m3epZAufCrP1izl4ecWc/MpQ6ldE7zwxgfc/nQNt317T9q1bcOVJwwCYPaC\npfzmvqTLu392C95cupyad5evs6/f3vcCvxyxMx2q2vJ/895icp55RZXsxBO+xmP/fJQlS5bQu2d3\nfv6LXzJ6zEml7lZZqaqq4tLLruDwQ4dTW1vLqNFj6Ne/f6m7VVY8RoXJ8ji15tSmmPQpJloXjaSv\nAgdHxMnp781Jbo0bERGLJR0HDI+IMZIWAr0iYoWkLSLiXUl3A2MjYnJ6GWl5RKzOc5z9gR9FxGHp\n71HAbhFxpqQdgb9FxJC0UPkKsBewCTAD2BMYABwNnEoSeNxFUiT9q4Hz+iAiOtVrexfYCTgB6J+e\nU1/gQWBHoO77yyQvo7s+Il5qbPw2rt4p+p56ZWOrZN7//ewLpe6Cmdk6OrbTU409mfnT2LRH39jt\n+38pxq7XeuyH+xSt/8VUrpfGZpM8U+BCSfuSPFlyAPBQmt6cy8dPlZwF3Jw+pruu2JkMXJKmRVvk\nK4IaMAE4TFI7YAxwXc6yiRGxLCKWAI8AewBfSj8zSJ6L0JfkgU/NsQ9wE0BEPE+SAu2YplPbAxcD\nW5EkSDs38xhmZpZRvmssv7K8NBYRL0raHTgE+BXwD2BORAzLs/qhwH7A4cA5kgZGxFhJ96bbT5Y0\nPC0umjruR5IeInkXyrHA4NzF9VcnSYF+GxF/Xs9TBEDS9kAtHz9OvKF+fQDcDtwuaQ3JeT3XnGOa\nmZnZx8oyEZLUDfgoIm4iSUL2BLaRNCxd3k5S//SOqx7pXJqzgM2BTpJ6R8TsiLgQmEaS1OTzPrBp\nvbZrgcuBaRHxTk77CEkbSdoa2D/d7yRgTN1dXJKqJRU0QUfSNsBVwBXpg6AeA45Pl+0IbAe8IOlz\nSl5Uh6T2QD9y5gyZmZkVwneN5VeWiRAwELg4TT9WAd8muex1eTpfqAr4A/AicFPaJuDydI7QBZIO\nANYAc2j40duzgFpJzwDXRcSlEfGUpKXAX/Os+wjJZOkLImIhsDC9TPVE+h/BByRzfRpKeDqml/ba\npedzI8kEcIA/AVdKmp0uG53Oe+qdtoukcL0X+HuTI2hmZmZNKstCKCImkaQt9e2Xp22fPNt/r8Dj\nrALWmTWbplFtSCYo55oVESfm2cdlfHyHWlPHa9vIsuXAN/O03wDcUMj+zczM8mrl83iKqSwvjZWK\npBOBJ4FzImJNU+ubmZlZ61aWidCGJmkgyWWoXCsiYs/chobSl4g4fz2OtTXJbe71HRgR2XpAjpmZ\nlQXRuufxFFMmCqGImA0MaqFjvdVSxzIzM7NPJxOFkJmZWdY5EMrPc4TMzMwss5wImZmZZUAbR0J5\nOREyMzOzzHIiZGZmlgEOhPJzImRmZmaZ5UTIzMyswiVviHcklI8TITMzM8ssJ0JmZmYZ0MaBUF5O\nhMzMzCyznAiZmZllgOcI5edEyMzMzDLLiZCZmVkGOBDKz4mQmZmZZZYTITMzswonQDgSyseJkJmZ\nmWWWEyEzM7MM8HOE8nMiZGZmZpnlRMjMzKzSSX6OUAOcCJmZmVlmOREyMzPLAAdC+TkRMjMzs8xy\nImRmZlbhBLRxJJSXEyEzMzPLLCdCZmZmGeBAKD8nQmZmZpZZToTMzMwywM8Rys+FUAXavvMm3PSt\nPUvdDTMzs7LnQsjMzKzCSZ4j1BDPETIzM7PMciJkZmaWAX6OUH5OhMzMzCyznAiZmZllgPOg/JwI\nmZmZWWY5ETIzM8sAP0coPydCZmZmlllOhMzMzCpc8vb5UveiPDkRMjMzs8xyImRmZlbpJM8RaoAT\nITMzM8uzYlJLAAAgAElEQVQsJ0JmZmYZ4EAovwYLIUmbNbZhRCzd8N0xMzMzazmNJUJzgGDdh1HW\n/Q5guyL2y8zMzDYgzxHKr8FCKCJ6tGRHzMzMzFpaQXOEJI0Eto+I30jqDnSNiKeK2zUzMzPbEPwc\noYY1edeYpCuAA4BvpE0fAVcVs1NmZmZmLaGQRGjviNhd0gyAiHhbUvsi98vMzMw2IM8Ryq+Q5wit\nktSGZII0krYG1hS1V2ZmZmYtoJBC6I/A34FtJP0SeBy4sKi9MjMzsw1KRf60Vk1eGouIGyQ9BXwx\nbTomIp4tbrfMzMzMiq/QJ0u3BVaRXB7zaznMzMxaEQnaeI5QXoXcNXYOcAvQDegO/E3S2cXumJmZ\nmVmxFZIInQjsFhEfAUj6NTAD+G0xO2ZmZmYbjgOh/Aq5zPU66xZMVWmbmZmZWavW2EtXLyWZE/Q2\nMEfSpPT3l4BpLdM9MzMz2xD8HKH8Grs0Vndn2Bzg3pz2KcXrjpmZmVnLaeylq39pyY6YmZlZ8TgQ\nyq+Qu8Z6SxonaZakF+s+LdE5K1+v1yxg9NFf5vD9B3PEAUO48do/AvDuO2/zrZGH8+XP7cq3Rh7O\ne+++A8CsGdM56qBhHHXQML7yxb343/vvAuDDD95f237UQcP43IDt+O0vflKy8yqVByc9wC79d6J/\n3z5cfNHYUnenLHmMmuYxKozHyXIpIhpfQXoM+BXwO+BI4JtARMTPi989a44Bu+4et97/WFGPsfjN\nN1i86A36DRzEhx+8zzEH78vl/3MLd956M5tvsSUnf/eHXHPF71n63rv88JwLWLbsI9q1a09VVRWL\n33yDow7ai0eenkdV1bqh5DEH78NZ549lyF77FLX/23fZpKj7Xx+1tbUM7Lcj997/ENXdu7PPXkO5\n/qZb2Llfv1J3rWx4jJrmMSpMOY9Tx3Z6KiKGFGPfXXoPiK9edGsxdr3WVUf3L1r/i6mQu8Y2johJ\nABHxckScC3y5uN2ycrdN18/Qb+AgADbptCnb77ATi954nUcm3cuRxxwPwJHHHM8/HrgHgI4dN15b\n9KxYsTzvpL1XXn6Jt5csZvCen2uhsygP06ZOpXfvPvTafnvat2/PMceN5J67J5a6W2XFY9Q0j1Fh\nPE5WXyGF0Ir0pasvSzpN0uHApkXul7UiNa+9ynPPPsMuuw3hrSWL2KbrZwDo3KUrby1ZtHa9WU9P\n44gDhnDkgXvyi7GXfSINuu+u2zj4iK9m7s6GhQtr6N69x9rf1dXdqampKWGPyo/HqGkeo8JkdpyU\nzBEq5qe1KqQQ+j6wCXAG8DngZGBMMTtlrceHH37AmScfz09/eSGdNt1snWWS1ilqdtl9KHc9Mp3x\n9/2Ta674PSuWL19n/fsn3sYhRx7TIv02MzODAgqhiHgyIt6PiP9ExDci4oiImNwSnfs0JPWU9PUW\nPF6tpJmSnpU0QdLG67n9B8XqW7GsWrWKM08+nkO/chwHHTICgK07d2Hxm28AyTyirbbe5hPb9d6h\nLxtvvAkvvTB3bdvzc2ZTu7qW/rvs1jKdLyPdulWzYMFra3/X1Cygurq6hD0qPx6jpnmMCpPlcar7\ny2mxPgX2YQtJt0l6XtJzkoZJ2krSQ5JeSv+5Zc76Z0uaJ+kFScNz2gdLmp0uu1yf4lJCg4WQpDsk\n3d7Qp7kHbEE9gRYrhIBlETEoIgYAK4HTchcqUTEvrI0IfvHD77B9n50Yfer31rYf8KVDuHPCzQDc\nOeFmDhh+KAAL/vMKq1evBmDhgv8w/+UXqe6x3drt7ps4gUOOPLoFz6B8DBk6lHnzXuKV+fNZuXIl\nE8aP49DDjih1t8qKx6hpHqPCeJxK7jLggYjoC+wKPAf8FHg4InYAHk5/I6kfMBLoDxwM/ElS23Q/\nV5Jcodoh/Rzc3A419kDFK5q708ZI2gS4leQFrm2BC4B5wCVAJ2AJMDoiXpd0BklBsRqYGxEjJX2e\nZCAhedL1fhHxfp5DjQV2ljQTuB74CnBGRMxM+/E4cHra3hvoA3QGLoqIa9J1fgwcC3QA7oiI8wo8\nzceAXST1BCYBTwKDgUMk7Q38DBBwb0SclTM2l5I8ufsNYGRELJbUG/gjsA3wEXByRDyfZ1xPAU4B\n2La6R/3FG9zT057grr/fwo479+eog4YBcOZPz+dbp/+AH5x2IrffcgPduvfg91fdkKw/9Qmu/ePv\nqapqR5s2bfj5by5ly606r93fpLtv58ob/170fpejqqoqLr3sCg4/dDi1tbWMGj2Gfv37l7pbZcVj\n1DSPUWGyPE6l/pu4pM2B/YDRABGxElgpaQSwf7ra9cCjwFnACGBcRKwA5kuaB+wh6RVgs4iYku73\nBpK72u9vVr+aun1+Q5P0VeDgiDg5/b05SedHpH/wHwcMj4gxkhYCvSJihaQtIuJdSXcDYyNisqRO\nwPKIWJ3nOPsDP4qIw9Lfo0heHnumpB2Bv0XEEEnnkxRDe5HMhZoB7AkMAI4GTiUpWu4iKZL+1cB5\nfRARnSRVAX8HHkjP69/A3hExRVI3kidzDwbeAR4ELo+IOyUFcEJE3CzpF0CXiPiupIeB0yLiJUl7\nAr+NiC80NsYtcft8a1dOt8+bmUGRb5/vMyCOu3hCMXa91hVH9XuVJMyoc3VEXF33Q9Ig4GpgLkka\n9BTwX0BNRGyRriPgnYjYQtIVwJSIuCld9heSP1dfIakDvpi27wucVffn/foq5O3zG9ps4PeSLgTu\nISkIBgAPpZf42vLxS11nATdLuhO4M22bDFwi6Wbg9ohYUOBxJwA/T1OeMcB1OcsmRsQyYJmkR4A9\ngH1I0pkZ6TqdSOK3vIUQ0DFNnyBJhP4CdANeratagaHAoxGxGCA9h/3Sc1sDjE/Xuwm4PS309gYm\n5Fz+7FDg+ZqZmQHJ3+Zb4I7cJU0UclXA7sD3IuJJSZeRXgarExGRBgMtpsULoYh4UdLuwCEkD2r8\nBzAnIoblWf1QkkLhcOAcSQMjYqyke9PtJ0sanu9SUZ7jfiTpIZKo7ViSVGbt4vqrk/x389uI+HOB\np7YsIgblNqT/0X1Y4Pb1BUmS+W79/ZqZmbVCC4AFEfFk+vs2kkLoTUnbplNitgXqnrtSA+TO9eie\nttWk3+u3N0vBlwwlbZAkIr089FEadV1MchlqG0nD0uXtJPVPJxb3iIhHSK4Vbg50ktQ7ImZHxIXA\nNKBvA4d6n08+7+ha4HJgWkS8k9M+QtJGkrYmuU45jWRuz5g0lUFStaQun/L0pwKfl9Q5nfD1NeCf\n6bI2JJfiIJnk/XhELCW5LnpM2gdJ2vVT9sHMzDKojYr7aUpEvAG8JmmntOlAkstkdwGj0rZRQN0T\nLu8CRkrqIKkXyVWZqRHxOrBU0l7ppbQTc7ZZb00mQpL2ILnMszmwXfoH8bci4nuNb9mggcDFktYA\nq4Bvk0yGvjydL1QF/AF4EbgpbRPJXJp3JV0g6QCSS0lzaHhy1CygVtIzwHURcWlEPCVpKfDXPOs+\nQjJZ+oKIWAgslLQz8ESa7HwAnMDHlep6S6vdn6bHqpssXfcv70OSSWDnpsc4Lm0/HrgybW8HjAOe\naW4fzMzMSuh7JFNe2pPMof0mSRBwq6STgFdJrtoQEXMk3UpSLK0GTo+I2nQ/3yGZ4tKRpA5o1kRp\nKOxdY1NI/lC+MyJ2S9ueTW8Tb1XSNOpRoG9ErEnbzgc+iIjflbBrG5QnSzfNk6XNrNwUc7J01z4D\n4vhLbivGrte6dMTOFfuusTYR8Wq9ttq8a5YxSSeS3MZ+Tl0RZGZmZtlWyGTp19LLY5HOa/keyWWr\nsiBpIHBjveYVEbFnbkNE3ADcUH/7iDh/PY61NcnDnuo7MCLeKnQ/ZmZmLSl5H1grfiFYERVSCH2b\nZILxdsCbwP+mbWUhImYDLXJXVVrs+A4uMzOzCtFkIRQRi0gecW1mZmatVCF3dmVRIXeNXcMnn7ND\nRJxSlB6ZmZmZtZBCLo39b873jUheR/FaA+uamZlZGfIUofwKuTQ2Pve3pBuBx4vWIzMzM7MW0pxX\nbPQCum7ojpiZmVlxCGjjSCivQuYIvcPHc4TaAG9T7yVpZmZmZq1Ro4VQ+g6PXfn4ZWZroqlHUZuZ\nmVnZKfjlohnT6LikRc99EVGbflwEmZmZWcUopECcKWm3ovfEzMzMiiZ5unTxPq1Vg5fGJFVFxGpg\nN2CapJdJ3pAukrBo9xbqo5mZmVlRNDZHaCqwO3BEC/XFzMzMikCS7xprQGOFkAAi4uUW6ouZmZlZ\ni2qsENpG0g8aWhgRlxShP2ZmZlYEDoTya6wQagt0Ik2GzMzMzCpNY4XQ6xHx3y3WEzMzMysav30+\nv8Zun/eQmZmZWUVrLBE6sMV6YWZmZkXjd401rMFEKCLebsmOmJmZmbW05rx93szMzFoZB0L5+R1s\nZmZmlllOhMzMzCqdfNdYQ5wImZmZWWY5ETIzM8sA+ak4eTkRMjMzs8xyImRmZlbhkucIlboX5cmJ\nkJmZmWWWE6EK1L6qDZ/tvHGpu2FmZmXEiVB+ToTMzMwss5wImZmZZYD8aOm8nAiZmZlZZjkRMjMz\nq3C+a6xhToTMzMwss5wImZmZVTr57fMNcSJkZmZmmeVEyMzMLAPaOBLKy4mQmZmZZZYTITMzswrn\nu8Ya5kTIzMzMMsuJkJmZWQZ4ilB+ToTMzMwss5wImZmZVTzRBkdC+TgRMjMzs8xyImRmZlbhhOcI\nNcSJkJmZmWWWEyEzM7NKJz9HqCFOhMzMzCyznAiZmZllgN81lp8TITMzM8ssJ0JmZmYVzneNNcyJ\nkJmZmWWWEyEzM7MM8Byh/JwImZmZWWY5ETIzM8sAB0L5OREyMzOzzHIiZGZmVuGEk4+GeFzMzMws\ns1wI2Qbx7VPG0LN7V4buNnBt26xnZnLAvsMYNnQ39h02lOnTpgIwfdpUhg3djWFDd2OvIYO4a+Id\npep22Xhw0gPs0n8n+vftw8UXjS11d8qSx6hpHqPCZHKcBJKK+mmtXAjZBnH8N0Zz5933r9N27tln\ncfY5v+CJaTM49xe/5NyfnQVAv/4DeOyJaTwxbQZ33n0/Z5x+GqtXry5Ft8tCbW0tZ55xOhPvvp8Z\ns+YyYdwtPDd3bqm7VVY8Rk3zGBXG42T1uRCyDWKfffdjyy23WqdNEkvfXwrAe0vfY9ttuwGw8cYb\nU1WVTE9bvnx5q/6bxIYwbepUevfuQ6/tt6d9+/Ycc9xI7rl7Yqm7VVY8Rk3zGBUmy+OkIn9aK0+W\ntqK58HeXcuThB3POT3/MmjVrePjRyWuXTZv6JN8+5SRe+8+rXPPXG9YWRlm0cGEN3bv3WPu7uro7\nU6c+WcIelR+PUdM8RoXxOFl9ToSsaK69+krGXnwJL7z8H8ZefAnfOfVba5cN3WNPps98ln9Onsrv\nLxrL8uXLS9hTM7PKJpInSxfz01plvhCS1FPS11vweLWSZkp6VtLdkrZI2wdJekLSHEmzJB2Xs82j\nkoa0VB83lL/ddAMjjjwKgKO+egxPTZ/6iXX67rwzm3TqxNw5z7Z098pGt27VLFjw2trfNTULqK6u\nLmGPyo/HqGkeo8J4nKy+zBdCQE+gxQohYFlEDIqIAcDbwOlp+0fAiRHRHzgY+ENdkdRafWbbbjz2\nr38C8Ogj/6B3nx0AeGX+/LWTo//z6qu8+MLzbPfZnqXqZskNGTqUefNe4pX581m5ciUTxo/j0MOO\nKHW3yorHqGkeo8JkeZw8Ryi/sp2YIWkT4FagO9AWuACYB1wCdAKWAKMj4nVJZwCnAauBuRExUtLn\ngcvS3QWwX0S8n+dQY4GdJc0Erge+ApwRETPTfjxOUqx8BegN9AE6AxdFxDXpOj8GjgU6AHdExHkF\nnuYTwC4AEfFiXWNELJS0CNgGeLeQHUk6BTgFoMd22xV4+A1n9De+zmP/epS3lixhx+17cM7Pz+eK\nK6/mJz88k9WrV7PRRhvx//70ZwCe+L/H+f3FF9KuXTvatGnDpZf9kc6dO7d4n8tFVVUVl152BYcf\nOpza2lpGjR5Dv/79S92tsuIxaprHqDAeJ6tPEVHqPuQl6avAwRFxcvp7c+B+YERELE4vHQ2PiDGS\nFgK9ImKFpC0i4l1JdwNjI2KypE7A8oj4xD3akvYHfhQRh6W/RwG7RcSZknYE/hYRQySdT1IM7QVs\nAswA9gQGAEcDp5IUxXeRFEn/auC8PoiITpLaAuOAv0TEA/XW2YOkKOsfEWskPZr2cXohY7f74CHx\n2BPTClk1s9q2ac1/fzGzStSxnZ6KiKJMg9i+3y7xq5vuK8au1zp+cI+i9b+YyvnS2GzgIEkXStoX\n6EFSdDyUpjfnkqRFALOAmyWdQJIKAUwGLknToi3yFUENmAAcJqkdMAa4LmfZxIhYFhFLgEeAPYAv\npZ8ZwNNAX2CHRvbfMe3/G0BX4KHchZK2BW4EvhkRawrss5mZmTVD2V4ai4gXJe0OHAL8CvgHMCci\nhuVZ/VBgP+Bw4BxJAyNirKR70+0nSxoeEc8XcNyPJD0EjCC53DU4d3H91UlSoN9GxJ8LPLVlETFI\n0sbAJJLLbpcDSNoMuBc4JyKmFLg/MzOzJrTupz8XU9kmQpK6AR9FxE3AxSSXobaRNCxd3k5Sf0lt\ngB4R8QhwFrA50ElS74iYHREXAtNIkpp83gc2rdd2LUlxMi0i3slpHyFpI0lbA/un+50EjEkvvyGp\nWlKXps4vIj4CzgB+KKlKUnvgDuCGiLitqe3NzMzs0yvbRAgYCFwsaQ2wCvg2yWWvy9P5QlXAH4AX\ngZvSNgGXp3OELpB0ALAGmEMyvyifWUCtpGeA6yLi0oh4StJS4K951n2EZLL0BRGxEFgoaWfgibTa\n/gA4AVjU1AlGxAxJs4CvkU7oBraWNDpdZXTdpG3gXkmr0u9PRMQxTe3fzMwM/Pb5xpRtIRQRk0jS\nlvr2y9O2T57tv1fgcVYBX8htS9OoNsCD9VafFREn5tnHZXx8h1pTx+tU7/fhOT9vamCb/QvZt5mZ\nma2fsi2ESkXSicCvgR94srKZmVUKzxHKLzOFkKSBJHdj5VoREXvmNkTEDcAN9bePiPPX41hbAw/n\nWXRgRLxV6H7MzMysuDJTCEXEbGBQCx3rrZY6lpmZWSGcB+XnuVNmZmbWYiS1lTRD0j3p760kPSTp\npfSfW+ase7akeZJekDQ8p32wpNnpssv1Ka77uRAyMzOrdErmCBXzsx7+C3gu5/dPgYcjYgeSaSU/\nBZDUDxgJ1L2D80/pWxkArgROJnmA8Q7p8mZxIWRmZmYtQlJ3kocgX5vTPILktVKk/zwyp31cRKyI\niPkk7xvdI30Dw2YRMSWS94TdkLPNesvMHCEzM7OsaqHnCHWWlPtOzKsj4up66/wB+AnrPsi4a0S8\nnn6ve/0UQDWQ+5aFBWnbqvR7/fZmcSFkZmZmG8KSxl66KukwYFH60OL9860TESGpRd8G70LIzMws\nA8rgOUKfA46QdAiwEbCZpJuANyVtGxGvp5e96t7MUEPywvU63dO2Gj5+6Xpue7N4jpCZmZkVXUSc\nHRHdI6InySTof0TECcBdwKh0tVHAxPT7XcBISR0k9SKZFD01vYy2VNJe6d1iJ+Zss96cCJmZmWVA\nyfOgho0FbpV0EvAqcCxARMyRdCswl+Rdo6dHRG26zXeA64COJO8Sbeh9ok1yIWRmZmYtKiIeBR5N\nv78FHNjAer8mee1V/fbpwIAN0RcXQmZmZhlQ+ilC5clzhMzMzCyznAiZmZlVuOQ5Qo6E8nEiZGZm\nZpnlRMjMzCwDPEcoPydCZmZmlllOhMzMzCqekOcI5eVEyMzMzDLLiZCZmVkGeI5Qfk6EzMzMLLOc\nCJmZmVU4P0eoYU6EzMzMLLOcCJmZmVU6eY5QQ5wImZmZWWY5ETIzM8sAJ0L5OREyMzOzzHIiZGZm\nlgF+snR+LoQqkIC2bfwfvJmZWVNcCJmZmVU4Af77cX6eI2RmZmaZ5UTIzMwsAzxHKD8nQmZmZpZZ\nToTMzMwywM8Rys+JkJmZmWWWEyEzM7MM8Byh/JwImZmZWWY5ETIzM6twfo5Qw5wImZmZWWY5ETIz\nM6t48hyhBjgRMjMzs8xyImRmZlbp5OcINcSJkJmZmWWWEyEzM7MMcCCUnxMhMzMzyywnQmZmZhUu\neY6QM6F8nAiZmZlZZjkRMjMzywDnQfk5ETIzM7PMciJkZmaWBY6E8nIiZGZmZpnlRMjMzCwD/K6x\n/JwImZmZWWY5ETIzM8sAP0YoPydCZmZmlllOhMzMzDLAgVB+ToTMzMwss5wImZmZZYEjobycCNkG\nd+q3xrBdty4MHjRgbdvfb5vA7rv2Z+P2bXhq+vQS9q48PTjpAXbpvxP9+/bh4ovGlro7Zclj1DSP\nUWE8TpbLhZBtcN8YNZqJ9zywTlv//gMYd+vt7LPvfiXqVfmqra3lzDNOZ+Ld9zNj1lwmjLuF5+bO\nLXW3yorHqGkeo8JkdZxE8hyhYv6vtXIhZBvcPvvux1ZbbbVOW9+dd2bHnXYqUY/K27SpU+nduw+9\ntt+e9u3bc8xxI7nn7oml7lZZ8Rg1zWNUGI+T1edCyKzEFi6soXv3Hmt/V1d3p6ampoQ9Kj8eo6Z5\njAqT2XFS8hyhYn5aKxdCZmZmllmZLYQk9ZT09RY81rP12s6X9KP0+8WSnpc0S9IdkrZI2/eX9J6k\nmeny37VEf61ldetWzYIFr639XVOzgOrq6hL2qPx4jJrmMSpMlsdJRf60VpkthICeQIsUQgV4CBgQ\nEbsALwJn5yx7LCIGAbsBh0n6XCk6aMUzZOhQ5s17iVfmz2flypVMGD+OQw87otTdKiseo6Z5jArj\ncbL6yq4QkrSJpHslPSPpWUnHSRos6Z+SnpI0SdK26bpnSJqbJinj0rbPpwnKTEkzJG3awKHGAvum\n631f0r8kDcrpx+OSdk2TmxslPSHpJUkn56zzY0nT0uP/srnnHBEPRsTq9OcUoHuedZYBM4G8f3WR\ndIqk6ZKmL16yuLld2SBOPOFr7L/vMF584QV69+zOdf/zFybeeQe9e3bnySlPcNSIQzn8kOEl7WM5\nqaqq4tLLruDwQ4czaODOfPWYY+nXv3+pu1VWPEZN8xgVJtPj5EgoL0VEqfuwDklfBQ6OiJPT35sD\n9wMjImKxpOOA4RExRtJCoFdErJC0RUS8K+luYGxETJbUCVieU2TkHmd/4EcRcVj6exSwW0ScKWlH\n4G8RMUTS+cBXgL2ATYAZwJ7AAOBo4FSS/wTuAi6KiH/lOVZP4J6IGJDTdj7wQUT8rt66dwPjI+Km\n3D5K2hL4X+DQiHijsTEcPHhITH7Sz+oxM2tNOrbTUxH/v717j7d0rvs//no7T4gQIt3kkOjgHJ2l\nW4NEOilC3NIoUXcH4S4pmXKnuNUdHUxufkqlHJJJcohQTOQskQ5IVI4RM+/fH9/vaq7Zs/bsPWP2\nvvbe1/s5j/2Yta91rbW++3qstb6f6/P9fL+XNxuJ597gRRv7lLMvHomn/pdN11xuxNo/ksZcRgi4\nDvh3SZ+V9ApgDUrQcb6ka4DDmJ0x+TVwqqTdgV6wcxlwjKT3A8v3C4IG8R3K0NPiwN7AtMZ9Z9r+\nh+37gAuBLYBt68+vgBnA+sC6gzz3YNHmHNslHVr/jlMbm18h6VrgT8D0oYKgiIiIuY30KkLjNyU0\n5i6xYftWSZsA2wOfBn4K3GB7qz677wC8EtgROFTSC21PlfTD+vjLJL3O9s3DeN1HJZ0P7AS8Fdi0\neffA3SlZoKNsnzCMP+t+4BkDtq0A3NH7RdJewOuBbTxnmu5nNSO0FnCFpNNtXzOM14yIiIghjLmM\nkKTVgEdtnwIcTRmGeqakrer9i0vaUNIiwBq2LwQ+CiwHLCNpbdvX2f4s8EtKpqafh4CB9UNfA44D\nfmn7b43tO0laStKKwKvr804H9q7Db0haXdLK/V7I9sPA3ZJeU/ddAZgMXFp/nwx8BHiD7UcHeY47\nKHVNHx3k74mIiBhU1hHqb8xlhIAXAkdLmgU8AUyhDBcdV+uFFgO+SJlddUrdJuC4WiP0KUlbA7OA\nGyj1Rf38GphZh52m2f6C7aslPQic1GffC4GVgE/Zvgu4S9LzgctV3gEPA7sD9w7yensAX5J0TP39\nk7Z/W28fDyxJGf4DuML2e/o8x1eAD0la0/bvBnmdiIiIGKYxFwjZnk7JtgzU7yJVL+/z+AOG+TpP\nAK9pbqvZqEWAHw/Y/de29+jzHMcCxw7z9W4Eth7kvnUG2X4RcFHj938wyKyxiIiIwYzziV0jaswN\njbVF0h7AlcChtme13Z6IiIgYeWMuI7SwSXoh8H8DNj9u+yXNDbZPBk4e+Hjbh8/Ha60IXNDnrm1s\n3z/c54mIiFjokhLqa8IHQravAzYacseF81r3j9ZrRURExFM34QOhiIiIYFyv9TOSUiMUERERnZWM\nUERERAeM57V+RlIyQhEREdFZyQhFRER0QBJC/SUjFBEREZ2VjFBERMREl6WlB5WMUERERHRWMkIR\nEREdkHWE+ktGKCIiIjorGaGIiIgJTmQdocEkIxQRERGdlUAoIiKiAzTCP0O+vrSGpAsl3SjpBkkH\n1u0rSDpf0m/q/89oPOZjkm6TdIuk1zW2byrpunrfcdKC57sSCEVERMRoeBL4T9sbAFsC75W0AXAw\ncIHtdYEL6u/U+3YFNgQmA1+WtGh9rv8F9gXWrT+TF7RRCYQiIiK6oOWUkO27bc+otx8CbgJWB3YC\nvll3+yawc729E/At24/bvgO4DdhC0rOAp9u+wraBkxuPmW8plo6IiIiFYSVJVzV+P9H2if12lLQm\nsDFwJbCK7bvrXfcAq9TbqwNXNB72x7rtiXp74PYFkkAoIiKiA0ZhHaH7bG82ZDukZYDvAQfZfrBZ\n3mPbkjyCbZxLhsYiIiJiVEhanBIEnWr7jLr5z3W4i/r/vXX7n4A1Gg9/dt32p3p74PYFkkAoIiKi\nAzfHtvUAABslSURBVKSR/Rn69SXg68BNto9p3HUWsGe9vSdwZmP7rpKWlLQWpSj6F3UY7UFJW9bn\n3KPxmPmWobGIiIgYDS8D3glcJ+mauu0QYCpwuqR9gDuBtwLYvkHS6cCNlBln77U9sz5uf2AaMAn4\nUf1ZIAmEIiIiOqDthaVtXzqPZmwzyGOOBI7ss/0q4AULo10ZGouIiIjOSkYoIiKiC9pOCY1RyQhF\nREREZyUjFBERMcGVxZ+TEuonGaGIiIjorGSEIiIiJrphrvXTRQmEJqAZM66+b9LiurPtdgywEnBf\n240Y43KMhifHaWg5RkMbi8fo39puQBclEJqAbD+z7TYMJOmq4VyDpstyjIYnx2loOUZD6+IxSkKo\nv9QIRURERGclIxQREdEFSQn1lYxQjJYT227AOJBjNDw5TkPLMRpajlEAINtttyEiIiJG0As32tRn\nnn/ZiL7G2itPuno81l0lIxQRERGdlRqhiIiIDsg6Qv0lIxQRERGdlYxQRETEBCcyaWwwyQjFhCJp\nZUmrtd2OsULSupImt92OiIixKoFQTDRHAEdJenbbDWmbpOcD3wU2lrRU2+0ZD6S5qygkdfp7UtLi\nbbdhPJC0oqS1+r2HxgyN8M841ekPeExIH6B8JD/W5WCo/u3fBI60fZTtx9pu01gnSbYtaXtJUyUd\nLWld27PabltbJK0MHFeD6hhEPT4/pJx4fFHSC1tuUsyHBEIxYdSO7B/AfsDywCEdDoaeBfzW9ukA\nkl4v6RhJR0jauuW2jUk1CHodcDhwJrAxJbvY2e9J2/cCSwMHS1qvt11Vey0bOyStD3wb+ATwRmA5\nYLsB+4yJY6UR/jdedfYDHhNH40tmLUnr12Bob2AZ4NCOBkMPA09ImiLpXGBPYJX6s5ukVVtt3di1\nOeVYrQwsBXzQ9ixJS7fbrNElaWlJzwSwvQfwEPBxSev1Mmc1cNxE0hbttrY9khYF3gY8Dbjc9u+B\nTwMvaWbRnJWLx7QEQjHu1S/knYDTgKmSPgc8E/gPYEngU5LWaLONo832TcDFwPOB+4BP297N9n7A\nmsBKLTZvTBjkLP1pwBeAA4A9bP++vrf2qZ3ehCdpA+As4NuSvgBg+33Ag8DHgfXrfq8EvkfJGHWS\n7ZnACcDJwDckLQe8CtgK+JGkb0r6qqRV2mxnjzSyP+NVAqEY9yRtCXyEko6+FHgHpVZoVeA9wOKU\ndPWE1uvYJa1dA79v2n6/7T1sX1vvezHwDOCJFpvaOkmL9M7SJW0u6aV16OcLwDqUs/vba2f/OeD6\n2ulNaJLWoZxQfAN4J7CZpP8CsL0/8HfgQEl7AqcA/2n7wrba2xZJz5E0uWbD/gwcB1wPXAK8D1iX\nEhCdRDnpWKettsbQso5QTBQfALagpKnfQRmv/zxwGPDOLqSma2ZsB8rffQuwmqQptq+StCJl2Odo\n4FDbt7TZ1jbVAuBzaye2AaXA9WJgNWA68FrgbElrAc+jDI/9tK32jpYaSG8OnGL71LrtA8AHG8Nh\n75P0v8BRwHttf793X4tNH1U1YzYNuB2YBfzG9icknQBMAl4ILGL7TuBO4KKWmjqXcZy0GVEJhGLc\naczuWc32XbavqNs/C0y1fYmk6cArgJkT/Uu6cTyeA3wU2L0GPwcD/yVpf2BZYAfgY7bP6Vrn1WT7\nXknXA78BzgPeZvuKGvhMA34PbEmpMZtk+3dttXU01ffQeZTOvFf/8iRleHUZSp0QtqdI+rzt27r2\nPqpDXF8H/tv26ZJeBewlaVnbd0v6H0qN2emS9rd9R6sNjmHJ0FiMO70pzsCZks6W9BqVtU7uBD4p\naXfgzcBnbd/WamNHkKQl681l6//3U9L0swBsTwXuBT5h+2bgsK4HQb06H9t7UTJBUyh1QdRO61jg\nJbYfsf3nLgRBktaU9FZJmwOP2r4L/lX/8jvgftsPSXqZpMMkLdr7XHXpfVTfOzOB7/ZmYwJXUIa9\nNgWw/UdKMH01ZQh67Bjh+qDxXCOUjFCMG43Mx3KUQugpwMsptQwrAD8AFgXeChxh+9LWGjvC6oyU\ngyUtCywp6VzbX5J0H7CppN/bvg84g1KrgO0H6v+d6bya6vtnpqQVbP/V9kdrWdUpkp5n+yFgCeAF\nkibV2YcTmqTnUWqC7qFkfG6Q9BnbT9ZdHgTukvRGymKlh3WhVmqgOoNuT8o0+S/WbUvYflzSbcDj\nddvqtv8k6fDGMYwxLoFQjBs1CHotpQj6IdtXAVdJ2o9SKL2Y7f+R9FXbj03UzEej8/oScBslo3Gy\nyno3X6LUb7xY0t+AXYCD22rrWFLfP9tRZoA9BpzfCIb+IOk4YEPgSx0JgtYFfgLsVoeTdwImM+dI\nwYp122uAd9k+f6J+robwXEoB9O6Uk4tbmD3h4EmAWnP2OUnvtn1rK60c0jhO24ygBEIxbtTZYV8F\nvgVsK2mq7YNtn1CHiXaUdJHte2BiZj5q53UBsI/t6XX20yxJrwYuBP4CvIvSca0NTKmdXBc7rznU\nGXNfppzZvwB4nqRP12BoKeDDwOa2r+/I8VoGWJ06hGP7TEkHUGpebgB+afsvkv4P+Knt8+t+E/24\nzMX2lXVobBfgzZK+0wh2/gYcSBkiO3zsBkExmARCMS7UmRp7UupdTpY0DfiqpCNtH2r7OElr9IKg\nCWwZyuym3po2qin66yS9GTgS+FGjhgHoZucFcwynLkZZJPF825cAl9TA+oOS1rF9oKQvu86mm+jH\nqwbQv6rFvudIele963nAm4CdgXUlfZ5SGHx3R4LDf5G0JrC+7fMAbP+8Zl13pgRDp9W6skcpExF2\ntn1BW+0dihjfdTwjKcXSMeZJ2ogyLX5dYBNJK9cO6z+AyZKOBrD9hxabOeJ6nRel5uc0SXvVeo0n\na7H4Xylfyp1eI6inEQRtTwkQ7wa2lDQZwGW24UzKpTSgzCIbM5dDGEk1i7iI7Z8Bb6Bcl+5422vY\nfp3t3jG70vbd9TGdCYKqDYGvSdqxt6HWHX6fskL7i+rmn1JmHl7QhffORJRAKMak3heKpI2B/6XU\nvhxDqVn4d0kr1RT0Oyhj9hPegM5rR+BYSXvanmX7CcoQxyOUgt/Oq0HQKyhB9Hm2r6csfPdmSfvW\nobIXAL+t+/dm23Wiw2+8ny4GtgGWqXVCvfun1cC7k2z/kLI44mcGHJfLgBuA99Rs7CW2zx0PQVAu\nPt9fhsZiTKqd2GbAQcDZLhd/PLfOktqOMlPqLHdsYcBG53VJPVM9W9JfgBspweLHbP+93Va2R2Ut\npRdT1gK6HXgLJePx3rrL+ZQlBj4IbE0Zap3RQlPHhMb76Rcqi3FeLGlv29PabttYYPsHdTjsiJpg\nPKvedQ3wMmYPUXcmgJ6IEgjFWPYkZbbGLEkr2r7f9rfrMND2lMXwOmdAMPR6yqrIDwJvt/2jrtVy\n9AyYCv434CbKDLr1KEM/b3Jd7Vdlwc1FJvLswsEM/HubmUZJW1OuzxeV7TMkzQKOq4H2PcChlCB6\nXM0uHPs5q3ZkaCzGjMZw2AYq1326k1K4uRqwu8r6QdjuXePortYaO8oGpt0HDJNtRalR+FG9rzOd\nek9jKvhBtb7lO5TM0OKUdaWelHRqb3/b/7T9WL09oY9X43O1qqRJ9BnFaA6T2f6xqlFv7Bhl+weU\nqfObA68EDrF9Vo7RxJCMUIwZdThsB+BTlLP5VSjTnfegXARySUkn2H6gV8A5UTUKfVcFHqAs2DZH\nh93ovK7sPQZQr9alYwZOBf+ByvpSz3a5gvx+wDfrtOe3tNnQ0VbfRztRLkB8D3CtpG/3+Qz1AqYl\nbT8+2u0cK/plCOvn7FJJl/XuG4+ZRI3rSp6Rk4xQjBkqFwb9OKWe412U9POhlPU5PkSZorp8aw0c\nRY3O6yRKMPh+Sc/qs2uz83IXg6ABs+lOlrSLpDdQLj3SuxTE34G9gM+01tCWSHoR5eLD76DMkpsM\nPNTMZqhcNmOmpOWBSyX9WzutHX3DzZj1dh/NtsXoSCAUrWt8Ic+iFLJeX4curgS+BmxdZ/y8odZ4\nTHjpvIZvkKngJwKvcrnA6uJ1v791dBbUqpTrqr2UMiV8iu2HKScYzffRcsDpwIe68jmD4Z901OM0\nS/Uaf+MtGwRk2tggEghFaxqd+jOhdFSUtXC+1djtcWDNOnPj4dFtYavSec2HPlPBJ1FmF0K9BEJX\nNDIcvRlNt1NmyH0O2NX2HTVjdrSk5ev76BmUZSg+VY9hZ+SkIxIIRWvqmdj2wE8knVDPyvYH7pN0\nuaT9KUNip7mslTNhL/aYzuupa04FB14PnKWy6OT4O3N/CurnalvgE5I+TKktuxY4G3iVyvX6jgS+\n3lhqYQrwmZpV65rOnHQkIdRfiqWjNSrrBO1KCXaeT7k+1oq295T0H3W3AzyGl61fWBqd18slPUQJ\ncK6lrFfyKkl3UTqvQ9J5zZap4HOT9FLKwpGfpUw0mARcDSwNbAs8Rllv6pzG8TuqKwFjYyLCovXk\n6nbKulJ7ANvbvrOedOxdA+m/15OO71KuJdbZz9tEpY6892OMqYXRlwBX295D5aKXbwK2oMwYO6lL\nM1dq5/UNZndeFwI3UzqvbSid1w+andd4nLXyVPWbTdevQLwGQ7N6j4FxWtMxn1SWEfgEcJXtL9Ys\nxsHAUrY/UPeZZPsfXTouA/VOOoDeSce7KZmzG4G7gC9QTjrOrvsfQrncyLg9Kdtok03944uvGNHX\nWOXpS1xte7MRfZERkIxQtML2/ZI+Q7lMxBttf1/S6ZR1XzajpKvHZfp5ftXOa3/gK7ZPknQGpfPa\nqnZeJw3svKCbHVimgg+unlysAzwd2EbSObZvk3Q4cLmk9W3f7LoIYBffP5CMWcwtNULRGtunAvsB\nn5S0s8v1sv4fZbinK0HQwM5rHdsPAIdThsTWB2h2Xl3+Qk5h69xUPIsydHMzJYi+A3iLpOcDz6Fc\nf+7R9lo5Ngw86aBcSX4S5aTjJNu7U4bjz5mIJx0a4X/jVQKhaJXt71HWDvqCpDe5TJuf8CtGp/Na\nYJ0pbB2uGhvfDVxGWXH9RuBcYAPKicX/UI7D75ude9fkpCMGk0AoRs1gX8Iuy9d/CLh3dFvUnnRe\nw5PZdP01jssatYMH+CqwRK2POg/4OvCz+nMZTJzMxvzISUdDpo31lUAoRkzjy3rQFVt7bH9vos/G\nSOc1/xqz6TIVnFLoXAM+S9oQOAU4TNLnKTVT6wIfAbB9EXAOsCawl+rCkl2Tk44YSoqlY8QMt7C1\nMZwxIQtbaxC4ZJ2GuyFl9doZKle0PoTZnddU2xdJWgJ4C6Xz+kqtneqkFLbOVjvpFwFvknQbpSP/\nAGUR0iPrz4PAv0uaZvselwuozqSs1t6Z91FjduEawKO276ecdBzaO+mQ9BiwC2U1+06cdCTK6y8Z\noRgxKWydo/M6RNK7gX0pndexlHqXZue1KoDtH1NW1z69S53XQF0vbB2o/l3XUYZyPkuZIj/D9u9s\n7wacRsmUrU5ZhqL3uAts/7mNNo+2ZMxiQSQjFCNpYGHrO2w/XDu433ShsLV+IV9HCX72pXTcM+rd\nu0naFNiYUiO1BXBWfdy4Xa9kYchU8P5sP1qzQY8AkyX9wvat9e5f2b5aZfHNfSSdZ/uf7bV2dCVj\nNrQM/PWXjFAsNCls7c/2o5SroJ9B6bzWa9z9K9tfo6xsu2cdFuusFLYOrvf5sn0YpYO/FfiMpGUl\nPQd4c911aWCldlrZnmTMYkElIxQLTaOwNZeJqHq1CrYPk/R04CBK5/Uu4BnAlpRsWCc7r4FqZ3a3\npF5h6/sknQvsBryVUs/xr8LWrmSC4F+fr9776UFJ3wD2Bn4CrEDJOEKpm3pPl7JBPcmYzcv4Xutn\nJCUQioUmha1zS+c1tBS2Dq3xHmpeV+2PkqYCl1OO22X1rnO6dGx6ctIRCyrXGouFQrnG0VwGy1jU\n4a9X0ei8upbdgMFn0wG92XTnAdNtT637b0uZTfdrSgH1hK3paASHmwE31uHVIR9D+U6f69prXdH8\nHEl6NuWkYwfqSYfLrMwdgdts39RiU0fdxpts5p9eeuWIvsYKSy+Wa41FN6WwtRhu51WzPuf3HkMH\nO68Uts5bfR9tRwkOdwN+3m8/SYvZflKzr6Q+IT9bQ0nGLJ6KFEvHAkth65wandd3gI0G20/SYvX/\nRet3d6eCIEhh61BqQf0xwC62fy7puSoLcS7Z2GfRGgQtD0yXtEJrDR5ljYkZm0l62mCBjcsle863\nfVn9vlokQVAMlEAoFljtxLNia5XOa/44s+nmMOAz8hgla7hBzWqcWn+2qvsu7tnrb50OfNr2X0e7\nzW3JSUcsTAmEYr40zsRymQjSeS2oRp1YpoIzx7DqNpKmAH+iDHNtC/ySUlN2EfW9ZPsJlaUnvkNZ\neuKiVhrekpx0LBhpZH/GqwRCMSzKiq1zSee14Opx6wVDDwLfoBRB/wS4gNkX4O3EbLp6PHaidO53\n1nqfg2zvaft7lCzrTsAVAJIWoQwpHuUJuvTEQDnpiJGSYukYUgpb+2t0XkdQlgWYKemgxqyVjSid\n10H19851Xv2ksHVuKtO99wLeANwl6WXAtpI+SVl5/Bjg47YvrA8xZTj6oTbaO9qaJx3AesCJzD7p\nOAv4OOVyPlsBFzVOOk4Hjujy560p6wj1l0AohlS/gHKZiAHSeQ1PoxPLbLrBPUZZd+uYevuvwCsp\nxeL7Anvb/m0ji2agM++jnHTESEogFMPirNjaTzqvYWgUtmYqeNUIDl8OLAvcR1kw8kOUtZOuVJl5\n+QnKWku/hYlbazeUnHQsBOO8jmckpUYohpTC1qJRKP7y2rG/mNJ5XQscZ/sAYFdgGQZ0Xl3twCCF\nrf3UIGhH4FjKEgLTgO1sH1GDoF2AbwOn2X6sxaaOFc2TjmmUz9nOlCGyGZSTjjNV1Y9cgqAYlgRC\nMaQUthbpvIYvha3zJulplFWPt6NkCf8OXCZpsTq54I3AYb3OvcWmtiInHQufRuFnvEogFPM0YFiH\nevuPwFRKceJejdlP53gCL1ufzmt4Mpuuv0bnvgGwNmU4bC9gP+Bdtu+hvLf+rf5+Vu9YttTk1uSk\nI0ZTAqGYQ+PLOiu2ks5rQTQKWzMVvKHRuZ8K/A34A2W204G2b611L0cBy9h+sveY1hrcopx0jJCk\nhPpKIBRzaBS2ZsVW0nktiAGFrefXY3S4pEXqDMNj6V/Y+tNWGjxK6symTwFvr1nVb1GyGsdL+ghw\nAmVG1DUtNrM1OemItmTWWMxhQGHrryQ9F3gCuNf243WfZmHrdyW9daLWdDQ6r11d1rr5FuWL+HhJ\nZwF70OHOaxCZTdff48A1wKslvQnYGvgj8ACl9uXdtZi8k51746TjCGBHZp90vHzAScfuOelYMFlH\nqL8EQsGAL95mYevbKPUcT1C+kC6qha1PdKiwNZ3XEBo1QZkKPm9/AK6iBM//DXwfeAXwkO3pvZ06\neFyAnHREezI01nEpbB1Ss/O6CXg/ZfXjU21Pt/1z6G7nBSlsHS7bD9s+Hni17TMoy00cAPy53ZaN\nGc2TjkMp604typwnHWenJmjB5Vpj/SUQ6rgUts5bOq+hpbB1vs2stVLHA4d4gq/APh9y0hGtyNBY\nxykrtg5XOq+GRiZxA8pZe6+wdQdqYWvNEt1Uf3+yy8OHTS5rJt1MGQK6I8elsP0wZRjsRNv/lLQ5\n5aTjwJabNmHkLKS/BEKRwtZhSOc1pxS2PjW2HwHuqLdzXOaUk44YVQmEOiaFrQsunddsKWyNkZKT\njhGUlFBfqRHqmBS2xkKSwtYYMbYfsZ2TjhgVyQh1zIDC1tfSKGylnC/0CluzWFnMS6aCR4wzWUeo\nv2SEOiArtsbCltl0EbEgJE2WdIuk2yQd3HZ7IIFQJzSGw3KZiFjYUtgaMQ6I9tcRkrQo8CXKifcG\nwNvrCXqrMjTWASlsjZGSwtaImA9bALfZvh2g9kU7ATe22agEQt2Qy0TEiMlsuoixb8aMq6dPWlwr\njfDLLCXpqsbvJ9o+sfH76pQRiZ4/Ai8Z4TYNKYFQN6SwNSKiw2xPbrsNY1VqhDogha0RETEG/AlY\no/H7s+u2ViUQ6pYUtkZERFt+CawraS1JSwC7Ame13CaU0ZBukbQ0sHIKWyMiYrRJ2h74ImUB1m/Y\nPrLlJiUQioiIiO7K0FhERER0VgKhiIiI6KwEQhEREdFZCYQiIiKisxIIRcSgJM2UdI2k6yV9R9LT\nnsJzvVrSOfX2G+Z1wUVJy0vafwFe43BJHxru9gH7TJP05vl4rTUlXT+/bYyIsSWBUETMyz9sb2T7\nBcA/gfc071Qx398jts+yPXUeuywPzHcgFBExvxIIRcRw/QxYp2ZCbpF0MnA9sIakbSVdLmlGzRwt\nAyBpsqSbJc0Aduk9kaS9JB1fb68i6fuSrq0/LwWmAmvXbNTRdb8PS/qlpF9L+mTjuQ6VdKukS4Hn\nDfVHSNq3Ps+1kr43IMv1WklX1ed7fd1/UUlHN157v6d6ICNi7EggFBFDkrQYsB1wXd20LvBl2xsC\njwCHAa+1vQnlunYflLQU8FVgR2BTYNVBnv444GLbLwY2AW4ADgZ+W7NRH5a0bX3NLYCNgE0lvbKu\nlL5r3bY9sPkw/pwzbG9eX+8mYJ/GfWvW19gB+Er9G/YBHrC9eX3+fSWtNYzXiYhxIBddjYh5mSTp\nmnr7Z8DXgdWAO21fUbdvCWwAXCYJYAngcmB94A7bvwGQdArw7j6v8RrKBYGxPRN4QNIzBuyzbf35\nVf19GUpgtCzwfduP1tcYznL9L5D0acrw2zLA9MZ9p9ueBfxG0u31b9gWeFGjfmi5+tq3DuO1ImKM\nSyAUEfPyD9sbNTfUYOeR5ibgfNtvH7DfHI97igQcZfuEAa9x0AI81zRgZ9vXStoLeHXjvoFL7bu+\n9gG2mwETktZcgNeOiDEmQ2MR8VRdAbxM0jpQrmcnaT3gZmBNSWvX/d4+yOMvAKbUxy4qaTngIUq2\np2c6sHej9mh1SSsDlwA7S5okaVnKMNxQlgXulrQ4sNuA+94iaZHa5ucCt9TXnlL3R9J69Zp9ETEB\nJCMUEU+J7b/UzMppkpasmw+zfaukdwM/lPQoZWht2T5PcSBwoqR9gJnAFNuXS7qsTk//Ua0Tej5w\nec1IPQzsbnuGpG8D1wL3Uq5uPZT/Aq4E/lL/b7bp98AvgKcD77H9mKSvUWqHZqi8+F+AnYd3dCJi\nrMtFVyMiIqKzMjQWERERnZVAKCIiIjorgVBERER0VgKhiIiI6KwEQhEREdFZCYQiIiKisxIIRURE\nRGf9fxERtiHe6hxOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4245cb3e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_5labels = confusion_matrix(y_pred = labels5_pred_vae_arr[:,0], y_true = labels5_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[8,8])\n",
    "plot_confusion_matrix(cm_5labels, output_columns_5labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Implemented Grad clip, getting good accuracy!",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
