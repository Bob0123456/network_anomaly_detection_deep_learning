{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"x\"]\n",
    "\n",
    "kdd_train = pd.read_csv(\"dataset/KDDTrain+.txt\",names = col_names,)\n",
    "kdd_test = pd.read_csv(\"dataset/KDDTest+.txt\",names = col_names,)\n",
    "\n",
    "kdd_train = kdd_train.drop(\"x\", axis = 1)\n",
    "kdd_test = kdd_test.drop(\"x\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"protocol_type\",\"service\",\"flag\"]\n",
    "for cv in category_variables:\n",
    "    kdd_train[cv] = kdd_train[cv].astype(\"category\")\n",
    "kdd_train[\"label\"] = kdd_train[\"label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125966</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125967</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>udp</td>\n",
       "      <td>private</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>klogin</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0              0           tcp  ftp_data   SF        491          0     0   \n",
       "1              0           udp     other   SF        146          0     0   \n",
       "2              0           tcp   private   S0          0          0     0   \n",
       "3              0           tcp      http   SF        232       8153     0   \n",
       "4              0           tcp      http   SF        199        420     0   \n",
       "5              0           tcp   private  REJ          0          0     0   \n",
       "6              0           tcp   private   S0          0          0     0   \n",
       "...          ...           ...       ...  ...        ...        ...   ...   \n",
       "125966         0           tcp   private   S0          0          0     0   \n",
       "125967         0           tcp      http   SF        359        375     0   \n",
       "125968         0           tcp   private   S0          0          0     0   \n",
       "125969         8           udp   private   SF        105        145     0   \n",
       "125970         0           tcp      smtp   SF       2231        384     0   \n",
       "125971         0           tcp    klogin   S0          0          0     0   \n",
       "125972         0           tcp  ftp_data   SF        151          0     0   \n",
       "\n",
       "        wrong_fragment  urgent  hot   ...     dst_host_srv_count  \\\n",
       "0                    0       0    0   ...                     25   \n",
       "1                    0       0    0   ...                      1   \n",
       "2                    0       0    0   ...                     26   \n",
       "3                    0       0    0   ...                    255   \n",
       "4                    0       0    0   ...                    255   \n",
       "5                    0       0    0   ...                     19   \n",
       "6                    0       0    0   ...                      9   \n",
       "...                ...     ...  ...   ...                    ...   \n",
       "125966               0       0    0   ...                     13   \n",
       "125967               0       0    0   ...                    255   \n",
       "125968               0       0    0   ...                     25   \n",
       "125969               0       0    0   ...                    244   \n",
       "125970               0       0    0   ...                     30   \n",
       "125971               0       0    0   ...                      8   \n",
       "125972               0       0    0   ...                     77   \n",
       "\n",
       "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                         0.17                    0.03   \n",
       "1                         0.00                    0.60   \n",
       "2                         0.10                    0.05   \n",
       "3                         1.00                    0.00   \n",
       "4                         1.00                    0.00   \n",
       "5                         0.07                    0.07   \n",
       "6                         0.04                    0.05   \n",
       "...                        ...                     ...   \n",
       "125966                    0.05                    0.07   \n",
       "125967                    1.00                    0.00   \n",
       "125968                    0.10                    0.06   \n",
       "125969                    0.96                    0.01   \n",
       "125970                    0.12                    0.06   \n",
       "125971                    0.03                    0.05   \n",
       "125972                    0.30                    0.03   \n",
       "\n",
       "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                              0.17                         0.00   \n",
       "1                              0.88                         0.00   \n",
       "2                              0.00                         0.00   \n",
       "3                              0.03                         0.04   \n",
       "4                              0.00                         0.00   \n",
       "5                              0.00                         0.00   \n",
       "6                              0.00                         0.00   \n",
       "...                             ...                          ...   \n",
       "125966                         0.00                         0.00   \n",
       "125967                         0.33                         0.04   \n",
       "125968                         0.00                         0.00   \n",
       "125969                         0.01                         0.00   \n",
       "125970                         0.00                         0.00   \n",
       "125971                         0.00                         0.00   \n",
       "125972                         0.30                         0.00   \n",
       "\n",
       "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                       0.00                      0.00                  0.05   \n",
       "1                       0.00                      0.00                  0.00   \n",
       "2                       1.00                      1.00                  0.00   \n",
       "3                       0.03                      0.01                  0.00   \n",
       "4                       0.00                      0.00                  0.00   \n",
       "5                       0.00                      0.00                  1.00   \n",
       "6                       1.00                      1.00                  0.00   \n",
       "...                      ...                       ...                   ...   \n",
       "125966                  1.00                      1.00                  0.00   \n",
       "125967                  0.33                      0.00                  0.00   \n",
       "125968                  1.00                      1.00                  0.00   \n",
       "125969                  0.00                      0.00                  0.00   \n",
       "125970                  0.72                      0.00                  0.01   \n",
       "125971                  1.00                      1.00                  0.00   \n",
       "125972                  0.00                      0.00                  0.00   \n",
       "\n",
       "        dst_host_srv_rerror_rate    label  \n",
       "0                           0.00   normal  \n",
       "1                           0.00   normal  \n",
       "2                           0.00  neptune  \n",
       "3                           0.01   normal  \n",
       "4                           0.00   normal  \n",
       "5                           1.00  neptune  \n",
       "6                           0.00  neptune  \n",
       "...                          ...      ...  \n",
       "125966                      0.00  neptune  \n",
       "125967                      0.00   normal  \n",
       "125968                      0.00  neptune  \n",
       "125969                      0.00   normal  \n",
       "125970                      0.00   normal  \n",
       "125971                      0.00  neptune  \n",
       "125972                      0.00   normal  \n",
       "\n",
       "[125973 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125973.00000</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.14465</td>\n",
       "      <td>4.556674e+04</td>\n",
       "      <td>1.977911e+04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.279250</td>\n",
       "      <td>...</td>\n",
       "      <td>182.148945</td>\n",
       "      <td>115.653005</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.284452</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.120240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2604.51531</td>\n",
       "      <td>5.870331e+06</td>\n",
       "      <td>4.021269e+06</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>23.942042</td>\n",
       "      <td>...</td>\n",
       "      <td>99.206213</td>\n",
       "      <td>110.702741</td>\n",
       "      <td>0.448949</td>\n",
       "      <td>0.188922</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.306557</td>\n",
       "      <td>0.319459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42908.00000</td>\n",
       "      <td>1.379964e+09</td>\n",
       "      <td>1.309937e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7479.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes           land  \\\n",
       "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
       "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
       "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
       "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
       "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
       "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
       "mean         0.022687       0.000111       0.204409           0.001222   \n",
       "std          0.253530       0.014366       2.149968           0.045239   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      77.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  125973.000000    125973.000000            ...              \n",
       "mean        0.395736         0.279250            ...              \n",
       "std         0.489010        23.942042            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         1.000000         0.000000            ...              \n",
       "max         1.000000      7479.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   125973.000000       125973.000000           125973.000000   \n",
       "mean       182.148945          115.653005                0.521242   \n",
       "std         99.206213          110.702741                0.448949   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%         82.000000           10.000000                0.050000   \n",
       "50%        255.000000           63.000000                0.510000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           125973.000000                125973.000000   \n",
       "mean                 0.082951                     0.148379   \n",
       "std                  0.188922                     0.308997   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.020000                     0.000000   \n",
       "75%                  0.070000                     0.060000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                125973.000000         125973.000000   \n",
       "mean                      0.032542              0.284452   \n",
       "std                       0.112564              0.444784   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.020000              1.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             125973.000000         125973.000000   \n",
       "mean                   0.278485              0.118832   \n",
       "std                    0.445669              0.306557   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    1.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             125973.000000  \n",
       "mean                   0.120240  \n",
       "std                    0.319459  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22544.000000</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.859076</td>\n",
       "      <td>1.039545e+04</td>\n",
       "      <td>2.056019e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.105394</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>193.869411</td>\n",
       "      <td>140.750532</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.226683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1407.176612</td>\n",
       "      <td>4.727864e+05</td>\n",
       "      <td>2.121930e+04</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.928428</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>7.269597</td>\n",
       "      <td>...</td>\n",
       "      <td>94.035663</td>\n",
       "      <td>111.783972</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.273139</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.400875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870000e+02</td>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57715.000000</td>\n",
       "      <td>6.282565e+07</td>\n",
       "      <td>1.345927e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
       "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
       "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
       "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
       "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
       "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
       "mean       0.000710      0.105394           0.021647      0.442202   \n",
       "std        0.036473      0.928428           0.150328      0.496659   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      1.000000   \n",
       "max        3.000000    101.000000           4.000000      1.000000   \n",
       "\n",
       "       num_compromised            ...             dst_host_count  \\\n",
       "count     22544.000000            ...               22544.000000   \n",
       "mean          0.119899            ...                 193.869411   \n",
       "std           7.269597            ...                  94.035663   \n",
       "min           0.000000            ...                   0.000000   \n",
       "25%           0.000000            ...                 121.000000   \n",
       "50%           0.000000            ...                 255.000000   \n",
       "75%           0.000000            ...                 255.000000   \n",
       "max         796.000000            ...                 255.000000   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        22544.000000            22544.000000            22544.000000   \n",
       "mean           140.750532                0.608722                0.090540   \n",
       "std            111.783972                0.435688                0.220717   \n",
       "min              0.000000                0.000000                0.000000   \n",
       "25%             15.000000                0.070000                0.000000   \n",
       "50%            168.000000                0.920000                0.010000   \n",
       "75%            255.000000                1.000000                0.060000   \n",
       "max            255.000000                1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 22544.000000                 22544.000000   \n",
       "mean                      0.132261                     0.019638   \n",
       "std                       0.306268                     0.085394   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.030000                     0.010000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          22544.000000              22544.000000          22544.000000   \n",
       "mean               0.097814                  0.099426              0.233385   \n",
       "std                0.273139                  0.281866              0.387229   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.360000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              22544.000000  \n",
       "mean                   0.226683  \n",
       "std                    0.400875  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.170000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          normal\n",
       "1          normal\n",
       "2         neptune\n",
       "3          normal\n",
       "4          normal\n",
       "5         neptune\n",
       "6         neptune\n",
       "           ...   \n",
       "125966    neptune\n",
       "125967     normal\n",
       "125968    neptune\n",
       "125969     normal\n",
       "125970     normal\n",
       "125971    neptune\n",
       "125972     normal\n",
       "Name: label, dtype: category\n",
       "Categories (23, object): [back, buffer_overflow, ftp_write, guess_passwd, ..., spy, teardrop, warezclient, warezmaster]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "#scatter_matrix(kdd_train, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_types = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "is_sess = {\n",
    "    \"DoS\":\"Attack\",\n",
    "    \"R2L\":\"Attack\",\n",
    "    \"U2R\":\"Attack\",\n",
    "    \"Probe\":\"Attack\",\n",
    "    \"normal\":\"Normal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_train[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])\n",
    "kdd_test[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_test[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_sess_type_group = kdd_train.groupby(\"sess_type\")\n",
    "kdd_is_sess_group = kdd_train.groupby(\"is_sess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_type\n",
       "DoS       45927\n",
       "Probe     11656\n",
       "R2L         995\n",
       "U2R          52\n",
       "normal    67343\n",
       "Name: is_sess, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.is_sess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sess\n",
       "Attack    58630\n",
       "Normal    67343\n",
       "Name: sess_type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.sess_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sess_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">DoS</th>\n",
       "      <th>count</th>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.090034</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>1.692015e+02</td>\n",
       "      <td>244.600475</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.747922</td>\n",
       "      <td>26.524005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.748494</td>\n",
       "      <td>1.176321e+03</td>\n",
       "      <td>32.656346</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.445748</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>1.168004e+03</td>\n",
       "      <td>41.324475</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.188947</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>48.303117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299931</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>7.686120e+03</td>\n",
       "      <td>94.667526</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "sess_type                                                                    \n",
       "DoS       count  45927.000000   45927.000000  4.592700e+04    45927.000000   \n",
       "          mean     178.090034       0.065403  1.692015e+02      244.600475   \n",
       "          std      104.445748       0.064023  1.168004e+03       41.324475   \n",
       "          min        1.000000       0.000000  0.000000e+00        1.000000   \n",
       "          25%      109.000000       0.050000  0.000000e+00      255.000000   \n",
       "          50%      172.000000       0.060000  0.000000e+00      255.000000   \n",
       "          75%      249.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                       ...            ...           ...             ...   \n",
       "normal    mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "          std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "          min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "          25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "          50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "          75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "          max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "                 dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "sess_type                                                       \n",
       "DoS       count            45927.000000          45927.000000   \n",
       "          mean                 0.066333              0.157569   \n",
       "          std                  0.058079              0.358934   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.050000              0.000000   \n",
       "          50%                  0.070000              0.000000   \n",
       "          75%                  0.070000              0.000000   \n",
       "...                                 ...                   ...   \n",
       "normal    mean                 0.040134              0.046589   \n",
       "          std                  0.128529              0.195306   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.000000              0.000000   \n",
       "          50%                  0.000000              0.000000   \n",
       "          75%                  0.020000              0.000000   \n",
       "          max                  1.000000              1.000000   \n",
       "\n",
       "                 dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "sess_type                                                              \n",
       "DoS       count                 45927.000000            45927.000000   \n",
       "          mean                      0.049492                0.123423   \n",
       "          std                       0.188947                0.228287   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.020000   \n",
       "          50%                       0.000000                0.050000   \n",
       "          75%                       0.000000                0.080000   \n",
       "...                                      ...                     ...   \n",
       "normal    mean                      0.121726                0.811875   \n",
       "          std                       0.254382                0.324091   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.750000   \n",
       "          50%                       0.010000                1.000000   \n",
       "          75%                       0.080000                1.000000   \n",
       "          max                       1.000000                1.000000   \n",
       "\n",
       "                 dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "sess_type                                                       ...         \n",
       "DoS       count          45927.000000        45927.000000       ...         \n",
       "          mean               0.747922           26.524005       ...         \n",
       "          std                0.431707           48.303117       ...         \n",
       "          min                0.000000            1.000000       ...         \n",
       "          25%                0.180000            6.000000       ...         \n",
       "          50%                1.000000           13.000000       ...         \n",
       "          75%                1.000000           20.000000       ...         \n",
       "...                               ...                 ...       ...         \n",
       "normal    mean               0.013930          190.285761       ...         \n",
       "          std                0.092006           92.608377       ...         \n",
       "          min                0.000000            0.000000       ...         \n",
       "          25%                0.000000          121.000000       ...         \n",
       "          50%                0.000000          255.000000       ...         \n",
       "          75%                0.000000          255.000000       ...         \n",
       "          max                1.000000          255.000000       ...         \n",
       "\n",
       "                 same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "sess_type                                                                  \n",
       "DoS       count   45927.000000  45927.000000  4.592700e+04  45927.000000   \n",
       "          mean        0.191887      0.748494  1.176321e+03     32.656346   \n",
       "          std         0.299931      0.432559  7.686120e+03     94.667526   \n",
       "          min         0.000000      0.000000  0.000000e+00      1.000000   \n",
       "          25%         0.040000      0.290000  0.000000e+00      5.000000   \n",
       "          50%         0.070000      1.000000  0.000000e+00     11.000000   \n",
       "          75%         0.150000      1.000000  0.000000e+00     18.000000   \n",
       "...                        ...           ...           ...           ...   \n",
       "normal    mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "          std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "          min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "          25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "          50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "          75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "          max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "                 srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "sess_type                                                               \n",
       "DoS       count        45927.000000     45927.000000     45927.000000   \n",
       "          mean             0.005317         0.153000         0.746678   \n",
       "          std              0.056390         0.357561         0.434050   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         1.000000   \n",
       "          75%              0.000000         0.000000         1.000000   \n",
       "...                             ...              ...              ...   \n",
       "normal    mean             0.126263         0.044629         0.012083   \n",
       "          std              0.271621         0.202264         0.086426   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         0.000000   \n",
       "          75%              0.110000         0.000000         0.000000   \n",
       "          max              1.000000         1.000000         1.000000   \n",
       "\n",
       "                 su_attempted        urgent  wrong_fragment  \n",
       "sess_type                                                    \n",
       "DoS       count  45927.000000  45927.000000    45927.000000  \n",
       "          mean       0.000000      0.000000        0.062229  \n",
       "          std        0.000000      0.000000        0.416951  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "...                       ...           ...             ...  \n",
       "normal    mean       0.002049      0.000148        0.000000  \n",
       "          std        0.061622      0.017233        0.000000  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "          max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[40 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Attack</th>\n",
       "      <th>count</th>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.849991</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>3.752448e+04</td>\n",
       "      <td>222.025260</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.595177</td>\n",
       "      <td>29.929081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>8.282014e+04</td>\n",
       "      <td>27.797885</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0.209114</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.048746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.334483</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>5.893991e+06</td>\n",
       "      <td>79.196259</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>52.289254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>8.593025e+06</td>\n",
       "      <td>84.710761</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.369916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "is_sess                                                                    \n",
       "Attack  count  58630.000000   58630.000000  5.863000e+04    58630.000000   \n",
       "        mean     154.849991       0.102410  3.752448e+04      222.025260   \n",
       "        std      124.334483       0.206408  5.893991e+06       79.196259   \n",
       "        min        0.000000       0.000000  0.000000e+00        1.000000   \n",
       "        25%       40.000000       0.050000  0.000000e+00      255.000000   \n",
       "        50%      138.000000       0.060000  0.000000e+00      255.000000   \n",
       "        75%      241.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                     ...            ...           ...             ...   \n",
       "Normal  mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "        std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "        min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "        25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "        50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "        75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "        max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "               dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "is_sess                                                       \n",
       "Attack  count            58630.000000          58630.000000   \n",
       "        mean                 0.132131              0.201810   \n",
       "        std                  0.230626              0.381090   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.050000              0.000000   \n",
       "        50%                  0.070000              0.000000   \n",
       "        75%                  0.080000              0.020000   \n",
       "...                               ...                   ...   \n",
       "Normal  mean                 0.040134              0.046589   \n",
       "        std                  0.128529              0.195306   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.000000              0.000000   \n",
       "        50%                  0.000000              0.000000   \n",
       "        75%                  0.020000              0.000000   \n",
       "        max                  1.000000              1.000000   \n",
       "\n",
       "               dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "is_sess                                                              \n",
       "Attack  count                 58630.000000            58630.000000   \n",
       "        mean                      0.178993                0.187417   \n",
       "        std                       0.359262                0.322430   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.020000   \n",
       "        50%                       0.000000                0.050000   \n",
       "        75%                       0.020000                0.090000   \n",
       "...                                    ...                     ...   \n",
       "Normal  mean                      0.121726                0.811875   \n",
       "        std                       0.254382                0.324091   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.750000   \n",
       "        50%                       0.010000                1.000000   \n",
       "        75%                       0.080000                1.000000   \n",
       "        max                       1.000000                1.000000   \n",
       "\n",
       "               dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "is_sess                                                       ...         \n",
       "Attack  count          58630.000000        58630.000000       ...         \n",
       "        mean               0.595177           29.929081       ...         \n",
       "        std                0.484495           52.289254       ...         \n",
       "        min                0.000000            1.000000       ...         \n",
       "        25%                0.000000            4.000000       ...         \n",
       "        50%                1.000000           12.000000       ...         \n",
       "        75%                1.000000           21.000000       ...         \n",
       "...                             ...                 ...       ...         \n",
       "Normal  mean               0.013930          190.285761       ...         \n",
       "        std                0.092006           92.608377       ...         \n",
       "        min                0.000000            0.000000       ...         \n",
       "        25%                0.000000          121.000000       ...         \n",
       "        50%                0.000000          255.000000       ...         \n",
       "        75%                0.000000          255.000000       ...         \n",
       "        max                1.000000          255.000000       ...         \n",
       "\n",
       "               same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "is_sess                                                                  \n",
       "Attack  count   58630.000000  58630.000000  5.863000e+04  58630.000000   \n",
       "        mean        0.306659      0.595808  8.282014e+04     27.797885   \n",
       "        std         0.395655      0.486588  8.593025e+06     84.710761   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         0.040000      0.000000  0.000000e+00      3.000000   \n",
       "        50%         0.080000      1.000000  0.000000e+00     10.000000   \n",
       "        75%         0.500000      1.000000  0.000000e+00     18.000000   \n",
       "...                      ...           ...           ...           ...   \n",
       "Normal  mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "        std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "        50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "        75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "        max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "               srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "is_sess                                                               \n",
       "Attack  count        58630.000000     58630.000000     58630.000000   \n",
       "        mean             0.064079         0.209114         0.593072   \n",
       "        std              0.241348         0.404487         0.490234   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         1.000000   \n",
       "        75%              0.000000         0.000000         1.000000   \n",
       "...                           ...              ...              ...   \n",
       "Normal  mean             0.126263         0.044629         0.012083   \n",
       "        std              0.271621         0.202264         0.086426   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         0.000000   \n",
       "        75%              0.110000         0.000000         0.000000   \n",
       "        max              1.000000         1.000000         1.000000   \n",
       "\n",
       "               su_attempted        urgent  wrong_fragment  \n",
       "is_sess                                                    \n",
       "Attack  count  58630.000000  58630.000000    58630.000000  \n",
       "        mean       0.000017      0.000068        0.048746  \n",
       "        std        0.004130      0.010116        0.369916  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "...                     ...           ...             ...  \n",
       "Normal  mean       0.002049      0.000148        0.000000  \n",
       "        std        0.061622      0.017233        0.000000  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "        max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[16 rows x 38 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kdd_is_sess_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kdd_sess_type_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_dummy(train, test):\n",
    "    dummy_variables_2labels = [*category_variables, \"is_sess\"]\n",
    "    dummy_variables_5labels = [*category_variables, \"sess_type\"]\n",
    "\n",
    "    drop_variables = [*category_variables, \"is_sess\", \"sess_type\", \"duration\", \"label\"]\n",
    "    \n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    def dummy(kdd):\n",
    "        kdd_one_hot_2labels = pd.get_dummies(kdd[dummy_variables_2labels], prefix=dummy_variables_2labels, drop_first=False)\n",
    "        kdd_one_hot_5labels = pd.get_dummies(kdd[dummy_variables_5labels], prefix=dummy_variables_5labels, drop_first=False)\n",
    "\n",
    "        kdd_2labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                       , kdd_one_hot_2labels]\n",
    "                                      , axis = 1)\n",
    "        kdd_5labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                      , kdd_one_hot_5labels]\n",
    "                                      , axis = 1)\n",
    "\n",
    "        return kdd_2labels, kdd_5labels\n",
    "    \n",
    "    kdd_2labels, kdd_5labels = dummy(pd.concat([train, test], axis = 0))\n",
    "    \n",
    "    kdd_2labels_train, kdd_2labels_test = kdd_2labels.iloc[:train_size,:], kdd_2labels.iloc[train_size:,:]\n",
    "    kdd_5labels_train, kdd_5labels_test = kdd_5labels.iloc[:train_size,:], kdd_5labels.iloc[train_size:,:]\n",
    "    \n",
    "    return kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_columns_2labels = ['is_sess_Normal', 'is_sess_Attack']\n",
    "output_columns_5labels = ['sess_type_normal', 'sess_type_DoS', 'sess_type_Probe', 'sess_type_R2L', 'sess_type_U2R']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test = encode_dummy(kdd_train, kdd_test)\n",
    "\n",
    "x_kdd_train = kdd_2labels_train.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_train = kdd_2labels_train.loc[:,output_columns_2labels].values \n",
    "y_5labels_train = kdd_5labels_train.loc[:,output_columns_5labels].values\n",
    "\n",
    "x_kdd_test = kdd_2labels_test.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_test = kdd_2labels_test.loc[:,output_columns_2labels].values \n",
    "y_5labels_test = kdd_5labels_test.loc[:,output_columns_5labels].values\n",
    "\n",
    "        \n",
    "ss = pp.StandardScaler()\n",
    "x_kdd_train = ss.fit_transform(x_kdd_train)\n",
    "x_test = ss.transform(x_kdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sanity before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: mean:-0.0000, std:0.9959, shape:(125973, 121)\n",
      "Testing  data: mean:0.0170, std:1.4175, shape:(22544, 121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_kdd_train.mean(), x_kdd_train.std(), x_kdd_train.shape))\n",
    "print(\"Testing  data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_test.mean(), x_test.std(), x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "                      input_dim = 121, classes = 2, hidden_layers = 8, epochs = 10, hidden_units = 5):\n",
    "   \n",
    "    model_dense = Sequential()\n",
    "    model_dense.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        model_dense.add(Dense(hidden_units, kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    model_dense.add(Dense(classes, kernel_initializer='uniform' ,activation = 'softmax'))\n",
    "\n",
    "    model_dense.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'],\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                  activity_regularizer=keras.regularizers.l1(0.01)) \n",
    "\n",
    "    model_dense.fit(x_train, y_train, \n",
    "                  epochs = epochs, batch_size=500, \n",
    "                  validation_data = (x_valid, y_valid),\n",
    "                  verbose=1)\n",
    "    \n",
    "    scores_train = model_dense.evaluate(x_train, y_train)\n",
    "    scores_valid = model_dense.evaluate(x_valid, y_valid)\n",
    "    scores_test = model_dense.evaluate(x_test, y_test)\n",
    "    train_loss = scores_train[0]\n",
    "    valid_accuracy = scores_valid[1] \n",
    "    test_accuracy = scores_test[1]\n",
    "    \n",
    "    print(\"\\n Test loss: {}, accuracy: {}\".format(scores_test[0], scores_test[1]))\n",
    "    return train_loss, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_vae(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5):\n",
    "    \n",
    "    graph_vae = tf.Graph()\n",
    "    with graph_vae.as_default():\n",
    "        latent_dim = hidden_units\n",
    "        dense_hidden_units = hidden_units\n",
    "        hidden_encoder_dim = 60\n",
    "        hidden_decoder_dim = 60\n",
    "\n",
    "        lam = 0.01\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            initial = tf.constant(0.01, shape=shape)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        l2_loss = tf.constant(0.001)\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "                b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "\n",
    "                # Hidden layer encoder\n",
    "                hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "                tf.summary.histogram(\"Weights_Encoder\", W_encoder_input_hidden)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "\n",
    "            # Mu encoder\n",
    "            mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "            tf.summary.histogram(\"Weights_Mean\", W_encoder_hidden_mu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "\n",
    "            # Sigma encoder\n",
    "            logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "            tf.summary.histogram(\"Weights_Variance\", W_encoder_hidden_logvar)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "                b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "\n",
    "                # Hidden layer decoder\n",
    "                hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "                tf.summary.histogram(\"Weights_Decoder\", W_decoder_z_hidden)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "            b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "            l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "\n",
    "            x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "            tf.summary.histogram(\"Weights_Reconstruction\", W_decoder_hidden_reconstruction)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), \n",
    "                                 reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), \n",
    "                                        reduction_indices=1)\n",
    "            \n",
    "            reconst_loss = tf.abs(tf.reduce_mean(BCE + KLD))\n",
    "            regularized_loss = reconst_loss + lam * l2_loss\n",
    "\n",
    "            tf.summary.scalar(\"BCE\", tf.reduce_mean(BCE))\n",
    "            tf.summary.scalar(\"KLD\", tf.reduce_mean(KLD))\n",
    "\n",
    "            tf.summary.scalar(\"Total_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph_vae) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                sess.run(optimizer, feed_dict={x: x_train[i,:], keep_prob:0.6})\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train, train_reduction_loss, train_z = sess.run([regularized_loss, summary_op, reconst_loss, z], feed_dict={x: x_train, keep_prob:1})\n",
    "            summary_str_valid, valid_reduction_loss, valid_z = sess.run([summary_op, reconst_loss, z], feed_dict={x: x_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}, Reduction Loss: {:.4f} | \"\n",
    "                      \"Validation - Reduction Loss: {:.4f}\"\n",
    "                      .format(epoch, train_loss, train_reduction_loss, valid_reduction_loss))\n",
    "\n",
    "        test_reduction_loss, test_z = sess.run([reconst_loss, z], feed_dict={x: x_test, keep_prob:1})\n",
    "        print(\"Test - Feature reduction loss:{:.4f}\".format(test_reduction_loss))\n",
    "    return train_reduction_loss, valid_reduction_loss, test_reduction_loss, train_z, valid_z, test_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_softmax(z_train, z_valid, y_train, y_valid, z_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5,\n",
    "              only_vae = False):\n",
    "\n",
    "    latent_dim = hidden_units\n",
    "    dense_hidden_units = hidden_units\n",
    "    \n",
    "    graph_softmax = tf.Graph()\n",
    "    with graph_softmax.as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"Sampled_Distribution\"):\n",
    "            z = tf.placeholder(\"float32\", shape=[None, latent_dim], name=\"Z\")\n",
    "            y_ = tf.placeholder(\"float\", shape=[None, classes], name = \"y_\")\n",
    "            keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "\n",
    "        z_h = z\n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):    \n",
    "            z_h = tf.layers.dense(z_h,dense_hidden_units, activation=tf.nn.relu)\n",
    "            z_h = tf.nn.dropout(z_h, keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z_h, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            regularized_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "            pred = tf.argmax(y, 1)\n",
    "            actual = tf.argmax(y_, 1)\n",
    "            \n",
    "            correct_prediction = tf.equal(actual, pred)\n",
    "            tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "            tf.summary.scalar(\"Softmax_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "\n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(z_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    with tf.Session(graph=graph_softmax) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/softmax/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/softmax/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                feed_dict={z: z_train[i,:], y_: y_train[i,:], keep_prob:0.6}\n",
    "                sess.run(optimizer, feed_dict = feed_dict)\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train = sess.run([regularized_loss, summary_op], feed_dict={z: z_train, y_: y_train, keep_prob:1})\n",
    "            valid_accuracy, summary_str_valid = sess.run([tf_accuracy, summary_op], feed_dict={z: z_valid, y_:y_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}| \"\n",
    "                      \"Validation - Acc: {:.4f}\"\n",
    "                      .format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "        test_accuracy, y_pred, y_actual = sess.run([tf_accuracy, pred, actual], feed_dict={z: z_test, y_:y_test, keep_prob:1})\n",
    "        print(\"Test - Accuracy: {:.4f}\".format(test_accuracy))\n",
    "\n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "\n",
    "        outputs = np.hstack((y_pred, y_actual))\n",
    "        \n",
    "    return train_loss, valid_accuracy, test_accuracy, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 8, 8),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 4),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 256, 8),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "    Hyper_parameters(10, 1024, 8)\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_scenario_vae(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"])\n",
    "    \n",
    "    print(\"Training for {} labels on VAE Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"*********** Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{} **************\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        print(\"VAE for Feature reduction:\")\n",
    "        t_r_l, v_r_l, te_r_l, t_z, v_z, te_z = train_vae(x_train, x_valid, \n",
    "                                           y_train, y_valid, \n",
    "                                           x_kdd_test, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        print(\"Softmax for Prediction:\")\n",
    "        t_l, v_a, te_a, op = train_softmax(t_z, v_z, \n",
    "                                           y_train, y_valid, \n",
    "                                           te_z, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a, t_r_l, v_r_l, te_r_l]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    \n",
    "    return df_results, op\n",
    "        \n",
    "def run_scenario_dense(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"])\n",
    "    \n",
    "    print(\"Training for {} labels on Dense Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a = train_dense(x_train, x_valid, y_train, y_valid, x_kdd_test, y_test_labels, \n",
    "                 classes = classes, \n",
    "                 hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                 hidden_units = Hyper_parameters.reduced_features,\n",
    "                 epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2094: UserWarning: Expected no kwargs, you passed 2\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.4385 - acc: 0.8114 - val_loss: 0.2695 - val_acc: 0.9734\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1983 - acc: 0.9795 - val_loss: 0.1429 - val_acc: 0.9857\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1093 - acc: 0.9872 - val_loss: 0.0857 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0720 - acc: 0.9889 - val_loss: 0.0642 - val_acc: 0.9886\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0566 - acc: 0.9899 - val_loss: 0.0540 - val_acc: 0.9895\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0494 - acc: 0.9903 - val_loss: 0.0487 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0457 - acc: 0.9903 - val_loss: 0.0463 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0442 - acc: 0.9904 - val_loss: 0.0467 - val_acc: 0.9897\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0432 - acc: 0.9904 - val_loss: 0.0443 - val_acc: 0.9906\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0424 - acc: 0.9908 - val_loss: 0.0444 - val_acc: 0.9906\n",
      "20800/22544 [==========================>...] - ETA: 0s\n",
      " Test loss: 3.340306894757208, accuracy: 0.4706352022711143\n",
      "Training for 2 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5850, Reduction Loss: 0.5060 | Validation - Reduction Loss: 0.5063\n",
      "Step  1 | Training - Loss: 0.4480, Reduction Loss: 0.2911 | Validation - Reduction Loss: 0.2921\n",
      "Step  2 | Training - Loss: 0.3827, Reduction Loss: 0.1994 | Validation - Reduction Loss: 0.2002\n",
      "Step  3 | Training - Loss: 0.3280, Reduction Loss: 0.1664 | Validation - Reduction Loss: 0.1670\n",
      "Step  4 | Training - Loss: 0.2666, Reduction Loss: 0.1426 | Validation - Reduction Loss: 0.1431\n",
      "Step  5 | Training - Loss: 0.2112, Reduction Loss: 0.1155 | Validation - Reduction Loss: 0.1163\n",
      "Step  6 | Training - Loss: 0.1803, Reduction Loss: 0.0957 | Validation - Reduction Loss: 0.0965\n",
      "Step  7 | Training - Loss: 0.1616, Reduction Loss: 0.0821 | Validation - Reduction Loss: 0.0830\n",
      "Step  8 | Training - Loss: 0.1466, Reduction Loss: 0.0725 | Validation - Reduction Loss: 0.0734\n",
      "Step  9 | Training - Loss: 0.1347, Reduction Loss: 0.0654 | Validation - Reduction Loss: 0.0664\n",
      "Test - Feature reduction loss:12541856407617536.0000\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.6963| Validation - Acc: 0.5054\n",
      "Step  1 | Training - Loss: 0.6914| Validation - Acc: 0.5248\n",
      "Step  2 | Training - Loss: 0.6910| Validation - Acc: 0.5287\n",
      "Step  3 | Training - Loss: 0.6908| Validation - Acc: 0.5288\n",
      "Step  4 | Training - Loss: 0.6907| Validation - Acc: 0.5309\n",
      "Step  5 | Training - Loss: 0.6906| Validation - Acc: 0.5311\n",
      "Step  6 | Training - Loss: 0.6906| Validation - Acc: 0.5308\n",
      "Step  7 | Training - Loss: 0.6906| Validation - Acc: 0.5315\n",
      "Step  8 | Training - Loss: 0.6906| Validation - Acc: 0.5308\n",
      "Step  9 | Training - Loss: 0.6907| Validation - Acc: 0.5309\n",
      "Test - Accuracy: 0.5342\n",
      "Training for 5 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.8367 - acc: 0.8586 - val_loss: 0.2627 - val_acc: 0.8876\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.2388 - acc: 0.9163 - val_loss: 0.2126 - val_acc: 0.9732\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1984 - acc: 0.9727 - val_loss: 0.1808 - val_acc: 0.9766\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1697 - acc: 0.9756 - val_loss: 0.1576 - val_acc: 0.9786\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1503 - acc: 0.9774 - val_loss: 0.1406 - val_acc: 0.9802\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1350 - acc: 0.9787 - val_loss: 0.1263 - val_acc: 0.9817\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1179 - acc: 0.9794 - val_loss: 0.0705 - val_acc: 0.9822\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0546 - acc: 0.9797 - val_loss: 0.0432 - val_acc: 0.9826\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0467 - acc: 0.9802 - val_loss: 0.0388 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0405 - acc: 0.9826 - val_loss: 0.0347 - val_acc: 0.9892\n",
      "21536/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 10.789834467209003, accuracy: 0.28180447125621005\n",
      "Training for 5 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5836, Reduction Loss: 0.5050 | Validation - Reduction Loss: 0.5057\n",
      "Step  1 | Training - Loss: 0.4466, Reduction Loss: 0.2897 | Validation - Reduction Loss: 0.2913\n",
      "Step  2 | Training - Loss: 0.3816, Reduction Loss: 0.1987 | Validation - Reduction Loss: 0.2005\n",
      "Step  3 | Training - Loss: 0.3271, Reduction Loss: 0.1659 | Validation - Reduction Loss: 0.1679\n",
      "Step  4 | Training - Loss: 0.2657, Reduction Loss: 0.1417 | Validation - Reduction Loss: 0.1442\n",
      "Step  5 | Training - Loss: 0.2106, Reduction Loss: 0.1156 | Validation - Reduction Loss: 0.1182\n",
      "Step  6 | Training - Loss: 0.1799, Reduction Loss: 0.0964 | Validation - Reduction Loss: 0.0992\n",
      "Step  7 | Training - Loss: 0.1612, Reduction Loss: 0.0833 | Validation - Reduction Loss: 0.0863\n",
      "Step  8 | Training - Loss: 0.1462, Reduction Loss: 0.0737 | Validation - Reduction Loss: 0.0769\n",
      "Step  9 | Training - Loss: 0.1343, Reduction Loss: 0.0663 | Validation - Reduction Loss: 0.0696\n",
      "Test - Feature reduction loss:nan\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.5533| Validation - Acc: 0.4408\n",
      "Step  1 | Training - Loss: 1.5047| Validation - Acc: 0.4525\n",
      "Step  2 | Training - Loss: 1.4611| Validation - Acc: 0.4621\n",
      "Step  3 | Training - Loss: 1.4239| Validation - Acc: 0.5298\n",
      "Step  4 | Training - Loss: 1.3977| Validation - Acc: 0.5298\n",
      "Step  5 | Training - Loss: 1.3824| Validation - Acc: 0.5298\n",
      "Step  6 | Training - Loss: 1.3744| Validation - Acc: 0.5298\n",
      "Step  7 | Training - Loss: 1.3702| Validation - Acc: 0.5298\n",
      "Step  8 | Training - Loss: 1.3678| Validation - Acc: 0.5298\n",
      "Step  9 | Training - Loss: 1.3663| Validation - Acc: 0.5298\n",
      "Test - Accuracy: 0.5342\n"
     ]
    }
   ],
   "source": [
    "# Scenario for classes = 2\n",
    "df_results_2label_dense = run_scenario_dense(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "df_results_2label_vae, labels2_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "\n",
    "# Scenario for classes = 5\n",
    "df_results_5label_dense = run_scenario_dense(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)\n",
    "df_results_5label_vae, labels5_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.990554</td>\n",
       "      <td>0.470635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.041496             0.990554   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.470635  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.690656</td>\n",
       "      <td>0.530859</td>\n",
       "      <td>0.534244</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>0.066375</td>\n",
       "      <td>1.254186e+16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.690656             0.530859   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534244                      0.065428                      0.066375   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                 1.254186e+16  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037086</td>\n",
       "      <td>0.989165</td>\n",
       "      <td>0.281804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.037086             0.989165   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.281804  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.366343</td>\n",
       "      <td>0.529827</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.066278</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    1.366343             0.529827   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534155                      0.066278                      0.069552   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                          NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12041     1]\n",
      " [10499     3]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAGdCAYAAABkXrYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzszMcs2BzszMcs2BzszMcu3/A4kFKjLb9e++AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b6c14b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = labels2_pred_vae_arr[:,0], y_true = labels2_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, output_columns_2labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12042     0     0     0     0]\n",
      " [ 8271     0     0     0     0]\n",
      " [ 2037     0     0     0     0]\n",
      " [  183     0     0     0     0]\n",
      " [   11     0     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAIkCAYAAADyCkRHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWP6LMjegeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b6c110e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_5labels = confusion_matrix(y_pred = labels5_pred_vae_arr[:,0], y_true = labels5_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[8,8])\n",
    "plot_confusion_matrix(cm_5labels, output_columns_5labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Implemented Grad clip, getting good accuracy!",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
