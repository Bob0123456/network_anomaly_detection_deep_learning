{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"x\"]\n",
    "\n",
    "kdd_train = pd.read_csv(\"dataset/KDDTrain+.txt\",names = col_names,)\n",
    "kdd_test = pd.read_csv(\"dataset/KDDTest+.txt\",names = col_names,)\n",
    "\n",
    "kdd_train = kdd_train.drop(\"x\", axis = 1)\n",
    "kdd_test = kdd_test.drop(\"x\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"protocol_type\",\"service\",\"flag\"]\n",
    "for cv in category_variables:\n",
    "    kdd_train[cv] = kdd_train[cv].astype(\"category\")\n",
    "kdd_train[\"label\"] = kdd_train[\"label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125966</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125967</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>udp</td>\n",
       "      <td>private</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>klogin</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0              0           tcp  ftp_data   SF        491          0     0   \n",
       "1              0           udp     other   SF        146          0     0   \n",
       "2              0           tcp   private   S0          0          0     0   \n",
       "3              0           tcp      http   SF        232       8153     0   \n",
       "4              0           tcp      http   SF        199        420     0   \n",
       "5              0           tcp   private  REJ          0          0     0   \n",
       "6              0           tcp   private   S0          0          0     0   \n",
       "...          ...           ...       ...  ...        ...        ...   ...   \n",
       "125966         0           tcp   private   S0          0          0     0   \n",
       "125967         0           tcp      http   SF        359        375     0   \n",
       "125968         0           tcp   private   S0          0          0     0   \n",
       "125969         8           udp   private   SF        105        145     0   \n",
       "125970         0           tcp      smtp   SF       2231        384     0   \n",
       "125971         0           tcp    klogin   S0          0          0     0   \n",
       "125972         0           tcp  ftp_data   SF        151          0     0   \n",
       "\n",
       "        wrong_fragment  urgent  hot   ...     dst_host_srv_count  \\\n",
       "0                    0       0    0   ...                     25   \n",
       "1                    0       0    0   ...                      1   \n",
       "2                    0       0    0   ...                     26   \n",
       "3                    0       0    0   ...                    255   \n",
       "4                    0       0    0   ...                    255   \n",
       "5                    0       0    0   ...                     19   \n",
       "6                    0       0    0   ...                      9   \n",
       "...                ...     ...  ...   ...                    ...   \n",
       "125966               0       0    0   ...                     13   \n",
       "125967               0       0    0   ...                    255   \n",
       "125968               0       0    0   ...                     25   \n",
       "125969               0       0    0   ...                    244   \n",
       "125970               0       0    0   ...                     30   \n",
       "125971               0       0    0   ...                      8   \n",
       "125972               0       0    0   ...                     77   \n",
       "\n",
       "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                         0.17                    0.03   \n",
       "1                         0.00                    0.60   \n",
       "2                         0.10                    0.05   \n",
       "3                         1.00                    0.00   \n",
       "4                         1.00                    0.00   \n",
       "5                         0.07                    0.07   \n",
       "6                         0.04                    0.05   \n",
       "...                        ...                     ...   \n",
       "125966                    0.05                    0.07   \n",
       "125967                    1.00                    0.00   \n",
       "125968                    0.10                    0.06   \n",
       "125969                    0.96                    0.01   \n",
       "125970                    0.12                    0.06   \n",
       "125971                    0.03                    0.05   \n",
       "125972                    0.30                    0.03   \n",
       "\n",
       "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                              0.17                         0.00   \n",
       "1                              0.88                         0.00   \n",
       "2                              0.00                         0.00   \n",
       "3                              0.03                         0.04   \n",
       "4                              0.00                         0.00   \n",
       "5                              0.00                         0.00   \n",
       "6                              0.00                         0.00   \n",
       "...                             ...                          ...   \n",
       "125966                         0.00                         0.00   \n",
       "125967                         0.33                         0.04   \n",
       "125968                         0.00                         0.00   \n",
       "125969                         0.01                         0.00   \n",
       "125970                         0.00                         0.00   \n",
       "125971                         0.00                         0.00   \n",
       "125972                         0.30                         0.00   \n",
       "\n",
       "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                       0.00                      0.00                  0.05   \n",
       "1                       0.00                      0.00                  0.00   \n",
       "2                       1.00                      1.00                  0.00   \n",
       "3                       0.03                      0.01                  0.00   \n",
       "4                       0.00                      0.00                  0.00   \n",
       "5                       0.00                      0.00                  1.00   \n",
       "6                       1.00                      1.00                  0.00   \n",
       "...                      ...                       ...                   ...   \n",
       "125966                  1.00                      1.00                  0.00   \n",
       "125967                  0.33                      0.00                  0.00   \n",
       "125968                  1.00                      1.00                  0.00   \n",
       "125969                  0.00                      0.00                  0.00   \n",
       "125970                  0.72                      0.00                  0.01   \n",
       "125971                  1.00                      1.00                  0.00   \n",
       "125972                  0.00                      0.00                  0.00   \n",
       "\n",
       "        dst_host_srv_rerror_rate    label  \n",
       "0                           0.00   normal  \n",
       "1                           0.00   normal  \n",
       "2                           0.00  neptune  \n",
       "3                           0.01   normal  \n",
       "4                           0.00   normal  \n",
       "5                           1.00  neptune  \n",
       "6                           0.00  neptune  \n",
       "...                          ...      ...  \n",
       "125966                      0.00  neptune  \n",
       "125967                      0.00   normal  \n",
       "125968                      0.00  neptune  \n",
       "125969                      0.00   normal  \n",
       "125970                      0.00   normal  \n",
       "125971                      0.00  neptune  \n",
       "125972                      0.00   normal  \n",
       "\n",
       "[125973 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125973.00000</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.14465</td>\n",
       "      <td>4.556674e+04</td>\n",
       "      <td>1.977911e+04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.279250</td>\n",
       "      <td>...</td>\n",
       "      <td>182.148945</td>\n",
       "      <td>115.653005</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.284452</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.120240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2604.51531</td>\n",
       "      <td>5.870331e+06</td>\n",
       "      <td>4.021269e+06</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>23.942042</td>\n",
       "      <td>...</td>\n",
       "      <td>99.206213</td>\n",
       "      <td>110.702741</td>\n",
       "      <td>0.448949</td>\n",
       "      <td>0.188922</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.306557</td>\n",
       "      <td>0.319459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42908.00000</td>\n",
       "      <td>1.379964e+09</td>\n",
       "      <td>1.309937e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7479.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes           land  \\\n",
       "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
       "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
       "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
       "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
       "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
       "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
       "mean         0.022687       0.000111       0.204409           0.001222   \n",
       "std          0.253530       0.014366       2.149968           0.045239   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      77.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  125973.000000    125973.000000            ...              \n",
       "mean        0.395736         0.279250            ...              \n",
       "std         0.489010        23.942042            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         1.000000         0.000000            ...              \n",
       "max         1.000000      7479.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   125973.000000       125973.000000           125973.000000   \n",
       "mean       182.148945          115.653005                0.521242   \n",
       "std         99.206213          110.702741                0.448949   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%         82.000000           10.000000                0.050000   \n",
       "50%        255.000000           63.000000                0.510000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           125973.000000                125973.000000   \n",
       "mean                 0.082951                     0.148379   \n",
       "std                  0.188922                     0.308997   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.020000                     0.000000   \n",
       "75%                  0.070000                     0.060000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                125973.000000         125973.000000   \n",
       "mean                      0.032542              0.284452   \n",
       "std                       0.112564              0.444784   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.020000              1.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             125973.000000         125973.000000   \n",
       "mean                   0.278485              0.118832   \n",
       "std                    0.445669              0.306557   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    1.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             125973.000000  \n",
       "mean                   0.120240  \n",
       "std                    0.319459  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22544.000000</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.859076</td>\n",
       "      <td>1.039545e+04</td>\n",
       "      <td>2.056019e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.105394</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>193.869411</td>\n",
       "      <td>140.750532</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.226683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1407.176612</td>\n",
       "      <td>4.727864e+05</td>\n",
       "      <td>2.121930e+04</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.928428</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>7.269597</td>\n",
       "      <td>...</td>\n",
       "      <td>94.035663</td>\n",
       "      <td>111.783972</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.273139</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.400875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870000e+02</td>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57715.000000</td>\n",
       "      <td>6.282565e+07</td>\n",
       "      <td>1.345927e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
       "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
       "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
       "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
       "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
       "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
       "mean       0.000710      0.105394           0.021647      0.442202   \n",
       "std        0.036473      0.928428           0.150328      0.496659   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      1.000000   \n",
       "max        3.000000    101.000000           4.000000      1.000000   \n",
       "\n",
       "       num_compromised            ...             dst_host_count  \\\n",
       "count     22544.000000            ...               22544.000000   \n",
       "mean          0.119899            ...                 193.869411   \n",
       "std           7.269597            ...                  94.035663   \n",
       "min           0.000000            ...                   0.000000   \n",
       "25%           0.000000            ...                 121.000000   \n",
       "50%           0.000000            ...                 255.000000   \n",
       "75%           0.000000            ...                 255.000000   \n",
       "max         796.000000            ...                 255.000000   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        22544.000000            22544.000000            22544.000000   \n",
       "mean           140.750532                0.608722                0.090540   \n",
       "std            111.783972                0.435688                0.220717   \n",
       "min              0.000000                0.000000                0.000000   \n",
       "25%             15.000000                0.070000                0.000000   \n",
       "50%            168.000000                0.920000                0.010000   \n",
       "75%            255.000000                1.000000                0.060000   \n",
       "max            255.000000                1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 22544.000000                 22544.000000   \n",
       "mean                      0.132261                     0.019638   \n",
       "std                       0.306268                     0.085394   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.030000                     0.010000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          22544.000000              22544.000000          22544.000000   \n",
       "mean               0.097814                  0.099426              0.233385   \n",
       "std                0.273139                  0.281866              0.387229   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.360000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              22544.000000  \n",
       "mean                   0.226683  \n",
       "std                    0.400875  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.170000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          normal\n",
       "1          normal\n",
       "2         neptune\n",
       "3          normal\n",
       "4          normal\n",
       "5         neptune\n",
       "6         neptune\n",
       "           ...   \n",
       "125966    neptune\n",
       "125967     normal\n",
       "125968    neptune\n",
       "125969     normal\n",
       "125970     normal\n",
       "125971    neptune\n",
       "125972     normal\n",
       "Name: label, dtype: category\n",
       "Categories (23, object): [back, buffer_overflow, ftp_write, guess_passwd, ..., spy, teardrop, warezclient, warezmaster]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "#scatter_matrix(kdd_train, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_types = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "is_sess = {\n",
    "    \"DoS\":\"Attack\",\n",
    "    \"R2L\":\"Attack\",\n",
    "    \"U2R\":\"Attack\",\n",
    "    \"Probe\":\"Attack\",\n",
    "    \"normal\":\"Normal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_train[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])\n",
    "kdd_test[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_test[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_sess_type_group = kdd_train.groupby(\"sess_type\")\n",
    "kdd_is_sess_group = kdd_train.groupby(\"is_sess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_type\n",
       "DoS       45927\n",
       "Probe     11656\n",
       "R2L         995\n",
       "U2R          52\n",
       "normal    67343\n",
       "Name: is_sess, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.is_sess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sess\n",
       "Attack    58630\n",
       "Normal    67343\n",
       "Name: sess_type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.sess_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sess_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">DoS</th>\n",
       "      <th>count</th>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.090034</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>1.692015e+02</td>\n",
       "      <td>244.600475</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.747922</td>\n",
       "      <td>26.524005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.748494</td>\n",
       "      <td>1.176321e+03</td>\n",
       "      <td>32.656346</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.445748</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>1.168004e+03</td>\n",
       "      <td>41.324475</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.188947</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>48.303117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299931</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>7.686120e+03</td>\n",
       "      <td>94.667526</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "sess_type                                                                    \n",
       "DoS       count  45927.000000   45927.000000  4.592700e+04    45927.000000   \n",
       "          mean     178.090034       0.065403  1.692015e+02      244.600475   \n",
       "          std      104.445748       0.064023  1.168004e+03       41.324475   \n",
       "          min        1.000000       0.000000  0.000000e+00        1.000000   \n",
       "          25%      109.000000       0.050000  0.000000e+00      255.000000   \n",
       "          50%      172.000000       0.060000  0.000000e+00      255.000000   \n",
       "          75%      249.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                       ...            ...           ...             ...   \n",
       "normal    mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "          std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "          min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "          25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "          50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "          75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "          max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "                 dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "sess_type                                                       \n",
       "DoS       count            45927.000000          45927.000000   \n",
       "          mean                 0.066333              0.157569   \n",
       "          std                  0.058079              0.358934   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.050000              0.000000   \n",
       "          50%                  0.070000              0.000000   \n",
       "          75%                  0.070000              0.000000   \n",
       "...                                 ...                   ...   \n",
       "normal    mean                 0.040134              0.046589   \n",
       "          std                  0.128529              0.195306   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.000000              0.000000   \n",
       "          50%                  0.000000              0.000000   \n",
       "          75%                  0.020000              0.000000   \n",
       "          max                  1.000000              1.000000   \n",
       "\n",
       "                 dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "sess_type                                                              \n",
       "DoS       count                 45927.000000            45927.000000   \n",
       "          mean                      0.049492                0.123423   \n",
       "          std                       0.188947                0.228287   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.020000   \n",
       "          50%                       0.000000                0.050000   \n",
       "          75%                       0.000000                0.080000   \n",
       "...                                      ...                     ...   \n",
       "normal    mean                      0.121726                0.811875   \n",
       "          std                       0.254382                0.324091   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.750000   \n",
       "          50%                       0.010000                1.000000   \n",
       "          75%                       0.080000                1.000000   \n",
       "          max                       1.000000                1.000000   \n",
       "\n",
       "                 dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "sess_type                                                       ...         \n",
       "DoS       count          45927.000000        45927.000000       ...         \n",
       "          mean               0.747922           26.524005       ...         \n",
       "          std                0.431707           48.303117       ...         \n",
       "          min                0.000000            1.000000       ...         \n",
       "          25%                0.180000            6.000000       ...         \n",
       "          50%                1.000000           13.000000       ...         \n",
       "          75%                1.000000           20.000000       ...         \n",
       "...                               ...                 ...       ...         \n",
       "normal    mean               0.013930          190.285761       ...         \n",
       "          std                0.092006           92.608377       ...         \n",
       "          min                0.000000            0.000000       ...         \n",
       "          25%                0.000000          121.000000       ...         \n",
       "          50%                0.000000          255.000000       ...         \n",
       "          75%                0.000000          255.000000       ...         \n",
       "          max                1.000000          255.000000       ...         \n",
       "\n",
       "                 same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "sess_type                                                                  \n",
       "DoS       count   45927.000000  45927.000000  4.592700e+04  45927.000000   \n",
       "          mean        0.191887      0.748494  1.176321e+03     32.656346   \n",
       "          std         0.299931      0.432559  7.686120e+03     94.667526   \n",
       "          min         0.000000      0.000000  0.000000e+00      1.000000   \n",
       "          25%         0.040000      0.290000  0.000000e+00      5.000000   \n",
       "          50%         0.070000      1.000000  0.000000e+00     11.000000   \n",
       "          75%         0.150000      1.000000  0.000000e+00     18.000000   \n",
       "...                        ...           ...           ...           ...   \n",
       "normal    mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "          std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "          min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "          25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "          50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "          75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "          max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "                 srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "sess_type                                                               \n",
       "DoS       count        45927.000000     45927.000000     45927.000000   \n",
       "          mean             0.005317         0.153000         0.746678   \n",
       "          std              0.056390         0.357561         0.434050   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         1.000000   \n",
       "          75%              0.000000         0.000000         1.000000   \n",
       "...                             ...              ...              ...   \n",
       "normal    mean             0.126263         0.044629         0.012083   \n",
       "          std              0.271621         0.202264         0.086426   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         0.000000   \n",
       "          75%              0.110000         0.000000         0.000000   \n",
       "          max              1.000000         1.000000         1.000000   \n",
       "\n",
       "                 su_attempted        urgent  wrong_fragment  \n",
       "sess_type                                                    \n",
       "DoS       count  45927.000000  45927.000000    45927.000000  \n",
       "          mean       0.000000      0.000000        0.062229  \n",
       "          std        0.000000      0.000000        0.416951  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "...                       ...           ...             ...  \n",
       "normal    mean       0.002049      0.000148        0.000000  \n",
       "          std        0.061622      0.017233        0.000000  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "          max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[40 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Attack</th>\n",
       "      <th>count</th>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.849991</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>3.752448e+04</td>\n",
       "      <td>222.025260</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.595177</td>\n",
       "      <td>29.929081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>8.282014e+04</td>\n",
       "      <td>27.797885</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0.209114</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.048746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.334483</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>5.893991e+06</td>\n",
       "      <td>79.196259</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>52.289254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>8.593025e+06</td>\n",
       "      <td>84.710761</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.369916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "is_sess                                                                    \n",
       "Attack  count  58630.000000   58630.000000  5.863000e+04    58630.000000   \n",
       "        mean     154.849991       0.102410  3.752448e+04      222.025260   \n",
       "        std      124.334483       0.206408  5.893991e+06       79.196259   \n",
       "        min        0.000000       0.000000  0.000000e+00        1.000000   \n",
       "        25%       40.000000       0.050000  0.000000e+00      255.000000   \n",
       "        50%      138.000000       0.060000  0.000000e+00      255.000000   \n",
       "        75%      241.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                     ...            ...           ...             ...   \n",
       "Normal  mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "        std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "        min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "        25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "        50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "        75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "        max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "               dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "is_sess                                                       \n",
       "Attack  count            58630.000000          58630.000000   \n",
       "        mean                 0.132131              0.201810   \n",
       "        std                  0.230626              0.381090   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.050000              0.000000   \n",
       "        50%                  0.070000              0.000000   \n",
       "        75%                  0.080000              0.020000   \n",
       "...                               ...                   ...   \n",
       "Normal  mean                 0.040134              0.046589   \n",
       "        std                  0.128529              0.195306   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.000000              0.000000   \n",
       "        50%                  0.000000              0.000000   \n",
       "        75%                  0.020000              0.000000   \n",
       "        max                  1.000000              1.000000   \n",
       "\n",
       "               dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "is_sess                                                              \n",
       "Attack  count                 58630.000000            58630.000000   \n",
       "        mean                      0.178993                0.187417   \n",
       "        std                       0.359262                0.322430   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.020000   \n",
       "        50%                       0.000000                0.050000   \n",
       "        75%                       0.020000                0.090000   \n",
       "...                                    ...                     ...   \n",
       "Normal  mean                      0.121726                0.811875   \n",
       "        std                       0.254382                0.324091   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.750000   \n",
       "        50%                       0.010000                1.000000   \n",
       "        75%                       0.080000                1.000000   \n",
       "        max                       1.000000                1.000000   \n",
       "\n",
       "               dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "is_sess                                                       ...         \n",
       "Attack  count          58630.000000        58630.000000       ...         \n",
       "        mean               0.595177           29.929081       ...         \n",
       "        std                0.484495           52.289254       ...         \n",
       "        min                0.000000            1.000000       ...         \n",
       "        25%                0.000000            4.000000       ...         \n",
       "        50%                1.000000           12.000000       ...         \n",
       "        75%                1.000000           21.000000       ...         \n",
       "...                             ...                 ...       ...         \n",
       "Normal  mean               0.013930          190.285761       ...         \n",
       "        std                0.092006           92.608377       ...         \n",
       "        min                0.000000            0.000000       ...         \n",
       "        25%                0.000000          121.000000       ...         \n",
       "        50%                0.000000          255.000000       ...         \n",
       "        75%                0.000000          255.000000       ...         \n",
       "        max                1.000000          255.000000       ...         \n",
       "\n",
       "               same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "is_sess                                                                  \n",
       "Attack  count   58630.000000  58630.000000  5.863000e+04  58630.000000   \n",
       "        mean        0.306659      0.595808  8.282014e+04     27.797885   \n",
       "        std         0.395655      0.486588  8.593025e+06     84.710761   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         0.040000      0.000000  0.000000e+00      3.000000   \n",
       "        50%         0.080000      1.000000  0.000000e+00     10.000000   \n",
       "        75%         0.500000      1.000000  0.000000e+00     18.000000   \n",
       "...                      ...           ...           ...           ...   \n",
       "Normal  mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "        std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "        50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "        75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "        max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "               srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "is_sess                                                               \n",
       "Attack  count        58630.000000     58630.000000     58630.000000   \n",
       "        mean             0.064079         0.209114         0.593072   \n",
       "        std              0.241348         0.404487         0.490234   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         1.000000   \n",
       "        75%              0.000000         0.000000         1.000000   \n",
       "...                           ...              ...              ...   \n",
       "Normal  mean             0.126263         0.044629         0.012083   \n",
       "        std              0.271621         0.202264         0.086426   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         0.000000   \n",
       "        75%              0.110000         0.000000         0.000000   \n",
       "        max              1.000000         1.000000         1.000000   \n",
       "\n",
       "               su_attempted        urgent  wrong_fragment  \n",
       "is_sess                                                    \n",
       "Attack  count  58630.000000  58630.000000    58630.000000  \n",
       "        mean       0.000017      0.000068        0.048746  \n",
       "        std        0.004130      0.010116        0.369916  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "...                     ...           ...             ...  \n",
       "Normal  mean       0.002049      0.000148        0.000000  \n",
       "        std        0.061622      0.017233        0.000000  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "        max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[16 rows x 38 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kdd_is_sess_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kdd_sess_type_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_dummy(train, test):\n",
    "    dummy_variables_2labels = [*category_variables, \"is_sess\"]\n",
    "    dummy_variables_5labels = [*category_variables, \"sess_type\"]\n",
    "\n",
    "    drop_variables = [*category_variables, \"is_sess\", \"sess_type\", \"duration\", \"label\"]\n",
    "    \n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    def dummy(kdd):\n",
    "        kdd_one_hot_2labels = pd.get_dummies(kdd[dummy_variables_2labels], prefix=dummy_variables_2labels, drop_first=False)\n",
    "        kdd_one_hot_5labels = pd.get_dummies(kdd[dummy_variables_5labels], prefix=dummy_variables_5labels, drop_first=False)\n",
    "\n",
    "        kdd_2labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                       , kdd_one_hot_2labels]\n",
    "                                      , axis = 1)\n",
    "        kdd_5labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                      , kdd_one_hot_5labels]\n",
    "                                      , axis = 1)\n",
    "\n",
    "        return kdd_2labels, kdd_5labels\n",
    "    \n",
    "    kdd_2labels, kdd_5labels = dummy(pd.concat([train, test], axis = 0))\n",
    "    \n",
    "    kdd_2labels_train, kdd_2labels_test = kdd_2labels.iloc[:train_size,:], kdd_2labels.iloc[train_size:,:]\n",
    "    kdd_5labels_train, kdd_5labels_test = kdd_5labels.iloc[:train_size,:], kdd_5labels.iloc[train_size:,:]\n",
    "    \n",
    "    return kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_columns_2labels = ['is_sess_Normal', 'is_sess_Attack']\n",
    "output_columns_5labels = ['sess_type_normal', 'sess_type_DoS', 'sess_type_Probe', 'sess_type_R2L', 'sess_type_U2R']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test = encode_dummy(kdd_train, kdd_test)\n",
    "\n",
    "x_kdd_train = kdd_2labels_train.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_train = kdd_2labels_train.loc[:,output_columns_2labels].values \n",
    "y_5labels_train = kdd_5labels_train.loc[:,output_columns_5labels].values\n",
    "\n",
    "x_kdd_test = kdd_2labels_test.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_test = kdd_2labels_test.loc[:,output_columns_2labels].values \n",
    "y_5labels_test = kdd_5labels_test.loc[:,output_columns_5labels].values\n",
    "\n",
    "        \n",
    "ss = pp.StandardScaler()\n",
    "x_kdd_train = ss.fit_transform(x_kdd_train)\n",
    "x_test = ss.transform(x_kdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sanity before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "                      input_dim = 121, classes = 2, hidden_layers = 8, epochs = 10, hidden_units = 5):\n",
    "   \n",
    "    model_dense = Sequential()\n",
    "    model_dense.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        model_dense.add(Dense(hidden_units, kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    model_dense.add(Dense(classes, kernel_initializer='uniform' ,activation = 'softmax'))\n",
    "\n",
    "    model_dense.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'],\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                  activity_regularizer=keras.regularizers.l1(0.01)) \n",
    "\n",
    "    model_dense.fit(x_train, y_train, \n",
    "                  epochs = epochs, batch_size=500, \n",
    "                  validation_data = (x_valid, y_valid),\n",
    "                  verbose=0)\n",
    "    \n",
    "    scores_train = model_dense.evaluate(x_train, y_train, verbose=0)\n",
    "    scores_valid = model_dense.evaluate(x_valid, y_valid, verbose=0)\n",
    "    scores_test = model_dense.evaluate(x_test, y_test, verbose=0)\n",
    "    train_loss = scores_train[0]\n",
    "    valid_accuracy = scores_valid[1] \n",
    "    test_accuracy = scores_test[1]\n",
    "    \n",
    "    print(\"Train loss: {:.4f}, accuracy: {:.4f}\".format(scores_train[0], scores_train[1]))\n",
    "    print(\"Valid loss: {:.4f}, accuracy: {:.4f}\".format(scores_valid[0], scores_valid[1]))\n",
    "    print(\"Test loss: {:.4f}, accuracy: {:.4f}\".format(scores_test[0], scores_test[1]))\n",
    "    return train_loss, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: mean:-0.0000, std:0.9959, shape:(125973, 121)\n",
      "Testing  data: mean:0.0170, std:1.4175, shape:(22544, 121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_kdd_train.mean(), x_kdd_train.std(), x_kdd_train.shape))\n",
    "print(\"Testing  data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_test.mean(), x_test.std(), x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_vae(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5):\n",
    "    \n",
    "    graph_vae = tf.Graph()\n",
    "    with graph_vae.as_default():\n",
    "        latent_dim = hidden_units\n",
    "        dense_hidden_units = hidden_units\n",
    "        hidden_encoder_dim = 60\n",
    "        hidden_decoder_dim = 60\n",
    "\n",
    "        lam = 0.01\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            initial = tf.constant(0.01, shape=shape)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        l2_loss = tf.constant(0.001)\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "                b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "\n",
    "                # Hidden layer encoder\n",
    "                hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "                tf.summary.histogram(\"Weights_Encoder\", W_encoder_input_hidden)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "\n",
    "            # Mu encoder\n",
    "            mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "            tf.summary.histogram(\"Weights_Mean\", W_encoder_hidden_mu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "\n",
    "            # Sigma encoder\n",
    "            logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "            tf.summary.histogram(\"Weights_Variance\", W_encoder_hidden_logvar)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "                b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "\n",
    "                # Hidden layer decoder\n",
    "                hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "                tf.summary.histogram(\"Weights_Decoder\", W_decoder_z_hidden)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "            b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "            l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "\n",
    "            x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "            tf.summary.histogram(\"Weights_Reconstruction\", W_decoder_hidden_reconstruction)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), \n",
    "                                 reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), \n",
    "                                        reduction_indices=1)\n",
    "            \n",
    "            reconst_loss = tf.abs(tf.reduce_mean(BCE + KLD))\n",
    "            regularized_loss = reconst_loss + lam * l2_loss\n",
    "\n",
    "            tf.summary.scalar(\"BCE\", tf.reduce_mean(BCE))\n",
    "            tf.summary.scalar(\"KLD\", tf.reduce_mean(KLD))\n",
    "\n",
    "            tf.summary.scalar(\"Total_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph_vae) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                sess.run(optimizer, feed_dict={x: x_train[i,:], keep_prob:0.6})\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train, train_reduction_loss, train_z = sess.run([regularized_loss, summary_op, reconst_loss, z], feed_dict={x: x_train, keep_prob:1})\n",
    "            summary_str_valid, valid_reduction_loss, valid_z = sess.run([summary_op, reconst_loss, z], feed_dict={x: x_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}, Reduction Loss: {:.4f} | \"\n",
    "                      \"Validation - Reduction Loss: {:.4f}\"\n",
    "                      .format(epoch, train_loss, train_reduction_loss, valid_reduction_loss))\n",
    "\n",
    "        test_reduction_loss, test_z = sess.run([reconst_loss, z], feed_dict={x: x_test, keep_prob:1})\n",
    "        print(\"Test - Feature reduction loss:{:.4f}\".format(test_reduction_loss))\n",
    "    return train_reduction_loss, valid_reduction_loss, test_reduction_loss, train_z, valid_z, test_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_softmax(z_train, z_valid, y_train, y_valid, z_test, y_test, \n",
    "              input_dim = 121, classes = 2, \n",
    "              hidden_units = 8, hidden_layers = 4, epochs = 5,\n",
    "              only_vae = False):\n",
    "\n",
    "    latent_dim = hidden_units\n",
    "    dense_hidden_units = hidden_units\n",
    "    \n",
    "    graph_softmax = tf.Graph()\n",
    "    with graph_softmax.as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"Sampled_Distribution\"):\n",
    "            z = tf.placeholder(\"float32\", shape=[None, latent_dim], name=\"Z\")\n",
    "            y_ = tf.placeholder(\"float\", shape=[None, classes], name = \"y_\")\n",
    "            keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "\n",
    "        z_h = z\n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):    \n",
    "            z_h = tf.layers.dense(z_h,dense_hidden_units, activation=tf.nn.relu)\n",
    "            z_h = tf.nn.dropout(z_h, keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z_h, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            regularized_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "            pred = tf.argmax(y, 1)\n",
    "            actual = tf.argmax(y_, 1)\n",
    "            \n",
    "            correct_prediction = tf.equal(actual, pred)\n",
    "            tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "            tf.summary.scalar(\"Softmax_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "\n",
    "    batch_iterations = 10\n",
    "    batch_indices = np.array_split(np.arange(z_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    with tf.Session(graph=graph_softmax) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/softmax/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/softmax/validation')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                feed_dict={z: z_train[i,:], y_: y_train[i,:], keep_prob:0.6}\n",
    "                sess.run(optimizer, feed_dict = feed_dict)\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train = sess.run([regularized_loss, summary_op], feed_dict={z: z_train, y_: y_train, keep_prob:1})\n",
    "            valid_accuracy, summary_str_valid = sess.run([tf_accuracy, summary_op], feed_dict={z: z_valid, y_:y_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}| \"\n",
    "                      \"Validation - Acc: {:.4f}\"\n",
    "                      .format(epoch, train_loss, valid_accuracy))\n",
    "\n",
    "        test_accuracy, y_pred, y_actual = sess.run([tf_accuracy, pred, actual], feed_dict={z: z_test, y_:y_test, keep_prob:1})\n",
    "        print(\"Test - Accuracy: {:.4f}\".format(test_accuracy))\n",
    "\n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "\n",
    "        outputs = np.hstack((y_pred, y_actual))\n",
    "        \n",
    "    return train_loss, valid_accuracy, test_accuracy, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"reduced_features\", \"hidden_layers\"])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 1024, 4)\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    Hyper_parameters(10, 8, 4),\n",
    "    Hyper_parameters(10, 16, 8),\n",
    "    Hyper_parameters(10, 128, 8),\n",
    "    Hyper_parameters(10, 256, 4),\n",
    "    Hyper_parameters(10, 1024, 4),\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_scenario_vae(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"])\n",
    "     \n",
    "    print(\"Training for {} labels on VAE Softmax Network:\".format(classes))\n",
    "    \n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"*********** Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{} **************\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        print(\"VAE for Feature reduction:\")\n",
    "        t_r_l, v_r_l, te_r_l, t_z, v_z, te_z = train_vae(x_train, x_valid, \n",
    "                                           y_train, y_valid, \n",
    "                                           x_test, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        print(\"Softmax for Prediction:\")\n",
    "        t_l, v_a, te_a, op = train_softmax(t_z, v_z, \n",
    "                                           y_train, y_valid, \n",
    "                                           te_z, y_test_labels, \n",
    "                                           classes = classes, \n",
    "                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                           hidden_units = Hyper_parameters.reduced_features,\n",
    "                                           epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a, t_r_l, v_r_l, te_r_l]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    \n",
    "    return df_results, op\n",
    "        \n",
    "def run_scenario_dense(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"])\n",
    "    \n",
    "    print(\"Training for {} labels on Dense Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.reduced_features, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a = train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test_labels, \n",
    "                 classes = classes, \n",
    "                 hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                 hidden_units = Hyper_parameters.reduced_features,\n",
    "                 epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results = df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"]), ignore_index=True)\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results], axis = 1)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2094: UserWarning: Expected no kwargs, you passed 2\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0163, accuracy: 0.9947\n",
      "Valid loss: 0.0179, accuracy: 0.9946\n",
      "Test loss: 4.8423, accuracy: 0.5034\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train loss: 0.6909, accuracy: 0.5336\n",
      "Valid loss: 0.6903, accuracy: 0.5386\n",
      "Test loss: 0.6908, accuracy: 0.5342\n",
      "Current parameters: epochs:10, hidden_units:8, hidden_layers:4\n",
      "Train loss: 0.0861, accuracy: 0.9798\n",
      "Valid loss: 0.0860, accuracy: 0.9798\n",
      "Test loss: 3.5819, accuracy: 0.5036\n",
      "Current parameters: epochs:10, hidden_units:16, hidden_layers:8\n",
      "Train loss: 0.0155, accuracy: 0.9952\n",
      "Valid loss: 0.0179, accuracy: 0.9946\n",
      "Test loss: 4.8254, accuracy: 0.5040\n",
      "Current parameters: epochs:10, hidden_units:128, hidden_layers:8\n",
      "Train loss: 0.0130, accuracy: 0.9960\n",
      "Valid loss: 0.0157, accuracy: 0.9955\n",
      "Test loss: 6.1472, accuracy: 0.5024\n",
      "Current parameters: epochs:10, hidden_units:256, hidden_layers:4\n",
      "Train loss: 0.0106, accuracy: 0.9962\n",
      "Valid loss: 0.0142, accuracy: 0.9958\n",
      "Test loss: 6.2712, accuracy: 0.5031\n",
      "Current parameters: epochs:10, hidden_units:1024, hidden_layers:4\n",
      "Train loss: 0.0102, accuracy: 0.9958\n",
      "Valid loss: 0.0123, accuracy: 0.9960\n",
      "Test loss: 6.1307, accuracy: 0.5022\n",
      "Training for 2 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5836, Reduction Loss: 0.5049 | Validation - Reduction Loss: 0.5044\n",
      "Step  1 | Training - Loss: 0.4467, Reduction Loss: 0.2916 | Validation - Reduction Loss: 0.2915\n",
      "Step  2 | Training - Loss: 0.3824, Reduction Loss: 0.2017 | Validation - Reduction Loss: 0.2003\n",
      "Step  3 | Training - Loss: 0.3276, Reduction Loss: 0.1685 | Validation - Reduction Loss: 0.1669\n",
      "Step  4 | Training - Loss: 0.2665, Reduction Loss: 0.1442 | Validation - Reduction Loss: 0.1426\n",
      "Step  5 | Training - Loss: 0.2114, Reduction Loss: 0.1174 | Validation - Reduction Loss: 0.1157\n",
      "Step  6 | Training - Loss: 0.1808, Reduction Loss: 0.0981 | Validation - Reduction Loss: 0.0963\n",
      "Step  7 | Training - Loss: 0.1622, Reduction Loss: 0.0848 | Validation - Reduction Loss: 0.0830\n",
      "Step  8 | Training - Loss: 0.1472, Reduction Loss: 0.0754 | Validation - Reduction Loss: 0.0735\n",
      "Step  9 | Training - Loss: 0.1355, Reduction Loss: 0.0679 | Validation - Reduction Loss: 0.0659\n",
      "Test - Feature reduction loss:0.1128\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.6968| Validation - Acc: 0.5164\n",
      "Step  1 | Training - Loss: 0.6920| Validation - Acc: 0.5312\n",
      "Step  2 | Training - Loss: 0.6912| Validation - Acc: 0.5363\n",
      "Step  3 | Training - Loss: 0.6910| Validation - Acc: 0.5365\n",
      "Step  4 | Training - Loss: 0.6909| Validation - Acc: 0.5365\n",
      "Step  5 | Training - Loss: 0.6909| Validation - Acc: 0.5362\n",
      "Step  6 | Training - Loss: 0.6909| Validation - Acc: 0.5364\n",
      "Step  7 | Training - Loss: 0.6909| Validation - Acc: 0.5365\n",
      "Step  8 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  9 | Training - Loss: 0.6909| Validation - Acc: 0.5366\n",
      "Test - Accuracy: 0.5342\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5838, Reduction Loss: 0.5041 | Validation - Reduction Loss: 0.5037\n",
      "Step  1 | Training - Loss: 0.4467, Reduction Loss: 0.2919 | Validation - Reduction Loss: 0.2914\n",
      "Step  2 | Training - Loss: 0.3827, Reduction Loss: 0.2030 | Validation - Reduction Loss: 0.2017\n",
      "Step  3 | Training - Loss: 0.3282, Reduction Loss: 0.1695 | Validation - Reduction Loss: 0.1674\n",
      "Step  4 | Training - Loss: 0.2668, Reduction Loss: 0.1447 | Validation - Reduction Loss: 0.1432\n",
      "Step  5 | Training - Loss: 0.2116, Reduction Loss: 0.1175 | Validation - Reduction Loss: 0.1158\n",
      "Step  6 | Training - Loss: 0.1808, Reduction Loss: 0.0980 | Validation - Reduction Loss: 0.0963\n",
      "Step  7 | Training - Loss: 0.1621, Reduction Loss: 0.0849 | Validation - Reduction Loss: 0.0830\n",
      "Step  8 | Training - Loss: 0.1471, Reduction Loss: 0.0754 | Validation - Reduction Loss: 0.0735\n",
      "Step  9 | Training - Loss: 0.1354, Reduction Loss: 0.0681 | Validation - Reduction Loss: 0.0660\n",
      "Test - Feature reduction loss:0.1123\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.6967| Validation - Acc: 0.5355\n",
      "Step  1 | Training - Loss: 0.6931| Validation - Acc: 0.5363\n",
      "Step  2 | Training - Loss: 0.6917| Validation - Acc: 0.5366\n",
      "Step  3 | Training - Loss: 0.6911| Validation - Acc: 0.5366\n",
      "Step  4 | Training - Loss: 0.6909| Validation - Acc: 0.5366\n",
      "Step  5 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  6 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  7 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  8 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Step  9 | Training - Loss: 0.6908| Validation - Acc: 0.5366\n",
      "Test - Accuracy: 0.5342\n",
      "*********** Current parameters: epochs:10, hidden_units:8, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5640, Reduction Loss: 0.4529 | Validation - Reduction Loss: 0.4527\n",
      "Step  1 | Training - Loss: 0.4399, Reduction Loss: 0.2443 | Validation - Reduction Loss: 0.2438\n",
      "Step  2 | Training - Loss: 0.3808, Reduction Loss: 0.1755 | Validation - Reduction Loss: 0.1736\n",
      "Step  3 | Training - Loss: 0.3060, Reduction Loss: 0.1100 | Validation - Reduction Loss: 0.1074\n",
      "Step  4 | Training - Loss: 0.2237, Reduction Loss: 0.0108 | Validation - Reduction Loss: 0.0146\n",
      "Step  5 | Training - Loss: 0.2054, Reduction Loss: 0.0164 | Validation - Reduction Loss: 0.0082\n",
      "Step  6 | Training - Loss: 0.3712, Reduction Loss: 0.1721 | Validation - Reduction Loss: 0.0121\n",
      "Step  7 | Training - Loss: 0.1982, Reduction Loss: 0.0288 | Validation - Reduction Loss: 0.0256\n",
      "Step  8 | Training - Loss: 0.1805, Reduction Loss: 0.0042 | Validation - Reduction Loss: 0.0014\n",
      "Step  9 | Training - Loss: 0.1753, Reduction Loss: 0.0015 | Validation - Reduction Loss: 0.0034\n",
      "Test - Feature reduction loss:0.0158\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.6807| Validation - Acc: 0.5894\n",
      "Step  1 | Training - Loss: 0.5540| Validation - Acc: 0.7571\n",
      "Step  2 | Training - Loss: 0.4941| Validation - Acc: 0.8242\n",
      "Step  3 | Training - Loss: 0.4753| Validation - Acc: 0.8385\n",
      "Step  4 | Training - Loss: 0.4676| Validation - Acc: 0.8452\n",
      "Step  5 | Training - Loss: 0.4635| Validation - Acc: 0.8488\n",
      "Step  6 | Training - Loss: 0.4611| Validation - Acc: 0.8506\n",
      "Step  7 | Training - Loss: 0.4597| Validation - Acc: 0.8515\n",
      "Step  8 | Training - Loss: 0.4589| Validation - Acc: 0.8525\n",
      "Step  9 | Training - Loss: 0.4583| Validation - Acc: 0.8527\n",
      "Test - Accuracy: 0.5108\n",
      "*********** Current parameters: epochs:10, hidden_units:16, hidden_layers:8 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5374, Reduction Loss: 0.3739 | Validation - Reduction Loss: 0.3732\n",
      "Step  1 | Training - Loss: 0.3783, Reduction Loss: 0.1437 | Validation - Reduction Loss: 0.1405\n",
      "Step  2 | Training - Loss: 0.2329, Reduction Loss: 0.0141 | Validation - Reduction Loss: 0.0077\n",
      "Step  3 | Training - Loss: 0.2358, Reduction Loss: 0.0554 | Validation - Reduction Loss: 0.0488\n",
      "Step  4 | Training - Loss: 0.2137, Reduction Loss: 0.0299 | Validation - Reduction Loss: 0.0344\n",
      "Step  5 | Training - Loss: 0.1752, Reduction Loss: 0.0090 | Validation - Reduction Loss: 0.0043\n",
      "Step  6 | Training - Loss: 0.1829, Reduction Loss: 0.0203 | Validation - Reduction Loss: 0.0254\n",
      "Step  7 | Training - Loss: 0.2233, Reduction Loss: 0.0580 | Validation - Reduction Loss: 0.0478\n",
      "Step  8 | Training - Loss: 0.1727, Reduction Loss: 0.0126 | Validation - Reduction Loss: 0.0182\n",
      "Step  9 | Training - Loss: 0.2101, Reduction Loss: 0.0646 | Validation - Reduction Loss: 0.0411\n",
      "Test - Feature reduction loss:0.0319\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.5882| Validation - Acc: 0.7106\n",
      "Step  1 | Training - Loss: 0.5337| Validation - Acc: 0.7846\n",
      "Step  2 | Training - Loss: 0.5151| Validation - Acc: 0.7977\n",
      "Step  3 | Training - Loss: 0.5080| Validation - Acc: 0.8013\n",
      "Step  4 | Training - Loss: 0.5047| Validation - Acc: 0.8027\n",
      "Step  5 | Training - Loss: 0.5027| Validation - Acc: 0.8048\n",
      "Step  6 | Training - Loss: 0.5013| Validation - Acc: 0.8066\n",
      "Step  7 | Training - Loss: 0.5003| Validation - Acc: 0.8073\n",
      "Step  8 | Training - Loss: 0.4994| Validation - Acc: 0.8081\n",
      "Step  9 | Training - Loss: 0.4986| Validation - Acc: 0.8087\n",
      "Test - Accuracy: 0.5023\n",
      "*********** Current parameters: epochs:10, hidden_units:128, hidden_layers:8 **************\n",
      "VAE for Feature reduction:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 | Training - Loss: 0.4277, Reduction Loss: 0.0849 | Validation - Reduction Loss: 0.0893\n",
      "Step  1 | Training - Loss: 0.2146, Reduction Loss: 0.0583 | Validation - Reduction Loss: 0.0613\n",
      "Step  2 | Training - Loss: 0.1917, Reduction Loss: 0.0406 | Validation - Reduction Loss: 0.0433\n",
      "Step  3 | Training - Loss: 0.1542, Reduction Loss: 0.0257 | Validation - Reduction Loss: 0.0346\n",
      "Step  4 | Training - Loss: 0.1262, Reduction Loss: 0.0316 | Validation - Reduction Loss: 0.0265\n",
      "Step  5 | Training - Loss: 0.1624, Reduction Loss: 0.0837 | Validation - Reduction Loss: 0.0768\n",
      "Step  6 | Training - Loss: 0.1065, Reduction Loss: 0.0074 | Validation - Reduction Loss: 0.0010\n",
      "Step  7 | Training - Loss: 0.1029, Reduction Loss: 0.0062 | Validation - Reduction Loss: 0.0120\n",
      "Step  8 | Training - Loss: 0.0968, Reduction Loss: 0.0092 | Validation - Reduction Loss: 0.0009\n",
      "Step  9 | Training - Loss: 0.1239, Reduction Loss: 0.0295 | Validation - Reduction Loss: 0.0368\n",
      "Test - Feature reduction loss:0.0027\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.4741| Validation - Acc: 0.8409\n",
      "Step  1 | Training - Loss: 0.4489| Validation - Acc: 0.8635\n",
      "Step  2 | Training - Loss: 0.4428| Validation - Acc: 0.8678\n",
      "Step  3 | Training - Loss: 0.4400| Validation - Acc: 0.8692\n",
      "Step  4 | Training - Loss: 0.4387| Validation - Acc: 0.8692\n",
      "Step  5 | Training - Loss: 0.4367| Validation - Acc: 0.8699\n",
      "Step  6 | Training - Loss: 0.4351| Validation - Acc: 0.8706\n",
      "Step  7 | Training - Loss: 0.4337| Validation - Acc: 0.8703\n",
      "Step  8 | Training - Loss: 0.4326| Validation - Acc: 0.8704\n",
      "Step  9 | Training - Loss: 0.4315| Validation - Acc: 0.8706\n",
      "Test - Accuracy: 0.4963\n",
      "*********** Current parameters: epochs:10, hidden_units:256, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.2913, Reduction Loss: 0.0542 | Validation - Reduction Loss: 0.0513\n",
      "Step  1 | Training - Loss: 0.1875, Reduction Loss: 0.0772 | Validation - Reduction Loss: 0.0741\n",
      "Step  2 | Training - Loss: 0.1548, Reduction Loss: 0.0554 | Validation - Reduction Loss: 0.0493\n",
      "Step  3 | Training - Loss: 0.1238, Reduction Loss: 0.0242 | Validation - Reduction Loss: 0.0314\n",
      "Step  4 | Training - Loss: 0.1200, Reduction Loss: 0.0273 | Validation - Reduction Loss: 0.0335\n",
      "Step  5 | Training - Loss: 0.0925, Reduction Loss: 0.0082 | Validation - Reduction Loss: 0.0153\n",
      "Step  6 | Training - Loss: 0.1186, Reduction Loss: 0.0539 | Validation - Reduction Loss: 0.0488\n",
      "Step  7 | Training - Loss: 0.0862, Reduction Loss: 0.0094 | Validation - Reduction Loss: 0.0035\n",
      "Step  8 | Training - Loss: 0.1258, Reduction Loss: 0.0346 | Validation - Reduction Loss: 0.0402\n",
      "Step  9 | Training - Loss: 0.0895, Reduction Loss: 0.0212 | Validation - Reduction Loss: 0.0156\n",
      "Test - Feature reduction loss:0.0322\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.7776| Validation - Acc: 0.5377\n",
      "Step  1 | Training - Loss: 0.5901| Validation - Acc: 0.7130\n",
      "Step  2 | Training - Loss: 0.5138| Validation - Acc: 0.7936\n",
      "Step  3 | Training - Loss: 0.4561| Validation - Acc: 0.8499\n",
      "Step  4 | Training - Loss: 0.4414| Validation - Acc: 0.8634\n",
      "Step  5 | Training - Loss: 0.4370| Validation - Acc: 0.8647\n",
      "Step  6 | Training - Loss: 0.4323| Validation - Acc: 0.8654\n",
      "Step  7 | Training - Loss: 0.4278| Validation - Acc: 0.8657\n",
      "Step  8 | Training - Loss: 0.4243| Validation - Acc: 0.8678\n",
      "Step  9 | Training - Loss: 0.4210| Validation - Acc: 0.8681\n",
      "Test - Accuracy: 0.4952\n",
      "*********** Current parameters: epochs:10, hidden_units:1024, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.3033, Reduction Loss: 0.1218 | Validation - Reduction Loss: 0.1198\n",
      "Step  1 | Training - Loss: 0.2028, Reduction Loss: 0.0902 | Validation - Reduction Loss: 0.0921\n",
      "Step  2 | Training - Loss: 0.1720, Reduction Loss: 0.0350 | Validation - Reduction Loss: 0.0303\n",
      "Step  3 | Training - Loss: 0.1102, Reduction Loss: 0.0048 | Validation - Reduction Loss: 0.0033\n",
      "Step  4 | Training - Loss: 0.1201, Reduction Loss: 0.0304 | Validation - Reduction Loss: 0.0346\n",
      "Step  5 | Training - Loss: 0.1298, Reduction Loss: 0.0530 | Validation - Reduction Loss: 0.0471\n",
      "Step  6 | Training - Loss: 0.0970, Reduction Loss: 0.0078 | Validation - Reduction Loss: 0.0062\n",
      "Step  7 | Training - Loss: 0.0883, Reduction Loss: 0.0250 | Validation - Reduction Loss: 0.0219\n",
      "Step  8 | Training - Loss: 0.0880, Reduction Loss: 0.0197 | Validation - Reduction Loss: 0.0239\n",
      "Step  9 | Training - Loss: 0.0945, Reduction Loss: 0.0147 | Validation - Reduction Loss: 0.0201\n",
      "Test - Feature reduction loss:0.0563\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  1 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  2 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  3 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  4 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  5 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  6 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  7 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  8 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Step  9 | Training - Loss: 0.7792| Validation - Acc: 0.5366\n",
      "Test - Accuracy: 0.5342\n",
      "Training for 5 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Train loss: 0.0723, accuracy: 0.9880\n",
      "Valid loss: 0.0749, accuracy: 0.9867\n",
      "Test loss: 5.0786, accuracy: 0.4395\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train loss: 0.1779, accuracy: 0.9791\n",
      "Valid loss: 0.1881, accuracy: 0.9779\n",
      "Test loss: 3.0446, accuracy: 0.4416\n",
      "Current parameters: epochs:10, hidden_units:8, hidden_layers:4\n",
      "Train loss: 0.0273, accuracy: 0.9935\n",
      "Valid loss: 0.0333, accuracy: 0.9927\n",
      "Test loss: 6.6737, accuracy: 0.4410\n",
      "Current parameters: epochs:10, hidden_units:16, hidden_layers:8\n",
      "Train loss: 0.0558, accuracy: 0.9902\n",
      "Valid loss: 0.0567, accuracy: 0.9896\n",
      "Test loss: 6.6186, accuracy: 0.4438\n",
      "Current parameters: epochs:10, hidden_units:128, hidden_layers:8\n",
      "Train loss: 0.0276, accuracy: 0.9926\n",
      "Valid loss: 0.0312, accuracy: 0.9913\n",
      "Test loss: 6.5160, accuracy: 0.4422\n",
      "Current parameters: epochs:10, hidden_units:256, hidden_layers:4\n",
      "Train loss: 0.0150, accuracy: 0.9952\n",
      "Valid loss: 0.0187, accuracy: 0.9942\n",
      "Test loss: 6.5806, accuracy: 0.4351\n",
      "Current parameters: epochs:10, hidden_units:1024, hidden_layers:4\n",
      "Train loss: 0.0138, accuracy: 0.9954\n",
      "Valid loss: 0.0162, accuracy: 0.9948\n",
      "Test loss: 7.1311, accuracy: 0.4371\n",
      "Training for 5 labels on VAE Softmax Network:\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:2 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5839, Reduction Loss: 0.5055 | Validation - Reduction Loss: 0.5058\n",
      "Step  1 | Training - Loss: 0.4466, Reduction Loss: 0.2912 | Validation - Reduction Loss: 0.2923\n",
      "Step  2 | Training - Loss: 0.3818, Reduction Loss: 0.2005 | Validation - Reduction Loss: 0.2020\n",
      "Step  3 | Training - Loss: 0.3278, Reduction Loss: 0.1674 | Validation - Reduction Loss: 0.1688\n",
      "Step  4 | Training - Loss: 0.2664, Reduction Loss: 0.1427 | Validation - Reduction Loss: 0.1438\n",
      "Step  5 | Training - Loss: 0.2111, Reduction Loss: 0.1151 | Validation - Reduction Loss: 0.1167\n",
      "Step  6 | Training - Loss: 0.1801, Reduction Loss: 0.0953 | Validation - Reduction Loss: 0.0972\n",
      "Step  7 | Training - Loss: 0.1613, Reduction Loss: 0.0820 | Validation - Reduction Loss: 0.0841\n",
      "Step  8 | Training - Loss: 0.1462, Reduction Loss: 0.0726 | Validation - Reduction Loss: 0.0747\n",
      "Step  9 | Training - Loss: 0.1344, Reduction Loss: 0.0653 | Validation - Reduction Loss: 0.0676\n",
      "Test - Feature reduction loss:0.1100\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.5716| Validation - Acc: 0.3260\n",
      "Step  1 | Training - Loss: 1.5322| Validation - Acc: 0.3723\n",
      "Step  2 | Training - Loss: 1.4963| Validation - Acc: 0.3789\n",
      "Step  3 | Training - Loss: 1.4574| Validation - Acc: 0.4249\n",
      "Step  4 | Training - Loss: 1.4240| Validation - Acc: 0.4667\n",
      "Step  5 | Training - Loss: 1.3983| Validation - Acc: 0.5309\n",
      "Step  6 | Training - Loss: 1.3796| Validation - Acc: 0.5311\n",
      "Step  7 | Training - Loss: 1.3701| Validation - Acc: 0.5311\n",
      "Step  8 | Training - Loss: 1.3668| Validation - Acc: 0.5311\n",
      "Step  9 | Training - Loss: 1.3656| Validation - Acc: 0.5311\n",
      "Test - Accuracy: 0.5342\n",
      "*********** Current parameters: epochs:10, hidden_units:4, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 | Training - Loss: 0.5836, Reduction Loss: 0.5033 | Validation - Reduction Loss: 0.5034\n",
      "Step  1 | Training - Loss: 0.4465, Reduction Loss: 0.2900 | Validation - Reduction Loss: 0.2909\n",
      "Step  2 | Training - Loss: 0.3813, Reduction Loss: 0.1989 | Validation - Reduction Loss: 0.1999\n",
      "Step  3 | Training - Loss: 0.3267, Reduction Loss: 0.1657 | Validation - Reduction Loss: 0.1673\n",
      "Step  4 | Training - Loss: 0.2655, Reduction Loss: 0.1414 | Validation - Reduction Loss: 0.1429\n",
      "Step  5 | Training - Loss: 0.2106, Reduction Loss: 0.1144 | Validation - Reduction Loss: 0.1161\n",
      "Step  6 | Training - Loss: 0.1799, Reduction Loss: 0.0952 | Validation - Reduction Loss: 0.0970\n",
      "Step  7 | Training - Loss: 0.1613, Reduction Loss: 0.0820 | Validation - Reduction Loss: 0.0839\n",
      "Step  8 | Training - Loss: 0.1463, Reduction Loss: 0.0725 | Validation - Reduction Loss: 0.0746\n",
      "Step  9 | Training - Loss: 0.1345, Reduction Loss: 0.0652 | Validation - Reduction Loss: 0.0675\n",
      "Test - Feature reduction loss:0.1100\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.5915| Validation - Acc: 0.2938\n",
      "Step  1 | Training - Loss: 1.5490| Validation - Acc: 0.4003\n",
      "Step  2 | Training - Loss: 1.5127| Validation - Acc: 0.4329\n",
      "Step  3 | Training - Loss: 1.4802| Validation - Acc: 0.4405\n",
      "Step  4 | Training - Loss: 1.4512| Validation - Acc: 0.4499\n",
      "Step  5 | Training - Loss: 1.4255| Validation - Acc: 0.4580\n",
      "Step  6 | Training - Loss: 1.4033| Validation - Acc: 0.5172\n",
      "Step  7 | Training - Loss: 1.3865| Validation - Acc: 0.5305\n",
      "Step  8 | Training - Loss: 1.3760| Validation - Acc: 0.5311\n",
      "Step  9 | Training - Loss: 1.3707| Validation - Acc: 0.5311\n",
      "Test - Accuracy: 0.5342\n",
      "*********** Current parameters: epochs:10, hidden_units:8, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5652, Reduction Loss: 0.4542 | Validation - Reduction Loss: 0.4546\n",
      "Step  1 | Training - Loss: 0.4426, Reduction Loss: 0.2445 | Validation - Reduction Loss: 0.2458\n",
      "Step  2 | Training - Loss: 0.3868, Reduction Loss: 0.1804 | Validation - Reduction Loss: 0.1819\n",
      "Step  3 | Training - Loss: 0.3200, Reduction Loss: 0.1558 | Validation - Reduction Loss: 0.1581\n",
      "Step  4 | Training - Loss: 0.2587, Reduction Loss: 0.1280 | Validation - Reduction Loss: 0.1299\n",
      "Step  5 | Training - Loss: 0.2220, Reduction Loss: 0.1027 | Validation - Reduction Loss: 0.1057\n",
      "Step  6 | Training - Loss: 0.1976, Reduction Loss: 0.0916 | Validation - Reduction Loss: 0.0930\n",
      "Step  7 | Training - Loss: 0.1703, Reduction Loss: 0.0838 | Validation - Reduction Loss: 0.0850\n",
      "Step  8 | Training - Loss: 0.1498, Reduction Loss: 0.0743 | Validation - Reduction Loss: 0.0765\n",
      "Step  9 | Training - Loss: 0.1365, Reduction Loss: 0.0662 | Validation - Reduction Loss: 0.0684\n",
      "Test - Feature reduction loss:0.1121\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.6116| Validation - Acc: 0.2438\n",
      "Step  1 | Training - Loss: 1.5340| Validation - Acc: 0.4214\n",
      "Step  2 | Training - Loss: 1.4672| Validation - Acc: 0.4778\n",
      "Step  3 | Training - Loss: 1.4159| Validation - Acc: 0.5104\n",
      "Step  4 | Training - Loss: 1.3875| Validation - Acc: 0.5304\n",
      "Step  5 | Training - Loss: 1.3747| Validation - Acc: 0.5314\n",
      "Step  6 | Training - Loss: 1.3695| Validation - Acc: 0.5311\n",
      "Step  7 | Training - Loss: 1.3676| Validation - Acc: 0.5311\n",
      "Step  8 | Training - Loss: 1.3668| Validation - Acc: 0.5311\n",
      "Step  9 | Training - Loss: 1.3664| Validation - Acc: 0.5311\n",
      "Test - Accuracy: 0.5342\n",
      "*********** Current parameters: epochs:10, hidden_units:16, hidden_layers:8 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.5389, Reduction Loss: 0.3786 | Validation - Reduction Loss: 0.3787\n",
      "Step  1 | Training - Loss: 0.3977, Reduction Loss: 0.1583 | Validation - Reduction Loss: 0.1601\n",
      "Step  2 | Training - Loss: 0.2663, Reduction Loss: 0.0473 | Validation - Reduction Loss: 0.0514\n",
      "Step  3 | Training - Loss: 0.1993, Reduction Loss: 0.0057 | Validation - Reduction Loss: 0.0021\n",
      "Step  4 | Training - Loss: 0.2225, Reduction Loss: 0.0417 | Validation - Reduction Loss: 0.0389\n",
      "Step  5 | Training - Loss: 0.1638, Reduction Loss: 0.0020 | Validation - Reduction Loss: 0.0049\n",
      "Step  6 | Training - Loss: 0.1570, Reduction Loss: 0.0032 | Validation - Reduction Loss: 0.0023\n",
      "Step  7 | Training - Loss: 0.1585, Reduction Loss: 0.0014 | Validation - Reduction Loss: 0.0052\n",
      "Step  8 | Training - Loss: 0.1725, Reduction Loss: 0.0184 | Validation - Reduction Loss: 0.0212\n",
      "Step  9 | Training - Loss: 0.1544, Reduction Loss: 0.0017 | Validation - Reduction Loss: 0.0037\n",
      "Test - Feature reduction loss:0.0043\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.3918| Validation - Acc: 0.5930\n",
      "Step  1 | Training - Loss: 1.2273| Validation - Acc: 0.7147\n",
      "Step  2 | Training - Loss: 1.1623| Validation - Acc: 0.7582\n",
      "Step  3 | Training - Loss: 1.1335| Validation - Acc: 0.7796\n",
      "Step  4 | Training - Loss: 1.1221| Validation - Acc: 0.7883\n",
      "Step  5 | Training - Loss: 1.1173| Validation - Acc: 0.7918\n",
      "Step  6 | Training - Loss: 1.1148| Validation - Acc: 0.7935\n",
      "Step  7 | Training - Loss: 1.1130| Validation - Acc: 0.7943\n",
      "Step  8 | Training - Loss: 1.1116| Validation - Acc: 0.7947\n",
      "Step  9 | Training - Loss: 1.1107| Validation - Acc: 0.7961\n",
      "Test - Accuracy: 0.4695\n",
      "*********** Current parameters: epochs:10, hidden_units:128, hidden_layers:8 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.3925, Reduction Loss: 0.0496 | Validation - Reduction Loss: 0.0432\n",
      "Step  1 | Training - Loss: 0.1700, Reduction Loss: 0.0233 | Validation - Reduction Loss: 0.0174\n",
      "Step  2 | Training - Loss: 0.1593, Reduction Loss: 0.0383 | Validation - Reduction Loss: 0.0328\n",
      "Step  3 | Training - Loss: 0.1257, Reduction Loss: 0.0293 | Validation - Reduction Loss: 0.0337\n",
      "Step  4 | Training - Loss: 0.1313, Reduction Loss: 0.0467 | Validation - Reduction Loss: 0.0498\n",
      "Step  5 | Training - Loss: 0.1531, Reduction Loss: 0.0524 | Validation - Reduction Loss: 0.0486\n",
      "Step  6 | Training - Loss: 0.1449, Reduction Loss: 0.0725 | Validation - Reduction Loss: 0.0771\n",
      "Step  7 | Training - Loss: 0.0863, Reduction Loss: 0.0007 | Validation - Reduction Loss: 0.0071\n",
      "Step  8 | Training - Loss: 0.1418, Reduction Loss: 0.0483 | Validation - Reduction Loss: 0.0421\n",
      "Step  9 | Training - Loss: 0.0968, Reduction Loss: 0.0074 | Validation - Reduction Loss: 0.0003\n",
      "Test - Feature reduction loss:0.0239\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.1461| Validation - Acc: 0.7766\n",
      "Step  1 | Training - Loss: 1.0958| Validation - Acc: 0.8140\n",
      "Step  2 | Training - Loss: 1.0871| Validation - Acc: 0.8183\n",
      "Step  3 | Training - Loss: 1.0841| Validation - Acc: 0.8198\n",
      "Step  4 | Training - Loss: 1.0810| Validation - Acc: 0.8212\n",
      "Step  5 | Training - Loss: 1.0792| Validation - Acc: 0.8221\n",
      "Step  6 | Training - Loss: 1.0774| Validation - Acc: 0.8220\n",
      "Step  7 | Training - Loss: 1.0761| Validation - Acc: 0.8219\n",
      "Step  8 | Training - Loss: 1.0747| Validation - Acc: 0.8224\n",
      "Step  9 | Training - Loss: 1.0734| Validation - Acc: 0.8231\n",
      "Test - Accuracy: 0.4684\n",
      "*********** Current parameters: epochs:10, hidden_units:256, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.3301, Reduction Loss: 0.0843 | Validation - Reduction Loss: 0.0895\n",
      "Step  1 | Training - Loss: 0.1818, Reduction Loss: 0.0133 | Validation - Reduction Loss: 0.0084\n",
      "Step  2 | Training - Loss: 0.1908, Reduction Loss: 0.0471 | Validation - Reduction Loss: 0.0464\n",
      "Step  3 | Training - Loss: 0.1312, Reduction Loss: 0.0268 | Validation - Reduction Loss: 0.0272\n",
      "Step  4 | Training - Loss: 0.1344, Reduction Loss: 0.0371 | Validation - Reduction Loss: 0.0378\n",
      "Step  5 | Training - Loss: 0.1172, Reduction Loss: 0.0143 | Validation - Reduction Loss: 0.0140\n",
      "Step  6 | Training - Loss: 0.0936, Reduction Loss: 0.0008 | Validation - Reduction Loss: 0.0003\n",
      "Step  7 | Training - Loss: 0.1071, Reduction Loss: 0.0270 | Validation - Reduction Loss: 0.0259\n",
      "Step  8 | Training - Loss: 0.1224, Reduction Loss: 0.0510 | Validation - Reduction Loss: 0.0499\n",
      "Step  9 | Training - Loss: 0.0940, Reduction Loss: 0.0163 | Validation - Reduction Loss: 0.0153\n",
      "Test - Feature reduction loss:0.2469\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.1361| Validation - Acc: 0.7712\n",
      "Step  1 | Training - Loss: 1.1084| Validation - Acc: 0.8002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2 | Training - Loss: 1.1043| Validation - Acc: 0.8014\n",
      "Step  3 | Training - Loss: 1.1020| Validation - Acc: 0.8018\n",
      "Step  4 | Training - Loss: 1.1008| Validation - Acc: 0.8017\n",
      "Step  5 | Training - Loss: 1.1000| Validation - Acc: 0.8021\n",
      "Step  6 | Training - Loss: 1.0988| Validation - Acc: 0.8023\n",
      "Step  7 | Training - Loss: 1.0981| Validation - Acc: 0.8019\n",
      "Step  8 | Training - Loss: 1.0974| Validation - Acc: 0.8023\n",
      "Step  9 | Training - Loss: 1.0968| Validation - Acc: 0.8021\n",
      "Test - Accuracy: 0.5179\n",
      "*********** Current parameters: epochs:10, hidden_units:1024, hidden_layers:4 **************\n",
      "VAE for Feature reduction:\n",
      "Step  0 | Training - Loss: 0.3182, Reduction Loss: 0.1507 | Validation - Reduction Loss: 0.1537\n",
      "Step  1 | Training - Loss: 0.1523, Reduction Loss: 0.0156 | Validation - Reduction Loss: 0.0167\n",
      "Step  2 | Training - Loss: 0.1586, Reduction Loss: 0.0785 | Validation - Reduction Loss: 0.0767\n",
      "Step  3 | Training - Loss: 0.1391, Reduction Loss: 0.0476 | Validation - Reduction Loss: 0.0488\n",
      "Step  4 | Training - Loss: 0.1147, Reduction Loss: 0.0371 | Validation - Reduction Loss: 0.0385\n",
      "Step  5 | Training - Loss: 0.0797, Reduction Loss: 0.0013 | Validation - Reduction Loss: 0.0014\n",
      "Step  6 | Training - Loss: 0.1055, Reduction Loss: 0.0470 | Validation - Reduction Loss: 0.0500\n",
      "Step  7 | Training - Loss: 0.1120, Reduction Loss: 0.0630 | Validation - Reduction Loss: 0.0666\n",
      "Step  8 | Training - Loss: 0.1012, Reduction Loss: 0.0228 | Validation - Reduction Loss: 0.0180\n",
      "Step  9 | Training - Loss: 0.1084, Reduction Loss: 0.0281 | Validation - Reduction Loss: 0.0331\n",
      "Test - Feature reduction loss:0.1265\n",
      "Softmax for Prediction:\n",
      "Step  0 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  1 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  2 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  3 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  4 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  5 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  6 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  7 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  8 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Step  9 | Training - Loss: 1.5415| Validation - Acc: 0.3694\n",
      "Test - Accuracy: 0.3669\n"
     ]
    }
   ],
   "source": [
    "# Scenario for classes = 2\n",
    "df_results_2label_dense = run_scenario_dense(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "df_results_2label_vae, labels2_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "\n",
    "# Scenario for classes = 5\n",
    "df_results_5label_dense = run_scenario_dense(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)\n",
    "df_results_5label_vae, labels5_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016348</td>\n",
       "      <td>0.994602</td>\n",
       "      <td>0.503416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.690915</td>\n",
       "      <td>0.538599</td>\n",
       "      <td>0.534155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086071</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.503637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>0.994562</td>\n",
       "      <td>0.503992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>0.995515</td>\n",
       "      <td>0.502395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.995793</td>\n",
       "      <td>0.503061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>0.995991</td>\n",
       "      <td>0.502218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.016348             0.994602   \n",
       "1      10                 4              4    0.690915             0.538599   \n",
       "2      10                 8              4    0.086071             0.979798   \n",
       "3      10                16              8    0.015496             0.994562   \n",
       "4      10               128              8    0.013025             0.995515   \n",
       "5      10               256              4    0.010597             0.995793   \n",
       "6      10              1024              4    0.010249             0.995991   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.503416  \n",
       "1       0.534155  \n",
       "2       0.503637  \n",
       "3       0.503992  \n",
       "4       0.502395  \n",
       "5       0.503061  \n",
       "6       0.502218  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.690862</td>\n",
       "      <td>0.536614</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.067930</td>\n",
       "      <td>0.065904</td>\n",
       "      <td>0.112750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.690836</td>\n",
       "      <td>0.536614</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.068066</td>\n",
       "      <td>0.066047</td>\n",
       "      <td>0.112254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.458287</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>0.510779</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.015819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.498637</td>\n",
       "      <td>0.808732</td>\n",
       "      <td>0.502262</td>\n",
       "      <td>0.064647</td>\n",
       "      <td>0.041120</td>\n",
       "      <td>0.031928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.431506</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.496318</td>\n",
       "      <td>0.029489</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.420968</td>\n",
       "      <td>0.868148</td>\n",
       "      <td>0.495165</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>0.032180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.536614</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>0.056312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.690862             0.536614   \n",
       "1      10                 4              4    0.690836             0.536614   \n",
       "2      10                 8              4    0.458287             0.852749   \n",
       "3      10                16              8    0.498637             0.808732   \n",
       "4      10               128              8    0.431506             0.870570   \n",
       "5      10               256              4    0.420968             0.868148   \n",
       "6      10              1024              4    0.779200             0.536614   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534155                      0.067930                      0.065904   \n",
       "1       0.534155                      0.068066                      0.066047   \n",
       "2       0.510779                      0.001534                      0.003362   \n",
       "3       0.502262                      0.064647                      0.041120   \n",
       "4       0.496318                      0.029489                      0.036761   \n",
       "5       0.495165                      0.021164                      0.015571   \n",
       "6       0.534155                      0.014735                      0.020117   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                     0.112750  \n",
       "1                     0.112254  \n",
       "2                     0.015819  \n",
       "3                     0.031928  \n",
       "4                     0.002735  \n",
       "5                     0.032180  \n",
       "6                     0.056312  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072257</td>\n",
       "      <td>0.986743</td>\n",
       "      <td>0.439496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.177946</td>\n",
       "      <td>0.977932</td>\n",
       "      <td>0.441581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>0.441049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.055772</td>\n",
       "      <td>0.989601</td>\n",
       "      <td>0.443799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.991308</td>\n",
       "      <td>0.442246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.994245</td>\n",
       "      <td>0.435105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.437101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    0.072257             0.986743   \n",
       "1      10                 4              4    0.177946             0.977932   \n",
       "2      10                 8              4    0.027333             0.992657   \n",
       "3      10                16              8    0.055772             0.989601   \n",
       "4      10               128              8    0.027580             0.991308   \n",
       "5      10               256              4    0.014988             0.994245   \n",
       "6      10              1024              4    0.013778             0.994801   \n",
       "\n",
       "   test_accuracy  \n",
       "0       0.439496  \n",
       "1       0.441581  \n",
       "2       0.441049  \n",
       "3       0.443799  \n",
       "4       0.442246  \n",
       "5       0.435105  \n",
       "6       0.437101  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>reduced_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.365627</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.065284</td>\n",
       "      <td>0.067570</td>\n",
       "      <td>0.109996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.370677</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.067470</td>\n",
       "      <td>0.110005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.366432</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.534155</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.068429</td>\n",
       "      <td>0.112057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1.110683</td>\n",
       "      <td>0.796110</td>\n",
       "      <td>0.469526</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.004343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>1.073440</td>\n",
       "      <td>0.823060</td>\n",
       "      <td>0.468373</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.023865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>1.096815</td>\n",
       "      <td>0.802104</td>\n",
       "      <td>0.517920</td>\n",
       "      <td>0.016256</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>0.246865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1.541539</td>\n",
       "      <td>0.369399</td>\n",
       "      <td>0.366883</td>\n",
       "      <td>0.028109</td>\n",
       "      <td>0.033143</td>\n",
       "      <td>0.126534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  reduced_features  hidden_layers  train_loss  validation_accuracy  \\\n",
       "0      10                 4              2    1.365627             0.531097   \n",
       "1      10                 4              4    1.370677             0.531097   \n",
       "2      10                 8              4    1.366432             0.531097   \n",
       "3      10                16              8    1.110683             0.796110   \n",
       "4      10               128              8    1.073440             0.823060   \n",
       "5      10               256              4    1.096815             0.802104   \n",
       "6      10              1024              4    1.541539             0.369399   \n",
       "\n",
       "   test_accuracy  train_feature_reduction_loss  valid_feature_reduction_loss  \\\n",
       "0       0.534155                      0.065284                      0.067570   \n",
       "1       0.534155                      0.065186                      0.067470   \n",
       "2       0.534155                      0.066152                      0.068429   \n",
       "3       0.469526                      0.001695                      0.003663   \n",
       "4       0.468373                      0.007354                      0.000286   \n",
       "5       0.517920                      0.016256                      0.015258   \n",
       "6       0.366883                      0.028109                      0.033143   \n",
       "\n",
       "   test_feature_reduction_loss  \n",
       "0                     0.109996  \n",
       "1                     0.110005  \n",
       "2                     0.112057  \n",
       "3                     0.004343  \n",
       "4                     0.023865  \n",
       "5                     0.246865  \n",
       "6                     0.126534  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12042     0]\n",
      " [10502     0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAGdCAYAAABkXrYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVNX9//HXG5CiiKIoKqigohGMFdFo4s8WNTZMYsES\nG9EYTdSosUQT04jGFHtsMfYo2GIPGqJfKwJiwS6KKAiCih1Fls/vj3tWhw27LMvszM6976ePeXDn\n1jMs7mc+n3PuuYoIzMzM8qpdtRtgZmbWmhzozMws1xzozMws1xzozMws1xzozMws1xzozMws1xzo\nzMws1xzozMysbCT9Q9IMSc+WrPuTpBclPSPpVknLlmw7RdJESS9J2rFk/SaSJqRt50lSWt9J0vC0\n/nFJfRbWJgc6MzMrpyuBnRqsuw9YLyLWB14GTgGQ1B8YAgxIx/xNUvt0zEXAYUC/9Ko/51BgVkSs\nBZwN/HFhDXKgMzOzsomIB4H3Gqy7NyLmprejgd5peTBwQ0R8HhGTgInAIEkrA90iYnRk03ddDexR\ncsxVafkmYLv6bK8xHRb3Q5mZWdvWvtvqEXNnl+VcMXvmyIhomLEtikOB4Wm5F1ngqzclrfsiLTdc\nX3/MmwARMVfSB8DywDuNXdCBzsws52LubDqts3dZzvXZUxd+TdK4klWXRsSlzTlW0qnAXOC6sjSm\nmRzozMxyT6Cy9VS9ExEDF7kF0sHArsB28dXTBKYCq5bs1jutm8pX5c3S9aXHTJHUAVgGeLepa7uP\nzsws7wRI5Xm15PLSTsCJwO4R8WnJptuBIWkkZV+yQSdjImIa8KGkzVP/24HAbSXHHJSW9wT+Gwt5\nDI8zOjOzIihfRtf0ZaTrga2BHpKmAKeTjbLsBNyXxo2MjogjIuI5SSOA58lKmkdFRF061ZFkIzi7\nAPekF8DlwDWSJpINehmy0Db5eXRmZvnWbqme0Wnd/cpyrs+eOOeJlpQuq8kZnZlZEbSw7JgHDnRm\nZrlX1sEoNae4n9zMzArBGZ2ZWRG4dGlmZrklXLo0MzPLK2d0Zma51/KbvfPAgc7MrAgKXLp0oDMz\nK4ICZ3TFDfFmZlYIzujMzHKv2DeMO9CZmeVd/dMLCqq4Id7MzArBGZ2ZWRG4dGlmZvlV7D664n5y\nMzMrBGd0ZmZF0K64g1Ec6MzM8q7gkzo70JmZFYFvLzAzM8snZ3RmZrlX7FGXDnRmZkXg0qWZmVk+\nOaMzMysCly7NzCy3VOwnjBc3xJuZWSE4ozMzKwKXLs3MLNdcujQzM8snZ3RmZrnnG8bNzCzvCly6\ndKAzM8u7gj+9oLif3MzMCsGBzqwFJHWRdIekDyTduBjn2V/SveVsW7VI+pakl6rdDluQ1EdXjlcN\nqs1WmzWTpP0kjZP0saRpku6R9M0ynHpPoCewfETs1dKTRMR1EbFDGdrTqiSFpLWa2iciHoqIdSrV\nJltE9bOjLO6rBjnQWW5JOg44B/gDWVBaDbgQ2L0Mp18deDki5pbhXDVPkvv7rc1yoLNckrQM8Fvg\nqIi4JSI+iYgvIuLOiDgx7dNJ0jmS3kqvcyR1Stu2ljRF0vGSZqRs8JC07TfAr4B9UqY4VNKvJV1b\ncv0+KQvqkN4fLOk1SR9JmiRp/5L1D5cct4WksakkOlbSFiXbHpD0O0mPpPPcK6lHI5+/vv0nlrR/\nD0k7S3pZ0nuSflGy/yBJj0l6P+17gaSOaduDaben0+fdp+T8J0maDlxRvy4ds2a6xsbp/SqSZkra\nerF+sNZyLl2a5c43gM7ArU3scyqwObAhsAEwCDitZPtKwDJAL2AocKGk7hFxOlmWODwiukbE5U01\nRNJSwHnAdyJiaWAL4KkF7LcccFfad3ngr8BdkpYv2W0/4BBgRaAjcEITl16J7O+gF1lgvgw4ANgE\n+BbwS0l90751wM+AHmR/d9sBRwJExFZpnw3S5x1ecv7lyLLbw0svHBGvAicB10paErgCuCoiHmii\nvdaaXLo0y53lgXcWUlrcH/htRMyIiJnAb4AflGz/Im3/IiLuBj4GWtoHNQ9YT1KXiJgWEc8tYJ9d\ngFci4pqImBsR1wMvAruV7HNFRLwcEbOBEWRBujFfAMMi4gvgBrIgdm5EfJSu/zxZgCcinoiI0em6\nrwOXAP+vGZ/p9Ij4PLVnPhFxGTAReBxYmeyLhVnFOdBZXr0L9FhI39EqwOSS95PTui/P0SBQfgp0\nXdSGRMQnwD7AEcA0SXdJ+loz2lPfpl4l76cvQnvejYi6tFwfiN4u2T67/nhJa0u6U9J0SR+SZawL\nLIuWmBkRny1kn8uA9YDzI+LzhexrrUUedWmWR48BnwN7NLHPW2Rlt3qrpXUt8QmwZMn7lUo3RsTI\niPg2WWbzIlkAWFh76ts0tYVtWhQXkbWrX0R0A35BdptxU6KpjZK6kg0Guhz4dSrNWrW4dGmWLxHx\nAVm/1IVpEMaSkpaQ9B1JZ6XdrgdOk7RCGtTxK+Daxs65EE8BW0laLQ2EOaV+g6SekganvrrPyUqg\n8xZwjruBtdMtER0k7QP0B+5sYZsWxdLAh8DHKdv8cYPtbwNrLOI5zwXGRcQPyfoeL17sVlqLSSrL\nqxY50FluRcRfgOPIBpjMBN4EfgL8K+3ye2Ac8AwwARif1rXkWvcBw9O5nmD+4NQuteMt4D2yvq+G\ngYSIeBfYFTierPR6IrBrRLzTkjYtohPIBrp8RJZtDm+w/dfAVWlU5t4LO5mkwcBOfPU5jwM2rh9t\nalZJimiy+mBmZjWuffc+0Xm708tyrk9vPvSJiBhYlpNViG/yNDPLO7HwHtccc+nSzMxyzRmdmVnu\n1e5AknJwoDMzKwAHOmuz1KFLqOPS1W6GVdFG665W7SZYlY0f/8Q7EbFCtdtRqxzo2jh1XJpO6yx0\nNLfl2COPX1DtJliVdVlCDWfMWWTO6MzMLNcc6MzMLL98e4GZmVl5SPpHegbisyXrlpN0n6RX0p/d\nS7adImmipJck7ViyfhNJE9K285RSUmXPkRye1j8uqc/C2uRAZ2aWc6I881w2s/x5Jdn0b6VOBkZF\nRD9gVHqPpP7AEGBAOuZvktqnYy4CDgP6pVf9OYcCsyJiLeBs4I8La5ADnZlZAVQq0EXEg2RzupYa\nDFyVlq/iq6eKDAZuSM80nET2/MJBklYGuqVnJAZwdYNj6s91E7CdFtIwBzozM2ttPSNiWlqeDvRM\ny73IJluvNyWt65WWG66f75j0vMgPyB603CgPRjEzK4AyjrrsIWlcyftLI+LS5h4cESGpok8TcKAz\nMyuAMga6d1rw9IK3Ja0cEdNSWXJGWj8VWLVkv95p3dS03HB96TFTJHUAliF7rFWjXLo0M7PWdjtw\nUFo+CLitZP2QNJKyL9mgkzGpzPmhpM1T/9uBDY6pP9eewH9jIc+bc0ZnZpZ3FbyPTtL1wNZkJc4p\nwOnAmcAISUOBycDeABHxnKQRwPPAXOCoiKhLpzqSbARnF+Ce9AK4HLhG0kSyQS9DFtYmBzozswKo\n1MwoEbFvI5u2a2T/YcCwBawfB6y3gPWfAXstSptcujQzs1xzRmdmlnPy8+jMzCzvHOjMzCzfihvn\n3EdnZmb55ozOzCzv5NKlmZnlXJEDnUuXZmaWa87ozMwKoMgZnQOdmVnOFf0+Opcuzcws15zRmZkV\nQXETOgc6M7Pc8+0FZmaWd0UOdO6jMzOzXHNGZ2ZWAEXO6BzozMyKoLhxzqVLMzPLN2d0ZmYF4NKl\nmZnlluSZUczMzHLLGZ2ZWQEUOaNzoDMzK4AiBzqXLs3MLNec0ZmZFUFxEzoHOjOzIihy6dKBzsws\n7wr+9AL30ZmZWa45ozMzyzkBBU7oHOjMzPLPM6OYmZnlljM6M7MCKHBC50BnZlYELl2amZnllDM6\nM7O8k0uXZmaWYwLatStupHOgMzMrgCJndO6jMzOzXHNGZ2ZWAEUedelAZ2aWdwUfjOLSpZmZ5Zoz\nOjOznMsmdS5uSudAZ2aWe57U2czMLLcc6KwiLj59fyaPOoNxN/7iy3V/OHYPnrrlNMYMP4XhfzmM\nZbp2+XLbCYfuwLO3nc7Tt/6S7b+x7v+c78ZzfjTfuY4+YFvG33wqY4afwt0X/5TVVu7euh/IKuLe\nkf9m/QHrMOBra/Gns86sdnNqmlSeVy1yoLOKuOaO0Qw+6sL51o0a/SKb7PUHBu1zBq9MnsHPD90B\ngK+tsRJ77bgxG+85jN2P+hvnnrL3fLM6DN52Az759PP5zvXUi2+y5f5nMWifM7h11JMMO2aP1v9Q\n1qrq6uo49uijuO2Oe3jymee58YbreeH556vdrJolqSyvWuRAZxXxyPhXee+DT+dbN2r0i9TVzQNg\nzIRJ9Oq5LAC7br0+N44cz5wv5jL5rXd59c132HS9PgAs1aUjRx+wLWf+/d/znevBca8w+7MvsnM9\n8/qX57LaNXbMGNZccy36rrEGHTt2ZK99hnDnHbdVu1m1qUzZXI3GOQc6axsOHPwNRj6SfVvvtcIy\nTJk+68ttU2fMYpUVlwHg9CN35dxrRvHp7DmNnuvgPb46l9Wut96aSu/eq375vlev3kydOrWKLbJa\n5UBnVXfi0B2pq5vHDXePbXK/9dfuRd9VV+D2+59pdJ8hO2/Kxv1X4+yrRpW7mWY1q/72Apcuy0zS\no6117nKQFJL+UvL+BEm/rnAbrpS0ZyWv2dYcsNtm7LzVehx86pVfrps68wN6r/TVYJJeK3bnrRkf\nsNkGfdmk/2q8eNdv+O8VP6Pf6isy8rJjvtxvm83W4aShO7LnsZcw54u5lfwY1gpWWaUXU6a8+eX7\nqVOn0KtXryq2qLa5dNkKImKL1jp3mXwOfE9Sj5YcLMn3IC6mb2+xLscdvD17HnvJl/1rAHc98Ax7\n7bgxHZfowOqrLM9aq63A2Gdf57IbH2aNHU7la7uczraHnM0rk2ew42HnArDBOr254NQh7PmzS5g5\n6+NqfSQro4GbbsrEia/w+qRJzJkzhxuH38Auu+5e7WZZDWq1X9aSPo6IrpJWBoYD3dL1fhwRDy1g\n//bA5cBAIIB/RMTZktYELgRWAD4FDouIFyXtBZwO1AEfRMRWkgYAVwAdyYL49yPilUaaOBe4FPgZ\ncGqDtvQB/gH0AGYCh0TEG5KuBD4DNgIekfQh0BdYA1gtnWtz4DvAVGC3iPhC0q+A3YAuwKPAjyIi\nmv2XmQNXnXEw39qkHz2W7crEf/+O3118Nz8/ZAc6dezAnRf9BIAxE17n6GE38MJr07n53id58uZT\nmVs3j2PPHMG8eU3/df3hZ3uw1JKduO6soQC8OX0Wex17Sat/Lms9HTp04OxzL2C3XXakrq6Ogw4+\nlP4DBlS7WTWrVsuO5aDW+n1bEuiOBzpHxLAUzJaMiI8WsP8mwJkR8e30ftmIeF/SKOCIiHhF0mbA\nGRGxraQJwE4RMbVk3/OB0RFxnaSOQPuImN1Y+4BVgGeADYDDgK4R8WtJdwA3RcRVkg4Fdo+IPVKg\n6wEMjoi6VOrcHtgG6A88RhZc75F0K3BVRPxL0nIR8V667jXAiIi4I53vzoi4qUHbDgcOB2CJrpt0\nHnBQS34ElhOzxl5Q7SZYlXVZQk9ExMCWHr9Ur3ViwFHl+eI39tRtmmyLpJ8BPyRLWCYAhwBLkiU8\nfYDXgb0jYlba/xRgKFnScnREjEzrNwGuJEsQ7gaOaWmCUInBKGOBQ1JQ+PqCglzyGrCGpPMl7QR8\nKKkrsAVwo6SngEuAldP+jwBXSjoMaJ/WPQb8QtJJwOqNBbl6EfEhcDVwdINN3wD+mZavAb5Zsu3G\niKgreX9PRHxB9gNtD9SPe59A9kMF2EbS4yk4bws0+bU0Ii6NiIERMVAdujS1q5lZmyGpF9nv04ER\nsR7Z78QhwMnAqIjoB4xK75HUP20fAOwE/C0lRAAXkSUg/dJrp5a2q9UDXUQ8CGxFVsq7UtKBjew3\niyyzegA4Avh7at/7EbFhyWvdtP8RwGnAqsATkpaPiH8CuwOzgbslbduMJp5D9m1iqWZ+pE8avP88\ntWce8EXJN455QAdJnYG/AXtGxNeBy4DOzbyWmdniU0VHXXYAuqRxDEsCbwGDgavS9quA+hkdBgM3\nRMTnETEJmAgMSl1e3SJidPqdenXJMYus1QOdpNWBtyPiMrLgtXEj+/UA2kXEzWQBbOOUcU1K/XEo\ns0FaXjMiHo+IX5H1o60qaQ3gtYg4D7gNWH9h7UslxRFkwa7eo2TfMgD2B/6nT3ER1Ae1d1KGWuhR\nlmZWedntBa0/6jIipgJ/Bt4AppGNn7gX6BkR09Ju04GeabkX8GbJKaakdb3ScsP1LVKJ0uXWwNOS\nngT2Ac5tZL9ewAOpRHktcEpavz8wVNLTwHNk3wAA/iRpgqRnyQLT08DewLPpHOuRfQtojr+Q9b3V\n+ylZufUZ4AfAMQs8qhki4n2yLO5ZYCRZKdfMrFb1kDSu5HV4/QZJ3cl+R/clGwOxlKQDSg9OGVpF\nB+O12qjLiOia/ryKr1LWpvZ/mgVkeymd/Z/abER8bwGnOTO9mt2+tPw2WYpd/34yWV9aw2MObvD+\n102c89cly6eRZalNns/MrHWU9Wbvd5oYjLI9MCkiZgJIuoVsnMXbklaOiGmpLDkj7T+VrPupXu+0\nbmpabri+RTwziplZAVTohvE3gM0lLakssm4HvADcDtQPHz+IrGuJtH6IpE6S+pINOhmTypwfSto8\nnefAkmMWWVVuepb0ONCpweofRMSEMl9nebIRPg1tFxHvlvNaZmZtWSXuo4uIxyXdBIwnu1f5SbL7\nlbsCIyQNBSaTdTMREc9JGgE8n/Y/qmRU+5F8dXvBPenVIlUJdBGxWYWu8y6wYSWuZWZmEBGnk03m\nUepzsuxuQfsPA4YtYP04srEWi83TWJmZ5V0Nz1NZDg50ZmY5V//0gqLyYBQzM8s1Z3RmZgVQ5IzO\ngc7MrAAKHOdcujQzs3xzRmdmVgAuXZqZWX759gIzM8szlXeuy5rjPjozM8s1Z3RmZgVQ4ITOgc7M\nrAjaFTjSuXRpZma55ozOzKwACpzQOdCZmeVd9tDU4kY6ly7NzCzXnNGZmRVAu+ImdA50ZmZFUOTS\npQOdmVkBFDjOuY/OzMzyzRmdmVnOiWy+y6JyoDMzK4AiD0Zx6dLMzHLNGZ2ZWd6p2I/pcaAzMyuA\nAsc5ly7NzCzfnNGZmeWcKPZjehzozMwKoMBxzqVLMzPLN2d0ZmYF4FGXZmaWW9nz6KrdiuppNNBJ\n6tbUgRHxYfmbY2ZmrcGDURbsOSBgvgnS6t8HsFortsvMzKwsGg10EbFqJRtiZmatp7j5XDNHXUoa\nIukXabm3pE1at1lmZlZOStOALe6rFi000Em6ANgG+EFa9SlwcWs2yszMrFyaM+pyi4jYWNKTABHx\nnqSOrdwuMzMrk2xmlGq3onqaE+i+kNSObAAKkpYH5rVqq8zMrHxquOxYDs3po7sQuBlYQdJvgIeB\nP7Zqq8zMzMpkoRldRFwt6Qlg+7Rqr4h4tnWbZWZm5VTghK7ZM6O0B74gK196fkwzsxrj0mUTJJ0K\nXA+sAvQG/inplNZumJmZlUf9YJRyvGpRczK6A4GNIuJTAEnDgCeBM1qzYWZmZuXQnEA3rcF+HdI6\nMzOrEUUuXTY1qfPZZH1y7wHPSRqZ3u8AjK1M88zMrByKG+aazujqR1Y+B9xVsn506zXHzMysvJqa\n1PnySjbEzMxah+TH9DRJ0prAMKA/0Ll+fUSs3YrtMjOzMipwnGvWPXFXAleQlXi/A4wAhrdim8zM\nzMqmOYFuyYgYCRARr0bEaWQBz8zMakSRH9PTnNsLPk+TOr8q6QhgKrB06zbLzMzKqUZjVFk0J9D9\nDFgKOJqsr24Z4NDWbJSZmZWPUKEHoyy0dBkRj0fERxHxRkT8ICJ2j4hHKtE4MzOrLZKWlXSTpBcl\nvSDpG5KWk3SfpFfSn91L9j9F0kRJL0nasWT9JpImpG3naTHqpk3dMH4r6Rl0CxIR32vpRc3MrIJU\n0dLlucC/I2LP9JDuJYFfAKMi4kxJJwMnAydJ6g8MAQaQzaf8H0lrR0QdcBFwGPA4cDewE3BPSxrU\nVOnygpac0MzM2p5KDCSRtAywFXAwQETMAeZIGgxsnXa7CngAOAkYDNwQEZ8DkyRNBAZJeh3oFhGj\n03mvBvag3IEuIka15IRWZp2WhL4bVbsVZmbN0ReYCVwhaQPgCeAYoGdE1M+RPB3omZZ7Mf9sW1PS\nui/ScsP1LeJny5mZFUC7Mr2AHpLGlbwOL7lMB2Bj4KKI2Aj4hKxM+aWICJroFmsNzX3wqpmZ1ShR\n1tLlOxExsJFtU4ApEfF4en8TWaB7W9LKETFN0srAjLR9KrBqyfG907qpabnh+hZpdkYnqVNLL2Jm\nZvkXEdOBNyWtk1ZtBzwP3A4clNYdBNyWlm8HhkjqJKkv0A8Yk8qcH0raPI22PLDkmEXWnLkuBwGX\nk90/t1qqu/4wIn7a0ouamVllVfDp4D8FrksjLl8DDiFLqkZIGgpMBvYGiIjnJI0gC4ZzgaPSiEuA\nI8mmoOxCNgilRQNRoHmly/OAXYF/pYY9LWmbll7QzMwqr1KBLiKeAhZU2tyukf2HkU1G0nD9OGC9\ncrSpOaXLdhExucG6ugXuaWZm1sY0J6N7M5UvQ1J7srT05dZtlpmZlYtUmfvo2qrmBLofk5UvVwPe\nBv6T1pmZWY2oYB9dm7PQQBcRM8imaDEzsxpV4ISuWaMuL2MBN/dFxOEL2N3MzKxNaU7p8j8ly52B\n7wJvtk5zzMys3ASFfkxPc0qXw0vfS7oGeLjVWmRmZmVX5PkeW/LZ+/LVhJxmZmZtWnP66GbxVR9d\nO+A9GkzSaWZmbVuBK5dNB7o0x9gGfDWZ5rw087SZmdUISYXuo2uydJmC2t0RUZdeDnJmZlZTmtNH\n95QkP/nTzKyGZbOjLP6rFjVaupTUISLmAhsBYyW9SvYQPZElextXqI1mZraYPDPKgo0he1Ls7hVq\ni5mZtQLfR9c4AUTEqxVqi5mZWdk1FehWkHRcYxsj4q+t0B4zM2sFBU7omgx07YGupMzOzMxqlNxH\n15hpEfHbirXEzMysFSy0j87MzGqfCvwrvalAt13FWmFmZq0mG3VZ7VZUT6M3jEfEe5VsiJmZWWto\nzvPozMysxhU5o3OgMzMrABX4/oIiP4vPzMwKwBmdmVnOFX0wigOdmVne1fCTB8rBgc7MrACKPKmz\n++jMzCzXnNGZmeWc++jMzCz3Cly5dOnSzMzyzRmdmVnuiXae1NnMzPJKuHRpZmaWW87ozMzyzk8Y\nNzOzvCvyDeMOdGZmOec+OjMzsxxzRmdmVgAuXZqZWa4VOM65dGlmZvnmjM7MLOdEsbMaBzozs7wT\nqMC1yyIHeTMzKwBndGZmBVDcfM6Bzsws97IHrxY31DnQmZkVQHHDnPvozMws55zRmZkVQIErlw50\nZmb5J99eYGZmllfO6KwiLj7qW3xn4KrM/OAzBh57CwDdu3bkmuO3ZfUVujJ55scc8Of/8v4nc1ht\nha48dd73efmtDwAY8/IMjr7kUQA2WmN5Lv3pVnTp2IGR49/k+MtHA3D0butx8PZrM7cueOfDzzji\nwod4Y+bH1fmwVjb3jvw3Jxx3DHV1dRx86A/5+YknV7tJNanoM6MU+bNbBV1z/ysM/t3I+dad8N0N\neOCZt/j6T27igWfe4oTvbfDlttfe/ojNj/8Xmx//ry+DHMB5P9qSoy56mPWOupE1V+7GDhv1BuCp\nSe+y5c9vY9Bxt3LrY5MYduCmlflg1mrq6uo49uijuO2Oe3jymee58YbreeH556vdrJolqSyvZl6r\nvaQnJd2Z3i8n6T5Jr6Q/u5fse4qkiZJekrRjyfpNJE1I287TYtReHeisIh55fjrvffT5fOt2HbQa\n1z7wCgDXPvAKuw1arclzrNS9C0t3WYIxL88E4J8PTGS3zVYH4MFnpzF7Th0AY16eSa/llyr3R7AK\nGztmDGuuuRZ911iDjh07stc+Q7jzjtuq3SxrnmOAF0renwyMioh+wKj0Hkn9gSHAAGAn4G+S2qdj\nLgIOA/ql104tbYwDnVXNist2Yfqs2QBMnzWbFZft8uW2Pit2ZfRf9uDe3+3Mluv2BGCV5ZZi6ruf\nfLnP1Hc/YZXllvyf8x683dqMHD+llVtvre2tt6bSu/eqX77v1as3U6dOrWKLapvK9FrodaTewC7A\n30tWDwauSstXAXuUrL8hIj6PiEnARGCQpJWBbhExOiICuLrkmEXmPjprMyKyP6fP+pS1Dx/Oex9/\nzkZrLM+Ik7dn42NuadY5hmy1Jhuv1YNvn3ZXK7bUrMZUdlLnc4ATgaVL1vWMiGlpeTrQMy33AkaX\n7DclrfsiLTdc3yIVy+gkPbrwvapL0h6SQtLXStb1kbRfyfsNJe28GNd4XVKPxW1rHsx4fzYrdc+y\nuJW6d2HmB1l2N2fuPN77OCtzPvnau7w2/SP6rbIMb733yXwlyV7LL8Vb73365ftt1l+Fk/bckD3P\nuI85c+dV8JNYa1hllV5MmfLml++nTp1Cr14t/l1n5dND0riS1+H1GyTtCsyIiCcaOzhlaFGJhtar\nWKCLiC0qda3FsC/wcPqzXh9gv5L3GwItDnT2lbvGvsEBW/cD4ICt+3HnmDcA6NGtM+3aZd8++/Rc\nmrVW7saktz9k+qzZfDT7CwatvQIA+229FneOmQzABn2X54IjtmTPM+5j5gefVeHTWLkN3HRTJk58\nhdcnTWLOnDncOPwGdtl192o3qybVj7osxwt4JyIGlrwuLbnUlsDukl4HbgC2lXQt8HYqR5L+nJH2\nnwqsWnJ877RualpuuL5FKla6lPRxRHRNH3I40C1d/8cR8dAC9m8PXA4MJIv+/4iIsyWtCVwIrAB8\nChwWES9K2gs4HagDPoiIrSQNAK4AOpL9jL4fEa800r6uwDeBbYA70rkAzgTWlfQUcD1wFNBF0jeB\nM4BJwLlAZ2A2cEhEvJTa/0eyDtR5wGURcX7J9boAtwC3RMRli/r3WWuu+tnWfGu9lemxdGcmXjaE\n390wnj8TgU95AAAXFElEQVTf8gzXnrAtB223Nm/M/JgD/vJfAL7ZfyV+OWRjvqibx7wIfnrJI8z6\neA4Ax1z6aLq9oD33jp/yZV/cHw7clKU6L8F1J2wLwJvvfMxeZ/ynOh/WyqJDhw6cfe4F7LbLjtTV\n1XHQwYfSf8CAajerZlWidBkRpwCnpOttDZwQEQdI+hNwENnv04OA+lFFtwP/lPRXYBWyQSdjIqJO\n0oeSNgceBw4EzqeFFFGZDLIk0B0PdI6IYSkYLBkRHy1g/02AMyPi2+n9shHxvqRRwBER8YqkzYAz\nImJbSROAnSJiasm+5wOjI+I6SR2B9hExu5H27Q9sGxFDU5n1pxHxRMkPa9e038HAwIj4SXrfDfg0\nIuZK2p4scH9f0o+B7YAhadtyEfFe+qazNVlH7dURcfUC2nI4kJUDuiy3SeedzmrB37jlxawRQ6vd\nBKuyLkvoiYgY2NLj1xqwQfz5+pEL37EZvrvBys1qS+nvTknLAyOA1YDJwN4R8V7a71TgUGAucGxE\n3JPWDwSuBLoA95D9Tm5RwKrGYJSxwD8kLQH8KyKeamS/14A1UrC6C7g3ZV1bADeWfDvplP58BLhS\n0giyTAngMeDUNArolsayuWRfsswMspR7X6DROnOJZYCrJPUjyzyXSOu3By6OiLkA9T/U5DbgrIi4\nbkEnTKWASwHade9T0Vq2mVk5RMQDwANp+V2yL/4L2m8YMGwB68cB65WjLRW/vSAiHgS2Iqu3Xinp\nwEb2mwVsQPYXdQRZBtQOeD8iNix5rZv2PwI4jaze+4Sk5SPin8DuZCXFuyVtu6BrSVoO2Bb4e8q4\nfg7s3cwbFH8H3B8R6wG7kZUwF+YRYKfFuQHSzGxRSOV51aKKBzpJqwNvp36pvwMbN7JfD6BdRNxM\nFsA2jogPgUmpPw5lNkjLa0bE4xHxK2AmsKqkNYDXIuI8sixq/UaatSdwTUSsHhF9ImJVsr63bwEf\nMf8w2Ybvl+GrTtKDS9bfB/xIUofUvuVKtv0KmEXW12hm1qqywSgqy6sWVeOG8a2BpyU9CezDV+XC\nhnoBD6RBINeSOjiB/YGhkp4GniO74RDgT2m6mGeBR4Gngb2BZ9M51iO76XBB9gVubbDu5rT+GaBO\n0tOSfgbcD/SX9JSkfYCzgDPS5yktBf8deAN4JrV1v/lPzzFkg1rcAWdm1ooqNhjFWqZd9z7RaZtf\nVrsZVkUejGKLOxil34AN4uzh95alLbt9faXFaks1eGYUM7PcE6rRsmM5tIlAJ+lxvho9We8HETGh\nzNdZnmxC0Ya2S6OCzMwsZ9pEoIuIzSp0nXfJZjYxMyuUWh0xWQ5tItCZmVnrqR91WVQOdGZmeVfD\n98CVg59HZ2ZmueaMzsysAIqc0TnQmZkVQJFvL3Dp0szMcs0ZnZlZzgloV9yEzoHOzKwIXLo0MzPL\nKWd0ZmYF4FGXZmaWa0UuXTrQmZnlXNEHo7iPzszMcs0ZnZlZ7vl5dGZmlmee1NnMzCy/nNGZmRVA\ngRM6Bzozs7zLRl0WN9S5dGlmZrnmjM7MrACKm8850JmZFUOBI51Ll2ZmlmvO6MzMCsA3jJuZWa4V\neNClA52ZWREUOM65j87MzPLNGZ2ZWREUOKVzoDMzyzlR7MEoLl2amVmuOaMzM8u7gj+mx4HOzKwA\nChznXLo0M7N8c0ZnZlYEBU7pHOjMzHJPhR516UBnZlYARR6M4j46MzPLNWd0ZmY5JwrdRedAZ2ZW\nCAWOdC5dmplZrjmjMzMrAI+6NDOzXPOoSzMzs5xyRmdmVgAFTugc6MzMcq/g9xc40JmZFUCRB6O4\nj87MzMpC0qqS7pf0vKTnJB2T1i8n6T5Jr6Q/u5ccc4qkiZJekrRjyfpNJE1I286TWj6cxoHOzCzn\nRDbqshyvhZgLHB8R/YHNgaMk9QdOBkZFRD9gVHpP2jYEGADsBPxNUvt0rouAw4B+6bVTSz+/A52Z\nWQGoTK+mRMS0iBiflj8CXgB6AYOBq9JuVwF7pOXBwA0R8XlETAImAoMkrQx0i4jRERHA1SXHLDIH\nOjMzKztJfYCNgMeBnhExLW2aDvRMy72AN0sOm5LW9UrLDde3iAejmJkVQfnGovSQNK7k/aURcel8\nl5K6AjcDx0bEh6XdaxERkqJsrWkGBzozswIo46jLdyJiYKPXkZYgC3LXRcQtafXbklaOiGmpLDkj\nrZ8KrFpyeO+0bmpabri+RVy6NDOzskgjIy8HXoiIv5Zsuh04KC0fBNxWsn6IpE6S+pINOhmTypwf\nSto8nfPAkmMWmTM6M7MCqNBcl1sCPwAmSHoqrfsFcCYwQtJQYDKwN0BEPCdpBPA82YjNoyKiLh13\nJHAl0AW4J71axIHOzKwAKhHnIuLhJi61XSPHDAOGLWD9OGC9crTLpUszM8s1Z3RtXLw/+Z3Pbv3h\n5Gq3o8p6AO9UuxHV0mWJH1a7CW1Bof8NAKsv9hmKOwOYA11bFxErVLsN1SZpXFOjvCz//G9g8WQ3\nexc30jnQmZnlXfOm78ot99GZmVmuOaOzWnDpwnexnPO/gcVU4ITOgc7avobTC1nx+N9AGRQ40rl0\naWZmueaMzsws9+RRl2Zmlm8edWlWQJK6S1r8G3Gt5klFDgP550BnhSSpE/B74MA0a7oVSH1gk7SS\npHbpKda5Va6ni9fqtwEHOiukiPgcuAboC3xP0hpVbpJVgKRuJctfB84GulavRRVU4EjnQGeFoyQi\nRgOXAOvjYJd7kjoD90v6Ucrg3gE+Sk/Abp/2qdFf5QunMv1XizwYxQolBbiQ1EfSjIh4XNInwM/T\n5psiYlK122nlFxGfSToJuFDSXOCBkm116c9clzCLyoHOCiUFud2BE4EXJL1A9kTkPwAnAftJuj4i\nXqtmO628Uj/cvIj4j6QDgOHAJsByks4EppFVuD4HLspjwMtvrrpwLl1aoUj6BvArYE/gM+Ag4GRg\nJvAXoH/1WmetIWXx8yTtIOm3ETEW2B/YHugNTACWAnoCT+YxyEGhu+gc6KwYJNX/W18Z+BGwMfAN\n4LfARsBvyPpsDnc2ly8pi98GuAj4b1r3GFmw6w7Mjog/RMTJab3ljAOd5VrJ4IJlACLiloh4AtgN\nGBoRNwOTgW7AMhHxSXVaaq0hjTtqBwwGhkXEA5I6pFLmWOBw4GxJq9cPSMml9JiecrxqkQOd5Vr6\nNv8dYKSkMyRtlzYtBQxLpcxNgAsi4uWqNdRaRWTmATOA3pI6RsTcVMrcFHgc+HpETK4fkJJfxS1e\nOtBZrknqRVaq/D0wF9hV0k7AEcB7wOlk3/THVq+VVk4lN4P3lbSCpI7AGLL+1w0kdZG0PnAesFZE\nfFjF5loFeNSl5ZakQWSDDSZGxO2SHgF+AOwEdIqIAyUtHREf1d92UNUGW1mkLH5n4EzgLmANsv64\nvsBxZGXqFYAzI+LZqjW0gkTtlh3LwRmd5ZKkrYHbyEbWHSdph4h4F7iSbCj5DpJWjIiPwPdP5Ymk\ngcCfgO8DU4GtgFHAdcBQslG2B0TELXm+Qbyh4hYundFZDqUZTo4ADo6IkZIeAm6StHdE/FvSJcCy\nETGjui21ckjzlraPiE8lrQjUkQW51YCDyUbVXgTcD+wcERPqjy3SF5zihPT/5YzOcqGkX2ZjsjLV\nSsC2kpaKiOuBHwJ3S9o5It6PiNer11orlzRSclPgAEn7Ab8A3gJeAnYEzomI6cCjZDeDr1Ottlr1\nOKOzXEj9MluSDS45DphE9gvw+5JujIgRaZh5zkfWFUtE1EmaQVaO3JTsPsi3JS0BzAP6S9of+C5w\nSES8VMXmVlWtzlNZDs7oLBfSTPQHAremAQY3As8DG5J92+8SETekUmZx/4/PCUlLSlorvW0HLElW\nmlxP0koR8QVwBdltJLsCfy1ykAMK3UnnjM7yYgCwHjBPUs/0rf4fZLcWrA8sRzYwoVD9Mjm2GnCk\npNnAmmSl6aWBA4BjyTK8qWRfeMZExByPrC0uZ3RWk0r65PqkZ4zdRPYEgmWBbST1SN/qLwXOioip\n1WuttYKJZH1uRwDPpGnbngHuBpaQdHt6Pyci5oC/4BQ4oXNGZ7Wnfib6dOP3H4GngX5kI+0uIRtC\n3knSXRHxDvBm9Vpr5SKpe0TMAoiIuZKeBM4huwl8z4i4iex5czPIBp18GBFjqtjkNqOWp+8qBwc6\nqxmpn212CnJrkAW5n5CNqDsWGE02WfO/yILevVVrrJWVpGWBVyTdCTwdEWdHxD/TtgOBQyTNAl4l\nu3fy/PTvxOVKc6Cz2iCpO9mN349ExL+BWcBTEfGQpPYR8RdJqwCHRsRZkkZHxLTqttrKaB7wINnP\nfX1J9wHnA+Mi4ur0INVfk81+cmia37Lw5cpSHnVp1vYtS/bLbjtJ2wKfAutKOr5kMt7XgK5p+a0q\ntNFaSZqP8j9kj1Y6HLiQ7AkU96SJup8Adie7IdyZ/IIUuJPOGZ21aZKWAj6PiElpFOW+ZPdEzSB7\neOojklYmK1kdRvbkcH+Tz5H68mNE/E3SJmRPmxhLVrp+BDgG+Bg4NiKeqWJTrY1yRmdtVuqHexz4\nh6TNgCXIBh9MJbtnrhswiKyctQpwkr/N50+aDEBppO2TZP2y9wCXRcShwFHAiZ7SrWkFTuic0Vmb\n9j7Z/W8HAOPJRlNeQVaefJvsHrmLI2JY/QEefJBP9T9TSdeQlS5fiIg/p20eVdsMRR516YzO2qQ0\nwOQ9shvBXwJ6ks1C/z6wNllGdyRwhqRl6o9zkMuvdFvJB2RTvM2QtJxnuWkule2/WuSMztqkNIdh\n+4iYJen/kWV0cyLidLJS5kCyuQ1fT7/8LAeaysjrR1KSPTB3A6Czv9hYczjQWZuVgl2HiJiRnkow\nVlLXiDg+IsYB48Dlyryo/zlK2hHoDNydZreZT0SMl7RfRHhkbTP5watmbViaAaNDGmgwEDhY0tkN\n9nGQy4GSJ4OfA3y0oCCXxqS0j4gpkpZKz6Iza5IDnbUJTfW1pGDXPiJmAusCt1euZVYJktqlvtYT\ngaMi4r+SvinpIEkbluzaLmX6ywIPA2tUpcFWU1y6tKprTsmqtIxJdg+dS5Y5UPIz7BIRH0gaC3xP\n0lHAbLJRt6sAT6Wf/9wUEG8Ejo6IF6rX+tri0qVZFTWnZFW/K3z5LLKODnK1reQLzs7A8JTV3ws8\nR/b8uAOAi4FvpnlO56ap4O4AfhMRD1Wv9bXHoy7NqiQ99XtpGpSsyJ4x9nREPJX2a19Ssrof2A/w\nt/kaVpLF/wk4Jn1xuS+9SKNt/0B2M/jsdNiuwOkR8XA12my1yYHOqsIlK0tfcgaSPXligqR9yJ4v\ndxHZFF97ks12c3fJNGDXVK/FNcyP6TGrrAYlqyMl7UZWsloLuD4iHpG0O/Cj+kfzpJLVbcAv/G2+\ndpX2q6bH6LwN/IWs3/Xe9DoMGEkW5D6tWmNzpJan7yoHBzqrOJesiqnkC853gG8BdWT/Bh4HZqVb\nBnoBg4HlImJS/bHujy2DAkc6D0axiltQyUrS/ZL2ltSXBiUrgIi4JiLur2KzbTGlILcT8FvgTmBr\nssEmz6Ygtw9wN/DH0iBntric0VlFuGRlyebAD4B1yHKMk1MAbA/MAU6IiPt860j51eqIyXJwoLNW\n55JVcZX87JeNiPeBLsBZwJLAgRHxhqQ9gJ4RcUn9cf65l1+RB6O4dGmtziWrYioJcjsBv5LUGbiK\n7MGp/xcRr0n6FtkDVCdWs62Wb87orFJcsiqY9PPdDjgXOCQiPgOeTwORhktaB/gacFxEjKpmW4ug\nwAmdA521DpesLH2J2Yvs5/6YpP2AbwOPAhuS3Su5pLP4CqlQpEsZ/LlAe+DvEXFmZa7cOJcurexc\nsjLI5icF7iGb9eY+4OvAg8B3gRUi4m0HuXxJX24uBL4D9Af2ldS/uq1yRmetwCUrqxcRt0l6E/gg\nIl6V9HXgx8C8hRxqZVahUZeDgIkR8RqApBvIBpk9X4mLN8aBzsrOJSurl7L78Wl5R+Bssv7Z6dVt\nWbFU8MGrvYA3S95PATaryJWb4EBnZZcmX76HLNDtSzZv4YNkwe+u8JOhc6WkVN0uIubL1Or7XCUt\nSdY/e2xE3OtBR5U1fvwTI7ssoR5lOl1nSeNK3l8aEZeW6dytwoHOWoVLVsVQEuS2A7pKGplK1fOJ\niE8l3Z767TzoqMIiYqcKXWoqsGrJ+95pXVV5MIq1ivqSVQpyOwLDgd+7ZJUf6dFJ9YOOLiK7+f9/\nglzJvnWSukhavrIttQoaC/ST1FdSR2AIcHuV2+RAZy1XPw9lmrtyPo2UrG6vP8Zql6S1JC2dAld3\n4JfAERHxoKRvSTpI0qCS/UufJfgAWR+t5VBEzAV+QjaV3wvAiIh4rrqtArmCYC3RsGQFLLBklfZt\nX1+ystonaUuyp72PTvOW/h7oS/bFuR3wBfB6RJym+Z8leBPwu4h4sGqNt0JyRmeLzCWrYouIR4AJ\nwGuSugFXAmOA8yNiH2AEMEBSxxTkugO3Ar91kLNqcKCzZnPJyupFxEfAMWS3jLwTEedGxKNpIoDf\nkc2IMSftvi9Z/+xDVWquFZxLl9ZsLllZQ8qeEn8+2aw3n5HNdvOfiLjDtxBYW+FAZ4tE0tJkZav1\ngRWBXYCx6dv87sAhwD4RMSdlfTeTPRnc3+ZzKpWwryCbsHtORHxW0ofrYGdV50Bni0zSYGAY8M00\nYTOpZHUB8IuIuCutOxJ4MSL+W7XGWkVI2gX4OCL+r9ptMWvIgc5axCUrWxD/7K0tcqCzFnPJysxq\ngQOdLRaXrMysrXOgs7JwBmdmbZUDnZmZ5ZpvGDczs1xzoDMzs1xzoDMzs1xzoDNrQFKdpKckPSvp\nxvSooZaea2tJd6bl3SWd3MS+y6ab7Bf1Gr+WdEJz1zfY50pJey7CtfpIenZR22hWTQ50Zv9rdkRs\nGBHrAXOAI0o3KrPI/+9ExO0RcWYTuywLLHKgM7OmOdCZNe0hYK2Uybwk6WrgWWBVSTtIekzS+JT5\ndYXsRnpJL0oaD3yv/kSSDpZ0QVruKelWSU+n1xbAmcCaKZv8U9rv55LGSnpG0m9KznWqpJclPUx2\nw36TJB2WzvO0pJsbZKnbSxqXzrdr2r+9pD+VXPtHi/sXaVYtDnRmjZDUAfgO2STWAP2Av0XEAOAT\n4DRg+4jYGBgHHCepM3AZsBvZ9GgrNXL684D/i4gNgI2B54CTgVdTNvlzSTukaw4CNgQ2kbSVpE2A\nIWndzsCmzfg4t0TEpul6LwBDS7b1SdfYBbg4fYahwAcRsWk6/2GS+jbjOmZtTodqN8CsDeoi6am0\n/BBwObAKMDkiRqf1mwP9gUckAXQEHgO+BkyKiFcAJF0LHL6Aa2wLHAiQnr7+QXraQ6kd0uvJ9L4r\nWeBbGrg1Ij5N17i9GZ9pvfRYpWXTeUaWbBsREfOAVyS9lj7DDsD6Jf13y6Rrv9yMa5m1KQ50Zv9r\ndkRsWLoiBbNPSlcB90XEvg32m++4xSTgjIi4pME1jm3Bua4E9oiIpyUdDGxdsq3hrBGRrv3TiCgN\niEjq04Jrm1WVS5dmLTMa2FLSWgCSlpK0NvAi0EfSmmm/fRs5fhTw43Rs+/SA2o/IsrV6I4FDS/r+\neklaEXgQ2ENSl/R8wN2a0d6lgWmSlgD2b7BtL0ntUpvXAF5K1/5x2h9Ja0taqhnXMWtznNGZtUBE\nzEyZ0fWSOqXVp0XEy5IOB+6S9ClZ6XPpBZziGOBSSUOBOuDHEfGYpEfS8P17Uj/dusBjKaP8GDgg\nIsZLGg48DcwAxjajyb8EHgdmpj9L2/QGMAboBhyRnkLxd7K+u/HKLj4T2KN5fztmbYvnujQzs1xz\n6dLMzHLNgc7MzHLNgc7MzHLNgc7MzHLNgc7MzHLNgc7MzHLNgc7MzHLNgc7MzHLt/wPNHr4D+71W\ndQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f325d3e14e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = labels2_pred_vae_arr[:,0], y_true = labels2_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, output_columns_2labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[    0 12042     0     0     0]\n",
      " [    0  8271     0     0     0]\n",
      " [    0  2037     0     0     0]\n",
      " [    0   183     0     0     0]\n",
      " [    0    11     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAIkCAYAAADyCkRHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35206a5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_5labels = confusion_matrix(y_pred = labels5_pred_vae_arr[:,0], y_true = labels5_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[8,8])\n",
    "plot_confusion_matrix(cm_5labels, output_columns_5labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Implemented Grad clip, getting good accuracy!",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
