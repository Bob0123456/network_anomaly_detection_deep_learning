{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"x\"]\n",
    "\n",
    "kdd_train = pd.read_csv(\"dataset/KDDTrain+.txt\",names = col_names,)\n",
    "kdd_test = pd.read_csv(\"dataset/KDDTest+.txt\",names = col_names,)\n",
    "\n",
    "kdd_train = kdd_train.drop(\"x\", axis = 1)\n",
    "kdd_test = kdd_test.drop(\"x\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"protocol_type\",\"service\",\"flag\"]\n",
    "for cv in category_variables:\n",
    "    kdd_train[cv] = kdd_train[cv].astype(\"category\")\n",
    "kdd_train[\"label\"] = kdd_train[\"label\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125966</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125967</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>udp</td>\n",
       "      <td>private</td>\n",
       "      <td>SF</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>klogin</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0              0           tcp  ftp_data   SF        491          0     0   \n",
       "1              0           udp     other   SF        146          0     0   \n",
       "2              0           tcp   private   S0          0          0     0   \n",
       "3              0           tcp      http   SF        232       8153     0   \n",
       "4              0           tcp      http   SF        199        420     0   \n",
       "5              0           tcp   private  REJ          0          0     0   \n",
       "6              0           tcp   private   S0          0          0     0   \n",
       "...          ...           ...       ...  ...        ...        ...   ...   \n",
       "125966         0           tcp   private   S0          0          0     0   \n",
       "125967         0           tcp      http   SF        359        375     0   \n",
       "125968         0           tcp   private   S0          0          0     0   \n",
       "125969         8           udp   private   SF        105        145     0   \n",
       "125970         0           tcp      smtp   SF       2231        384     0   \n",
       "125971         0           tcp    klogin   S0          0          0     0   \n",
       "125972         0           tcp  ftp_data   SF        151          0     0   \n",
       "\n",
       "        wrong_fragment  urgent  hot   ...     dst_host_srv_count  \\\n",
       "0                    0       0    0   ...                     25   \n",
       "1                    0       0    0   ...                      1   \n",
       "2                    0       0    0   ...                     26   \n",
       "3                    0       0    0   ...                    255   \n",
       "4                    0       0    0   ...                    255   \n",
       "5                    0       0    0   ...                     19   \n",
       "6                    0       0    0   ...                      9   \n",
       "...                ...     ...  ...   ...                    ...   \n",
       "125966               0       0    0   ...                     13   \n",
       "125967               0       0    0   ...                    255   \n",
       "125968               0       0    0   ...                     25   \n",
       "125969               0       0    0   ...                    244   \n",
       "125970               0       0    0   ...                     30   \n",
       "125971               0       0    0   ...                      8   \n",
       "125972               0       0    0   ...                     77   \n",
       "\n",
       "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                         0.17                    0.03   \n",
       "1                         0.00                    0.60   \n",
       "2                         0.10                    0.05   \n",
       "3                         1.00                    0.00   \n",
       "4                         1.00                    0.00   \n",
       "5                         0.07                    0.07   \n",
       "6                         0.04                    0.05   \n",
       "...                        ...                     ...   \n",
       "125966                    0.05                    0.07   \n",
       "125967                    1.00                    0.00   \n",
       "125968                    0.10                    0.06   \n",
       "125969                    0.96                    0.01   \n",
       "125970                    0.12                    0.06   \n",
       "125971                    0.03                    0.05   \n",
       "125972                    0.30                    0.03   \n",
       "\n",
       "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                              0.17                         0.00   \n",
       "1                              0.88                         0.00   \n",
       "2                              0.00                         0.00   \n",
       "3                              0.03                         0.04   \n",
       "4                              0.00                         0.00   \n",
       "5                              0.00                         0.00   \n",
       "6                              0.00                         0.00   \n",
       "...                             ...                          ...   \n",
       "125966                         0.00                         0.00   \n",
       "125967                         0.33                         0.04   \n",
       "125968                         0.00                         0.00   \n",
       "125969                         0.01                         0.00   \n",
       "125970                         0.00                         0.00   \n",
       "125971                         0.00                         0.00   \n",
       "125972                         0.30                         0.00   \n",
       "\n",
       "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                       0.00                      0.00                  0.05   \n",
       "1                       0.00                      0.00                  0.00   \n",
       "2                       1.00                      1.00                  0.00   \n",
       "3                       0.03                      0.01                  0.00   \n",
       "4                       0.00                      0.00                  0.00   \n",
       "5                       0.00                      0.00                  1.00   \n",
       "6                       1.00                      1.00                  0.00   \n",
       "...                      ...                       ...                   ...   \n",
       "125966                  1.00                      1.00                  0.00   \n",
       "125967                  0.33                      0.00                  0.00   \n",
       "125968                  1.00                      1.00                  0.00   \n",
       "125969                  0.00                      0.00                  0.00   \n",
       "125970                  0.72                      0.00                  0.01   \n",
       "125971                  1.00                      1.00                  0.00   \n",
       "125972                  0.00                      0.00                  0.00   \n",
       "\n",
       "        dst_host_srv_rerror_rate    label  \n",
       "0                           0.00   normal  \n",
       "1                           0.00   normal  \n",
       "2                           0.00  neptune  \n",
       "3                           0.01   normal  \n",
       "4                           0.00   normal  \n",
       "5                           1.00  neptune  \n",
       "6                           0.00  neptune  \n",
       "...                          ...      ...  \n",
       "125966                      0.00  neptune  \n",
       "125967                      0.00   normal  \n",
       "125968                      0.00  neptune  \n",
       "125969                      0.00   normal  \n",
       "125970                      0.00   normal  \n",
       "125971                      0.00  neptune  \n",
       "125972                      0.00   normal  \n",
       "\n",
       "[125973 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>125973.00000</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>1.259730e+05</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "      <td>125973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>287.14465</td>\n",
       "      <td>4.556674e+04</td>\n",
       "      <td>1.977911e+04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.279250</td>\n",
       "      <td>...</td>\n",
       "      <td>182.148945</td>\n",
       "      <td>115.653005</td>\n",
       "      <td>0.521242</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.284452</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.120240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2604.51531</td>\n",
       "      <td>5.870331e+06</td>\n",
       "      <td>4.021269e+06</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>23.942042</td>\n",
       "      <td>...</td>\n",
       "      <td>99.206213</td>\n",
       "      <td>110.702741</td>\n",
       "      <td>0.448949</td>\n",
       "      <td>0.188922</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>0.112564</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.306557</td>\n",
       "      <td>0.319459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.760000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42908.00000</td>\n",
       "      <td>1.379964e+09</td>\n",
       "      <td>1.309937e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7479.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes           land  \\\n",
       "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
       "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
       "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
       "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
       "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
       "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
       "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
       "mean         0.022687       0.000111       0.204409           0.001222   \n",
       "std          0.253530       0.014366       2.149968           0.045239   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      77.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  125973.000000    125973.000000            ...              \n",
       "mean        0.395736         0.279250            ...              \n",
       "std         0.489010        23.942042            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         1.000000         0.000000            ...              \n",
       "max         1.000000      7479.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   125973.000000       125973.000000           125973.000000   \n",
       "mean       182.148945          115.653005                0.521242   \n",
       "std         99.206213          110.702741                0.448949   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%         82.000000           10.000000                0.050000   \n",
       "50%        255.000000           63.000000                0.510000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           125973.000000                125973.000000   \n",
       "mean                 0.082951                     0.148379   \n",
       "std                  0.188922                     0.308997   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.020000                     0.000000   \n",
       "75%                  0.070000                     0.060000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                125973.000000         125973.000000   \n",
       "mean                      0.032542              0.284452   \n",
       "std                       0.112564              0.444784   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.020000              1.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             125973.000000         125973.000000   \n",
       "mean                   0.278485              0.118832   \n",
       "std                    0.445669              0.306557   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    1.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             125973.000000  \n",
       "mean                   0.120240  \n",
       "std                    0.319459  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22544.000000</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>2.254400e+04</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "      <td>22544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.859076</td>\n",
       "      <td>1.039545e+04</td>\n",
       "      <td>2.056019e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.105394</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>193.869411</td>\n",
       "      <td>140.750532</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.097814</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.226683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1407.176612</td>\n",
       "      <td>4.727864e+05</td>\n",
       "      <td>2.121930e+04</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.928428</td>\n",
       "      <td>0.150328</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>7.269597</td>\n",
       "      <td>...</td>\n",
       "      <td>94.035663</td>\n",
       "      <td>111.783972</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.273139</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.400875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870000e+02</td>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57715.000000</td>\n",
       "      <td>6.282565e+07</td>\n",
       "      <td>1.345927e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
       "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
       "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
       "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
       "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
       "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
       "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
       "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
       "\n",
       "             urgent           hot  num_failed_logins     logged_in  \\\n",
       "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
       "mean       0.000710      0.105394           0.021647      0.442202   \n",
       "std        0.036473      0.928428           0.150328      0.496659   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      1.000000   \n",
       "max        3.000000    101.000000           4.000000      1.000000   \n",
       "\n",
       "       num_compromised            ...             dst_host_count  \\\n",
       "count     22544.000000            ...               22544.000000   \n",
       "mean          0.119899            ...                 193.869411   \n",
       "std           7.269597            ...                  94.035663   \n",
       "min           0.000000            ...                   0.000000   \n",
       "25%           0.000000            ...                 121.000000   \n",
       "50%           0.000000            ...                 255.000000   \n",
       "75%           0.000000            ...                 255.000000   \n",
       "max         796.000000            ...                 255.000000   \n",
       "\n",
       "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count        22544.000000            22544.000000            22544.000000   \n",
       "mean           140.750532                0.608722                0.090540   \n",
       "std            111.783972                0.435688                0.220717   \n",
       "min              0.000000                0.000000                0.000000   \n",
       "25%             15.000000                0.070000                0.000000   \n",
       "50%            168.000000                0.920000                0.010000   \n",
       "75%            255.000000                1.000000                0.060000   \n",
       "max            255.000000                1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                 22544.000000                 22544.000000   \n",
       "mean                      0.132261                     0.019638   \n",
       "std                       0.306268                     0.085394   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.030000                     0.010000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count          22544.000000              22544.000000          22544.000000   \n",
       "mean               0.097814                  0.099426              0.233385   \n",
       "std                0.273139                  0.281866              0.387229   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.360000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count              22544.000000  \n",
       "mean                   0.226683  \n",
       "std                    0.400875  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.170000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          normal\n",
       "1          normal\n",
       "2         neptune\n",
       "3          normal\n",
       "4          normal\n",
       "5         neptune\n",
       "6         neptune\n",
       "           ...   \n",
       "125966    neptune\n",
       "125967     normal\n",
       "125968    neptune\n",
       "125969     normal\n",
       "125970     normal\n",
       "125971    neptune\n",
       "125972     normal\n",
       "Name: label, dtype: category\n",
       "Categories (23, object): [back, buffer_overflow, ftp_write, guess_passwd, ..., spy, teardrop, warezclient, warezmaster]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_types = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "is_sess = {\n",
    "    \"DoS\":\"Attack\",\n",
    "    \"R2L\":\"Attack\",\n",
    "    \"U2R\":\"Attack\",\n",
    "    \"Probe\":\"Attack\",\n",
    "    \"normal\":\"Normal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_train[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])\n",
    "kdd_test[\"sess_type\"] = kdd_train.label.map(lambda x: sess_types[x])\n",
    "kdd_test[\"is_sess\"] = kdd_train.sess_type.map(lambda x: is_sess[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_sess_type_group = kdd_train.groupby(\"sess_type\")\n",
    "kdd_is_sess_group = kdd_train.groupby(\"is_sess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess_type\n",
       "DoS       45927\n",
       "Probe     11656\n",
       "R2L         995\n",
       "U2R          52\n",
       "normal    67343\n",
       "Name: is_sess, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.is_sess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sess\n",
       "Attack    58630\n",
       "Normal    67343\n",
       "Name: sess_type, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.sess_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sess_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">DoS</th>\n",
       "      <th>count</th>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>4.592700e+04</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "      <td>45927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.090034</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>1.692015e+02</td>\n",
       "      <td>244.600475</td>\n",
       "      <td>0.066333</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.747922</td>\n",
       "      <td>26.524005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.748494</td>\n",
       "      <td>1.176321e+03</td>\n",
       "      <td>32.656346</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.445748</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>1.168004e+03</td>\n",
       "      <td>41.324475</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.188947</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.431707</td>\n",
       "      <td>48.303117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299931</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>7.686120e+03</td>\n",
       "      <td>94.667526</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "sess_type                                                                    \n",
       "DoS       count  45927.000000   45927.000000  4.592700e+04    45927.000000   \n",
       "          mean     178.090034       0.065403  1.692015e+02      244.600475   \n",
       "          std      104.445748       0.064023  1.168004e+03       41.324475   \n",
       "          min        1.000000       0.000000  0.000000e+00        1.000000   \n",
       "          25%      109.000000       0.050000  0.000000e+00      255.000000   \n",
       "          50%      172.000000       0.060000  0.000000e+00      255.000000   \n",
       "          75%      249.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                       ...            ...           ...             ...   \n",
       "normal    mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "          std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "          min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "          25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "          50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "          75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "          max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "                 dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "sess_type                                                       \n",
       "DoS       count            45927.000000          45927.000000   \n",
       "          mean                 0.066333              0.157569   \n",
       "          std                  0.058079              0.358934   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.050000              0.000000   \n",
       "          50%                  0.070000              0.000000   \n",
       "          75%                  0.070000              0.000000   \n",
       "...                                 ...                   ...   \n",
       "normal    mean                 0.040134              0.046589   \n",
       "          std                  0.128529              0.195306   \n",
       "          min                  0.000000              0.000000   \n",
       "          25%                  0.000000              0.000000   \n",
       "          50%                  0.000000              0.000000   \n",
       "          75%                  0.020000              0.000000   \n",
       "          max                  1.000000              1.000000   \n",
       "\n",
       "                 dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "sess_type                                                              \n",
       "DoS       count                 45927.000000            45927.000000   \n",
       "          mean                      0.049492                0.123423   \n",
       "          std                       0.188947                0.228287   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.020000   \n",
       "          50%                       0.000000                0.050000   \n",
       "          75%                       0.000000                0.080000   \n",
       "...                                      ...                     ...   \n",
       "normal    mean                      0.121726                0.811875   \n",
       "          std                       0.254382                0.324091   \n",
       "          min                       0.000000                0.000000   \n",
       "          25%                       0.000000                0.750000   \n",
       "          50%                       0.010000                1.000000   \n",
       "          75%                       0.080000                1.000000   \n",
       "          max                       1.000000                1.000000   \n",
       "\n",
       "                 dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "sess_type                                                       ...         \n",
       "DoS       count          45927.000000        45927.000000       ...         \n",
       "          mean               0.747922           26.524005       ...         \n",
       "          std                0.431707           48.303117       ...         \n",
       "          min                0.000000            1.000000       ...         \n",
       "          25%                0.180000            6.000000       ...         \n",
       "          50%                1.000000           13.000000       ...         \n",
       "          75%                1.000000           20.000000       ...         \n",
       "...                               ...                 ...       ...         \n",
       "normal    mean               0.013930          190.285761       ...         \n",
       "          std                0.092006           92.608377       ...         \n",
       "          min                0.000000            0.000000       ...         \n",
       "          25%                0.000000          121.000000       ...         \n",
       "          50%                0.000000          255.000000       ...         \n",
       "          75%                0.000000          255.000000       ...         \n",
       "          max                1.000000          255.000000       ...         \n",
       "\n",
       "                 same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "sess_type                                                                  \n",
       "DoS       count   45927.000000  45927.000000  4.592700e+04  45927.000000   \n",
       "          mean        0.191887      0.748494  1.176321e+03     32.656346   \n",
       "          std         0.299931      0.432559  7.686120e+03     94.667526   \n",
       "          min         0.000000      0.000000  0.000000e+00      1.000000   \n",
       "          25%         0.040000      0.290000  0.000000e+00      5.000000   \n",
       "          50%         0.070000      1.000000  0.000000e+00     11.000000   \n",
       "          75%         0.150000      1.000000  0.000000e+00     18.000000   \n",
       "...                        ...           ...           ...           ...   \n",
       "normal    mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "          std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "          min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "          25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "          50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "          75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "          max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "                 srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "sess_type                                                               \n",
       "DoS       count        45927.000000     45927.000000     45927.000000   \n",
       "          mean             0.005317         0.153000         0.746678   \n",
       "          std              0.056390         0.357561         0.434050   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         1.000000   \n",
       "          75%              0.000000         0.000000         1.000000   \n",
       "...                             ...              ...              ...   \n",
       "normal    mean             0.126263         0.044629         0.012083   \n",
       "          std              0.271621         0.202264         0.086426   \n",
       "          min              0.000000         0.000000         0.000000   \n",
       "          25%              0.000000         0.000000         0.000000   \n",
       "          50%              0.000000         0.000000         0.000000   \n",
       "          75%              0.110000         0.000000         0.000000   \n",
       "          max              1.000000         1.000000         1.000000   \n",
       "\n",
       "                 su_attempted        urgent  wrong_fragment  \n",
       "sess_type                                                    \n",
       "DoS       count  45927.000000  45927.000000    45927.000000  \n",
       "          mean       0.000000      0.000000        0.062229  \n",
       "          std        0.000000      0.000000        0.416951  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "...                       ...           ...             ...  \n",
       "normal    mean       0.002049      0.000148        0.000000  \n",
       "          std        0.061622      0.017233        0.000000  \n",
       "          min        0.000000      0.000000        0.000000  \n",
       "          25%        0.000000      0.000000        0.000000  \n",
       "          50%        0.000000      0.000000        0.000000  \n",
       "          75%        0.000000      0.000000        0.000000  \n",
       "          max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[40 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_sess_type_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>urgent</th>\n",
       "      <th>wrong_fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Attack</th>\n",
       "      <th>count</th>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>5.863000e+04</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "      <td>58630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.849991</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>3.752448e+04</td>\n",
       "      <td>222.025260</td>\n",
       "      <td>0.132131</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.187417</td>\n",
       "      <td>0.595177</td>\n",
       "      <td>29.929081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306659</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>8.282014e+04</td>\n",
       "      <td>27.797885</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0.209114</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.048746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.334483</td>\n",
       "      <td>0.206408</td>\n",
       "      <td>5.893991e+06</td>\n",
       "      <td>79.196259</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>52.289254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>8.593025e+06</td>\n",
       "      <td>84.710761</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.369916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">Normal</th>\n",
       "      <th>mean</th>\n",
       "      <td>22.517945</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>4.329685e+03</td>\n",
       "      <td>147.431923</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>190.285761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>1.313328e+04</td>\n",
       "      <td>27.685654</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.026086</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>6.546282e+04</td>\n",
       "      <td>101.785400</td>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.195306</td>\n",
       "      <td>0.254382</td>\n",
       "      <td>0.324091</td>\n",
       "      <td>0.092006</td>\n",
       "      <td>92.608377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>4.181131e+05</td>\n",
       "      <td>60.182334</td>\n",
       "      <td>0.271621</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.290000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.790000e+02</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.056000e+03</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.028652e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.958152e+07</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count  diff_srv_rate     dst_bytes  dst_host_count  \\\n",
       "is_sess                                                                    \n",
       "Attack  count  58630.000000   58630.000000  5.863000e+04    58630.000000   \n",
       "        mean     154.849991       0.102410  3.752448e+04      222.025260   \n",
       "        std      124.334483       0.206408  5.893991e+06       79.196259   \n",
       "        min        0.000000       0.000000  0.000000e+00        1.000000   \n",
       "        25%       40.000000       0.050000  0.000000e+00      255.000000   \n",
       "        50%      138.000000       0.060000  0.000000e+00      255.000000   \n",
       "        75%      241.000000       0.070000  0.000000e+00      255.000000   \n",
       "...                     ...            ...           ...             ...   \n",
       "Normal  mean      22.517945       0.028788  4.329685e+03      147.431923   \n",
       "        std       54.026086       0.145622  6.546282e+04      101.785400   \n",
       "        min        0.000000       0.000000  0.000000e+00        0.000000   \n",
       "        25%        1.000000       0.000000  1.050000e+02       40.000000   \n",
       "        50%        4.000000       0.000000  3.790000e+02      156.000000   \n",
       "        75%       14.000000       0.000000  2.056000e+03      255.000000   \n",
       "        max      511.000000       1.000000  7.028652e+06      255.000000   \n",
       "\n",
       "               dst_host_diff_srv_rate  dst_host_rerror_rate  \\\n",
       "is_sess                                                       \n",
       "Attack  count            58630.000000          58630.000000   \n",
       "        mean                 0.132131              0.201810   \n",
       "        std                  0.230626              0.381090   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.050000              0.000000   \n",
       "        50%                  0.070000              0.000000   \n",
       "        75%                  0.080000              0.020000   \n",
       "...                               ...                   ...   \n",
       "Normal  mean                 0.040134              0.046589   \n",
       "        std                  0.128529              0.195306   \n",
       "        min                  0.000000              0.000000   \n",
       "        25%                  0.000000              0.000000   \n",
       "        50%                  0.000000              0.000000   \n",
       "        75%                  0.020000              0.000000   \n",
       "        max                  1.000000              1.000000   \n",
       "\n",
       "               dst_host_same_src_port_rate  dst_host_same_srv_rate  \\\n",
       "is_sess                                                              \n",
       "Attack  count                 58630.000000            58630.000000   \n",
       "        mean                      0.178993                0.187417   \n",
       "        std                       0.359262                0.322430   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.020000   \n",
       "        50%                       0.000000                0.050000   \n",
       "        75%                       0.020000                0.090000   \n",
       "...                                    ...                     ...   \n",
       "Normal  mean                      0.121726                0.811875   \n",
       "        std                       0.254382                0.324091   \n",
       "        min                       0.000000                0.000000   \n",
       "        25%                       0.000000                0.750000   \n",
       "        50%                       0.010000                1.000000   \n",
       "        75%                       0.080000                1.000000   \n",
       "        max                       1.000000                1.000000   \n",
       "\n",
       "               dst_host_serror_rate  dst_host_srv_count       ...        \\\n",
       "is_sess                                                       ...         \n",
       "Attack  count          58630.000000        58630.000000       ...         \n",
       "        mean               0.595177           29.929081       ...         \n",
       "        std                0.484495           52.289254       ...         \n",
       "        min                0.000000            1.000000       ...         \n",
       "        25%                0.000000            4.000000       ...         \n",
       "        50%                1.000000           12.000000       ...         \n",
       "        75%                1.000000           21.000000       ...         \n",
       "...                             ...                 ...       ...         \n",
       "Normal  mean               0.013930          190.285761       ...         \n",
       "        std                0.092006           92.608377       ...         \n",
       "        min                0.000000            0.000000       ...         \n",
       "        25%                0.000000          121.000000       ...         \n",
       "        50%                0.000000          255.000000       ...         \n",
       "        75%                0.000000          255.000000       ...         \n",
       "        max                1.000000          255.000000       ...         \n",
       "\n",
       "               same_srv_rate   serror_rate     src_bytes     srv_count  \\\n",
       "is_sess                                                                  \n",
       "Attack  count   58630.000000  58630.000000  5.863000e+04  58630.000000   \n",
       "        mean        0.306659      0.595808  8.282014e+04     27.797885   \n",
       "        std         0.395655      0.486588  8.593025e+06     84.710761   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         0.040000      0.000000  0.000000e+00      3.000000   \n",
       "        50%         0.080000      1.000000  0.000000e+00     10.000000   \n",
       "        75%         0.500000      1.000000  0.000000e+00     18.000000   \n",
       "...                      ...           ...           ...           ...   \n",
       "Normal  mean        0.969360      0.013441  1.313328e+04     27.685654   \n",
       "        std         0.144371      0.094211  4.181131e+05     60.182334   \n",
       "        min         0.000000      0.000000  0.000000e+00      0.000000   \n",
       "        25%         1.000000      0.000000  1.290000e+02      2.000000   \n",
       "        50%         1.000000      0.000000  2.330000e+02      5.000000   \n",
       "        75%         1.000000      0.000000  3.240000e+02     18.000000   \n",
       "        max         1.000000      1.000000  8.958152e+07    511.000000   \n",
       "\n",
       "               srv_diff_host_rate  srv_rerror_rate  srv_serror_rate  \\\n",
       "is_sess                                                               \n",
       "Attack  count        58630.000000     58630.000000     58630.000000   \n",
       "        mean             0.064079         0.209114         0.593072   \n",
       "        std              0.241348         0.404487         0.490234   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         1.000000   \n",
       "        75%              0.000000         0.000000         1.000000   \n",
       "...                           ...              ...              ...   \n",
       "Normal  mean             0.126263         0.044629         0.012083   \n",
       "        std              0.271621         0.202264         0.086426   \n",
       "        min              0.000000         0.000000         0.000000   \n",
       "        25%              0.000000         0.000000         0.000000   \n",
       "        50%              0.000000         0.000000         0.000000   \n",
       "        75%              0.110000         0.000000         0.000000   \n",
       "        max              1.000000         1.000000         1.000000   \n",
       "\n",
       "               su_attempted        urgent  wrong_fragment  \n",
       "is_sess                                                    \n",
       "Attack  count  58630.000000  58630.000000    58630.000000  \n",
       "        mean       0.000017      0.000068        0.048746  \n",
       "        std        0.004130      0.010116        0.369916  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "...                     ...           ...             ...  \n",
       "Normal  mean       0.002049      0.000148        0.000000  \n",
       "        std        0.061622      0.017233        0.000000  \n",
       "        min        0.000000      0.000000        0.000000  \n",
       "        25%        0.000000      0.000000        0.000000  \n",
       "        50%        0.000000      0.000000        0.000000  \n",
       "        75%        0.000000      0.000000        0.000000  \n",
       "        max        2.000000      3.000000        0.000000  \n",
       "\n",
       "[16 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_is_sess_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kdd_is_sess_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kdd_sess_type_group.hist(figsize=[25,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_dummy(train, test):\n",
    "    dummy_variables_2labels = [*category_variables, \"is_sess\"]\n",
    "    dummy_variables_5labels = [*category_variables, \"sess_type\"]\n",
    "\n",
    "    drop_variables = [*category_variables, \"is_sess\", \"sess_type\", \"duration\", \"label\"]\n",
    "    \n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    def dummy(kdd):\n",
    "        kdd_one_hot_2labels = pd.get_dummies(kdd[dummy_variables_2labels], prefix=dummy_variables_2labels, drop_first=False)\n",
    "        kdd_one_hot_5labels = pd.get_dummies(kdd[dummy_variables_5labels], prefix=dummy_variables_5labels, drop_first=False)\n",
    "\n",
    "        kdd_2labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                       , kdd_one_hot_2labels]\n",
    "                                      , axis = 1)\n",
    "        kdd_5labels = pd.concat([kdd.drop(drop_variables, axis = 1)\n",
    "                                      , kdd_one_hot_5labels]\n",
    "                                      , axis = 1)\n",
    "\n",
    "        return kdd_2labels, kdd_5labels\n",
    "    \n",
    "    kdd_2labels, kdd_5labels = dummy(pd.concat([train, test], axis = 0))\n",
    "    \n",
    "    kdd_2labels_train, kdd_2labels_test = kdd_2labels.iloc[:train_size,:], kdd_2labels.iloc[train_size:,:]\n",
    "    kdd_5labels_train, kdd_5labels_test = kdd_5labels.iloc[:train_size,:], kdd_5labels.iloc[train_size:,:]\n",
    "    \n",
    "    return kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_columns_2labels = ['is_sess_Normal', 'is_sess_Attack']\n",
    "output_columns_5labels = ['sess_type_normal', 'sess_type_DoS', 'sess_type_Probe', 'sess_type_R2L', 'sess_type_U2R']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "kdd_2labels_train, kdd_2labels_test, kdd_5labels_train, kdd_5labels_test = encode_dummy(kdd_train, kdd_test)\n",
    "\n",
    "x_kdd_train = kdd_2labels_train.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_train = kdd_2labels_train.loc[:,output_columns_2labels].values \n",
    "y_5labels_train = kdd_5labels_train.loc[:,output_columns_5labels].values\n",
    "\n",
    "x_kdd_test = kdd_2labels_test.drop(output_columns_2labels, axis = 1).values\n",
    "y_2labels_test = kdd_2labels_test.loc[:,output_columns_2labels].values \n",
    "y_5labels_test = kdd_5labels_test.loc[:,output_columns_5labels].values\n",
    "\n",
    "        \n",
    "ss = pp.StandardScaler()\n",
    "x_kdd_train = ss.fit_transform(x_kdd_train)\n",
    "x_test = ss.transform(x_kdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sanity before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: mean:-0.0000, std:0.9959, shape:(125973, 121)\n",
      "Testing  data: mean:0.0170, std:1.4175, shape:(22544, 121)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_kdd_train.mean(), x_kdd_train.std(), x_kdd_train.shape))\n",
    "print(\"Testing  data: mean:{:.4f}, std:{:.4f}, shape:{}\".format(x_test.mean(), x_test.std(), x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_dense(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "                      input_dim = 121, classes = 2, hidden_layers = 8, epochs = 10, hidden_units = 5):\n",
    "   \n",
    "    model_dense = Sequential()\n",
    "    model_dense.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        model_dense.add(Dense(hidden_units, kernel_initializer='uniform' ,activation = 'relu'))\n",
    "    \n",
    "    model_dense.add(Dense(classes, kernel_initializer='uniform' ,activation = 'softmax'))\n",
    "\n",
    "    model_dense.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'],\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                  activity_regularizer=keras.regularizers.l1(0.01)) \n",
    "\n",
    "    model_dense.fit(x_train, y_train, \n",
    "                  epochs = epochs, batch_size=500, \n",
    "                  validation_data = (x_valid, y_valid),\n",
    "                  verbose=1)\n",
    "    \n",
    "    scores_train = model_dense.evaluate(x_train, y_train)\n",
    "    scores_valid = model_dense.evaluate(x_valid, y_valid)\n",
    "    scores_test = model_dense.evaluate(x_test, y_test)\n",
    "    train_loss = scores_train[0]\n",
    "    valid_accuracy = scores_valid[1] \n",
    "    test_accuracy = scores_test[1]\n",
    "    \n",
    "    print(\"\\n Test loss: {}, accuracy: {}\".format(scores_test[0], scores_test[1]))\n",
    "    return train_loss, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_vae(x_train, x_valid, y_train, y_valid, x_test, y_test, \n",
    "              input_dim = 121, classes = 2, hidden_units = 8, hidden_layers = 4, epochs = 5):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    # tf.reset_default_graph()\n",
    "    with graph.as_default():\n",
    "        latent_dim = hidden_units\n",
    "        dense_hidden_units = hidden_units\n",
    "        hidden_encoder_dim = latent_dim #60\n",
    "        hidden_decoder_dim = latent_dim #60\n",
    "\n",
    "        lam = 0.01\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            initial = tf.constant(0.01, shape=shape)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        l2_loss = tf.constant(0.001)\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "                b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "\n",
    "                # Hidden layer encoder\n",
    "                hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "                tf.summary.histogram(\"Weights_Encoder\", W_encoder_input_hidden)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "\n",
    "            # Mu encoder\n",
    "            mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "            tf.summary.histogram(\"Weights_Mean\", W_encoder_hidden_mu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "            b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "\n",
    "            # Sigma encoder\n",
    "            logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "            l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "            tf.summary.histogram(\"Weights_Variance\", W_encoder_hidden_logvar)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            for i in range(hidden_layers):\n",
    "                W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "                b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "\n",
    "                # Hidden layer decoder\n",
    "                hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, keep_prob=keep_prob)\n",
    "                l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "                tf.summary.histogram(\"Weights_Decoder\", W_decoder_z_hidden)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "            b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "            l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "\n",
    "            x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "            tf.summary.histogram(\"Weights_Reconstruction\", W_decoder_hidden_reconstruction)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):    \n",
    "            z = tf.layers.dense(z,dense_hidden_units, activation=tf.nn.relu)\n",
    "            z = tf.nn.dropout(z, keep_prob = keep_prob)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), \n",
    "                                 reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), \n",
    "                                        reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "            reconst_loss = tf.reduce_mean(BCE + KLD)\n",
    "            loss = tf.reduce_mean( reconst_loss + softmax_loss)\n",
    "            regularized_loss = tf.abs(loss + lam * l2_loss, name = \"Regularized_loss\")\n",
    "\n",
    "            pred = tf.argmax(y, 1)\n",
    "            actual = tf.argmax(y_, 1)\n",
    "            \n",
    "            correct_prediction = tf.equal(actual, pred)\n",
    "            tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "            tf.summary.scalar(\"BCE\", tf.reduce_mean(BCE))\n",
    "            tf.summary.scalar(\"KLD\", tf.reduce_mean(KLD))\n",
    "            tf.summary.scalar(\"Softmax_loss\", softmax_loss)\n",
    "\n",
    "            tf.summary.scalar(\"Total_loss\", regularized_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            grad_clip=5\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(regularized_loss, tvars), grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # add op for merging summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "\n",
    "    batch_iterations = 10\n",
    "\n",
    "    batch_indices = np.array_split(np.arange(x_train.shape[0]), batch_iterations)\n",
    "\n",
    "    outputs = []\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "        summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(0, epochs):\n",
    "            for i in batch_indices:\n",
    "                sess.run(optimizer, feed_dict={x: x_train[i,:], y_: y_train[i,:], keep_prob:0.6})\n",
    "\n",
    "            # For stats purpose\n",
    "            train_loss, summary_str_train, train_reduction_loss = sess.run([regularized_loss, summary_op, reconst_loss], feed_dict={x: x_train, y_: y_train, keep_prob:1})\n",
    "            valid_accuracy, summary_str_valid, valid_reduction_loss = sess.run([tf_accuracy, summary_op, reconst_loss], feed_dict={x: x_valid, y_:y_valid, keep_prob:1})\n",
    "            summary_writer_train.add_summary(summary_str_train, epoch)\n",
    "            summary_writer_valid.add_summary(summary_str_valid, epoch)\n",
    "\n",
    "            # Print Stats\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Step {:>2} | Training - Loss: {:.4f}, Reduction Loss: {:.4f} | \"\n",
    "                      \"Validation - Acc: {:.4f}, Reduction Loss: {:.4f}\"\n",
    "                      .format(epoch, train_loss, train_reduction_loss, valid_accuracy, valid_reduction_loss))\n",
    "\n",
    "        test_accuracy, y_pred, y_actual, test_reduction_loss = sess.run([tf_accuracy, pred, actual, reconst_loss], feed_dict={x: x_test, y_:y_test, keep_prob:1})\n",
    "        print(\"Test - Accuracy: {:.4f}, Feature reduction loss:{:.4f}\".format(test_accuracy, test_reduction_loss))\n",
    "        \n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "        \n",
    "        outputs = np.hstack((y_pred, y_actual))\n",
    "        \n",
    "    return train_loss, valid_accuracy, test_accuracy, train_reduction_loss, valid_reduction_loss, test_reduction_loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Hyper_parameters = namedtuple(\"Hyper_parameters\", [\"epochs\", \"hidden_units\", \"hidden_layers\",])\n",
    "\n",
    "dense_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "dense_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_2labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]\n",
    "\n",
    "vae_hyper_parameters_5labels = [\n",
    "    Hyper_parameters(10, 4, 2),\n",
    "    Hyper_parameters(10, 4, 4),\n",
    "    #Hyper_parameters(10, 8, 4),\n",
    "    #Hyper_parameters(10, 8, 8),\n",
    "    #Hyper_parameters(10, 16, 8),\n",
    "    #Hyper_parameters(10, 128, 8)\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_scenario_vae(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"])\n",
    "    \n",
    "    print(\"Training for {} labels on VAE Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.hidden_units, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a, t_r_l, v_r_l, te_r_l, op = train_vae(x_train, x_valid, \n",
    "                                                           y_train, y_valid, \n",
    "                                                           x_kdd_test, y_test_labels, \n",
    "                                                           classes = classes, \n",
    "                                                           hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                                                           hidden_units = Hyper_parameters.hidden_units,\n",
    "                                                           epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a, t_r_l, v_r_l, te_r_l]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\", \n",
    "                       \"train_feature_reduction_loss\",\n",
    "                       \"valid_feature_reduction_loss\",\n",
    "                       \"test_feature_reduction_loss\"]))\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results])\n",
    "    \n",
    "    return df_results, op\n",
    "        \n",
    "def run_scenario_dense(hyper_parameters_arr, classes, y_train_labels, y_test_labels):\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = ms.train_test_split(x_kdd_train, y_train_labels, test_size=0.2)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"])\n",
    "    \n",
    "    print(\"Training for {} labels on Dense Softmax Network:\".format(classes))\n",
    "    for Hyper_parameters in hyper_parameters_arr:\n",
    "        \n",
    "        print(\"Current parameters: epochs:{}, hidden_units:{}, hidden_layers:{}\"\n",
    "              .format(Hyper_parameters.epochs, Hyper_parameters.hidden_units, Hyper_parameters.hidden_layers))\n",
    "        \n",
    "        t_l, v_a, te_a = train_dense(x_train, x_valid, y_train, y_valid, x_kdd_test, y_test_labels, \n",
    "                 classes = classes, \n",
    "                 hidden_layers = Hyper_parameters.hidden_layers,\n",
    "                 hidden_units = Hyper_parameters.hidden_units,\n",
    "                 epochs = Hyper_parameters.epochs)\n",
    "        \n",
    "        df_results.append(pd.DataFrame(\n",
    "                        [[t_l, v_a, te_a]], \n",
    "                        columns=[\"train_loss\", \"validation_accuracy\", \"test_accuracy\"]))\n",
    "    \n",
    "    df_parameters = pd.DataFrame.from_dict(hyper_parameters_arr)\n",
    "    df_results = pd.concat([df_parameters, df_results])\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2094: UserWarning: Expected no kwargs, you passed 2\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.3017 - acc: 0.8924 - val_loss: 0.0773 - val_acc: 0.9749\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0653 - acc: 0.9781 - val_loss: 0.0527 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0455 - acc: 0.9831 - val_loss: 0.0376 - val_acc: 0.9848\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0342 - acc: 0.9847 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0270 - acc: 0.9907 - val_loss: 0.0238 - val_acc: 0.9927\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0210 - val_acc: 0.9929\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0201 - val_acc: 0.9939\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0187 - val_acc: 0.9940\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0178 - acc: 0.9939 - val_loss: 0.0183 - val_acc: 0.9942\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0174 - acc: 0.9940 - val_loss: 0.0182 - val_acc: 0.9940\n",
      "21312/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 8.579240212802603, accuracy: 0.46553406671398156\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.4765 - acc: 0.7215 - val_loss: 0.0901 - val_acc: 0.9727\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0775 - acc: 0.9742 - val_loss: 0.0656 - val_acc: 0.9761\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0567 - acc: 0.9785 - val_loss: 0.0476 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0406 - acc: 0.9849 - val_loss: 0.0341 - val_acc: 0.9872\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0303 - acc: 0.9902 - val_loss: 0.0270 - val_acc: 0.9921\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0249 - acc: 0.9932 - val_loss: 0.0233 - val_acc: 0.9940\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0220 - acc: 0.9936 - val_loss: 0.0214 - val_acc: 0.9939\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0204 - acc: 0.9940 - val_loss: 0.0202 - val_acc: 0.9940\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0188 - acc: 0.9944 - val_loss: 0.0192 - val_acc: 0.9944\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0178 - acc: 0.9946 - val_loss: 0.0215 - val_acc: 0.9921\n",
      "21696/22544 [===========================>..] - ETA: 0s\n",
      " Test loss: 8.5710803452242, accuracy: 0.465667139815472\n",
      "Training for 2 labels on VAE Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Step  0 | Training - Loss: 1.3349, Reduction Loss: 0.6423 | Validation - Acc: 0.5474, Reduction Loss: 0.6422\n",
      "Step  1 | Training - Loss: 1.2724, Reduction Loss: 0.5627 | Validation - Acc: 0.5516, Reduction Loss: 0.5627\n",
      "Step  2 | Training - Loss: 1.2075, Reduction Loss: 0.4755 | Validation - Acc: 0.5494, Reduction Loss: 0.4751\n",
      "Step  3 | Training - Loss: 1.1475, Reduction Loss: 0.3972 | Validation - Acc: 0.5608, Reduction Loss: 0.3977\n",
      "Step  4 | Training - Loss: 1.0879, Reduction Loss: 0.3474 | Validation - Acc: 0.6200, Reduction Loss: 0.3469\n",
      "Step  5 | Training - Loss: 1.0174, Reduction Loss: 0.3198 | Validation - Acc: 0.6976, Reduction Loss: 0.3194\n",
      "Step  6 | Training - Loss: 0.9566, Reduction Loss: 0.2943 | Validation - Acc: 0.7475, Reduction Loss: 0.2938\n",
      "Step  7 | Training - Loss: 0.9120, Reduction Loss: 0.2741 | Validation - Acc: 0.7688, Reduction Loss: 0.2746\n",
      "Step  8 | Training - Loss: 0.8862, Reduction Loss: 0.2457 | Validation - Acc: 0.7665, Reduction Loss: 0.2452\n",
      "Step  9 | Training - Loss: 0.8636, Reduction Loss: 0.2246 | Validation - Acc: 0.7673, Reduction Loss: 0.2250\n",
      "Test - Accuracy: 0.4886, Feature reduction loss:168.3580\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Step  0 | Training - Loss: 1.3365, Reduction Loss: 0.6395 | Validation - Acc: 0.5377, Reduction Loss: 0.6394\n",
      "Step  1 | Training - Loss: 1.2733, Reduction Loss: 0.5617 | Validation - Acc: 0.5460, Reduction Loss: 0.5616\n",
      "Step  2 | Training - Loss: 1.2076, Reduction Loss: 0.4701 | Validation - Acc: 0.5417, Reduction Loss: 0.4695\n",
      "Step  3 | Training - Loss: 1.1473, Reduction Loss: 0.3907 | Validation - Acc: 0.5488, Reduction Loss: 0.3911\n",
      "Step  4 | Training - Loss: 1.0949, Reduction Loss: 0.3334 | Validation - Acc: 0.5746, Reduction Loss: 0.3337\n",
      "Step  5 | Training - Loss: 1.0306, Reduction Loss: 0.3010 | Validation - Acc: 0.6630, Reduction Loss: 0.3010\n",
      "Step  6 | Training - Loss: 0.9552, Reduction Loss: 0.2753 | Validation - Acc: 0.7313, Reduction Loss: 0.2746\n",
      "Step  7 | Training - Loss: 0.9037, Reduction Loss: 0.2474 | Validation - Acc: 0.7657, Reduction Loss: 0.2472\n",
      "Step  8 | Training - Loss: 0.8638, Reduction Loss: 0.2178 | Validation - Acc: 0.7826, Reduction Loss: 0.2188\n",
      "Step  9 | Training - Loss: 0.8320, Reduction Loss: 0.1898 | Validation - Acc: 0.7968, Reduction Loss: 0.1896\n",
      "Test - Accuracy: 0.4854, Feature reduction loss:3177330.0000\n",
      "Training for 5 labels on Dense Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.8833 - acc: 0.8631 - val_loss: 0.2672 - val_acc: 0.8903\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.2471 - acc: 0.8910 - val_loss: 0.2219 - val_acc: 0.9274\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1939 - acc: 0.9104 - val_loss: 0.1506 - val_acc: 0.9602\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.1061 - acc: 0.9734 - val_loss: 0.0753 - val_acc: 0.9779\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0696 - acc: 0.9778 - val_loss: 0.0652 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0624 - acc: 0.9787 - val_loss: 0.0608 - val_acc: 0.9795\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0585 - acc: 0.9790 - val_loss: 0.0588 - val_acc: 0.9792\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0558 - acc: 0.9794 - val_loss: 0.0566 - val_acc: 0.9805\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0537 - acc: 0.9797 - val_loss: 0.0560 - val_acc: 0.9795\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.0521 - acc: 0.9797 - val_loss: 0.0554 - val_acc: 0.9806\n",
      "20672/22544 [==========================>...] - ETA: 0s\n",
      " Test loss: 6.5417808093781, accuracy: 0.3692334989354152\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 1s - loss: 1.2141 - acc: 0.5325 - val_loss: 0.7184 - val_acc: 0.5366\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.6212 - acc: 0.5513 - val_loss: 0.5396 - val_acc: 0.8626\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.4970 - acc: 0.8601 - val_loss: 0.4564 - val_acc: 0.8563\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.4309 - acc: 0.8540 - val_loss: 0.4055 - val_acc: 0.8535\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.3857 - acc: 0.8575 - val_loss: 0.3682 - val_acc: 0.8568\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.3476 - acc: 0.8849 - val_loss: 0.3323 - val_acc: 0.9544\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.3128 - acc: 0.9548 - val_loss: 0.2984 - val_acc: 0.9597\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.2810 - acc: 0.9600 - val_loss: 0.2700 - val_acc: 0.9646\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100778/100778 [==============================] - 1s - loss: 0.2514 - acc: 0.9653 - val_loss: 0.2439 - val_acc: 0.9683\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 1s - loss: 0.2272 - acc: 0.9699 - val_loss: 0.2230 - val_acc: 0.9746\n",
      "20960/22544 [==========================>...] - ETA: 0s\n",
      " Test loss: 7.35262975496464, accuracy: 0.5336231369765791\n",
      "Training for 5 labels on VAE Softmax Network:\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:2\n",
      "Step  0 | Training - Loss: 2.1590, Reduction Loss: 0.6385 | Validation - Acc: 0.5318, Reduction Loss: 0.6384\n",
      "Step  1 | Training - Loss: 2.0387, Reduction Loss: 0.5618 | Validation - Acc: 0.5321, Reduction Loss: 0.5617\n",
      "Step  2 | Training - Loss: 1.9348, Reduction Loss: 0.4755 | Validation - Acc: 0.5325, Reduction Loss: 0.4760\n",
      "Step  3 | Training - Loss: 1.8507, Reduction Loss: 0.4020 | Validation - Acc: 0.5448, Reduction Loss: 0.4019\n",
      "Step  4 | Training - Loss: 1.7811, Reduction Loss: 0.3501 | Validation - Acc: 0.5728, Reduction Loss: 0.3496\n",
      "Step  5 | Training - Loss: 1.7171, Reduction Loss: 0.3163 | Validation - Acc: 0.6165, Reduction Loss: 0.3162\n",
      "Step  6 | Training - Loss: 1.6594, Reduction Loss: 0.2966 | Validation - Acc: 0.6539, Reduction Loss: 0.2962\n",
      "Step  7 | Training - Loss: 1.6114, Reduction Loss: 0.2733 | Validation - Acc: 0.6718, Reduction Loss: 0.2733\n",
      "Step  8 | Training - Loss: 1.5726, Reduction Loss: 0.2524 | Validation - Acc: 0.6864, Reduction Loss: 0.2527\n",
      "Step  9 | Training - Loss: 1.5386, Reduction Loss: 0.2254 | Validation - Acc: 0.6901, Reduction Loss: 0.2248\n",
      "Test - Accuracy: 0.3910, Feature reduction loss:285636.0000\n",
      "Current parameters: epochs:10, hidden_units:4, hidden_layers:4\n",
      "Step  0 | Training - Loss: 2.1925, Reduction Loss: 0.6424 | Validation - Acc: 0.3763, Reduction Loss: 0.6424\n",
      "Step  1 | Training - Loss: 2.0757, Reduction Loss: 0.5649 | Validation - Acc: 0.4429, Reduction Loss: 0.5649\n",
      "Step  2 | Training - Loss: 1.9478, Reduction Loss: 0.4862 | Validation - Acc: 0.5790, Reduction Loss: 0.4854\n",
      "Step  3 | Training - Loss: 1.8309, Reduction Loss: 0.4125 | Validation - Acc: 0.6210, Reduction Loss: 0.4127\n",
      "Step  4 | Training - Loss: 1.7482, Reduction Loss: 0.3499 | Validation - Acc: 0.6260, Reduction Loss: 0.3498\n",
      "Step  5 | Training - Loss: 1.6846, Reduction Loss: 0.3097 | Validation - Acc: 0.6417, Reduction Loss: 0.3095\n",
      "Step  6 | Training - Loss: 1.6427, Reduction Loss: 0.2733 | Validation - Acc: 0.6501, Reduction Loss: 0.2738\n",
      "Step  7 | Training - Loss: 1.6084, Reduction Loss: 0.2438 | Validation - Acc: 0.6550, Reduction Loss: 0.2439\n",
      "Step  8 | Training - Loss: 1.5712, Reduction Loss: 0.2231 | Validation - Acc: 0.6680, Reduction Loss: 0.2237\n",
      "Step  9 | Training - Loss: 1.5466, Reduction Loss: 0.1970 | Validation - Acc: 0.6701, Reduction Loss: 0.1966\n",
      "Test - Accuracy: 0.4270, Feature reduction loss:1935793.3750\n"
     ]
    }
   ],
   "source": [
    "# Scenario for classes = 2\n",
    "df_results_2label_dense = run_scenario_dense(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "df_results_2label_vae, labels2_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_2labels, 2, y_2labels_train, y_2labels_test)\n",
    "\n",
    "# Scenario for classes = 5\n",
    "df_results_5label_dense = run_scenario_dense(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)\n",
    "df_results_5label_vae, labels5_pred_vae_arr = run_scenario_vae(dense_hyper_parameters_5labels, 5, y_5labels_train, y_5labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_layers  hidden_units test_accuracy train_loss  \\\n",
       "0    10.0            2.0           4.0           NaN        NaN   \n",
       "1    10.0            4.0           4.0           NaN        NaN   \n",
       "\n",
       "  validation_accuracy  \n",
       "0                 NaN  \n",
       "1                 NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_layers  hidden_units test_accuracy  \\\n",
       "0    10.0            2.0           4.0           NaN   \n",
       "1    10.0            4.0           4.0           NaN   \n",
       "\n",
       "  test_feature_reduction_loss train_feature_reduction_loss train_loss  \\\n",
       "0                         NaN                          NaN        NaN   \n",
       "1                         NaN                          NaN        NaN   \n",
       "\n",
       "  valid_feature_reduction_loss validation_accuracy  \n",
       "0                          NaN                 NaN  \n",
       "1                          NaN                 NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_2label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_layers  hidden_units test_accuracy train_loss  \\\n",
       "0    10.0            2.0           4.0           NaN        NaN   \n",
       "1    10.0            4.0           4.0           NaN        NaN   \n",
       "\n",
       "  validation_accuracy  \n",
       "0                 NaN  \n",
       "1                 NaN  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_feature_reduction_loss</th>\n",
       "      <th>train_feature_reduction_loss</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_feature_reduction_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  hidden_layers  hidden_units test_accuracy  \\\n",
       "0    10.0            2.0           4.0           NaN   \n",
       "1    10.0            4.0           4.0           NaN   \n",
       "\n",
       "  test_feature_reduction_loss train_feature_reduction_loss train_loss  \\\n",
       "0                         NaN                          NaN        NaN   \n",
       "1                         NaN                          NaN        NaN   \n",
       "\n",
       "  valid_feature_reduction_loss validation_accuracy  \n",
       "0                          NaN                 NaN  \n",
       "1                          NaN                 NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_5label_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[3411 8631]\n",
      " [2971 7531]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAGfCAYAAAAtY8c9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpZL/w9VAxt7PVEsNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa570d41ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = labels2_pred_vae_arr[:,0], y_true = labels2_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, output_columns_2labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[4461 7581    0    0    0]\n",
      " [3106 5165    0    0    0]\n",
      " [ 772 1265    0    0    0]\n",
      " [  65  118    0    0    0]\n",
      " [   1   10    0    0    0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAIkCAYAAAAaivFsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM2t5/w9UBswzQdNQEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa570df22b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_5labels = confusion_matrix(y_pred = labels5_pred_vae_arr[:,0], y_true = labels5_pred_vae_arr[:,1])\n",
    "plt.figure(figsize=[8,8])\n",
    "plot_confusion_matrix(cm_5labels, output_columns_5labels, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Implemented Grad clip, getting good accuracy!",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
