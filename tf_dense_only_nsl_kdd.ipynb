{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:08.173904Z",
     "start_time": "2017-06-18T21:09:07.874259Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:08.264211Z",
     "start_time": "2017-06-18T21:09:08.175411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:08.276510Z",
     "start_time": "2017-06-18T21:09:08.266044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:08.289157Z",
     "start_time": "2017-06-18T21:09:08.277916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:09.045294Z",
     "start_time": "2017-06-18T21:09:08.290790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:10.103250Z",
     "start_time": "2017-06-18T21:09:09.047005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:10.293242Z",
     "start_time": "2017-06-18T21:09:10.104957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:10.524205Z",
     "start_time": "2017-06-18T21:09:10.295168Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T21:09:10.586720Z",
     "start_time": "2017-06-18T21:09:10.525873Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 8, 32, 122]\n",
    "        hidden_layers_arr = [1, 3, 5]\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-5, 1e-5, 1e-6]\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/tf_dense_only_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:46.571640Z",
     "start_time": "2017-06-18T21:09:10.588314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8min 5s ± 28.9 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10\n",
    "%%capture\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T20:12:01.576364Z",
     "start_time": "2017-06-18T20:12:01.571823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T20:12:01.584850Z",
     "start_time": "2017-06-18T20:12:01.578340Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T20:12:01.591604Z",
     "start_time": "2017-06-18T20:12:01.586451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:46.590894Z",
     "start_time": "2017-06-18T22:39:46.573538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906414</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.783291</td>\n",
       "      <td>7.756121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.922289</td>\n",
       "      <td>0.869899</td>\n",
       "      <td>0.763207</td>\n",
       "      <td>26.090022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>0.858410</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>23.537852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938403</td>\n",
       "      <td>0.791474</td>\n",
       "      <td>0.608186</td>\n",
       "      <td>8.059961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "13      8               8              1     0.906414    0.880988   \n",
       "78     10              32              5     0.922289    0.869899   \n",
       "87      9             122              5     0.946896    0.858410   \n",
       "6       8               1              1     0.938403    0.791474   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "13       0.783291    7.756121  \n",
       "78       0.763207   26.090022  \n",
       "87       0.734599   23.537852  \n",
       "6        0.608186    8.059961  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:46.638483Z",
     "start_time": "2017-06-18T22:39:46.592200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903239</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>0.784051</td>\n",
       "      <td>5.512524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.922289</td>\n",
       "      <td>0.869899</td>\n",
       "      <td>0.763207</td>\n",
       "      <td>26.090022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>0.858410</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>23.537852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938403</td>\n",
       "      <td>0.791474</td>\n",
       "      <td>0.608186</td>\n",
       "      <td>8.059961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "11      6               8              1     0.903239    0.880323   \n",
       "78     10              32              5     0.922289    0.869899   \n",
       "87      9             122              5     0.946896    0.858410   \n",
       "6       8               1              1     0.938403    0.791474   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "11       0.784051    5.512524  \n",
       "78       0.763207   26.090022  \n",
       "87       0.734599   23.537852  \n",
       "6        0.608186    8.059961  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:46.684859Z",
     "start_time": "2017-06-18T22:39:46.639918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906414</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.783291</td>\n",
       "      <td>7.756121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905620</td>\n",
       "      <td>0.880944</td>\n",
       "      <td>0.783376</td>\n",
       "      <td>6.632713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903239</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>0.784051</td>\n",
       "      <td>5.512524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.922289</td>\n",
       "      <td>0.869899</td>\n",
       "      <td>0.763207</td>\n",
       "      <td>26.090022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.920464</td>\n",
       "      <td>0.869721</td>\n",
       "      <td>0.763122</td>\n",
       "      <td>22.827152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.909589</td>\n",
       "      <td>0.868879</td>\n",
       "      <td>0.761688</td>\n",
       "      <td>20.928807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.902524</td>\n",
       "      <td>0.867770</td>\n",
       "      <td>0.759241</td>\n",
       "      <td>19.046752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>0.858410</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>23.537852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825290</td>\n",
       "      <td>0.858011</td>\n",
       "      <td>0.761603</td>\n",
       "      <td>4.388568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.945388</td>\n",
       "      <td>0.853220</td>\n",
       "      <td>0.725148</td>\n",
       "      <td>20.603261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.942531</td>\n",
       "      <td>0.847276</td>\n",
       "      <td>0.714262</td>\n",
       "      <td>17.673385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888792</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>0.699578</td>\n",
       "      <td>15.241036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.941737</td>\n",
       "      <td>0.833171</td>\n",
       "      <td>0.688101</td>\n",
       "      <td>14.741884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.878949</td>\n",
       "      <td>0.827227</td>\n",
       "      <td>0.682025</td>\n",
       "      <td>13.333037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.935069</td>\n",
       "      <td>0.826872</td>\n",
       "      <td>0.676456</td>\n",
       "      <td>11.801530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.897127</td>\n",
       "      <td>0.822968</td>\n",
       "      <td>0.668017</td>\n",
       "      <td>9.087819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.876806</td>\n",
       "      <td>0.819198</td>\n",
       "      <td>0.671983</td>\n",
       "      <td>11.425825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.930862</td>\n",
       "      <td>0.818754</td>\n",
       "      <td>0.663460</td>\n",
       "      <td>8.846303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891729</td>\n",
       "      <td>0.818045</td>\n",
       "      <td>0.658903</td>\n",
       "      <td>7.585594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>0.814452</td>\n",
       "      <td>0.654008</td>\n",
       "      <td>12.186608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.939832</td>\n",
       "      <td>0.812766</td>\n",
       "      <td>0.652996</td>\n",
       "      <td>10.171448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.922369</td>\n",
       "      <td>0.812012</td>\n",
       "      <td>0.654852</td>\n",
       "      <td>8.131453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.811036</td>\n",
       "      <td>0.647426</td>\n",
       "      <td>6.084401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.905461</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.656962</td>\n",
       "      <td>6.087417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892999</td>\n",
       "      <td>0.802342</td>\n",
       "      <td>0.637468</td>\n",
       "      <td>3.400584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.913875</td>\n",
       "      <td>0.801544</td>\n",
       "      <td>0.634768</td>\n",
       "      <td>5.939004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.953643</td>\n",
       "      <td>0.800878</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>17.748106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944991</td>\n",
       "      <td>0.799459</td>\n",
       "      <td>0.620084</td>\n",
       "      <td>16.381602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>0.799370</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>11.232897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.853469</td>\n",
       "      <td>0.798527</td>\n",
       "      <td>0.645232</td>\n",
       "      <td>4.067227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951659</td>\n",
       "      <td>0.798394</td>\n",
       "      <td>0.623207</td>\n",
       "      <td>10.118115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.854342</td>\n",
       "      <td>0.797773</td>\n",
       "      <td>0.623797</td>\n",
       "      <td>4.573471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.922686</td>\n",
       "      <td>0.797551</td>\n",
       "      <td>0.616371</td>\n",
       "      <td>15.006444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947293</td>\n",
       "      <td>0.797152</td>\n",
       "      <td>0.621688</td>\n",
       "      <td>9.009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.911970</td>\n",
       "      <td>0.795777</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>13.633728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909589</td>\n",
       "      <td>0.792495</td>\n",
       "      <td>0.606076</td>\n",
       "      <td>12.256747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938403</td>\n",
       "      <td>0.791474</td>\n",
       "      <td>0.608186</td>\n",
       "      <td>8.059961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823940</td>\n",
       "      <td>0.786329</td>\n",
       "      <td>0.604641</td>\n",
       "      <td>3.070189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.786018</td>\n",
       "      <td>0.629958</td>\n",
       "      <td>7.647516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936974</td>\n",
       "      <td>0.785752</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>7.891977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932608</td>\n",
       "      <td>0.784954</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>6.924456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.781711</td>\n",
       "      <td>0.784732</td>\n",
       "      <td>0.657468</td>\n",
       "      <td>2.059196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892523</td>\n",
       "      <td>0.784732</td>\n",
       "      <td>0.592996</td>\n",
       "      <td>10.860647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865931</td>\n",
       "      <td>0.783623</td>\n",
       "      <td>0.637890</td>\n",
       "      <td>2.273588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916812</td>\n",
       "      <td>0.773377</td>\n",
       "      <td>0.575781</td>\n",
       "      <td>5.772116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.769606</td>\n",
       "      <td>0.572996</td>\n",
       "      <td>6.788192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897047</td>\n",
       "      <td>0.766102</td>\n",
       "      <td>0.583376</td>\n",
       "      <td>5.655944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906176</td>\n",
       "      <td>0.760956</td>\n",
       "      <td>0.552827</td>\n",
       "      <td>4.622861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873393</td>\n",
       "      <td>0.759448</td>\n",
       "      <td>0.582447</td>\n",
       "      <td>4.521235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.874742</td>\n",
       "      <td>0.753903</td>\n",
       "      <td>0.540084</td>\n",
       "      <td>8.165948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789332</td>\n",
       "      <td>0.752528</td>\n",
       "      <td>0.571561</td>\n",
       "      <td>1.565016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823940</td>\n",
       "      <td>0.732434</td>\n",
       "      <td>0.546751</td>\n",
       "      <td>3.401707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.679314</td>\n",
       "      <td>0.730837</td>\n",
       "      <td>0.741097</td>\n",
       "      <td>3.298299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871408</td>\n",
       "      <td>0.727999</td>\n",
       "      <td>0.491899</td>\n",
       "      <td>3.476577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.832513</td>\n",
       "      <td>0.713316</td>\n",
       "      <td>0.466160</td>\n",
       "      <td>5.464634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.761311</td>\n",
       "      <td>0.696283</td>\n",
       "      <td>0.504726</td>\n",
       "      <td>2.277307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805922</td>\n",
       "      <td>0.690605</td>\n",
       "      <td>0.427511</td>\n",
       "      <td>2.332918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.865296</td>\n",
       "      <td>0.673039</td>\n",
       "      <td>0.396034</td>\n",
       "      <td>44.906827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.871249</td>\n",
       "      <td>0.671753</td>\n",
       "      <td>0.393840</td>\n",
       "      <td>43.278384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.866884</td>\n",
       "      <td>0.670866</td>\n",
       "      <td>0.392152</td>\n",
       "      <td>41.646417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.866090</td>\n",
       "      <td>0.669846</td>\n",
       "      <td>0.390464</td>\n",
       "      <td>40.048072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.866010</td>\n",
       "      <td>0.669003</td>\n",
       "      <td>0.389114</td>\n",
       "      <td>38.440099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.864423</td>\n",
       "      <td>0.668027</td>\n",
       "      <td>0.387511</td>\n",
       "      <td>36.816779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.865534</td>\n",
       "      <td>0.666918</td>\n",
       "      <td>0.385907</td>\n",
       "      <td>35.223553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.860057</td>\n",
       "      <td>0.665410</td>\n",
       "      <td>0.383713</td>\n",
       "      <td>33.647811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718209</td>\n",
       "      <td>0.662615</td>\n",
       "      <td>0.490717</td>\n",
       "      <td>1.147233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.857358</td>\n",
       "      <td>0.661817</td>\n",
       "      <td>0.377722</td>\n",
       "      <td>32.087380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.856088</td>\n",
       "      <td>0.659466</td>\n",
       "      <td>0.373502</td>\n",
       "      <td>30.513274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859422</td>\n",
       "      <td>0.656361</td>\n",
       "      <td>0.367679</td>\n",
       "      <td>28.963678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.766868</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>0.361266</td>\n",
       "      <td>2.762487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.652246</td>\n",
       "      <td>0.645626</td>\n",
       "      <td>0.551392</td>\n",
       "      <td>3.848985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753612</td>\n",
       "      <td>0.642477</td>\n",
       "      <td>0.361603</td>\n",
       "      <td>1.182265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685188</td>\n",
       "      <td>0.641057</td>\n",
       "      <td>0.618987</td>\n",
       "      <td>1.147233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.832910</td>\n",
       "      <td>0.629791</td>\n",
       "      <td>0.318481</td>\n",
       "      <td>27.448016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.795126</td>\n",
       "      <td>0.610273</td>\n",
       "      <td>0.458819</td>\n",
       "      <td>3.025803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.783299</td>\n",
       "      <td>0.596744</td>\n",
       "      <td>0.258312</td>\n",
       "      <td>24.286118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.752104</td>\n",
       "      <td>0.587473</td>\n",
       "      <td>0.242110</td>\n",
       "      <td>22.724931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.721305</td>\n",
       "      <td>0.559484</td>\n",
       "      <td>0.341688</td>\n",
       "      <td>1.419023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500079</td>\n",
       "      <td>0.558597</td>\n",
       "      <td>0.745823</td>\n",
       "      <td>2.195987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.524530</td>\n",
       "      <td>0.758819</td>\n",
       "      <td>1.101968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.510398</td>\n",
       "      <td>0.513529</td>\n",
       "      <td>0.532152</td>\n",
       "      <td>1.949590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.709001</td>\n",
       "      <td>0.495963</td>\n",
       "      <td>0.224135</td>\n",
       "      <td>19.576402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.693443</td>\n",
       "      <td>0.490463</td>\n",
       "      <td>0.215105</td>\n",
       "      <td>17.982180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.655818</td>\n",
       "      <td>0.476269</td>\n",
       "      <td>0.192152</td>\n",
       "      <td>14.774089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.641372</td>\n",
       "      <td>0.471079</td>\n",
       "      <td>0.183966</td>\n",
       "      <td>13.169223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.533736</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.650742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.539054</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.472629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.271839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "13      8               8              1     0.906414    0.880988   \n",
       "12      7               8              1     0.905620    0.880944   \n",
       "11      6               8              1     0.903239    0.880323   \n",
       "78     10              32              5     0.922289    0.869899   \n",
       "74      6              32              5     0.920464    0.869721   \n",
       "72      4              32              5     0.909589    0.868879   \n",
       "79     11              32              5     0.902524    0.867770   \n",
       "87      9             122              5     0.946896    0.858410   \n",
       "10      5               8              1     0.825290    0.858011   \n",
       "86      8             122              5     0.945388    0.853220   \n",
       "85      7             122              5     0.942531    0.847276   \n",
       "77      9              32              5     0.888792    0.836719   \n",
       "84      6             122              5     0.941737    0.833171   \n",
       "76      8              32              5     0.878949    0.827227   \n",
       "83      5             122              5     0.935069    0.826872   \n",
       "43      7              32              3     0.897127    0.822968   \n",
       "75      7              32              5     0.876806    0.819198   \n",
       "82      4             122              5     0.930862    0.818754   \n",
       "42      6              32              3     0.891729    0.818045   \n",
       "49      7             122              3     0.944436    0.814452   \n",
       "48      6             122              3     0.939832    0.812766   \n",
       "47      5             122              3     0.922369    0.812012   \n",
       "41      5              32              3     0.872678    0.811036   \n",
       "46      4             122              3     0.905461    0.809617   \n",
       "26      4             122              1     0.892999    0.802342   \n",
       "81      3             122              5     0.913875    0.801544   \n",
       "34      8               8              3     0.953643    0.800878   \n",
       "32      6               8              3     0.944991    0.799459   \n",
       "23     11              32              1     0.953882    0.799370   \n",
       "45      3             122              3     0.853469    0.798527   \n",
       "22     10              32              1     0.951659    0.798394   \n",
       "40      4              32              3     0.854342    0.797773   \n",
       "30      4               8              3     0.922686    0.797551   \n",
       "21      9              32              1     0.947293    0.797152   \n",
       "37     11               8              3     0.911970    0.795777   \n",
       "36     10               8              3     0.909589    0.792495   \n",
       "6       8               1              1     0.938403    0.791474   \n",
       "39      3              32              3     0.823940    0.786329   \n",
       "73      5              32              5     0.830846    0.786018   \n",
       "20      8              32              1     0.936974    0.785752   \n",
       "5       7               1              1     0.932608    0.784954   \n",
       "44      2             122              3     0.781711    0.784732   \n",
       "35      9               8              3     0.892523    0.784732   \n",
       "25      3             122              1     0.865931    0.783623   \n",
       "4       6               1              1     0.916812    0.773377   \n",
       "19      7              32              1     0.925464    0.769606   \n",
       "18      6              32              1     0.897047    0.766102   \n",
       "3       5               1              1     0.906176    0.760956   \n",
       "17      5              32              1     0.873393    0.759448   \n",
       "33      7               8              3     0.874742    0.753903   \n",
       "38      2              32              3     0.789332    0.752528   \n",
       "16      4              32              1     0.823940    0.732434   \n",
       "9       4               8              1     0.679314    0.730837   \n",
       "2       4               1              1     0.871408    0.727999   \n",
       "31      5               8              3     0.832513    0.713316   \n",
       "15      3              32              1     0.761311    0.696283   \n",
       "1       3               1              1     0.805922    0.690605   \n",
       "69     33               8              5     0.865296    0.673039   \n",
       "68     30               8              5     0.871249    0.671753   \n",
       "67     27               8              5     0.866884    0.670866   \n",
       "66     24               8              5     0.866090    0.669846   \n",
       "65     21               8              5     0.866010    0.669003   \n",
       "61     18               8              5     0.864423    0.668027   \n",
       "64     15               8              5     0.865534    0.666918   \n",
       "58     12               8              5     0.860057    0.665410   \n",
       "14      2              32              1     0.718209    0.662615   \n",
       "52      9               8              5     0.857358    0.661817   \n",
       "56      6               8              5     0.856088    0.659466   \n",
       "63     22               8              5     0.859422    0.656361   \n",
       "29      3               8              3     0.766868    0.651925   \n",
       "71      3              32              5     0.652246    0.645626   \n",
       "0       2               1              1     0.753612    0.642477   \n",
       "24      2             122              1     0.685188    0.641057   \n",
       "62     20               8              5     0.832910    0.629791   \n",
       "80      2             122              5     0.795126    0.610273   \n",
       "60     16               8              5     0.783299    0.596744   \n",
       "59     14               8              5     0.752104    0.587473   \n",
       "28      2               8              3     0.721305    0.559484   \n",
       "8       3               8              1     0.500079    0.558597   \n",
       "7       2               8              1     0.418400    0.524530   \n",
       "70      2              32              5     0.510398    0.513529   \n",
       "53     10               8              5     0.709001    0.495963   \n",
       "57      8               8              5     0.693443    0.490463   \n",
       "55      4               8              5     0.655818    0.476269   \n",
       "54     11               8              5     0.641372    0.471079   \n",
       "51      2               8              5     0.533736    0.430758   \n",
       "50      2               1              5     0.539054    0.430758   \n",
       "27      2               1              3     0.541435    0.430758   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "13       0.783291    7.756121  \n",
       "12       0.783376    6.632713  \n",
       "11       0.784051    5.512524  \n",
       "78       0.763207   26.090022  \n",
       "74       0.763122   22.827152  \n",
       "72       0.761688   20.928807  \n",
       "79       0.759241   19.046752  \n",
       "87       0.734599   23.537852  \n",
       "10       0.761603    4.388568  \n",
       "86       0.725148   20.603261  \n",
       "85       0.714262   17.673385  \n",
       "77       0.699578   15.241036  \n",
       "84       0.688101   14.741884  \n",
       "76       0.682025   13.333037  \n",
       "83       0.676456   11.801530  \n",
       "43       0.668017    9.087819  \n",
       "75       0.671983   11.425825  \n",
       "82       0.663460    8.846303  \n",
       "42       0.658903    7.585594  \n",
       "49       0.654008   12.186608  \n",
       "48       0.652996   10.171448  \n",
       "47       0.654852    8.131453  \n",
       "41       0.647426    6.084401  \n",
       "46       0.656962    6.087417  \n",
       "26       0.637468    3.400584  \n",
       "81       0.634768    5.939004  \n",
       "34       0.622785   17.748106  \n",
       "32       0.620084   16.381602  \n",
       "23       0.624641   11.232897  \n",
       "45       0.645232    4.067227  \n",
       "22       0.623207   10.118115  \n",
       "40       0.623797    4.573471  \n",
       "30       0.616371   15.006444  \n",
       "21       0.621688    9.009806  \n",
       "37       0.612321   13.633728  \n",
       "36       0.606076   12.256747  \n",
       "6        0.608186    8.059961  \n",
       "39       0.604641    3.070189  \n",
       "73       0.629958    7.647516  \n",
       "20       0.600000    7.891977  \n",
       "5        0.596203    6.924456  \n",
       "44       0.657468    2.059196  \n",
       "35       0.592996   10.860647  \n",
       "25       0.637890    2.273588  \n",
       "4        0.575781    5.772116  \n",
       "19       0.572996    6.788192  \n",
       "18       0.583376    5.655944  \n",
       "3        0.552827    4.622861  \n",
       "17       0.582447    4.521235  \n",
       "33       0.540084    8.165948  \n",
       "38       0.571561    1.565016  \n",
       "16       0.546751    3.401707  \n",
       "9        0.741097    3.298299  \n",
       "2        0.491899    3.476577  \n",
       "31       0.466160    5.464634  \n",
       "15       0.504726    2.277307  \n",
       "1        0.427511    2.332918  \n",
       "69       0.396034   44.906827  \n",
       "68       0.393840   43.278384  \n",
       "67       0.392152   41.646417  \n",
       "66       0.390464   40.048072  \n",
       "65       0.389114   38.440099  \n",
       "61       0.387511   36.816779  \n",
       "64       0.385907   35.223553  \n",
       "58       0.383713   33.647811  \n",
       "14       0.490717    1.147233  \n",
       "52       0.377722   32.087380  \n",
       "56       0.373502   30.513274  \n",
       "63       0.367679   28.963678  \n",
       "29       0.361266    2.762487  \n",
       "71       0.551392    3.848985  \n",
       "0        0.361603    1.182265  \n",
       "24       0.618987    1.147233  \n",
       "62       0.318481   27.448016  \n",
       "80       0.458819    3.025803  \n",
       "60       0.258312   24.286118  \n",
       "59       0.242110   22.724931  \n",
       "28       0.341688    1.419023  \n",
       "8        0.745823    2.195987  \n",
       "7        0.758819    1.101968  \n",
       "70       0.532152    1.949590  \n",
       "53       0.224135   19.576402  \n",
       "57       0.215105   17.982180  \n",
       "55       0.192152   14.774089  \n",
       "54       0.183966   13.169223  \n",
       "51       0.181603    1.650742  \n",
       "50       0.181603    1.472629  \n",
       "27       0.181603    1.271839  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:46.927869Z",
     "start_time": "2017-06-18T22:39:46.686225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.248835Z",
     "start_time": "2017-06-18T22:39:46.929538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.404867Z",
     "start_time": "2017-06-18T22:39:47.250409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8616  0.1384]\n",
      " [ 0.0671  0.9329]]\n"
     ]
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.549421Z",
     "start_time": "2017-06-18T22:39:47.406457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.4596  0.5404]\n",
      " [ 0.0886  0.9114]]\n"
     ]
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.558895Z",
     "start_time": "2017-06-18T22:39:47.550916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.760094\n",
       "                3                0.713199\n",
       "                5                0.625079\n",
       "8               1                0.774272\n",
       "                3                0.753633\n",
       "                5                0.718815\n",
       "32              1                0.775530\n",
       "                3                0.769248\n",
       "                5                0.789377\n",
       "122             1                0.748377\n",
       "                3                0.810419\n",
       "                5                0.813828\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.test_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.599977Z",
     "start_time": "2017-06-18T22:39:47.560396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.605652</td>\n",
       "      <td>0.581308</td>\n",
       "      <td>0.310464</td>\n",
       "      <td>1.158116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.628671</td>\n",
       "      <td>0.606991</td>\n",
       "      <td>0.351730</td>\n",
       "      <td>2.297762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.956977</td>\n",
       "      <td>0.789567</td>\n",
       "      <td>0.605401</td>\n",
       "      <td>12.285068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.855056</td>\n",
       "      <td>0.727156</td>\n",
       "      <td>0.497975</td>\n",
       "      <td>4.561070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894269</td>\n",
       "      <td>0.757807</td>\n",
       "      <td>0.551646</td>\n",
       "      <td>5.691493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.914907</td>\n",
       "      <td>0.770538</td>\n",
       "      <td>0.574684</td>\n",
       "      <td>6.811489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.933005</td>\n",
       "      <td>0.776260</td>\n",
       "      <td>0.585148</td>\n",
       "      <td>7.927863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.943007</td>\n",
       "      <td>0.783446</td>\n",
       "      <td>0.596709</td>\n",
       "      <td>9.022055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951659</td>\n",
       "      <td>0.786861</td>\n",
       "      <td>0.601097</td>\n",
       "      <td>10.104383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.954040</td>\n",
       "      <td>0.789035</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>11.189151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.793301</td>\n",
       "      <td>0.771159</td>\n",
       "      <td>0.604810</td>\n",
       "      <td>1.131583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.873948</td>\n",
       "      <td>0.816403</td>\n",
       "      <td>0.666920</td>\n",
       "      <td>2.226308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.890618</td>\n",
       "      <td>0.825674</td>\n",
       "      <td>0.680084</td>\n",
       "      <td>3.314382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>0.833393</td>\n",
       "      <td>0.690127</td>\n",
       "      <td>4.405876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.920702</td>\n",
       "      <td>0.833570</td>\n",
       "      <td>0.690127</td>\n",
       "      <td>5.508591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.709478</td>\n",
       "      <td>0.691049</td>\n",
       "      <td>0.449536</td>\n",
       "      <td>1.133849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.793459</td>\n",
       "      <td>0.706574</td>\n",
       "      <td>0.467342</td>\n",
       "      <td>2.255247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.957850</td>\n",
       "      <td>0.766324</td>\n",
       "      <td>0.558397</td>\n",
       "      <td>12.141247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.852675</td>\n",
       "      <td>0.731946</td>\n",
       "      <td>0.496878</td>\n",
       "      <td>4.513144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.961819</td>\n",
       "      <td>0.768985</td>\n",
       "      <td>0.561772</td>\n",
       "      <td>13.249446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.965471</td>\n",
       "      <td>0.773510</td>\n",
       "      <td>0.569705</td>\n",
       "      <td>14.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.940308</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.539156</td>\n",
       "      <td>8.804977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.944515</td>\n",
       "      <td>0.758960</td>\n",
       "      <td>0.544726</td>\n",
       "      <td>9.914802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.950389</td>\n",
       "      <td>0.762509</td>\n",
       "      <td>0.551139</td>\n",
       "      <td>11.017808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.627040</td>\n",
       "      <td>0.491224</td>\n",
       "      <td>1.113718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.607001</td>\n",
       "      <td>0.724494</td>\n",
       "      <td>0.582532</td>\n",
       "      <td>2.222951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.696618</td>\n",
       "      <td>0.753992</td>\n",
       "      <td>0.628945</td>\n",
       "      <td>3.333286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.847357</td>\n",
       "      <td>0.805979</td>\n",
       "      <td>0.651055</td>\n",
       "      <td>4.452556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.874663</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>5.592731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.904112</td>\n",
       "      <td>0.819686</td>\n",
       "      <td>0.664388</td>\n",
       "      <td>6.711334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.926179</td>\n",
       "      <td>0.827848</td>\n",
       "      <td>0.677722</td>\n",
       "      <td>7.823106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.534291</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.344532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.921337</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.768354</td>\n",
       "      <td>10.175471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.926099</td>\n",
       "      <td>0.873669</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>11.436996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.931259</td>\n",
       "      <td>0.873891</td>\n",
       "      <td>0.768861</td>\n",
       "      <td>12.726343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.416733</td>\n",
       "      <td>0.407736</td>\n",
       "      <td>0.671224</td>\n",
       "      <td>1.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.713447</td>\n",
       "      <td>0.707328</td>\n",
       "      <td>0.758481</td>\n",
       "      <td>2.778562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.922686</td>\n",
       "      <td>0.884359</td>\n",
       "      <td>0.786329</td>\n",
       "      <td>15.028945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.860851</td>\n",
       "      <td>0.856680</td>\n",
       "      <td>0.779578</td>\n",
       "      <td>5.525958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.870773</td>\n",
       "      <td>0.860983</td>\n",
       "      <td>0.776709</td>\n",
       "      <td>6.875388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.889903</td>\n",
       "      <td>0.867592</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>8.217778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.899270</td>\n",
       "      <td>0.874645</td>\n",
       "      <td>0.782278</td>\n",
       "      <td>9.576630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909351</td>\n",
       "      <td>0.880234</td>\n",
       "      <td>0.785654</td>\n",
       "      <td>10.933760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.915383</td>\n",
       "      <td>0.881254</td>\n",
       "      <td>0.784979</td>\n",
       "      <td>12.290315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.923480</td>\n",
       "      <td>0.883517</td>\n",
       "      <td>0.786835</td>\n",
       "      <td>13.668214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.786871</td>\n",
       "      <td>0.705066</td>\n",
       "      <td>0.625992</td>\n",
       "      <td>1.555779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.873631</td>\n",
       "      <td>0.852555</td>\n",
       "      <td>0.736203</td>\n",
       "      <td>3.055902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.893078</td>\n",
       "      <td>0.870032</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>4.548212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.921257</td>\n",
       "      <td>0.871185</td>\n",
       "      <td>0.764219</td>\n",
       "      <td>8.352291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.932450</td>\n",
       "      <td>0.880944</td>\n",
       "      <td>0.782363</td>\n",
       "      <td>9.848957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.948722</td>\n",
       "      <td>0.856680</td>\n",
       "      <td>0.731055</td>\n",
       "      <td>21.057548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.937689</td>\n",
       "      <td>0.834768</td>\n",
       "      <td>0.694008</td>\n",
       "      <td>15.045598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.948008</td>\n",
       "      <td>0.857967</td>\n",
       "      <td>0.733080</td>\n",
       "      <td>22.571119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.949913</td>\n",
       "      <td>0.858987</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>24.086924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.950230</td>\n",
       "      <td>0.859830</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>25.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545722</td>\n",
       "      <td>0.653345</td>\n",
       "      <td>0.812658</td>\n",
       "      <td>2.049134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.846079</td>\n",
       "      <td>0.812911</td>\n",
       "      <td>4.086579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.890062</td>\n",
       "      <td>0.889017</td>\n",
       "      <td>0.807173</td>\n",
       "      <td>6.095935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.906969</td>\n",
       "      <td>0.896691</td>\n",
       "      <td>0.816878</td>\n",
       "      <td>8.145243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.752897</td>\n",
       "      <td>0.688121</td>\n",
       "      <td>0.634515</td>\n",
       "      <td>1.542210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.784251</td>\n",
       "      <td>0.710566</td>\n",
       "      <td>0.654346</td>\n",
       "      <td>3.023466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.830608</td>\n",
       "      <td>0.750133</td>\n",
       "      <td>0.613671</td>\n",
       "      <td>7.471663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.782071</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>8.963005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.851881</td>\n",
       "      <td>0.792983</td>\n",
       "      <td>0.625485</td>\n",
       "      <td>13.323386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.879346</td>\n",
       "      <td>0.794269</td>\n",
       "      <td>0.624388</td>\n",
       "      <td>18.708056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.518892</td>\n",
       "      <td>0.616350</td>\n",
       "      <td>0.814852</td>\n",
       "      <td>1.626448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.558978</td>\n",
       "      <td>0.642787</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>3.218406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.713526</td>\n",
       "      <td>0.788148</td>\n",
       "      <td>0.792405</td>\n",
       "      <td>4.775527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.827182</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>6.351990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.820368</td>\n",
       "      <td>0.862092</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>7.926014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.847119</td>\n",
       "      <td>0.872161</td>\n",
       "      <td>0.782869</td>\n",
       "      <td>9.501636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.919749</td>\n",
       "      <td>0.890082</td>\n",
       "      <td>0.803460</td>\n",
       "      <td>18.748190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.875377</td>\n",
       "      <td>0.889549</td>\n",
       "      <td>0.810211</td>\n",
       "      <td>12.678006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.928322</td>\n",
       "      <td>0.890880</td>\n",
       "      <td>0.804219</td>\n",
       "      <td>20.343794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.925861</td>\n",
       "      <td>0.891412</td>\n",
       "      <td>0.804641</td>\n",
       "      <td>21.945421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.569297</td>\n",
       "      <td>0.456086</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>1.932364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.603112</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.300928</td>\n",
       "      <td>3.818663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.884982</td>\n",
       "      <td>0.788458</td>\n",
       "      <td>0.606329</td>\n",
       "      <td>20.708272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.707176</td>\n",
       "      <td>0.666829</td>\n",
       "      <td>0.460675</td>\n",
       "      <td>7.601985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.728608</td>\n",
       "      <td>0.690295</td>\n",
       "      <td>0.499072</td>\n",
       "      <td>9.468606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.712340</td>\n",
       "      <td>0.523713</td>\n",
       "      <td>11.334867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.791713</td>\n",
       "      <td>0.743479</td>\n",
       "      <td>0.563629</td>\n",
       "      <td>13.198599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.940149</td>\n",
       "      <td>0.795023</td>\n",
       "      <td>0.616118</td>\n",
       "      <td>38.852246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.861724</td>\n",
       "      <td>0.780429</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>16.923508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.876647</td>\n",
       "      <td>0.787083</td>\n",
       "      <td>0.604473</td>\n",
       "      <td>18.802890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.905699</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.605738</td>\n",
       "      <td>28.185765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.789966</td>\n",
       "      <td>0.607173</td>\n",
       "      <td>30.030618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.943801</td>\n",
       "      <td>0.795644</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>44.564734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.926814</td>\n",
       "      <td>0.792140</td>\n",
       "      <td>0.610633</td>\n",
       "      <td>33.716603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.937927</td>\n",
       "      <td>0.794934</td>\n",
       "      <td>0.615949</td>\n",
       "      <td>35.564958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.941340</td>\n",
       "      <td>0.795112</td>\n",
       "      <td>0.616287</td>\n",
       "      <td>40.799320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.942848</td>\n",
       "      <td>0.795422</td>\n",
       "      <td>0.616878</td>\n",
       "      <td>42.681810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.943245</td>\n",
       "      <td>0.795910</td>\n",
       "      <td>0.617806</td>\n",
       "      <td>46.465843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.940705</td>\n",
       "      <td>0.796221</td>\n",
       "      <td>0.618397</td>\n",
       "      <td>48.353626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.943404</td>\n",
       "      <td>0.796443</td>\n",
       "      <td>0.618819</td>\n",
       "      <td>50.230263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.943166</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.619325</td>\n",
       "      <td>52.144946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.943721</td>\n",
       "      <td>0.796930</td>\n",
       "      <td>0.619747</td>\n",
       "      <td>54.035044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.508811</td>\n",
       "      <td>0.506166</td>\n",
       "      <td>0.771139</td>\n",
       "      <td>2.955071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.808382</td>\n",
       "      <td>0.857301</td>\n",
       "      <td>0.781435</td>\n",
       "      <td>5.857937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.881568</td>\n",
       "      <td>0.889461</td>\n",
       "      <td>0.809873</td>\n",
       "      <td>8.822795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                  2     0.605652    0.581308       0.310464   \n",
       "               1                  3     0.628671    0.606991       0.351730   \n",
       "               1                  4     0.956977    0.789567       0.605401   \n",
       "               1                  5     0.855056    0.727156       0.497975   \n",
       "               1                  6     0.894269    0.757807       0.551646   \n",
       "               1                  7     0.914907    0.770538       0.574684   \n",
       "               1                  8     0.933005    0.776260       0.585148   \n",
       "               1                  9     0.943007    0.783446       0.596709   \n",
       "               1                 10     0.951659    0.786861       0.601097   \n",
       "               1                 11     0.954040    0.789035       0.604726   \n",
       "8              1                  2     0.793301    0.771159       0.604810   \n",
       "               1                  3     0.873948    0.816403       0.666920   \n",
       "               1                  4     0.890618    0.825674       0.680084   \n",
       "               1                  5     0.907684    0.833393       0.690127   \n",
       "               1                  6     0.920702    0.833570       0.690127   \n",
       "32             1                  2     0.709478    0.691049       0.449536   \n",
       "               1                  3     0.793459    0.706574       0.467342   \n",
       "               1                  4     0.957850    0.766324       0.558397   \n",
       "               1                  5     0.852675    0.731946       0.496878   \n",
       "               1                  6     0.961819    0.768985       0.561772   \n",
       "               1                  8     0.965471    0.773510       0.569705   \n",
       "               1                  9     0.940308    0.755900       0.539156   \n",
       "               1                 10     0.944515    0.758960       0.544726   \n",
       "               1                 11     0.950389    0.762509       0.551139   \n",
       "122            1                  2     0.475155    0.627040       0.491224   \n",
       "               1                  3     0.607001    0.724494       0.582532   \n",
       "               1                  4     0.696618    0.753992       0.628945   \n",
       "               1                  5     0.847357    0.805979       0.651055   \n",
       "               1                  6     0.874663    0.817024       0.662700   \n",
       "               1                  7     0.904112    0.819686       0.664388   \n",
       "               1                  8     0.926179    0.827848       0.677722   \n",
       "1              3                  2     0.534291    0.430758       0.181603   \n",
       "               3                 11     0.921337    0.873181       0.768354   \n",
       "               3                  4     0.926099    0.873669       0.769114   \n",
       "               3                  6     0.931259    0.873891       0.768861   \n",
       "8              3                  2     0.416733    0.407736       0.671224   \n",
       "               3                  3     0.713447    0.707328       0.758481   \n",
       "               3                  4     0.922686    0.884359       0.786329   \n",
       "               3                  5     0.860851    0.856680       0.779578   \n",
       "               3                  6     0.870773    0.860983       0.776709   \n",
       "               3                  7     0.889903    0.867592       0.780000   \n",
       "               3                  8     0.899270    0.874645       0.782278   \n",
       "               3                  9     0.909351    0.880234       0.785654   \n",
       "               3                 10     0.915383    0.881254       0.784979   \n",
       "               3                 11     0.923480    0.883517       0.786835   \n",
       "32             3                  2     0.786871    0.705066       0.625992   \n",
       "               3                  3     0.873631    0.852555       0.736203   \n",
       "               3                  4     0.893078    0.870032       0.766667   \n",
       "               3                  7     0.921257    0.871185       0.764219   \n",
       "               3                  8     0.932450    0.880944       0.782363   \n",
       "...                             ...          ...         ...            ...   \n",
       "               3                 10     0.948722    0.856680       0.731055   \n",
       "               3                 11     0.937689    0.834768       0.694008   \n",
       "               3                 12     0.948008    0.857967       0.733080   \n",
       "               3                 14     0.949913    0.858987       0.734599   \n",
       "               3                 16     0.950230    0.859830       0.735949   \n",
       "122            3                  2     0.545722    0.653345       0.812658   \n",
       "               3                  3     0.739006    0.846079       0.812911   \n",
       "               3                  4     0.890062    0.889017       0.807173   \n",
       "               3                  5     0.906969    0.896691       0.816878   \n",
       "1              5                  2     0.752897    0.688121       0.634515   \n",
       "               5                  3     0.784251    0.710566       0.654346   \n",
       "               5                  7     0.830608    0.750133       0.613671   \n",
       "               5                  8     0.844023    0.782071       0.611814   \n",
       "               5                  4     0.851881    0.792983       0.625485   \n",
       "               5                 14     0.879346    0.794269       0.624388   \n",
       "8              5                  2     0.518892    0.616350       0.814852   \n",
       "               5                  3     0.558978    0.642787       0.791055   \n",
       "               5                  4     0.713526    0.788148       0.792405   \n",
       "               5                  5     0.771313    0.827182       0.777300   \n",
       "               5                  6     0.820368    0.862092       0.773333   \n",
       "               5                  7     0.847119    0.872161       0.782869   \n",
       "               5                  8     0.919749    0.890082       0.803460   \n",
       "               5                  9     0.875377    0.889549       0.810211   \n",
       "               5                 10     0.928322    0.890880       0.804219   \n",
       "               5                 12     0.925861    0.891412       0.804641   \n",
       "32             5                  2     0.569297    0.456086       0.227848   \n",
       "               5                  3     0.603112    0.495875       0.300928   \n",
       "               5                  4     0.884982    0.788458       0.606329   \n",
       "               5                  5     0.707176    0.666829       0.460675   \n",
       "               5                  6     0.728608    0.690295       0.499072   \n",
       "               5                  7     0.756787    0.712340       0.523713   \n",
       "               5                  8     0.791713    0.743479       0.563629   \n",
       "               5                  9     0.940149    0.795023       0.616118   \n",
       "               5                 10     0.861724    0.780429       0.597046   \n",
       "               5                 11     0.876647    0.787083       0.604473   \n",
       "               5                 14     0.905699    0.789123       0.605738   \n",
       "               5                 16     0.909033    0.789966       0.607173   \n",
       "               5                 18     0.943801    0.795644       0.617300   \n",
       "               5                 20     0.926814    0.792140       0.610633   \n",
       "               5                 22     0.937927    0.794934       0.615949   \n",
       "               5                 12     0.941340    0.795112       0.616287   \n",
       "               5                 15     0.942848    0.795422       0.616878   \n",
       "               5                 21     0.943245    0.795910       0.617806   \n",
       "               5                 24     0.940705    0.796221       0.618397   \n",
       "               5                 27     0.943404    0.796443       0.618819   \n",
       "               5                 30     0.943166    0.796709       0.619325   \n",
       "               5                 33     0.943721    0.796930       0.619747   \n",
       "122            5                  2     0.508811    0.506166       0.771139   \n",
       "               5                  3     0.808382    0.857301       0.781435   \n",
       "               5                  4     0.881568    0.889461       0.809873   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                1.158116  \n",
       "               1                2.297762  \n",
       "               1               12.285068  \n",
       "               1                4.561070  \n",
       "               1                5.691493  \n",
       "               1                6.811489  \n",
       "               1                7.927863  \n",
       "               1                9.022055  \n",
       "               1               10.104383  \n",
       "               1               11.189151  \n",
       "8              1                1.131583  \n",
       "               1                2.226308  \n",
       "               1                3.314382  \n",
       "               1                4.405876  \n",
       "               1                5.508591  \n",
       "32             1                1.133849  \n",
       "               1                2.255247  \n",
       "               1               12.141247  \n",
       "               1                4.513144  \n",
       "               1               13.249446  \n",
       "               1               14.350200  \n",
       "               1                8.804977  \n",
       "               1                9.914802  \n",
       "               1               11.017808  \n",
       "122            1                1.113718  \n",
       "               1                2.222951  \n",
       "               1                3.333286  \n",
       "               1                4.452556  \n",
       "               1                5.592731  \n",
       "               1                6.711334  \n",
       "               1                7.823106  \n",
       "1              3                1.344532  \n",
       "               3               10.175471  \n",
       "               3               11.436996  \n",
       "               3               12.726343  \n",
       "8              3                1.382200  \n",
       "               3                2.778562  \n",
       "               3               15.028945  \n",
       "               3                5.525958  \n",
       "               3                6.875388  \n",
       "               3                8.217778  \n",
       "               3                9.576630  \n",
       "               3               10.933760  \n",
       "               3               12.290315  \n",
       "               3               13.668214  \n",
       "32             3                1.555779  \n",
       "               3                3.055902  \n",
       "               3                4.548212  \n",
       "               3                8.352291  \n",
       "               3                9.848957  \n",
       "...                                  ...  \n",
       "               3               21.057548  \n",
       "               3               15.045598  \n",
       "               3               22.571119  \n",
       "               3               24.086924  \n",
       "               3               25.573333  \n",
       "122            3                2.049134  \n",
       "               3                4.086579  \n",
       "               3                6.095935  \n",
       "               3                8.145243  \n",
       "1              5                1.542210  \n",
       "               5                3.023466  \n",
       "               5                7.471663  \n",
       "               5                8.963005  \n",
       "               5               13.323386  \n",
       "               5               18.708056  \n",
       "8              5                1.626448  \n",
       "               5                3.218406  \n",
       "               5                4.775527  \n",
       "               5                6.351990  \n",
       "               5                7.926014  \n",
       "               5                9.501636  \n",
       "               5               18.748190  \n",
       "               5               12.678006  \n",
       "               5               20.343794  \n",
       "               5               21.945421  \n",
       "32             5                1.932364  \n",
       "               5                3.818663  \n",
       "               5               20.708272  \n",
       "               5                7.601985  \n",
       "               5                9.468606  \n",
       "               5               11.334867  \n",
       "               5               13.198599  \n",
       "               5               38.852246  \n",
       "               5               16.923508  \n",
       "               5               18.802890  \n",
       "               5               28.185765  \n",
       "               5               30.030618  \n",
       "               5               44.564734  \n",
       "               5               33.716603  \n",
       "               5               35.564958  \n",
       "               5               40.799320  \n",
       "               5               42.681810  \n",
       "               5               46.465843  \n",
       "               5               48.353626  \n",
       "               5               50.230263  \n",
       "               5               52.144946  \n",
       "               5               54.035044  \n",
       "122            5                2.955071  \n",
       "               5                5.857937  \n",
       "               5                8.822795  \n",
       "\n",
       "[970 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T22:39:47.611145Z",
     "start_time": "2017-06-18T22:39:47.602957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.760094\n",
       "                3                0.713199\n",
       "                5                0.625079\n",
       "8               1                0.774272\n",
       "                3                0.753633\n",
       "                5                0.718815\n",
       "32              1                0.775530\n",
       "                3                0.769248\n",
       "                5                0.789377\n",
       "122             1                0.748377\n",
       "                3                0.810419\n",
       "                5                0.813828\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.mean().test_score"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
