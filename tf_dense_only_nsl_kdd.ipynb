{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:36.061386Z",
     "start_time": "2017-07-19T18:29:35.574036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:36.097659Z",
     "start_time": "2017-07-19T18:29:36.063126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:36.355150Z",
     "start_time": "2017-07-19T18:29:36.099699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:43.673374Z",
     "start_time": "2017-07-19T18:29:42.544511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:43.874876Z",
     "start_time": "2017-07-19T18:29:43.674939Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "            #loss = tf.losses.mean_squared_error(labels = self.y_, predictions = self.y)\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "            \n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:44.453931Z",
     "start_time": "2017-07-19T18:29:43.876611Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 1000\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    \n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_data_temp.values, \n",
    "                                                                                  y_data_temp.values, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                                   net.regularized_loss, \n",
    "                                                                   ], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                                             net.y_: y_train[i,:], \n",
    "                                                                             net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                            train_batch()\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            #print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                                print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                             .format(epochs,h,f)))\n",
    "                                train_batch()\n",
    "\n",
    "\n",
    "                        valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                              feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "                    \n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(key, train_loss, valid_accuracy))\n",
    "                    \n",
    "                    end_time = time.perf_counter() \n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                               net.pred, \n",
    "                                                                               net.actual, net.y], \n",
    "                                                                              feed_dict={net.x: x_test, \n",
    "                                                                                         net.y_: y_test, \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        \n",
    "                        \n",
    "                        f1_score = me.f1_score(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        print(\"Key {} Test Accuracy: {} F1 score: {}, recall {}, precision {}\".format(key, accuracy, f1_score, recall, prec))\n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = accuracy\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\": actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):(curr_pred, \n",
    "                                                   Train.result(key, f, h, valid_accuracy, accuracy, f1_score, end_time - start_time))})\n",
    "\n",
    "                            #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:29:44.518372Z",
     "start_time": "2017-07-19T18:29:44.455584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        print(\"********************************** Training ******************************\")\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-4]\n",
    "        print(\"***************************** Entering Loop **********************\")\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - hidden layers:{} features count:{}\".format(h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = df_results#temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results, ignore_index=True).to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:45:17.093671Z",
     "start_time": "2017-07-19T18:29:44.519981Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "***************************** Entering Loop **********************\n",
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20140115 | Training Loss: 0.508328 | Validation Accuracy: 0.765363\n",
      "Key 20140107 | Training Loss: 0.496833 | Validation Accuracy: 0.810056\n",
      "Key 20140109 | Training Loss: 0.490550 | Validation Accuracy: 0.843575\n",
      "Key 20140104 | Training Loss: 0.586180 | Validation Accuracy: 0.826816\n",
      "Key 20140119 | Training Loss: 0.434777 | Validation Accuracy: 0.871508\n",
      "Key 20140120 | Training Loss: 0.508094 | Validation Accuracy: 0.905028\n",
      "Key 20140123 | Training Loss: 0.414506 | Validation Accuracy: 0.921788\n",
      "Key 20151224 Test Accuracy: 0.6933549642562866 F1 score: 0.8161100096732256, recall 0.6922981229754882, precision 0.9938524308656053\n",
      "Key 20151204 Test Accuracy: 0.719265341758728 F1 score: 0.8348292164943242, recall 0.7189905390251603, precision 0.995162821536746\n",
      "Key 20151216 Test Accuracy: 0.6729779839515686 F1 score: 0.800217617941152, recall 0.669094282628279, precision 0.9952601263966294\n",
      "Key 20151222 Test Accuracy: 0.6612095236778259 F1 score: 0.7930543834295651, recall 0.6578765567102478, precision 0.9981502864253444\n",
      "Key 20151214 Test Accuracy: 0.7167041301727295 F1 score: 0.8327487630295047, recall 0.7160296406893538, precision 0.9949313074279922\n",
      "Key 20151202 Test Accuracy: 0.6990077495574951 F1 score: 0.8181392525795543, recall 0.7100415387393896, precision 0.9650615718201531\n",
      "Key 20151227 Test Accuracy: 0.7542936205863953 F1 score: 0.8585137134403096, recall 0.7543548780634142, precision 0.9960442160523494\n",
      "Key 20151203 Test Accuracy: 0.7749700546264648 F1 score: 0.8610385583708499, recall 0.7692880475491869, precision 0.9776383381398702\n",
      "Key 20151223 Test Accuracy: 0.7098073363304138 F1 score: 0.8249441082660148, recall 0.708404793963006, precision 0.9873769988796998\n",
      "Key 20151205 Test Accuracy: 0.711763322353363 F1 score: 0.8299331860177823, recall 0.7129191966370855, precision 0.9929017116034118\n",
      "Key 20151229 Test Accuracy: 0.7506833076477051 F1 score: 0.8537393708690794, recall 0.748019882299149, precision 0.9942606760737339\n",
      "Key 20151208 Test Accuracy: 0.6905773878097534 F1 score: 0.8138996605510477, recall 0.6880773773422287, precision 0.9960353399996303\n",
      "Key 20151219 Test Accuracy: 0.6591798663139343 F1 score: 0.7752336391956267, recall 0.8532301637594637, precision 0.7103025860584959\n",
      "Key 20151206 Test Accuracy: 0.7218996286392212 F1 score: 0.836189985161339, recall 0.7204006348621667, precision 0.9963290655598924\n",
      "Key 20151225 Test Accuracy: 0.738126814365387 F1 score: 0.8472845966411969, recall 0.7368831143049986, precision 0.9965969956908702\n",
      "Key 20151210 Test Accuracy: 0.7742326259613037 F1 score: 0.871145660470378, recall 0.7734661096107774, precision 0.9970628095259687\n",
      "Key 20151217 Test Accuracy: 0.7098066210746765 F1 score: 0.8246406789254652, recall 0.7060595218734753, precision 0.9910925037323824\n",
      "Key 20151207 Test Accuracy: 0.7176651358604431 F1 score: 0.8325836771411731, recall 0.728097397947004, precision 0.9720833876599012\n",
      "Key 20151215 Test Accuracy: 0.7244996428489685 F1 score: 0.8371359362428004, recall 0.7219026556773204, precision 0.9961449849437157\n",
      "Key 20151213 Test Accuracy: 0.4589079022407532 F1 score: 0.6257662191626785, recall 0.45668142810678275, precision 0.9936688187964767\n",
      "Key 20151209 Test Accuracy: 0.6871610879898071 F1 score: 0.8117855965094747, recall 0.6868108047782772, precision 0.9923591686874763\n",
      "Key 20151228 Test Accuracy: 0.6550620198249817 F1 score: 0.7892932458774476, recall 0.6534506621760318, precision 0.996437682521567\n",
      "Key 20151226 Test Accuracy: 0.5797197222709656 F1 score: 0.7229520206704402, recall 0.6076180997584305, precision 0.892327545536119\n",
      "Key 20151218 Test Accuracy: 0.7327587604522705 F1 score: 0.8425696167631652, recall 0.7306059616550823, precision 0.9950602773233375\n",
      "Key 20151231 Test Accuracy: 0.7048707604408264 F1 score: 0.8172018445148529, recall 0.7128811532584867, precision 0.9572882724686129\n",
      "Key 20151212 Test Accuracy: 0.682628870010376 F1 score: 0.8073834039703549, recall 0.6799856161402048, precision 0.9935236957519648\n",
      "Key 20151211 Test Accuracy: 0.7438471913337708 F1 score: 0.851716671398616, recall 0.7430984570388277, precision 0.9975241178167367\n",
      "Key 20151221 Test Accuracy: 0.6426575779914856 F1 score: 0.7792390112599415, recall 0.6391636808321318, precision 0.9979420963871534\n",
      "Key 20151201 Test Accuracy: 0.7287975549697876 F1 score: 0.8379107479918987, recall 0.7229960103284465, precision 0.9962584855712526\n",
      "Key 20151220 Test Accuracy: 0.722233235836029 F1 score: 0.8326711925409765, recall 0.7365746078421003, precision 0.9576042189509004\n",
      "Key 20151230 Test Accuracy: 0.7434037923812866 F1 score: 0.8451053155587737, recall 0.7376830110653129, precision 0.9891459634849146\n",
      "Current Layer Attributes - hidden layers:1 features count:4\n",
      "Key 20140115 | Training Loss: 0.551857 | Validation Accuracy: 0.821229\n",
      "Key 20140107 | Training Loss: 0.425592 | Validation Accuracy: 0.882682\n",
      "Key 20140109 | Training Loss: 0.370806 | Validation Accuracy: 0.804469\n",
      "Key 20140104 | Training Loss: 0.562176 | Validation Accuracy: 0.837989\n",
      "Key 20140119 | Training Loss: 0.355267 | Validation Accuracy: 0.938547\n",
      "Key 20140120 | Training Loss: 0.596307 | Validation Accuracy: 0.905028\n",
      "Key 20140123 | Training Loss: 0.528710 | Validation Accuracy: 0.905028\n",
      "Key 20151224 Test Accuracy: 0.7115218043327332 F1 score: 0.8291861125652993, recall 0.7123745819397993, precision 0.9918196049695234\n",
      "Key 20151204 Test Accuracy: 0.7446222305297852 F1 score: 0.8520573805350122, recall 0.7452831215773372, precision 0.9945421614396683\n",
      "Key 20151216 Test Accuracy: 0.711578369140625 F1 score: 0.8282084513519445, recall 0.7102726250072828, precision 0.9931069488222282\n",
      "Key 20151222 Test Accuracy: 0.7111330032348633 F1 score: 0.8287444457891734, recall 0.7083377973496673, precision 0.9984689812476241\n",
      "Key 20151214 Test Accuracy: 0.6731284856796265 F1 score: 0.8021595104435342, recall 0.6727732368498072, precision 0.9931623075333588\n",
      "Key 20151202 Test Accuracy: 0.7171757817268372 F1 score: 0.8313257136708873, recall 0.7309356106538697, precision 0.9636823366668934\n",
      "Key 20151227 Test Accuracy: 0.7423011660575867 F1 score: 0.8506227488933492, recall 0.7424912102461131, precision 0.995618010673389\n",
      "Key 20151203 Test Accuracy: 0.7523719072341919 F1 score: 0.8500254218939676, recall 0.7743452457461473, precision 0.9421011194754212\n",
      "Key 20151223 Test Accuracy: 0.7147092819213867 F1 score: 0.8286611387769395, recall 0.7147530576616887, precision 0.9857585663953004\n",
      "Key 20151205 Test Accuracy: 0.7554518580436707 F1 score: 0.8593531372166583, recall 0.7573096683792621, precision 0.9931787109135725\n",
      "Key 20151229 Test Accuracy: 0.7375049591064453 F1 score: 0.8447650825907205, recall 0.7342291184235755, precision 0.9944812086402797\n",
      "Key 20151208 Test Accuracy: 0.7256147861480713 F1 score: 0.8390676780322557, recall 0.7274044753726817, precision 0.9912305885423464\n",
      "Key 20151219 Test Accuracy: 0.6011266708374023 F1 score: 0.7306353215706476, recall 0.7852983357235767, precision 0.6830870279146142\n",
      "Key 20151206 Test Accuracy: 0.7187826633453369 F1 score: 0.8343790077199688, recall 0.7189489022626444, precision 0.9939638580383288\n",
      "Key 20151225 Test Accuracy: 0.7525404691696167 F1 score: 0.8571872282271089, recall 0.753310239409219, precision 0.9942945106773221\n",
      "Key 20151210 Test Accuracy: 0.7220191359519958 F1 score: 0.836692991160226, recall 0.7217130641070235, precision 0.9952518257026597\n",
      "Key 20151217 Test Accuracy: 0.6737536191940308 F1 score: 0.8007310859352351, recall 0.6782826475849731, precision 0.9771294964365872\n",
      "Key 20151207 Test Accuracy: 0.7047960758209229 F1 score: 0.8237234554326164, recall 0.7153258534256386, precision 0.9708407581402884\n",
      "Key 20151215 Test Accuracy: 0.6864327192306519 F1 score: 0.8108095224681974, recall 0.6850717468593586, precision 0.9930787766843944\n",
      "Key 20151213 Test Accuracy: 0.6321316361427307 F1 score: 0.7729481752974885, recall 0.632107383221997, precision 0.9945440159349376\n",
      "Key 20151209 Test Accuracy: 0.6619428992271423 F1 score: 0.7935386350585442, recall 0.6613741624494036, precision 0.9917164409659647\n",
      "Key 20151228 Test Accuracy: 0.7533665895462036 F1 score: 0.858101656925875, recall 0.7542670662010935, precision 0.9950884122103597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151226 Test Accuracy: 0.64101243019104 F1 score: 0.7753146598116044, recall 0.6863092782206377, precision 0.8908457613814756\n",
      "Key 20151218 Test Accuracy: 0.7202648520469666 F1 score: 0.8346139439770807, recall 0.721101366795727, precision 0.9905404214758865\n",
      "Key 20151231 Test Accuracy: 0.6848719120025635 F1 score: 0.802463827491512, recall 0.6916928606642495, precision 0.9554787077330268\n",
      "Key 20151212 Test Accuracy: 0.7405572533607483 F1 score: 0.8484748878196465, recall 0.74257811760698, precision 0.9895984665052462\n",
      "Key 20151211 Test Accuracy: 0.7188960909843445 F1 score: 0.8349710245731741, recall 0.7183275136538634, precision 0.9968399647136296\n",
      "Key 20151221 Test Accuracy: 0.6696871519088745 F1 score: 0.7994458270261949, recall 0.6672091131000814, precision 0.9970561884039422\n",
      "Key 20151201 Test Accuracy: 0.7374376058578491 F1 score: 0.8447177193963108, recall 0.7365818167984669, precision 0.9900672410707437\n",
      "Key 20151220 Test Accuracy: 0.7124971747398376 F1 score: 0.8261105758697173, recall 0.7278482351572334, precision 0.9550454195321337\n",
      "Key 20151230 Test Accuracy: 0.7420543432235718 F1 score: 0.8456427110197343, recall 0.7446174543911369, precision 0.978384021734617\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20140115 | Training Loss: 0.762419 | Validation Accuracy: 0.770950\n",
      "Key 20140107 | Training Loss: 0.567865 | Validation Accuracy: 0.810056\n",
      "Key 20140109 | Training Loss: 0.468297 | Validation Accuracy: 0.860335\n",
      "Key 20140104 | Training Loss: 0.490211 | Validation Accuracy: 0.910615\n",
      "Key 20140119 | Training Loss: 0.403197 | Validation Accuracy: 0.849162\n",
      "Key 20140120 | Training Loss: 0.327890 | Validation Accuracy: 0.854749\n",
      "Key 20140123 | Training Loss: 0.354599 | Validation Accuracy: 0.882682\n",
      "Key 20151224 Test Accuracy: 0.6924943327903748 F1 score: 0.8163887656477274, recall 0.6955325572604916, precision 0.9880778406889567\n",
      "Key 20151204 Test Accuracy: 0.7185168862342834 F1 score: 0.8345126539239691, recall 0.7192554608369351, precision 0.9937574584582677\n",
      "Key 20151216 Test Accuracy: 0.7447214722633362 F1 score: 0.8510112066118242, recall 0.7448269371254935, precision 0.9925050464030195\n",
      "Key 20151222 Test Accuracy: 0.6859689354896545 F1 score: 0.8111604823523995, recall 0.6835227923438117, precision 0.9974123710228682\n",
      "Key 20151214 Test Accuracy: 0.6873751878738403 F1 score: 0.8125560493183903, recall 0.6879413972861887, precision 0.9923032212670929\n",
      "Key 20151202 Test Accuracy: 0.6813317537307739 F1 score: 0.8059532310335014, recall 0.6940366538524386, precision 0.9609032672890074\n",
      "Key 20151227 Test Accuracy: 0.7132094502449036 F1 score: 0.8309899836943861, recall 0.7134680228953589, precision 0.9948631569553748\n",
      "Key 20151203 Test Accuracy: 0.715136706829071 F1 score: 0.8235314609621304, recall 0.7334488673383004, precision 0.9388403494837172\n",
      "Key 20151223 Test Accuracy: 0.683365523815155 F1 score: 0.8081980609376824, recall 0.691146731924087, precision 0.9729801801801802\n",
      "Key 20151205 Test Accuracy: 0.7345365285873413 F1 score: 0.8454850904784178, recall 0.7362204577300326, precision 0.9928345754410364\n",
      "Key 20151229 Test Accuracy: 0.6899852752685547 F1 score: 0.8117978812590505, recall 0.6873368338766058, precision 0.9912996771946245\n",
      "Key 20151208 Test Accuracy: 0.7189915180206299 F1 score: 0.8346441567077338, recall 0.7212117342867175, precision 0.9904173242153252\n",
      "Key 20151219 Test Accuracy: 0.6337465643882751 F1 score: 0.757974451391145, recall 0.8325584497882134, precision 0.6956548395367242\n",
      "Key 20151206 Test Accuracy: 0.7294677495956421 F1 score: 0.8418924988770774, recall 0.731023200637029, precision 0.9924035739235945\n",
      "Key 20151225 Test Accuracy: 0.6740186810493469 F1 score: 0.8033772358251415, recall 0.675522094731409, precision 0.9909290118738215\n",
      "Key 20151210 Test Accuracy: 0.739784836769104 F1 score: 0.8487577465735769, recall 0.7399997655050045, precision 0.9949920124437718\n",
      "Key 20151217 Test Accuracy: 0.6998739838600159 F1 score: 0.8195502505012859, recall 0.7052463815254513, precision 0.9780728858896923\n",
      "Key 20151207 Test Accuracy: 0.6930151581764221 F1 score: 0.8157493170900185, recall 0.7047896004687602, precision 0.9681756525109186\n",
      "Key 20151215 Test Accuracy: 0.7252789735794067 F1 score: 0.8377203030997294, recall 0.7229571642506948, precision 0.9957938905503749\n",
      "Key 20151213 Test Accuracy: 0.549065113067627 F1 score: 0.7066969085859002, recall 0.5484075158247642, precision 0.9934373031304246\n",
      "Key 20151209 Test Accuracy: 0.694965660572052 F1 score: 0.8175764101296848, recall 0.6958634336462478, precision 0.9908927545444544\n",
      "Key 20151228 Test Accuracy: 0.7239826917648315 F1 score: 0.8384557452587902, recall 0.7244973488194197, precision 0.9949552967494728\n",
      "Key 20151226 Test Accuracy: 0.6053351163864136 F1 score: 0.7472775885887142, recall 0.6465526744292251, precision 0.8851772639857879\n",
      "Key 20151218 Test Accuracy: 0.7182614803314209 F1 score: 0.8337072746750754, recall 0.7215212016606801, precision 0.9872030125576423\n",
      "Key 20151231 Test Accuracy: 0.6480677127838135 F1 score: 0.7745879641814305, recall 0.653431559367428, precision 0.9508995807383889\n",
      "Key 20151212 Test Accuracy: 0.732363224029541 F1 score: 0.8429041498411264, recall 0.7340159358025626, precision 0.9897258700855922\n",
      "Key 20151211 Test Accuracy: 0.6965863108634949 F1 score: 0.8194626105790191, recall 0.6955710747008088, precision 0.9970524568563532\n",
      "Key 20151221 Test Accuracy: 0.6765184998512268 F1 score: 0.8044059616749467, recall 0.6741372008812375, precision 0.9970793364815506\n",
      "Key 20151201 Test Accuracy: 0.713324785232544 F1 score: 0.8285904105019696, recall 0.7146505114292915, precision 0.9857534534101196\n",
      "Key 20151220 Test Accuracy: 0.6817505359649658 F1 score: 0.8037972166389799, recall 0.6947726383262288, precision 0.9534070208797509\n",
      "Key 20151230 Test Accuracy: 0.7052080035209656 F1 score: 0.8220796715301102, recall 0.7177103696112468, precision 0.9619690185162774\n",
      "Current Layer Attributes - hidden layers:1 features count:16\n",
      "Key 20140115 | Training Loss: 0.631035 | Validation Accuracy: 0.759777\n",
      "Key 20140107 | Training Loss: 0.599690 | Validation Accuracy: 0.782123\n",
      "Key 20140109 | Training Loss: 0.832233 | Validation Accuracy: 0.759777\n",
      "Key 20140104 | Training Loss: 0.359997 | Validation Accuracy: 0.754190\n",
      "Key 20140119 | Training Loss: 0.506156 | Validation Accuracy: 0.826816\n",
      "Key 20140120 | Training Loss: 0.412588 | Validation Accuracy: 0.798883\n",
      "Key 20140123 | Training Loss: 0.548045 | Validation Accuracy: 0.821229\n",
      "Key 20151224 Test Accuracy: 0.5242146253585815 F1 score: 0.6808898533628143, recall 0.5164329357275802, precision 0.9990281377267679\n",
      "Key 20151204 Test Accuracy: 0.5444095730781555 F1 score: 0.7000620032958406, recall 0.5388183035568475, precision 0.9990243442920776\n",
      "Key 20151216 Test Accuracy: 0.5729433298110962 F1 score: 0.7214330681623076, recall 0.5649517987513838, precision 0.9978073820775253\n",
      "Key 20151222 Test Accuracy: 0.597284197807312 F1 score: 0.7437719035790245, recall 0.5923497227384061, precision 0.9991961014033951\n",
      "Key 20151214 Test Accuracy: 0.4871654510498047 F1 score: 0.6485384520676988, recall 0.4803759803526512, precision 0.9978501444984845\n",
      "Key 20151202 Test Accuracy: 0.5722621083259583 F1 score: 0.7112852829415851, recall 0.5525770358712385, precision 0.9978955542613085\n",
      "Key 20151227 Test Accuracy: 0.5042234659194946 F1 score: 0.6655235494589306, recall 0.4991260244713148, precision 0.9983518813354562\n",
      "Key 20151203 Test Accuracy: 0.655552327632904 F1 score: 0.7655452420336017, recall 0.6205195484422854, precision 0.9990366501113205\n",
      "Key 20151223 Test Accuracy: 0.5556730628013611 F1 score: 0.701364348911975, recall 0.5405700126453318, precision 0.9983170554142596\n",
      "Key 20151205 Test Accuracy: 0.5978752970695496 F1 score: 0.7443630518022768, recall 0.593457262961233, precision 0.9981836807803259\n",
      "Key 20151229 Test Accuracy: 0.5071487426757812 F1 score: 0.6609534619750284, recall 0.4938457794362749, precision 0.9989930182599356\n",
      "Key 20151208 Test Accuracy: 0.542975902557373 F1 score: 0.6975696896680653, recall 0.5359977016631021, precision 0.9985845970859352\n",
      "Key 20151219 Test Accuracy: 0.6818562150001526 F1 score: 0.700364822933211, recall 0.5397525264316446, precision 0.9970550544630095\n",
      "Key 20151206 Test Accuracy: 0.5696505308151245 F1 score: 0.720814924587621, recall 0.5638464413592117, precision 0.9988964061225469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151225 Test Accuracy: 0.5031386017799377 F1 score: 0.6632824597061642, recall 0.4963969508079565, precision 0.9992107970949341\n",
      "Key 20151210 Test Accuracy: 0.5240436792373657 F1 score: 0.68253825359387, recall 0.5185505082679027, precision 0.9982169457861237\n",
      "Key 20151217 Test Accuracy: 0.5110123157501221 F1 score: 0.6637516264713893, recall 0.49941453894942267, precision 0.9892853461161796\n",
      "Key 20151207 Test Accuracy: 0.5764872431755066 F1 score: 0.7187557538048934, recall 0.5612534994249007, precision 0.9991404039135768\n",
      "Key 20151215 Test Accuracy: 0.4819200038909912 F1 score: 0.6420281835528945, recall 0.47368305426691515, precision 0.9960042498152254\n",
      "Key 20151213 Test Accuracy: 0.30716460943222046 F1 score: 0.4625709880459399, recall 0.3009957717230854, precision 0.9986469435866786\n",
      "Key 20151209 Test Accuracy: 0.5108023881912231 F1 score: 0.66866948622538, recall 0.5025303578556988, precision 0.9989164007440146\n",
      "Key 20151228 Test Accuracy: 0.46322453022003174 F1 score: 0.6277518786069914, recall 0.45778335438318435, precision 0.9984696082197042\n",
      "Key 20151226 Test Accuracy: 0.49690181016921997 F1 score: 0.6145000261355915, recall 0.4443114639992189, precision 0.9960109293475575\n",
      "Key 20151218 Test Accuracy: 0.551456868648529 F1 score: 0.7030362574867268, recall 0.542426645519429, precision 0.9987652862925305\n",
      "Key 20151231 Test Accuracy: 0.6117003560066223 F1 score: 0.7347254549594057, recall 0.5810906726059153, precision 0.9987980480300007\n",
      "Key 20151212 Test Accuracy: 0.45122280716896057 F1 score: 0.6107751093136546, recall 0.4401726063175426, precision 0.9973156089193825\n",
      "Key 20151211 Test Accuracy: 0.6229443550109863 F1 score: 0.7650404213913944, recall 0.6200704330438416, precision 0.9984813752204649\n",
      "Key 20151221 Test Accuracy: 0.5364643335342407 F1 score: 0.6932179057728282, recall 0.5307648018880943, precision 0.998979034381465\n",
      "Key 20151201 Test Accuracy: 0.5537236332893372 F1 score: 0.7014569993954871, recall 0.5407502865050192, precision 0.9980778514065037\n",
      "Key 20151220 Test Accuracy: 0.5259827971458435 F1 score: 0.6624116031764543, recall 0.49564210236828465, precision 0.9983169107856191\n",
      "Key 20151230 Test Accuracy: 0.5156257152557373 F1 score: 0.6578817577483199, recall 0.4907924497636601, precision 0.9974676111131501\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20140115 | Training Loss: 0.805739 | Validation Accuracy: 0.821229\n",
      "Key 20140107 | Training Loss: 0.519910 | Validation Accuracy: 0.832402\n",
      "Key 20140109 | Training Loss: 0.472209 | Validation Accuracy: 0.815642\n",
      "Key 20140104 | Training Loss: 0.510068 | Validation Accuracy: 0.804469\n",
      "Key 20140119 | Training Loss: 0.326050 | Validation Accuracy: 0.849162\n",
      "Key 20140120 | Training Loss: 0.447819 | Validation Accuracy: 0.921788\n",
      "Key 20140123 | Training Loss: 0.422385 | Validation Accuracy: 0.893855\n",
      "Key 20151224 Test Accuracy: 0.6052812337875366 F1 score: 0.7502982450399098, recall 0.603346395471792, precision 0.991882452883617\n",
      "Key 20151204 Test Accuracy: 0.6525909900665283 F1 score: 0.7871661273515055, recall 0.6510725704310968, precision 0.9951906008697967\n",
      "Key 20151216 Test Accuracy: 0.663414478302002 F1 score: 0.7928831156210268, recall 0.6581856969349295, precision 0.996897825083494\n",
      "Key 20151222 Test Accuracy: 0.6275627017021179 F1 score: 0.7679367639209094, recall 0.6245091121348588, precision 0.9968862783950181\n",
      "Key 20151214 Test Accuracy: 0.6150722503662109 F1 score: 0.7582216297353753, recall 0.6127792600007635, precision 0.994191768001046\n",
      "Key 20151202 Test Accuracy: 0.6485546827316284 F1 score: 0.7811721334912789, recall 0.6578730101395804, precision 0.9613489839263049\n",
      "Key 20151227 Test Accuracy: 0.7075610756874084 F1 score: 0.8269352676848367, recall 0.7070082037702944, precision 0.9958589682915286\n",
      "Key 20151203 Test Accuracy: 0.6959567666053772 F1 score: 0.8093599830752647, recall 0.7121696311037634, precision 0.9372699538595253\n",
      "Key 20151223 Test Accuracy: 0.6677455306053162 F1 score: 0.7943779472657324, recall 0.6649294267620272, precision 0.9864128990119162\n",
      "Key 20151205 Test Accuracy: 0.7001334428787231 F1 score: 0.8218511502622445, recall 0.7011415226529659, precision 0.9927675784350034\n",
      "Key 20151229 Test Accuracy: 0.7228344082832336 F1 score: 0.8345385361550139, recall 0.7185430463576159, precision 0.9951942473966712\n",
      "Key 20151208 Test Accuracy: 0.6653399467468262 F1 score: 0.796207588645704, recall 0.6648194847894787, precision 0.9923194206213074\n",
      "Key 20151219 Test Accuracy: 0.5774716734886169 F1 score: 0.7045749534151513, recall 0.7314344795384051, precision 0.6796182094270042\n",
      "Key 20151206 Test Accuracy: 0.6757434606552124 F1 score: 0.8038371131890272, recall 0.6742918740892816, precision 0.9949962031893209\n",
      "Key 20151225 Test Accuracy: 0.6483323574066162 F1 score: 0.783667867558758, recall 0.646107118751737, precision 0.9956484165283694\n",
      "Key 20151210 Test Accuracy: 0.6521016359329224 F1 score: 0.7864560033138056, recall 0.6492736517514822, precision 0.9971369578526583\n",
      "Key 20151217 Test Accuracy: 0.5942327976226807 F1 score: 0.7369013165939755, recall 0.5880110587087332, precision 0.9867583647180831\n",
      "Key 20151207 Test Accuracy: 0.6825106143951416 F1 score: 0.8078092342770283, recall 0.6919909286225829, precision 0.9701895575975172\n",
      "Key 20151215 Test Accuracy: 0.6067041158676147 F1 score: 0.7496559701185475, recall 0.6003851885483298, precision 0.9977121856537003\n",
      "Key 20151213 Test Accuracy: 0.623288631439209 F1 score: 0.7661142031492113, recall 0.6228326953388876, precision 0.9950157282352801\n",
      "Key 20151209 Test Accuracy: 0.578521728515625 F1 score: 0.7287194489825826, recall 0.5762949036575368, precision 0.990768277114035\n",
      "Key 20151228 Test Accuracy: 0.7369213104248047 F1 score: 0.8469891661563826, recall 0.7364668719371091, precision 0.9965410420808852\n",
      "Key 20151226 Test Accuracy: 0.61866295337677 F1 score: 0.7576089352368769, recall 0.6603507954155341, precision 0.8884641950615924\n",
      "Key 20151218 Test Accuracy: 0.6819040775299072 F1 score: 0.8069285664795954, recall 0.6791062182208332, precision 0.9940254685739647\n",
      "Key 20151231 Test Accuracy: 0.6727613210678101 F1 score: 0.793129051194623, recall 0.6778855062987968, precision 0.9555822802279045\n",
      "Key 20151212 Test Accuracy: 0.7086623907089233 F1 score: 0.8260168713860849, recall 0.7070083464239075, precision 0.9931989790492396\n",
      "Key 20151211 Test Accuracy: 0.7104312777519226 F1 score: 0.8290279812992813, recall 0.7091533112483959, precision 0.9976739303858588\n",
      "Key 20151221 Test Accuracy: 0.6354835033416748 F1 score: 0.773990148930472, recall 0.632563915892253, precision 0.9968655563637454\n",
      "Key 20151201 Test Accuracy: 0.6611525416374207 F1 score: 0.7896073261709077, recall 0.6558183106427699, precision 0.9919729554772351\n",
      "Key 20151220 Test Accuracy: 0.6933127641677856 F1 score: 0.8121437972467331, recall 0.7065400196740038, precision 0.9548635184298365\n",
      "Key 20151230 Test Accuracy: 0.6594167947769165 F1 score: 0.783101435041196, recall 0.6479325336451425, precision 0.9895337051924681\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20140115 | Training Loss: 0.695975 | Validation Accuracy: 0.681564\n",
      "Key 20140107 | Training Loss: 0.691976 | Validation Accuracy: 0.731844\n",
      "Key 20140109 | Training Loss: 0.675096 | Validation Accuracy: 0.754190\n",
      "Key 20140104 | Training Loss: 0.693750 | Validation Accuracy: 0.698324\n",
      "Key 20140119 | Training Loss: 0.652543 | Validation Accuracy: 0.709497\n",
      "Key 20140120 | Training Loss: 0.666884 | Validation Accuracy: 0.715084\n",
      "Key 20140123 | Training Loss: 0.630677 | Validation Accuracy: 0.765363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 Test Accuracy: 0.01711343042552471 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151204 Test Accuracy: 0.013249633833765984 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151216 Test Accuracy: 0.02116244286298752 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151222 Test Accuracy: 0.013258238323032856 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151214 Test Accuracy: 0.015027950517833233 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151202 Test Accuracy: 0.0464799702167511 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151227 Test Accuracy: 0.011802698485553265 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151203 Test Accuracy: 0.09374673664569855 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151223 Test Accuracy: 0.03478793799877167 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151205 Test Accuracy: 0.013487710617482662 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151229 Test Accuracy: 0.02723914012312889 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151208 Test Accuracy: 0.016649192199110985 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151219 Test Accuracy: 0.3111410439014435 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151206 Test Accuracy: 0.014714673161506653 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151225 Test Accuracy: 0.014154382050037384 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151210 Test Accuracy: 0.013307882472872734 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151217 Test Accuracy: 0.03361067175865173 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151207 Test Accuracy: 0.03578229248523712 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151215 Test Accuracy: 0.019191473722457886 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151213 Test Accuracy: 0.009403099305927753 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151209 Test Accuracy: 0.01770460046827793 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151228 Test Accuracy: 0.011314482428133488 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151226 Test Accuracy: 0.09752998501062393 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151218 Test Accuracy: 0.02116948738694191 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151231 Test Accuracy: 0.07461465150117874 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151212 Test Accuracy: 0.02180875651538372 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151211 Test Accuracy: 0.010021804831922054 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151221 Test Accuracy: 0.013287071138620377 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151201 Test Accuracy: 0.03044763207435608 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151220 Test Accuracy: 0.06171160936355591 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151230 Test Accuracy: 0.051090482622385025 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Current Layer Attributes - hidden layers:3 features count:4\n",
      "Key 20140115 | Training Loss: 0.680664 | Validation Accuracy: 0.743017\n",
      "Key 20140107 | Training Loss: 0.548743 | Validation Accuracy: 0.698324\n",
      "Key 20140109 | Training Loss: 0.725903 | Validation Accuracy: 0.664804\n",
      "Key 20140104 | Training Loss: 0.554564 | Validation Accuracy: 0.743017\n",
      "Key 20140119 | Training Loss: 0.698162 | Validation Accuracy: 0.782123\n",
      "Key 20140120 | Training Loss: 0.665035 | Validation Accuracy: 0.720670\n",
      "Key 20140123 | Training Loss: 0.693877 | Validation Accuracy: 0.737430\n",
      "Key 20151224 Test Accuracy: 0.01765425130724907 F1 score: 0.0010998680158380994, recall 0.0005502366017387477, precision 1.0\n",
      "Key 20151204 Test Accuracy: 0.02348766103386879 F1 score: 0.02053790784879962, recall 0.010375499450196514, precision 1.0\n",
      "Key 20151216 Test Accuracy: 0.021381787955760956 F1 score: 0.0004480748464223464, recall 0.00022408762722575035, precision 1.0\n",
      "Key 20151222 Test Accuracy: 0.013305628672242165 F1 score: 9.604870408133108e-05, recall 4.80266584898202e-05, precision 1.0\n",
      "Key 20151214 Test Accuracy: 0.015078085474669933 F1 score: 0.00010179455314311891, recall 5.0899867236179627e-05, precision 1.0\n",
      "Key 20151202 Test Accuracy: 0.05908401310443878 F1 score: 0.02609197548678425, recall 0.01321843528814813, precision 1.0\n",
      "Key 20151227 Test Accuracy: 0.011846177279949188 F1 score: 0.00013598368195816502, recall 6.799809605331051e-05, precision 0.7391304347826086\n",
      "Key 20151203 Test Accuracy: 0.10396131128072739 F1 score: 0.022291179074517227, recall 0.011271213860534795, precision 1.0\n",
      "Key 20151223 Test Accuracy: 0.0349559485912323 F1 score: 0.0006243059713330979, recall 0.00031229361743519905, precision 0.6931818181818182\n",
      "Key 20151205 Test Accuracy: 0.017151767387986183 F1 score: 0.007400816770221019, recall 0.0037141522652965903, precision 1.0\n",
      "Key 20151229 Test Accuracy: 0.027511747553944588 F1 score: 0.0005603232180247131, recall 0.00028024012153571586, precision 1.0\n",
      "Key 20151208 Test Accuracy: 0.01910388097167015 F1 score: 0.004980066995274667, recall 0.002496249241868037, precision 1.0\n",
      "Key 20151219 Test Accuracy: 0.3111456334590912 F1 score: 1.3340804183676191e-05, recall 6.670446586398959e-06, precision 1.0\n",
      "Key 20151206 Test Accuracy: 0.020847121253609657 F1 score: 0.01237106742177911, recall 0.006224032674817315, precision 1.0\n",
      "Key 20151225 Test Accuracy: 0.016057616099715233 F1 score: 0.0038536790747207565, recall 0.001930559415571525, precision 1.0\n",
      "Key 20151210 Test Accuracy: 0.013408144004642963 F1 score: 0.00020320834717364546, recall 0.00010161449804392092, precision 1.0\n",
      "Key 20151217 Test Accuracy: 0.03369239717721939 F1 score: 0.00016911889058007776, recall 8.456659619450317e-05, precision 1.0\n",
      "Key 20151207 Test Accuracy: 0.04205988720059395 F1 score: 0.012936889540524805, recall 0.006510557954816728, precision 1.0\n",
      "Key 20151215 Test Accuracy: 0.019241750240325928 F1 score: 0.00010251641177913571, recall 5.1260833427921776e-05, precision 1.0\n",
      "Key 20151213 Test Accuracy: 0.009415491484105587 F1 score: 2.5019077046247765e-05, recall 1.2509695013635568e-05, precision 1.0\n",
      "Key 20151209 Test Accuracy: 0.01772146485745907 F1 score: 3.4338448331366026e-05, recall 1.7169518953002734e-05, precision 1.0\n",
      "Key 20151228 Test Accuracy: 0.011325364001095295 F1 score: 2.2012673796938588e-05, recall 1.1006458039254533e-05, precision 1.0\n",
      "Key 20151226 Test Accuracy: 0.09756125509738922 F1 score: 6.928741047751624e-05, recall 3.464490546665113e-05, precision 1.0\n",
      "Key 20151218 Test Accuracy: 0.024651123210787773 F1 score: 0.0070886546663102975, recall 0.0035569342725194756, precision 1.0\n",
      "Key 20151231 Test Accuracy: 0.07504820823669434 F1 score: 0.0009366011861285171, recall 0.00046852000125871047, precision 1.0\n",
      "Key 20151212 Test Accuracy: 0.021816162392497063 F1 score: 1.5140790424964134e-05, recall 7.57045252379961e-06, precision 1.0\n",
      "Key 20151211 Test Accuracy: 0.010024758987128735 F1 score: 5.968884206630833e-06, recall 2.984451010236667e-06, precision 1.0\n",
      "Key 20151221 Test Accuracy: 0.04049035534262657 F1 score: 0.05365982412839831, recall 0.027569602063199767, precision 1.0\n",
      "Key 20151201 Test Accuracy: 0.04752486199140549 F1 score: 0.03461730771926709, recall 0.017613520183743635, precision 1.0\n",
      "Key 20151220 Test Accuracy: 0.061741381883621216 F1 score: 6.346251480792013e-05, recall 3.173226430860685e-05, precision 1.0\n",
      "Key 20151230 Test Accuracy: 0.053446657955646515 F1 score: 0.004953772297604626, recall 0.0024830363471375107, precision 1.0\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20140115 | Training Loss: 0.775859 | Validation Accuracy: 0.385475\n",
      "Key 20140107 | Training Loss: 0.540562 | Validation Accuracy: 0.659218\n",
      "Key 20140109 | Training Loss: 0.672826 | Validation Accuracy: 0.636872\n",
      "Key 20140104 | Training Loss: 0.545637 | Validation Accuracy: 0.720670\n",
      "Key 20140119 | Training Loss: 0.621159 | Validation Accuracy: 0.703911\n",
      "Key 20140120 | Training Loss: 0.718945 | Validation Accuracy: 0.703911\n",
      "Key 20140123 | Training Loss: 0.507273 | Validation Accuracy: 0.698324\n",
      "Key 20151224 Test Accuracy: 0.017188675701618195 F1 score: 0.00015309759493246962, recall 7.655465763321706e-05, precision 1.0\n",
      "Key 20151204 Test Accuracy: 0.013360644690692425 F1 score: 0.00022497677659080351, recall 0.00011250104335645048, precision 1.0\n",
      "Key 20151216 Test Accuracy: 0.021215084940195084 F1 score: 0.00010755627658096524, recall 5.378103053418009e-05, precision 1.0\n",
      "Key 20151222 Test Accuracy: 0.013261883519589901 F1 score: 7.388689394275243e-06, recall 3.694358345370784e-06, precision 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151214 Test Accuracy: 0.015044662170112133 F1 score: 3.393266910133568e-05, recall 1.6966622412059877e-05, precision 1.0\n",
      "Key 20151202 Test Accuracy: 0.04686129093170166 F1 score: 0.0007994945131465268, recall 0.0003999071183467066, precision 1.0\n",
      "Key 20151227 Test Accuracy: 0.013470728881657124 F1 score: 0.0033702167081288512, recall 0.0016879527373233549, precision 1.0\n",
      "Key 20151203 Test Accuracy: 0.09379895776510239 F1 score: 0.00012409477295088504, recall 6.205151161914555e-05, precision 0.9333333333333333\n",
      "Key 20151223 Test Accuracy: 0.034862060099840164 F1 score: 0.00015357523138668196, recall 7.679351248406535e-05, precision 1.0\n",
      "Key 20151205 Test Accuracy: 0.013524572364985943 F1 score: 7.472864162011695e-05, recall 3.736571695469407e-05, precision 1.0\n",
      "Key 20151229 Test Accuracy: 0.028569892048835754 F1 score: 0.0027396352308253153, recall 0.001371701647516925, precision 0.9973190348525469\n",
      "Key 20151208 Test Accuracy: 0.016655471175909042 F1 score: 1.276845680431063e-05, recall 6.3842691607878185e-06, precision 1.0\n",
      "Key 20151219 Test Accuracy: 0.3113064467906952 F1 score: 0.0004934879595606623, recall 0.0002468065236967615, precision 0.9736842105263158\n",
      "Key 20151206 Test Accuracy: 0.014741359278559685 F1 score: 5.416765974042858e-05, recall 2.708456342392217e-05, precision 1.0\n",
      "Key 20151225 Test Accuracy: 0.015773842111229897 F1 score: 0.0032800368632541726, recall 0.0016427125104220431, precision 1.0\n",
      "Key 20151210 Test Accuracy: 0.01335030049085617 F1 score: 8.597780209473192e-05, recall 4.299074917242808e-05, precision 1.0\n",
      "Key 20151217 Test Accuracy: 0.033613815903663635 F1 score: 6.5051016259501525e-06, recall 3.252561392096276e-06, precision 1.0\n",
      "Key 20151207 Test Accuracy: 0.0357927568256855 F1 score: 2.170162436658384e-05, recall 1.0850929924694546e-05, precision 1.0\n",
      "Key 20151215 Test Accuracy: 0.01951468177139759 F1 score: 0.0006588507446843555, recall 0.00032953392917949714, precision 1.0\n",
      "Key 20151213 Test Accuracy: 0.012679561041295528 F1 score: 0.006593318903174966, recall 0.003307563361605244, precision 1.0\n",
      "Key 20151209 Test Accuracy: 0.017780495807528496 F1 score: 0.00015451373240796775, recall 7.72628352885123e-05, precision 1.0\n",
      "Key 20151228 Test Accuracy: 0.01248973049223423 F1 score: 0.0023745722884115925, recall 0.0011886974682394896, precision 1.0\n",
      "Key 20151226 Test Accuracy: 0.09859303385019302 F1 score: 0.002353081813634662, recall 0.0011779267858661384, precision 1.0\n",
      "Key 20151218 Test Accuracy: 0.021175194531679153 F1 score: 1.1662011580377498e-05, recall 5.8310397910155335e-06, precision 1.0\n",
      "Key 20151231 Test Accuracy: 0.07469876855611801 F1 score: 0.00018179720521757978, recall 9.090686591586919e-05, precision 1.0\n",
      "Key 20151212 Test Accuracy: 0.022212348878383636 F1 score: 0.0008248390050474094, recall 0.00041258966254707877, precision 1.0\n",
      "Key 20151211 Test Accuracy: 0.010252258740365505 F1 score: 0.0004654660030792367, recall 0.00023278717879846, precision 1.0\n",
      "Key 20151221 Test Accuracy: 0.013301156461238861 F1 score: 2.854940474491107e-05, recall 1.4274906142492113e-05, precision 1.0\n",
      "Key 20151201 Test Accuracy: 0.03048451617360115 F1 score: 7.608142614633312e-05, recall 3.8042160224068326e-05, precision 1.0\n",
      "Key 20151220 Test Accuracy: 0.07378000766038895 F1 score: 0.02539762106164562, recall 0.012862144466421976, precision 1.0\n",
      "Key 20151230 Test Accuracy: 0.051707372069358826 F1 score: 0.0012993634021665078, recall 0.0006501040617960028, precision 1.0\n",
      "Current Layer Attributes - hidden layers:3 features count:16\n",
      "Key 20140115 | Training Loss: 0.546517 | Validation Accuracy: 0.340782\n",
      "Key 20140107 | Training Loss: 0.637639 | Validation Accuracy: 0.653631\n",
      "Key 20140109 | Training Loss: 0.543644 | Validation Accuracy: 0.720670\n",
      "Key 20140104 | Training Loss: 0.484431 | Validation Accuracy: 0.826816\n",
      "Key 20140119 | Training Loss: 0.631640 | Validation Accuracy: 0.715084\n",
      "Key 20140120 | Training Loss: 0.708266 | Validation Accuracy: 0.782123\n",
      "Key 20140123 | Training Loss: 0.504712 | Validation Accuracy: 0.759777\n",
      "Key 20151224 Test Accuracy: 0.01711343042552471 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151204 Test Accuracy: 0.013249633833765984 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151216 Test Accuracy: 0.02116244286298752 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151222 Test Accuracy: 0.013258238323032856 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151214 Test Accuracy: 0.015027950517833233 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151202 Test Accuracy: 0.0464799702167511 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151227 Test Accuracy: 0.011802698485553265 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151203 Test Accuracy: 0.09374673664569855 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151223 Test Accuracy: 0.03478793799877167 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151205 Test Accuracy: 0.013487710617482662 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151229 Test Accuracy: 0.02723914012312889 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151208 Test Accuracy: 0.016649192199110985 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151219 Test Accuracy: 0.3111410439014435 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151206 Test Accuracy: 0.014714673161506653 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151225 Test Accuracy: 0.014154382050037384 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151210 Test Accuracy: 0.013307882472872734 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151217 Test Accuracy: 0.03361067175865173 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151207 Test Accuracy: 0.03578229248523712 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151215 Test Accuracy: 0.019191473722457886 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151213 Test Accuracy: 0.009403099305927753 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151209 Test Accuracy: 0.01770460046827793 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151228 Test Accuracy: 0.011314482428133488 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151226 Test Accuracy: 0.09752998501062393 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151218 Test Accuracy: 0.02116948738694191 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151231 Test Accuracy: 0.07461465150117874 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151212 Test Accuracy: 0.02180875651538372 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151211 Test Accuracy: 0.010021804831922054 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151221 Test Accuracy: 0.013287071138620377 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151201 Test Accuracy: 0.03044763207435608 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151220 Test Accuracy: 0.06171160936355591 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Key 20151230 Test Accuracy: 0.051090482622385025 F1 score: 0.0, recall 0.0, precision 0.0\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20140115 | Training Loss: 0.876492 | Validation Accuracy: 0.754190\n",
      "Key 20140107 | Training Loss: 0.536051 | Validation Accuracy: 0.720670\n",
      "Key 20140109 | Training Loss: 0.742776 | Validation Accuracy: 0.726257\n",
      "Key 20140104 | Training Loss: 0.316607 | Validation Accuracy: 0.675978\n",
      "Key 20140119 | Training Loss: 0.636794 | Validation Accuracy: 0.720670\n",
      "Key 20140120 | Training Loss: 0.381419 | Validation Accuracy: 0.743017\n",
      "Key 20140123 | Training Loss: 0.511417 | Validation Accuracy: 0.754190\n",
      "Key 20151224 Test Accuracy: 0.1474699079990387 F1 score: 0.23441868322141982, recall 0.13279362299701916, precision 0.9987405088344309\n",
      "Key 20151204 Test Accuracy: 0.14499396085739136 F1 score: 0.23580433815457197, recall 0.1336839011006957, precision 0.9987257347359288\n",
      "Key 20151216 Test Accuracy: 0.09515202790498734 F1 score: 0.14083333680452864, recall 0.0757640267650262, precision 0.9976983002832861\n",
      "Key 20151222 Test Accuracy: 0.09345654398202896 F1 score: 0.15058288275056445, recall 0.0814347410070082, precision 0.9980530652902291\n",
      "Key 20151214 Test Accuracy: 0.10618581622838974 F1 score: 0.16972096091900493, recall 0.09274804141552531, precision 0.9978551544745129\n",
      "Key 20151202 Test Accuracy: 0.1975070685148239 F1 score: 0.27380395823593584, recall 0.1586599241466498, precision 0.9982954545454545\n",
      "Key 20151227 Test Accuracy: 0.11745384335517883 F1 score: 0.1938141798697266, recall 0.10735299411616475, precision 0.9959182158892723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151203 Test Accuracy: 0.24458743631839752 F1 score: 0.2857023487587737, recall 0.16670138596483453, precision 0.9984602723725079\n",
      "Key 20151223 Test Accuracy: 0.15146589279174805 F1 score: 0.21624409502727127, recall 0.12127743448233494, precision 0.9967600774215265\n",
      "Key 20151205 Test Accuracy: 0.09783105552196503 F1 score: 0.15790777530811523, recall 0.08574311069593647, precision 0.9971320557945509\n",
      "Key 20151229 Test Accuracy: 0.14048925042152405 F1 score: 0.2088072825006686, recall 0.11659464003893863, precision 0.9985158050967885\n",
      "Key 20151208 Test Accuracy: 0.10235612094402313 F1 score: 0.16071564835087432, recall 0.08740064481118524, precision 0.9972319347319347\n",
      "Key 20151219 Test Accuracy: 0.38278621435165405 F1 score: 0.18899320754716983, recall 0.10439915952373012, precision 0.9962444302991725\n",
      "Key 20151206 Test Accuracy: 0.08011144399642944 F1 score: 0.12480450104604637, recall 0.06656843998331591, precision 0.997079107505071\n",
      "Key 20151225 Test Accuracy: 0.14659301936626434 F1 score: 0.23709405821505894, recall 0.13451383650295787, precision 0.9987103430487491\n",
      "Key 20151210 Test Accuracy: 0.11075504869222641 F1 score: 0.18011868064666375, recall 0.09899597059432756, precision 0.9976368649074439\n",
      "Key 20151217 Test Accuracy: 0.12161198258399963 F1 score: 0.16764312542257778, recall 0.09153358269637339, precision 0.994873970375084\n",
      "Key 20151207 Test Accuracy: 0.11908598244190216 F1 score: 0.15949407524981782, recall 0.08668265370342239, precision 0.9966936993137866\n",
      "Key 20151215 Test Accuracy: 0.11236564069986343 F1 score: 0.17374909742465167, recall 0.09515475279463079, precision 0.998348123391341\n",
      "Key 20151213 Test Accuracy: 0.05457713454961777 F1 score: 0.08737634125626519, recall 0.04568790812879982, precision 0.9981415687346269\n",
      "Key 20151209 Test Accuracy: 0.11104271560907364 F1 score: 0.17395154212637912, recall 0.09528653780942692, precision 0.9972148600691793\n",
      "Key 20151228 Test Accuracy: 0.08662284910678864 F1 score: 0.1418255897471289, recall 0.07633804134575962, precision 0.997806071068911\n",
      "Key 20151226 Test Accuracy: 0.1702660471200943 F1 score: 0.1495275914007942, recall 0.08082341491683648, precision 0.9972021450221497\n",
      "Key 20151218 Test Accuracy: 0.1090836450457573 F1 score: 0.16546104864708808, recall 0.09022950972617437, precision 0.9954326149887424\n",
      "Key 20151231 Test Accuracy: 0.1779608428478241 F1 score: 0.20115582218700678, recall 0.11184341641987784, precision 0.9985328546901826\n",
      "Key 20151212 Test Accuracy: 0.08075904846191406 F1 score: 0.11424126956943671, recall 0.06060147245301588, precision 0.9944717063171625\n",
      "Key 20151211 Test Accuracy: 0.07484149187803268 F1 score: 0.12336609769958873, recall 0.06575640910854448, precision 0.9957517964477788\n",
      "Key 20151221 Test Accuracy: 0.09363394230604172 F1 score: 0.1510057964130846, recall 0.08169052955143487, precision 0.996806595831156\n",
      "Key 20151201 Test Accuracy: 0.13543756306171417 F1 score: 0.19574457134769538, recall 0.10851526203915489, precision 0.9979009970264124\n",
      "Key 20151220 Test Accuracy: 0.14086583256721497 F1 score: 0.15597318708104813, recall 0.08460350535746396, precision 0.99713270585302\n",
      "Key 20151230 Test Accuracy: 0.18694764375686646 F1 score: 0.2507540227706981, recall 0.14337954790680035, precision 0.9985536865272756\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.294565Z",
     "start_time": "2017-07-19T18:45:17.095310Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.299812Z",
     "start_time": "2017-07-19T18:46:07.296169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.823053Z",
     "start_time": "2017-07-19T18:46:07.301246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.832314Z",
     "start_time": "2017-07-19T18:46:07.824663Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "#df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.838035Z",
     "start_time": "2017-07-19T18:46:07.834192Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.947845Z",
     "start_time": "2017-07-19T18:46:07.839674Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.954886Z",
     "start_time": "2017-07-19T18:46:07.949985Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:07.962281Z",
     "start_time": "2017-07-19T18:46:07.957168Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:08.014874Z",
     "start_time": "2017-07-19T18:46:07.964389Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.774233</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.774233</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.774970</td>\n",
       "      <td>0.861039</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.774970</td>\n",
       "      <td>0.861039</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>20151205</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.755452</td>\n",
       "      <td>0.859353</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20151205</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.755452</td>\n",
       "      <td>0.859353</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.754294</td>\n",
       "      <td>0.858514</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.754294</td>\n",
       "      <td>0.858514</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>20151228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.753367</td>\n",
       "      <td>0.858102</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20151228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.753367</td>\n",
       "      <td>0.858102</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.752540</td>\n",
       "      <td>0.857187</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.752540</td>\n",
       "      <td>0.857187</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.750683</td>\n",
       "      <td>0.853739</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.750683</td>\n",
       "      <td>0.853739</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20151204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.744622</td>\n",
       "      <td>0.852057</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>20151204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.744622</td>\n",
       "      <td>0.852057</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.743847</td>\n",
       "      <td>0.851717</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.743847</td>\n",
       "      <td>0.851717</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.744721</td>\n",
       "      <td>0.851011</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.744721</td>\n",
       "      <td>0.851011</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.742301</td>\n",
       "      <td>0.850623</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.742301</td>\n",
       "      <td>0.850623</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.752372</td>\n",
       "      <td>0.850025</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.752372</td>\n",
       "      <td>0.850025</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.848758</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.739785</td>\n",
       "      <td>0.848758</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>20151212</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.740557</td>\n",
       "      <td>0.848475</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20151212</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.740557</td>\n",
       "      <td>0.848475</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.738127</td>\n",
       "      <td>0.847285</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.738127</td>\n",
       "      <td>0.847285</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.736921</td>\n",
       "      <td>0.846989</td>\n",
       "      <td>15.092843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.736921</td>\n",
       "      <td>0.846989</td>\n",
       "      <td>15.092843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.742054</td>\n",
       "      <td>0.845643</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.742054</td>\n",
       "      <td>0.845643</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.734537</td>\n",
       "      <td>0.845485</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.734537</td>\n",
       "      <td>0.845485</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.743404</td>\n",
       "      <td>0.845105</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.743404</td>\n",
       "      <td>0.845105</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20151229</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.737505</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>20151229</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.737505</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.737438</td>\n",
       "      <td>0.844718</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.737438</td>\n",
       "      <td>0.844718</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>20151212</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.732363</td>\n",
       "      <td>0.842904</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>20151212</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.732363</td>\n",
       "      <td>0.842904</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.732759</td>\n",
       "      <td>0.842570</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.732759</td>\n",
       "      <td>0.842570</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.729468</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.729468</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.725615</td>\n",
       "      <td>0.839068</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.725615</td>\n",
       "      <td>0.839068</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>20151228</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>20151209</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>20151206</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.311141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.021809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.030448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>20151208</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>20151229</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>20151205</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>20151223</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>20151227</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>20151202</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>20151214</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>20151222</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>20151216</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>20151204</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>20151224</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "15   20151210               1              1     0.921788    0.774233   \n",
       "325  20151210               1              1     0.921788    0.774233   \n",
       "7    20151203               1              1     0.921788    0.774970   \n",
       "317  20151203               1              1     0.921788    0.774970   \n",
       "350  20151205               4              1     0.905028    0.755452   \n",
       "40   20151205               4              1     0.905028    0.755452   \n",
       "6    20151227               1              1     0.921788    0.754294   \n",
       "316  20151227               1              1     0.921788    0.754294   \n",
       "362  20151228               4              1     0.905028    0.753367   \n",
       "52   20151228               4              1     0.905028    0.753367   \n",
       "45   20151225               4              1     0.905028    0.752540   \n",
       "355  20151225               4              1     0.905028    0.752540   \n",
       "10   20151229               1              1     0.921788    0.750683   \n",
       "320  20151229               1              1     0.921788    0.750683   \n",
       "32   20151204               4              1     0.905028    0.744622   \n",
       "342  20151204               4              1     0.905028    0.744622   \n",
       "336  20151211               1              1     0.921788    0.743847   \n",
       "26   20151211               1              1     0.921788    0.743847   \n",
       "64   20151216               8              1     0.882682    0.744721   \n",
       "374  20151216               8              1     0.882682    0.744721   \n",
       "347  20151227               4              1     0.905028    0.742301   \n",
       "37   20151227               4              1     0.905028    0.742301   \n",
       "348  20151203               4              1     0.905028    0.752372   \n",
       "38   20151203               4              1     0.905028    0.752372   \n",
       "387  20151210               8              1     0.882682    0.739785   \n",
       "77   20151210               8              1     0.882682    0.739785   \n",
       "366  20151212               4              1     0.905028    0.740557   \n",
       "56   20151212               4              1     0.905028    0.740557   \n",
       "324  20151225               1              1     0.921788    0.738127   \n",
       "14   20151225               1              1     0.921788    0.738127   \n",
       "145  20151228              42              1     0.893855    0.736921   \n",
       "455  20151228              42              1     0.893855    0.736921   \n",
       "61   20151230               4              1     0.905028    0.742054   \n",
       "371  20151230               4              1     0.905028    0.742054   \n",
       "381  20151205               8              1     0.882682    0.734537   \n",
       "71   20151205               8              1     0.882682    0.734537   \n",
       "30   20151230               1              1     0.921788    0.743404   \n",
       "340  20151230               1              1     0.921788    0.743404   \n",
       "41   20151229               4              1     0.905028    0.737505   \n",
       "351  20151229               4              1     0.905028    0.737505   \n",
       "369  20151201               4              1     0.905028    0.737438   \n",
       "59   20151201               4              1     0.905028    0.737438   \n",
       "87   20151212               8              1     0.882682    0.732363   \n",
       "397  20151212               8              1     0.882682    0.732363   \n",
       "333  20151218               1              1     0.921788    0.732759   \n",
       "23   20151218               1              1     0.921788    0.732759   \n",
       "385  20151206               8              1     0.882682    0.729468   \n",
       "75   20151206               8              1     0.882682    0.729468   \n",
       "42   20151208               4              1     0.905028    0.725615   \n",
       "352  20151208               4              1     0.905028    0.725615   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "269  20151228              16              3     0.759777    0.011314   \n",
       "175  20151209               1              3     0.765363    0.017705   \n",
       "268  20151209              16              3     0.759777    0.017705   \n",
       "261  20151206              16              3     0.759777    0.014715   \n",
       "157  20151216               1              3     0.765363    0.021162   \n",
       "494  20151220               1              3     0.765363    0.061712   \n",
       "473  20151223               1              3     0.765363    0.034788   \n",
       "475  20151229               1              3     0.765363    0.027239   \n",
       "476  20151208               1              3     0.765363    0.016649   \n",
       "477  20151219               1              3     0.765363    0.311141   \n",
       "478  20151206               1              3     0.765363    0.014715   \n",
       "479  20151225               1              3     0.765363    0.014154   \n",
       "480  20151210               1              3     0.765363    0.013308   \n",
       "481  20151217               1              3     0.765363    0.033611   \n",
       "482  20151207               1              3     0.765363    0.035782   \n",
       "483  20151215               1              3     0.765363    0.019191   \n",
       "484  20151213               1              3     0.765363    0.009403   \n",
       "485  20151209               1              3     0.765363    0.017705   \n",
       "486  20151228               1              3     0.765363    0.011314   \n",
       "487  20151226               1              3     0.765363    0.097530   \n",
       "488  20151218               1              3     0.765363    0.021169   \n",
       "489  20151231               1              3     0.765363    0.074615   \n",
       "490  20151212               1              3     0.765363    0.021809   \n",
       "491  20151211               1              3     0.765363    0.010022   \n",
       "492  20151221               1              3     0.765363    0.013287   \n",
       "493  20151201               1              3     0.765363    0.030448   \n",
       "474  20151205               1              3     0.765363    0.013488   \n",
       "472  20151203               1              3     0.765363    0.093747   \n",
       "156  20151204               1              3     0.765363    0.013250   \n",
       "471  20151227               1              3     0.765363    0.011803   \n",
       "259  20151208              16              3     0.759777    0.016649   \n",
       "155  20151224               1              3     0.765363    0.017113   \n",
       "258  20151229              16              3     0.759777    0.027239   \n",
       "257  20151205              16              3     0.759777    0.013488   \n",
       "256  20151223              16              3     0.759777    0.034788   \n",
       "255  20151203              16              3     0.759777    0.093747   \n",
       "254  20151227              16              3     0.759777    0.011803   \n",
       "253  20151202              16              3     0.759777    0.046480   \n",
       "252  20151214              16              3     0.759777    0.015028   \n",
       "251  20151222              16              3     0.759777    0.013258   \n",
       "250  20151216              16              3     0.759777    0.021162   \n",
       "185  20151230               1              3     0.765363    0.051090   \n",
       "249  20151204              16              3     0.759777    0.013250   \n",
       "248  20151224              16              3     0.759777    0.017113   \n",
       "465  20151224               1              3     0.765363    0.017113   \n",
       "466  20151204               1              3     0.765363    0.013250   \n",
       "468  20151222               1              3     0.765363    0.013258   \n",
       "469  20151214               1              3     0.765363    0.015028   \n",
       "470  20151202               1              3     0.765363    0.046480   \n",
       "184  20151220               1              3     0.765363    0.061712   \n",
       "\n",
       "     f1_score  time_taken  \n",
       "15   0.871146   20.041186  \n",
       "325  0.871146   20.041186  \n",
       "7    0.861039   20.041186  \n",
       "317  0.861039   20.041186  \n",
       "350  0.859353   15.668566  \n",
       "40   0.859353   15.668566  \n",
       "6    0.858514   20.041186  \n",
       "316  0.858514   20.041186  \n",
       "362  0.858102   15.668566  \n",
       "52   0.858102   15.668566  \n",
       "45   0.857187   15.668566  \n",
       "355  0.857187   15.668566  \n",
       "10   0.853739   20.041186  \n",
       "320  0.853739   20.041186  \n",
       "32   0.852057   15.668566  \n",
       "342  0.852057   15.668566  \n",
       "336  0.851717   20.041186  \n",
       "26   0.851717   20.041186  \n",
       "64   0.851011   16.062345  \n",
       "374  0.851011   16.062345  \n",
       "347  0.850623   15.668566  \n",
       "37   0.850623   15.668566  \n",
       "348  0.850025   15.668566  \n",
       "38   0.850025   15.668566  \n",
       "387  0.848758   16.062345  \n",
       "77   0.848758   16.062345  \n",
       "366  0.848475   15.668566  \n",
       "56   0.848475   15.668566  \n",
       "324  0.847285   20.041186  \n",
       "14   0.847285   20.041186  \n",
       "145  0.846989   15.092843  \n",
       "455  0.846989   15.092843  \n",
       "61   0.845643   15.668566  \n",
       "371  0.845643   15.668566  \n",
       "381  0.845485   16.062345  \n",
       "71   0.845485   16.062345  \n",
       "30   0.845105   20.041186  \n",
       "340  0.845105   20.041186  \n",
       "41   0.844765   15.668566  \n",
       "351  0.844765   15.668566  \n",
       "369  0.844718   15.668566  \n",
       "59   0.844718   15.668566  \n",
       "87   0.842904   16.062345  \n",
       "397  0.842904   16.062345  \n",
       "333  0.842570   20.041186  \n",
       "23   0.842570   20.041186  \n",
       "385  0.841892   16.062345  \n",
       "75   0.841892   16.062345  \n",
       "42   0.839068   15.668566  \n",
       "352  0.839068   15.668566  \n",
       "..        ...         ...  \n",
       "269  0.000000   15.838929  \n",
       "175  0.000000   15.611936  \n",
       "268  0.000000   15.838929  \n",
       "261  0.000000   15.838929  \n",
       "157  0.000000   15.611936  \n",
       "494  0.000000   15.611936  \n",
       "473  0.000000   15.611936  \n",
       "475  0.000000   15.611936  \n",
       "476  0.000000   15.611936  \n",
       "477  0.000000   15.611936  \n",
       "478  0.000000   15.611936  \n",
       "479  0.000000   15.611936  \n",
       "480  0.000000   15.611936  \n",
       "481  0.000000   15.611936  \n",
       "482  0.000000   15.611936  \n",
       "483  0.000000   15.611936  \n",
       "484  0.000000   15.611936  \n",
       "485  0.000000   15.611936  \n",
       "486  0.000000   15.611936  \n",
       "487  0.000000   15.611936  \n",
       "488  0.000000   15.611936  \n",
       "489  0.000000   15.611936  \n",
       "490  0.000000   15.611936  \n",
       "491  0.000000   15.611936  \n",
       "492  0.000000   15.611936  \n",
       "493  0.000000   15.611936  \n",
       "474  0.000000   15.611936  \n",
       "472  0.000000   15.611936  \n",
       "156  0.000000   15.611936  \n",
       "471  0.000000   15.611936  \n",
       "259  0.000000   15.838929  \n",
       "155  0.000000   15.611936  \n",
       "258  0.000000   15.838929  \n",
       "257  0.000000   15.838929  \n",
       "256  0.000000   15.838929  \n",
       "255  0.000000   15.838929  \n",
       "254  0.000000   15.838929  \n",
       "253  0.000000   15.838929  \n",
       "252  0.000000   15.838929  \n",
       "251  0.000000   15.838929  \n",
       "250  0.000000   15.838929  \n",
       "185  0.000000   15.611936  \n",
       "249  0.000000   15.838929  \n",
       "248  0.000000   15.838929  \n",
       "465  0.000000   15.611936  \n",
       "466  0.000000   15.611936  \n",
       "468  0.000000   15.611936  \n",
       "469  0.000000   15.611936  \n",
       "470  0.000000   15.611936  \n",
       "184  0.000000   15.611936  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:08.046141Z",
     "start_time": "2017-07-19T18:46:08.016373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151210</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.774233</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151205</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.755452</td>\n",
       "      <td>0.859353</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151216</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.744721</td>\n",
       "      <td>0.851011</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151228</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.736921</td>\n",
       "      <td>0.846989</td>\n",
       "      <td>15.092843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151203</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.655552</td>\n",
       "      <td>0.765545</td>\n",
       "      <td>14.875491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20151203</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.244587</td>\n",
       "      <td>0.285702</td>\n",
       "      <td>19.912291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>20151221</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.040490</td>\n",
       "      <td>0.053660</td>\n",
       "      <td>15.115697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151220</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>0.073780</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>15.386132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151201</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.030448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>20151223</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "1              1              20151210     0.921788    0.774233  0.871146   \n",
       "4              1              20151205     0.905028    0.755452  0.859353   \n",
       "8              1              20151216     0.882682    0.744721  0.851011   \n",
       "42             1              20151228     0.893855    0.736921  0.846989   \n",
       "16             1              20151203     0.821229    0.655552  0.765545   \n",
       "42             3              20151203     0.754190    0.244587  0.285702   \n",
       "4              3              20151221     0.737430    0.040490  0.053660   \n",
       "8              3              20151220     0.698324    0.073780  0.025398   \n",
       "1              3              20151201     0.765363    0.030448  0.000000   \n",
       "16             3              20151223     0.759777    0.034788  0.000000   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1               20.041186  \n",
       "4              1               15.668566  \n",
       "8              1               16.062345  \n",
       "42             1               15.092843  \n",
       "16             1               14.875491  \n",
       "42             3               19.912291  \n",
       "4              3               15.115697  \n",
       "8              3               15.386132  \n",
       "1              3               15.611936  \n",
       "16             3               15.838929  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='f1_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:10.956246Z",
     "start_time": "2017-07-19T18:46:08.047608Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:10.979601Z",
     "start_time": "2017-07-19T18:46:10.957890Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151228_42_1'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:10.987151Z",
     "start_time": "2017-07-19T18:46:10.981125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.048328Z",
     "start_time": "2017-07-19T18:46:10.988960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84698916615638264"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.062688Z",
     "start_time": "2017-07-19T18:46:11.049858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0      4159\n",
       "1.0    363423\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.649141Z",
     "start_time": "2017-07-19T18:46:11.064119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a383ed7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.665171Z",
     "start_time": "2017-07-19T18:46:11.650694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.708751</td>\n",
       "      <td>0.824990</td>\n",
       "      <td>15.668566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.816935</td>\n",
       "      <td>20.041186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.814542</td>\n",
       "      <td>16.062345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.657731</td>\n",
       "      <td>0.787330</td>\n",
       "      <td>15.092843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.537324</td>\n",
       "      <td>0.681770</td>\n",
       "      <td>14.875491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.133364</td>\n",
       "      <td>0.179698</td>\n",
       "      <td>19.912291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>15.115697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.698324</td>\n",
       "      <td>0.039067</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>15.386132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.611936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.838929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "4              1                 0.905028    0.708751  0.824990   15.668566\n",
       "1              1                 0.921788    0.699422  0.816935   20.041186\n",
       "8              1                 0.882682    0.693750  0.814542   16.062345\n",
       "42             1                 0.893855    0.657731  0.787330   15.092843\n",
       "16             1                 0.821229    0.537324  0.681770   14.875491\n",
       "42             3                 0.754190    0.133364  0.179698   19.912291\n",
       "4              3                 0.737430    0.041669  0.006952   15.115697\n",
       "8              3                 0.698324    0.039067  0.001683   15.386132\n",
       "1              3                 0.765363    0.038258  0.000000   15.611936\n",
       "16             3                 0.759777    0.038258  0.000000   15.838929"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.682915Z",
     "start_time": "2017-07-19T18:46:11.666617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059806</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055328</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040470</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>0.004601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067645</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041730</td>\n",
       "      <td>0.032684</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.046025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "1              1                      0.0    0.059806  0.045952         0.0\n",
       "               3                      0.0    0.055450  0.000000         0.0\n",
       "4              1                      0.0    0.038176  0.028825         0.0\n",
       "               3                      0.0    0.055328  0.012376         0.0\n",
       "8              1                      0.0    0.040470  0.031042         0.0\n",
       "               3                      0.0    0.055584  0.004601         0.0\n",
       "16             1                      0.0    0.067645  0.056637         0.0\n",
       "               3                      0.0    0.055450  0.000000         0.0\n",
       "42             1                      0.0    0.041730  0.032684         0.0\n",
       "               3                      0.0    0.061007  0.046025         0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:46:11.754864Z",
     "start_time": "2017-07-19T18:46:11.684359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                   (0.726869908115, 0.906999569347)\n",
       "                3                                         (nan, nan)\n",
       "4               1                   (0.768494154497, 0.881485140892)\n",
       "                3                (-0.0173051632334, 0.0312090055208)\n",
       "8               1                    (0.75370055604, 0.875382846442)\n",
       "                3                (-0.00733499693879, 0.010700355621)\n",
       "16              1                   (0.570762836511, 0.792777409951)\n",
       "                3                                         (nan, nan)\n",
       "42              1                   (0.723271051649, 0.851388811447)\n",
       "                3                  (0.0894908728603, 0.269904941056)\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
