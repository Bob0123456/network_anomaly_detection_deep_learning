{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:02:19.483931Z",
     "start_time": "2017-07-20T20:02:12.858450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:41:49.162575Z",
     "start_time": "2017-07-19T18:41:49.151675Z"
    },
    "collapsed": true
   },
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:25.047158Z",
     "start_time": "2017-07-19T18:57:24.963312Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:25.053907Z",
     "start_time": "2017-07-19T18:57:25.048737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:25.059497Z",
     "start_time": "2017-07-19T18:57:25.055305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:25.796143Z",
     "start_time": "2017-07-19T18:57:25.060881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:26.889332Z",
     "start_time": "2017-07-19T18:57:25.797575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:27.074864Z",
     "start_time": "2017-07-19T18:57:26.890912Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:32.219837Z",
     "start_time": "2017-07-20T20:14:27.915881Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    \n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:57:27.439638Z",
     "start_time": "2017-07-19T18:57:27.350098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        print(\"********************************** Training ******************************\")\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [5]\n",
    "        lrs = [1e-5, 1e-6]\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "        past_scores.to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.107188Z",
     "start_time": "2017-07-19T18:57:27.441111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.673428 | Validation Accuracy: 0.688919\n",
      "Accuracy on Test data: 0.7459191083908081, 0.5784810185432434\n",
      "Step 2 | Training Loss: 0.626510 | Validation Accuracy: 0.768535\n",
      "Accuracy on Test data: 0.7801188826560974, 0.6107172966003418\n",
      "Step 3 | Training Loss: 0.587754 | Validation Accuracy: 0.838943\n",
      "Accuracy on Test data: 0.804737389087677, 0.6439662575721741\n",
      "Step 4 | Training Loss: 0.550928 | Validation Accuracy: 0.882283\n",
      "Accuracy on Test data: 0.8132097125053406, 0.6544303894042969\n",
      "Step 5 | Training Loss: 0.515119 | Validation Accuracy: 0.905858\n",
      "Accuracy on Test data: 0.815072774887085, 0.6548523306846619\n",
      "Step 1 | Training Loss: 0.530951 | Validation Accuracy: 0.908557\n",
      "Accuracy on Test data: 0.8155163526535034, 0.6555274128913879\n",
      "Step 2 | Training Loss: 0.514855 | Validation Accuracy: 0.906652\n",
      "Accuracy on Test data: 0.816847026348114, 0.6578059196472168\n",
      "Step 3 | Training Loss: 0.519569 | Validation Accuracy: 0.908081\n",
      "Accuracy on Test data: 0.8172462582588196, 0.6585654020309448\n",
      "Step 4 | Training Loss: 0.507900 | Validation Accuracy: 0.912605\n",
      "Accuracy on Test data: 0.8183552026748657, 0.6605907082557678\n",
      "Step 5 | Training Loss: 0.506539 | Validation Accuracy: 0.914590\n",
      "Accuracy on Test data: 0.821726381778717, 0.6670042276382446\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.692830 | Validation Accuracy: 0.672805\n",
      "Accuracy on Test data: 0.6295688152313232, 0.3814345896244049\n",
      "Step 2 | Training Loss: 0.623507 | Validation Accuracy: 0.798460\n",
      "Accuracy on Test data: 0.6870120763778687, 0.421434611082077\n",
      "Step 3 | Training Loss: 0.586485 | Validation Accuracy: 0.834259\n",
      "Accuracy on Test data: 0.699299156665802, 0.44075947999954224\n",
      "Step 4 | Training Loss: 0.537149 | Validation Accuracy: 0.869582\n",
      "Accuracy on Test data: 0.7130056619644165, 0.4636286795139313\n",
      "Step 5 | Training Loss: 0.507201 | Validation Accuracy: 0.905144\n",
      "Accuracy on Test data: 0.7208569645881653, 0.47637131810188293\n",
      "Step 1 | Training Loss: 0.516838 | Validation Accuracy: 0.906017\n",
      "Accuracy on Test data: 0.7213892936706543, 0.4772995710372925\n",
      "Step 2 | Training Loss: 0.522388 | Validation Accuracy: 0.912685\n",
      "Accuracy on Test data: 0.7222764492034912, 0.4789029657840729\n",
      "Step 3 | Training Loss: 0.497805 | Validation Accuracy: 0.908239\n",
      "Accuracy on Test data: 0.7232522964477539, 0.4805063307285309\n",
      "Step 4 | Training Loss: 0.498320 | Validation Accuracy: 0.915939\n",
      "Accuracy on Test data: 0.7242282032966614, 0.48236286640167236\n",
      "Step 5 | Training Loss: 0.525800 | Validation Accuracy: 0.924432\n",
      "Accuracy on Test data: 0.7257806658744812, 0.4853164553642273\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.698846 | Validation Accuracy: 0.491586\n",
      "Accuracy on Test data: 0.3939407467842102, 0.5848101377487183\n",
      "Step 2 | Training Loss: 0.641474 | Validation Accuracy: 0.779409\n",
      "Accuracy on Test data: 0.6709545850753784, 0.601772129535675\n",
      "Step 3 | Training Loss: 0.584427 | Validation Accuracy: 0.841562\n",
      "Accuracy on Test data: 0.7862846255302429, 0.6293671131134033\n",
      "Step 4 | Training Loss: 0.545607 | Validation Accuracy: 0.894348\n",
      "Accuracy on Test data: 0.8097941875457764, 0.6637974977493286\n",
      "Step 5 | Training Loss: 0.519370 | Validation Accuracy: 0.919908\n",
      "Accuracy on Test data: 0.8239886164665222, 0.6831223368644714\n",
      "Step 1 | Training Loss: 0.523935 | Validation Accuracy: 0.919194\n",
      "Accuracy on Test data: 0.8264726996421814, 0.6856539845466614\n",
      "Step 2 | Training Loss: 0.511311 | Validation Accuracy: 0.925226\n",
      "Accuracy on Test data: 0.8296664357185364, 0.6888607740402222\n",
      "Step 3 | Training Loss: 0.508930 | Validation Accuracy: 0.924036\n",
      "Accuracy on Test data: 0.8313964009284973, 0.6913080215454102\n",
      "Step 4 | Training Loss: 0.512686 | Validation Accuracy: 0.932767\n",
      "Accuracy on Test data: 0.8325053453445435, 0.6926582455635071\n",
      "Step 5 | Training Loss: 0.502806 | Validation Accuracy: 0.930386\n",
      "Accuracy on Test data: 0.834102213382721, 0.695189893245697\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.614766 | Validation Accuracy: 0.794729\n",
      "Accuracy on Test data: 0.758605420589447, 0.5828691720962524\n",
      "Step 2 | Training Loss: 0.579635 | Validation Accuracy: 0.855136\n",
      "Accuracy on Test data: 0.7795866131782532, 0.599746823310852\n",
      "Step 3 | Training Loss: 0.544866 | Validation Accuracy: 0.886728\n",
      "Accuracy on Test data: 0.7881476283073425, 0.6112236380577087\n",
      "Step 4 | Training Loss: 0.510033 | Validation Accuracy: 0.912923\n",
      "Accuracy on Test data: 0.7993701100349426, 0.6297046542167664\n",
      "Step 5 | Training Loss: 0.479695 | Validation Accuracy: 0.924274\n",
      "Accuracy on Test data: 0.805269718170166, 0.6396624445915222\n",
      "Step 1 | Training Loss: 0.480480 | Validation Accuracy: 0.929195\n",
      "Accuracy on Test data: 0.8053584098815918, 0.6397468447685242\n",
      "Step 2 | Training Loss: 0.486361 | Validation Accuracy: 0.932291\n",
      "Accuracy on Test data: 0.8053140640258789, 0.6396624445915222\n",
      "Step 3 | Training Loss: 0.473568 | Validation Accuracy: 0.932291\n",
      "Accuracy on Test data: 0.8052253127098083, 0.6394093036651611\n",
      "Step 4 | Training Loss: 0.473281 | Validation Accuracy: 0.931815\n",
      "Accuracy on Test data: 0.8052253127098083, 0.6393249034881592\n",
      "Step 5 | Training Loss: 0.476505 | Validation Accuracy: 0.939276\n",
      "Accuracy on Test data: 0.8057576417922974, 0.6403375267982483\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.640217 | Validation Accuracy: 0.753770\n",
      "Accuracy on Test data: 0.5986958742141724, 0.43248945474624634\n",
      "Step 2 | Training Loss: 0.592677 | Validation Accuracy: 0.797269\n",
      "Accuracy on Test data: 0.620963454246521, 0.4601687788963318\n",
      "Step 3 | Training Loss: 0.552844 | Validation Accuracy: 0.839181\n",
      "Accuracy on Test data: 0.6749467849731445, 0.5131645798683167\n",
      "Step 4 | Training Loss: 0.539862 | Validation Accuracy: 0.888712\n",
      "Accuracy on Test data: 0.7615773677825928, 0.5637130737304688\n",
      "Step 5 | Training Loss: 0.487263 | Validation Accuracy: 0.908081\n",
      "Accuracy on Test data: 0.7944907546043396, 0.6215189695358276\n",
      "Step 1 | Training Loss: 0.497111 | Validation Accuracy: 0.911573\n",
      "Accuracy on Test data: 0.796664297580719, 0.6254852414131165\n",
      "Step 2 | Training Loss: 0.481841 | Validation Accuracy: 0.913637\n",
      "Accuracy on Test data: 0.7974183559417725, 0.6267510652542114\n",
      "Step 3 | Training Loss: 0.471921 | Validation Accuracy: 0.916257\n",
      "Accuracy on Test data: 0.7984386086463928, 0.6281856298446655\n",
      "Step 4 | Training Loss: 0.483129 | Validation Accuracy: 0.917209\n",
      "Accuracy on Test data: 0.8000354766845703, 0.6308016777038574\n",
      "Step 5 | Training Loss: 0.507699 | Validation Accuracy: 0.919670\n",
      "Accuracy on Test data: 0.8025195002555847, 0.6351054906845093\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.693090 | Validation Accuracy: 0.533180\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693041 | Validation Accuracy: 0.539848\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693034 | Validation Accuracy: 0.538339\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692749 | Validation Accuracy: 0.539768\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692727 | Validation Accuracy: 0.534132\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692839 | Validation Accuracy: 0.528100\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692817 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692969 | Validation Accuracy: 0.528417\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693093 | Validation Accuracy: 0.527227\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692555 | Validation Accuracy: 0.534767\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.670121 | Validation Accuracy: 0.706303\n",
      "Accuracy on Test data: 0.8191536664962769, 0.805653989315033\n",
      "Step 2 | Training Loss: 0.680425 | Validation Accuracy: 0.734720\n",
      "Accuracy on Test data: 0.8466110825538635, 0.8163713216781616\n",
      "Step 3 | Training Loss: 0.662213 | Validation Accuracy: 0.774567\n",
      "Accuracy on Test data: 0.865241289138794, 0.8299577832221985\n",
      "Step 4 | Training Loss: 0.640617 | Validation Accuracy: 0.826957\n",
      "Accuracy on Test data: 0.8787704110145569, 0.8268354535102844\n",
      "Step 5 | Training Loss: 0.621713 | Validation Accuracy: 0.853786\n",
      "Accuracy on Test data: 0.8936746120452881, 0.8315612077713013\n",
      "Step 1 | Training Loss: 0.620505 | Validation Accuracy: 0.853548\n",
      "Accuracy on Test data: 0.893718957901001, 0.8311392664909363\n",
      "Step 2 | Training Loss: 0.629379 | Validation Accuracy: 0.851405\n",
      "Accuracy on Test data: 0.8939407467842102, 0.8310548663139343\n",
      "Step 3 | Training Loss: 0.641791 | Validation Accuracy: 0.858152\n",
      "Accuracy on Test data: 0.8941625356674194, 0.8306329250335693\n",
      "Step 4 | Training Loss: 0.616796 | Validation Accuracy: 0.855215\n",
      "Accuracy on Test data: 0.894561767578125, 0.8308860659599304\n",
      "Step 5 | Training Loss: 0.613019 | Validation Accuracy: 0.857755\n",
      "Accuracy on Test data: 0.8946061134338379, 0.8306329250335693\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.704950 | Validation Accuracy: 0.483569\n",
      "Accuracy on Test data: 0.4300922751426697, 0.39358648657798767\n",
      "Step 2 | Training Loss: 0.703830 | Validation Accuracy: 0.576361\n",
      "Accuracy on Test data: 0.5453335642814636, 0.5129113793373108\n",
      "Step 3 | Training Loss: 0.683545 | Validation Accuracy: 0.765121\n",
      "Accuracy on Test data: 0.6375088691711426, 0.5572995543479919\n",
      "Step 4 | Training Loss: 0.659705 | Validation Accuracy: 0.822035\n",
      "Accuracy on Test data: 0.6595990061759949, 0.5708860754966736\n",
      "Step 5 | Training Loss: 0.663966 | Validation Accuracy: 0.880854\n",
      "Accuracy on Test data: 0.7792760729789734, 0.6241350173950195\n",
      "Step 1 | Training Loss: 0.671056 | Validation Accuracy: 0.883632\n",
      "Accuracy on Test data: 0.7812721729278564, 0.6263291239738464\n",
      "Step 2 | Training Loss: 0.663193 | Validation Accuracy: 0.883870\n",
      "Accuracy on Test data: 0.782824695110321, 0.6286919713020325\n",
      "Step 3 | Training Loss: 0.678802 | Validation Accuracy: 0.888871\n",
      "Accuracy on Test data: 0.7843772172927856, 0.6301265954971313\n",
      "Step 4 | Training Loss: 0.665507 | Validation Accuracy: 0.884823\n",
      "Accuracy on Test data: 0.7857966423034668, 0.6318987607955933\n",
      "Step 5 | Training Loss: 0.670368 | Validation Accuracy: 0.881727\n",
      "Accuracy on Test data: 0.7869499921798706, 0.6333333253860474\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.708851 | Validation Accuracy: 0.582077\n",
      "Accuracy on Test data: 0.5699964761734009, 0.4411814212799072\n",
      "Step 2 | Training Loss: 0.679936 | Validation Accuracy: 0.808462\n",
      "Accuracy on Test data: 0.7515525221824646, 0.549957811832428\n",
      "Step 3 | Training Loss: 0.651904 | Validation Accuracy: 0.855691\n",
      "Accuracy on Test data: 0.8135645985603333, 0.6628692150115967\n",
      "Step 4 | Training Loss: 0.618064 | Validation Accuracy: 0.882600\n",
      "Accuracy on Test data: 0.8370298147201538, 0.705822765827179\n",
      "Step 5 | Training Loss: 0.610473 | Validation Accuracy: 0.896095\n",
      "Accuracy on Test data: 0.8529985547065735, 0.7331645488739014\n",
      "Step 1 | Training Loss: 0.593706 | Validation Accuracy: 0.891411\n",
      "Accuracy on Test data: 0.8538413643836975, 0.7345991730690002\n",
      "Step 2 | Training Loss: 0.588692 | Validation Accuracy: 0.894031\n",
      "Accuracy on Test data: 0.8549059629440308, 0.7364556789398193\n",
      "Step 3 | Training Loss: 0.575976 | Validation Accuracy: 0.896015\n",
      "Accuracy on Test data: 0.8557487726211548, 0.7378059029579163\n",
      "Step 4 | Training Loss: 0.608383 | Validation Accuracy: 0.894904\n",
      "Accuracy on Test data: 0.8564584851264954, 0.7389873266220093\n",
      "Step 5 | Training Loss: 0.587231 | Validation Accuracy: 0.891411\n",
      "Accuracy on Test data: 0.8573456406593323, 0.7405063509941101\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.652321 | Validation Accuracy: 0.855056\n",
      "Accuracy on Test data: 0.8872870802879333, 0.8203375339508057\n",
      "Step 2 | Training Loss: 0.604522 | Validation Accuracy: 0.890618\n",
      "Accuracy on Test data: 0.8813431262969971, 0.7956117987632751\n",
      "Step 3 | Training Loss: 0.555587 | Validation Accuracy: 0.907525\n",
      "Accuracy on Test data: 0.8613821864128113, 0.7469198107719421\n",
      "Step 4 | Training Loss: 0.526766 | Validation Accuracy: 0.929592\n",
      "Accuracy on Test data: 0.8393807411193848, 0.7027004361152649\n",
      "Step 5 | Training Loss: 0.517844 | Validation Accuracy: 0.948563\n",
      "Accuracy on Test data: 0.8342352509498596, 0.6905485391616821\n",
      "Step 1 | Training Loss: 0.500985 | Validation Accuracy: 0.948087\n",
      "Accuracy on Test data: 0.8347675800323486, 0.6910548806190491\n",
      "Step 2 | Training Loss: 0.470247 | Validation Accuracy: 0.951183\n",
      "Accuracy on Test data: 0.8348562717437744, 0.6911392211914062\n",
      "Step 3 | Training Loss: 0.481322 | Validation Accuracy: 0.949198\n",
      "Accuracy on Test data: 0.8350780606269836, 0.6913080215454102\n",
      "Step 4 | Training Loss: 0.497521 | Validation Accuracy: 0.948484\n",
      "Accuracy on Test data: 0.8353441953659058, 0.6915611624717712\n",
      "Step 5 | Training Loss: 0.472512 | Validation Accuracy: 0.950230\n",
      "Accuracy on Test data: 0.8352998495101929, 0.6913924217224121\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 10\n",
    "#capture\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T20:12:01.591604Z",
     "start_time": "2017-06-18T20:12:01.586451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.111270Z",
     "start_time": "2017-07-19T18:59:40.108666Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "#df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.154890Z",
     "start_time": "2017-07-19T18:59:40.112573Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "#df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.181403Z",
     "start_time": "2017-07-19T18:59:40.156195Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.187275Z",
     "start_time": "2017-07-19T18:59:40.182767Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train.predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:40.666714Z",
     "start_time": "2017-07-19T18:59:40.188625Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:03:07.148150Z",
     "start_time": "2017-07-20T20:03:07.077194Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:03:08.189637Z",
     "start_time": "2017-07-20T20:03:08.186973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:03:10.601869Z",
     "start_time": "2017-07-20T20:03:10.599348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:03:11.689097Z",
     "start_time": "2017-07-20T20:03:11.536182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:03:16.551532Z",
     "start_time": "2017-07-20T20:03:16.393602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.855215</td>\n",
       "      <td>0.894562</td>\n",
       "      <td>0.910884</td>\n",
       "      <td>0.830886</td>\n",
       "      <td>0.899950</td>\n",
       "      <td>13.064114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857755</td>\n",
       "      <td>0.894606</td>\n",
       "      <td>0.910878</td>\n",
       "      <td>0.830633</td>\n",
       "      <td>0.899755</td>\n",
       "      <td>14.526398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.858152</td>\n",
       "      <td>0.894163</td>\n",
       "      <td>0.910590</td>\n",
       "      <td>0.830633</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>11.607629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851405</td>\n",
       "      <td>0.893941</td>\n",
       "      <td>0.910493</td>\n",
       "      <td>0.831055</td>\n",
       "      <td>0.900170</td>\n",
       "      <td>10.226502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.853548</td>\n",
       "      <td>0.893719</td>\n",
       "      <td>0.910383</td>\n",
       "      <td>0.831139</td>\n",
       "      <td>0.900304</td>\n",
       "      <td>8.766340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.825687</td>\n",
       "      <td>0.890126</td>\n",
       "      <td>0.907881</td>\n",
       "      <td>0.833924</td>\n",
       "      <td>0.902158</td>\n",
       "      <td>3.292134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.855056</td>\n",
       "      <td>0.887287</td>\n",
       "      <td>0.904484</td>\n",
       "      <td>0.820338</td>\n",
       "      <td>0.893171</td>\n",
       "      <td>2.023987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.898635</td>\n",
       "      <td>0.885158</td>\n",
       "      <td>0.899608</td>\n",
       "      <td>0.797553</td>\n",
       "      <td>0.875924</td>\n",
       "      <td>7.526736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.826957</td>\n",
       "      <td>0.878770</td>\n",
       "      <td>0.899496</td>\n",
       "      <td>0.826835</td>\n",
       "      <td>0.898627</td>\n",
       "      <td>5.834742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.888316</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>0.891854</td>\n",
       "      <td>0.787679</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>16.187101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860533</td>\n",
       "      <td>0.873048</td>\n",
       "      <td>0.891739</td>\n",
       "      <td>0.781181</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>5.672433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.885934</td>\n",
       "      <td>0.874512</td>\n",
       "      <td>0.891663</td>\n",
       "      <td>0.788101</td>\n",
       "      <td>0.871435</td>\n",
       "      <td>14.600832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.874742</td>\n",
       "      <td>0.874290</td>\n",
       "      <td>0.891634</td>\n",
       "      <td>0.788776</td>\n",
       "      <td>0.872015</td>\n",
       "      <td>11.358085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.878711</td>\n",
       "      <td>0.874335</td>\n",
       "      <td>0.891585</td>\n",
       "      <td>0.788523</td>\n",
       "      <td>0.871750</td>\n",
       "      <td>12.995717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.873472</td>\n",
       "      <td>0.873625</td>\n",
       "      <td>0.891181</td>\n",
       "      <td>0.788861</td>\n",
       "      <td>0.872151</td>\n",
       "      <td>9.727539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863312</td>\n",
       "      <td>0.874823</td>\n",
       "      <td>0.889498</td>\n",
       "      <td>0.813840</td>\n",
       "      <td>0.881830</td>\n",
       "      <td>7.621298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.839736</td>\n",
       "      <td>0.869633</td>\n",
       "      <td>0.889457</td>\n",
       "      <td>0.797384</td>\n",
       "      <td>0.878707</td>\n",
       "      <td>6.505927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.747103</td>\n",
       "      <td>0.864088</td>\n",
       "      <td>0.889186</td>\n",
       "      <td>0.829789</td>\n",
       "      <td>0.900889</td>\n",
       "      <td>4.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.797269</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.886611</td>\n",
       "      <td>0.800169</td>\n",
       "      <td>0.884970</td>\n",
       "      <td>3.783092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.734720</td>\n",
       "      <td>0.846611</td>\n",
       "      <td>0.878624</td>\n",
       "      <td>0.816371</td>\n",
       "      <td>0.896074</td>\n",
       "      <td>2.967211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.930148</td>\n",
       "      <td>0.869943</td>\n",
       "      <td>0.876703</td>\n",
       "      <td>0.760591</td>\n",
       "      <td>0.837094</td>\n",
       "      <td>9.750888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.921178</td>\n",
       "      <td>0.867681</td>\n",
       "      <td>0.874606</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>7.226873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924988</td>\n",
       "      <td>0.869855</td>\n",
       "      <td>0.874390</td>\n",
       "      <td>0.757215</td>\n",
       "      <td>0.831587</td>\n",
       "      <td>11.076009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919987</td>\n",
       "      <td>0.869588</td>\n",
       "      <td>0.874068</td>\n",
       "      <td>0.756878</td>\n",
       "      <td>0.831254</td>\n",
       "      <td>9.937142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.922845</td>\n",
       "      <td>0.867770</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.753671</td>\n",
       "      <td>0.828607</td>\n",
       "      <td>8.869919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.691221</td>\n",
       "      <td>0.834191</td>\n",
       "      <td>0.871130</td>\n",
       "      <td>0.850549</td>\n",
       "      <td>0.914803</td>\n",
       "      <td>3.299453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891411</td>\n",
       "      <td>0.857346</td>\n",
       "      <td>0.869703</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.831793</td>\n",
       "      <td>16.152194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919114</td>\n",
       "      <td>0.861293</td>\n",
       "      <td>0.868949</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>0.825314</td>\n",
       "      <td>11.106206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894904</td>\n",
       "      <td>0.856458</td>\n",
       "      <td>0.868850</td>\n",
       "      <td>0.738987</td>\n",
       "      <td>0.830715</td>\n",
       "      <td>14.537344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921575</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.868848</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>0.825393</td>\n",
       "      <td>10.044874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918320</td>\n",
       "      <td>0.860850</td>\n",
       "      <td>0.868695</td>\n",
       "      <td>0.741181</td>\n",
       "      <td>0.825372</td>\n",
       "      <td>8.846274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918955</td>\n",
       "      <td>0.860628</td>\n",
       "      <td>0.868569</td>\n",
       "      <td>0.741266</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>7.764378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918320</td>\n",
       "      <td>0.860406</td>\n",
       "      <td>0.868420</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>0.825624</td>\n",
       "      <td>6.625149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.896015</td>\n",
       "      <td>0.855749</td>\n",
       "      <td>0.868148</td>\n",
       "      <td>0.737806</td>\n",
       "      <td>0.829837</td>\n",
       "      <td>13.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894031</td>\n",
       "      <td>0.854906</td>\n",
       "      <td>0.867340</td>\n",
       "      <td>0.736456</td>\n",
       "      <td>0.828867</td>\n",
       "      <td>11.367363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.928878</td>\n",
       "      <td>0.856547</td>\n",
       "      <td>0.867329</td>\n",
       "      <td>0.737468</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>7.816151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891411</td>\n",
       "      <td>0.853841</td>\n",
       "      <td>0.866269</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>0.827473</td>\n",
       "      <td>9.744935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917606</td>\n",
       "      <td>0.861648</td>\n",
       "      <td>0.865264</td>\n",
       "      <td>0.742194</td>\n",
       "      <td>0.819049</td>\n",
       "      <td>7.718013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925385</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.863651</td>\n",
       "      <td>0.731983</td>\n",
       "      <td>0.823261</td>\n",
       "      <td>5.856151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895221</td>\n",
       "      <td>0.853442</td>\n",
       "      <td>0.863324</td>\n",
       "      <td>0.736962</td>\n",
       "      <td>0.824325</td>\n",
       "      <td>4.412476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.706303</td>\n",
       "      <td>0.819154</td>\n",
       "      <td>0.860286</td>\n",
       "      <td>0.805654</td>\n",
       "      <td>0.891044</td>\n",
       "      <td>1.492567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.855926</td>\n",
       "      <td>0.858881</td>\n",
       "      <td>0.731392</td>\n",
       "      <td>0.809981</td>\n",
       "      <td>6.638997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931180</td>\n",
       "      <td>0.853797</td>\n",
       "      <td>0.857180</td>\n",
       "      <td>0.729789</td>\n",
       "      <td>0.809041</td>\n",
       "      <td>11.822337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924036</td>\n",
       "      <td>0.853664</td>\n",
       "      <td>0.857031</td>\n",
       "      <td>0.729620</td>\n",
       "      <td>0.808876</td>\n",
       "      <td>9.744194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.853575</td>\n",
       "      <td>0.856970</td>\n",
       "      <td>0.729620</td>\n",
       "      <td>0.808899</td>\n",
       "      <td>8.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924512</td>\n",
       "      <td>0.853353</td>\n",
       "      <td>0.856747</td>\n",
       "      <td>0.729451</td>\n",
       "      <td>0.808734</td>\n",
       "      <td>7.348495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.707970</td>\n",
       "      <td>0.813387</td>\n",
       "      <td>0.855772</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>0.892932</td>\n",
       "      <td>1.901208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912446</td>\n",
       "      <td>0.850293</td>\n",
       "      <td>0.853864</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>0.805963</td>\n",
       "      <td>4.894346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.907525</td>\n",
       "      <td>0.846966</td>\n",
       "      <td>0.852451</td>\n",
       "      <td>0.720422</td>\n",
       "      <td>0.805060</td>\n",
       "      <td>4.835380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.882600</td>\n",
       "      <td>0.837030</td>\n",
       "      <td>0.848869</td>\n",
       "      <td>0.705823</td>\n",
       "      <td>0.804859</td>\n",
       "      <td>6.584548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.759581</td>\n",
       "      <td>0.736304</td>\n",
       "      <td>0.556371</td>\n",
       "      <td>0.635209</td>\n",
       "      <td>2.240047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.788062</td>\n",
       "      <td>0.755633</td>\n",
       "      <td>0.735817</td>\n",
       "      <td>0.560253</td>\n",
       "      <td>0.646879</td>\n",
       "      <td>2.233437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706620</td>\n",
       "      <td>0.726180</td>\n",
       "      <td>0.733658</td>\n",
       "      <td>0.535274</td>\n",
       "      <td>0.669507</td>\n",
       "      <td>2.155548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.455469</td>\n",
       "      <td>0.584768</td>\n",
       "      <td>0.731369</td>\n",
       "      <td>0.848270</td>\n",
       "      <td>0.914511</td>\n",
       "      <td>1.652404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.861724</td>\n",
       "      <td>0.747738</td>\n",
       "      <td>0.730257</td>\n",
       "      <td>0.536962</td>\n",
       "      <td>0.628881</td>\n",
       "      <td>2.462379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720749</td>\n",
       "      <td>0.745520</td>\n",
       "      <td>0.726953</td>\n",
       "      <td>0.550380</td>\n",
       "      <td>0.643135</td>\n",
       "      <td>1.165165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.476822</td>\n",
       "      <td>0.571948</td>\n",
       "      <td>0.723781</td>\n",
       "      <td>0.807089</td>\n",
       "      <td>0.892686</td>\n",
       "      <td>1.648278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799413</td>\n",
       "      <td>0.718728</td>\n",
       "      <td>0.713220</td>\n",
       "      <td>0.502532</td>\n",
       "      <td>0.623395</td>\n",
       "      <td>1.216665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892761</td>\n",
       "      <td>0.738778</td>\n",
       "      <td>0.711309</td>\n",
       "      <td>0.514684</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>12.032783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892681</td>\n",
       "      <td>0.737979</td>\n",
       "      <td>0.710257</td>\n",
       "      <td>0.514177</td>\n",
       "      <td>0.592194</td>\n",
       "      <td>10.791958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888871</td>\n",
       "      <td>0.737136</td>\n",
       "      <td>0.709025</td>\n",
       "      <td>0.513165</td>\n",
       "      <td>0.590822</td>\n",
       "      <td>9.633597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687252</td>\n",
       "      <td>0.728930</td>\n",
       "      <td>0.708986</td>\n",
       "      <td>0.578312</td>\n",
       "      <td>0.681740</td>\n",
       "      <td>2.246917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887998</td>\n",
       "      <td>0.736781</td>\n",
       "      <td>0.708575</td>\n",
       "      <td>0.512489</td>\n",
       "      <td>0.590138</td>\n",
       "      <td>8.413865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924432</td>\n",
       "      <td>0.725781</td>\n",
       "      <td>0.705423</td>\n",
       "      <td>0.485316</td>\n",
       "      <td>0.586312</td>\n",
       "      <td>11.090278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880140</td>\n",
       "      <td>0.733721</td>\n",
       "      <td>0.703892</td>\n",
       "      <td>0.510127</td>\n",
       "      <td>0.585860</td>\n",
       "      <td>5.016164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915939</td>\n",
       "      <td>0.724228</td>\n",
       "      <td>0.703260</td>\n",
       "      <td>0.482363</td>\n",
       "      <td>0.582948</td>\n",
       "      <td>10.017707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.908239</td>\n",
       "      <td>0.723252</td>\n",
       "      <td>0.701897</td>\n",
       "      <td>0.480506</td>\n",
       "      <td>0.580825</td>\n",
       "      <td>8.869971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912685</td>\n",
       "      <td>0.722276</td>\n",
       "      <td>0.700531</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>7.803448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906017</td>\n",
       "      <td>0.721389</td>\n",
       "      <td>0.699287</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.577144</td>\n",
       "      <td>6.662382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652485</td>\n",
       "      <td>0.624645</td>\n",
       "      <td>0.690852</td>\n",
       "      <td>0.704304</td>\n",
       "      <td>0.809462</td>\n",
       "      <td>1.145454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869582</td>\n",
       "      <td>0.713006</td>\n",
       "      <td>0.687802</td>\n",
       "      <td>0.463629</td>\n",
       "      <td>0.562018</td>\n",
       "      <td>4.476040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.814574</td>\n",
       "      <td>0.707328</td>\n",
       "      <td>0.684336</td>\n",
       "      <td>0.477722</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>1.914489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845134</td>\n",
       "      <td>0.723119</td>\n",
       "      <td>0.684109</td>\n",
       "      <td>0.490211</td>\n",
       "      <td>0.557403</td>\n",
       "      <td>4.899869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.779409</td>\n",
       "      <td>0.670955</td>\n",
       "      <td>0.670925</td>\n",
       "      <td>0.601772</td>\n",
       "      <td>0.727619</td>\n",
       "      <td>2.216461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.710389</td>\n",
       "      <td>0.668898</td>\n",
       "      <td>0.471899</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>3.817973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797746</td>\n",
       "      <td>0.707993</td>\n",
       "      <td>0.665141</td>\n",
       "      <td>0.477131</td>\n",
       "      <td>0.544144</td>\n",
       "      <td>2.394279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.704223</td>\n",
       "      <td>0.658227</td>\n",
       "      <td>0.507679</td>\n",
       "      <td>0.581972</td>\n",
       "      <td>1.176690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.687012</td>\n",
       "      <td>0.649130</td>\n",
       "      <td>0.421435</td>\n",
       "      <td>0.508812</td>\n",
       "      <td>2.266125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815923</td>\n",
       "      <td>0.688565</td>\n",
       "      <td>0.634988</td>\n",
       "      <td>0.434346</td>\n",
       "      <td>0.487499</td>\n",
       "      <td>2.518926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822035</td>\n",
       "      <td>0.659599</td>\n",
       "      <td>0.633629</td>\n",
       "      <td>0.570886</td>\n",
       "      <td>0.682842</td>\n",
       "      <td>5.851403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.743610</td>\n",
       "      <td>0.663857</td>\n",
       "      <td>0.619425</td>\n",
       "      <td>0.510295</td>\n",
       "      <td>0.610929</td>\n",
       "      <td>2.978911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.771630</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.587092</td>\n",
       "      <td>0.381603</td>\n",
       "      <td>0.416375</td>\n",
       "      <td>1.325962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799016</td>\n",
       "      <td>0.626863</td>\n",
       "      <td>0.580491</td>\n",
       "      <td>0.487004</td>\n",
       "      <td>0.606002</td>\n",
       "      <td>2.907090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.672805</td>\n",
       "      <td>0.629569</td>\n",
       "      <td>0.566294</td>\n",
       "      <td>0.381435</td>\n",
       "      <td>0.469225</td>\n",
       "      <td>1.196492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641292</td>\n",
       "      <td>0.620564</td>\n",
       "      <td>0.538121</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.634356</td>\n",
       "      <td>1.184738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759168</td>\n",
       "      <td>0.597853</td>\n",
       "      <td>0.536076</td>\n",
       "      <td>0.441097</td>\n",
       "      <td>0.556129</td>\n",
       "      <td>1.521174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491586</td>\n",
       "      <td>0.393941</td>\n",
       "      <td>0.518993</td>\n",
       "      <td>0.584810</td>\n",
       "      <td>0.715377</td>\n",
       "      <td>1.142040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797269</td>\n",
       "      <td>0.620963</td>\n",
       "      <td>0.518836</td>\n",
       "      <td>0.460169</td>\n",
       "      <td>0.525762</td>\n",
       "      <td>2.296757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444991</td>\n",
       "      <td>0.464203</td>\n",
       "      <td>0.497274</td>\n",
       "      <td>0.509030</td>\n",
       "      <td>0.647180</td>\n",
       "      <td>1.092584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753770</td>\n",
       "      <td>0.598696</td>\n",
       "      <td>0.485644</td>\n",
       "      <td>0.432489</td>\n",
       "      <td>0.493485</td>\n",
       "      <td>1.191725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.576361</td>\n",
       "      <td>0.545334</td>\n",
       "      <td>0.480065</td>\n",
       "      <td>0.512911</td>\n",
       "      <td>0.611992</td>\n",
       "      <td>2.920725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.560814</td>\n",
       "      <td>0.475110</td>\n",
       "      <td>0.475105</td>\n",
       "      <td>0.581989</td>\n",
       "      <td>1.501227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.582077</td>\n",
       "      <td>0.569996</td>\n",
       "      <td>0.444788</td>\n",
       "      <td>0.441181</td>\n",
       "      <td>0.515369</td>\n",
       "      <td>1.660787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605414</td>\n",
       "      <td>0.537083</td>\n",
       "      <td>0.366671</td>\n",
       "      <td>0.395696</td>\n",
       "      <td>0.430627</td>\n",
       "      <td>1.272138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.690903</td>\n",
       "      <td>0.551943</td>\n",
       "      <td>0.362029</td>\n",
       "      <td>0.332574</td>\n",
       "      <td>0.322570</td>\n",
       "      <td>2.525971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.483569</td>\n",
       "      <td>0.430092</td>\n",
       "      <td>0.335471</td>\n",
       "      <td>0.393586</td>\n",
       "      <td>0.466677</td>\n",
       "      <td>1.525701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616368</td>\n",
       "      <td>0.525106</td>\n",
       "      <td>0.307324</td>\n",
       "      <td>0.308439</td>\n",
       "      <td>0.286585</td>\n",
       "      <td>1.392984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.533180</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.293707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.528973</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.279630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.531751</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.537956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "168     10              12              3     0.855215    0.894562  0.910884   \n",
       "169     12              12              3     0.857755    0.894606  0.910878   \n",
       "167      8              12              3     0.858152    0.894163  0.910590   \n",
       "166      6              12              3     0.851405    0.893941  0.910493   \n",
       "164      4              12              3     0.853548    0.893719  0.910383   \n",
       "118      3              48              3     0.825687    0.890126  0.907881   \n",
       "186      2             122              3     0.855056    0.887287  0.904484   \n",
       "55       5              48              3     0.898635    0.885158  0.899608   \n",
       "165      5              12              3     0.826957    0.878770  0.899496   \n",
       "47      12              12              3     0.888316    0.874867  0.891854   \n",
       "54       4              48              3     0.860533    0.873048  0.891739   \n",
       "46      10              12              3     0.885934    0.874512  0.891663   \n",
       "44       6              12              3     0.874742    0.874290  0.891634   \n",
       "45       8              12              3     0.878711    0.874335  0.891585   \n",
       "42       4              12              3     0.873472    0.873625  0.891181   \n",
       "51       6              24              3     0.863312    0.874823  0.889498   \n",
       "43       5              12              3     0.839736    0.869633  0.889457   \n",
       "50       4              24              3     0.747103    0.864088  0.889186   \n",
       "53       3              48              3     0.797269    0.861116  0.886611   \n",
       "163      3              12              3     0.734720    0.846611  0.878624   \n",
       "59       5             122              3     0.930148    0.869943  0.876703   \n",
       "58       4             122              3     0.921178    0.867681  0.874606   \n",
       "99      12             122              1     0.924988    0.869855  0.874390   \n",
       "98      10             122              1     0.919987    0.869588  0.874068   \n",
       "97       8             122              1     0.922845    0.867770  0.872066   \n",
       "49       3              24              3     0.691221    0.834191  0.871130   \n",
       "185     12              48              3     0.891411    0.857346  0.869703   \n",
       "75      12              12              1     0.919114    0.861293  0.868949   \n",
       "184     10              48              3     0.894904    0.856458  0.868850   \n",
       "74      10              12              1     0.921575    0.861116  0.868848   \n",
       "73       8              12              1     0.918320    0.860850  0.868695   \n",
       "72       6              12              1     0.918955    0.860628  0.868569   \n",
       "70       4              12              1     0.918320    0.860406  0.868420   \n",
       "183      8              48              3     0.896015    0.855749  0.868148   \n",
       "182      6              48              3     0.894031    0.854906  0.867340   \n",
       "122      5             122              3     0.928878    0.856547  0.867329   \n",
       "180      4              48              3     0.891411    0.853841  0.866269   \n",
       "96       6             122              1     0.917606    0.861648  0.865264   \n",
       "121      4             122              3     0.925385    0.852644  0.863651   \n",
       "71       5              12              1     0.895221    0.853442  0.863324   \n",
       "162      2              12              3     0.706303    0.819154  0.860286   \n",
       "94       4             122              1     0.915780    0.855926  0.858881   \n",
       "38      12             122              1     0.931180    0.853797  0.857180   \n",
       "37       8             122              1     0.924036    0.853664  0.857031   \n",
       "36       6             122              1     0.925464    0.853575  0.856970   \n",
       "34       4             122              1     0.924512    0.853353  0.856747   \n",
       "52       2              48              3     0.707970    0.813387  0.855772   \n",
       "35       5             122              1     0.912446    0.850293  0.853864   \n",
       "57       3             122              3     0.907525    0.846966  0.852451   \n",
       "181      5              48              3     0.882600    0.837030  0.848869   \n",
       "..     ...             ...            ...          ...         ...       ...   \n",
       "93       3             122              1     0.758771    0.759581  0.736304   \n",
       "77       3              24              1     0.788062    0.755633  0.735817   \n",
       "85       3              48              1     0.706620    0.726180  0.733658   \n",
       "48       2              24              3     0.455469    0.584768  0.731369   \n",
       "56       2             122              3     0.861724    0.747738  0.730257   \n",
       "76       2              24              1     0.720749    0.745520  0.726953   \n",
       "117      2              48              3     0.476822    0.571948  0.723781   \n",
       "16       2              24              1     0.799413    0.718728  0.713220   \n",
       "15      12              12              1     0.892761    0.738778  0.711309   \n",
       "14      10              12              1     0.892681    0.737979  0.710257   \n",
       "13       8              12              1     0.888871    0.737136  0.709025   \n",
       "61       3               1              1     0.687252    0.728930  0.708986   \n",
       "12       6              12              1     0.887998    0.736781  0.708575   \n",
       "138     12              12              1     0.924432    0.725781  0.705423   \n",
       "11       5              12              1     0.880140    0.733721  0.703892   \n",
       "137     10              12              1     0.915939    0.724228  0.703260   \n",
       "136      8              12              1     0.908239    0.723252  0.701897   \n",
       "135      6              12              1     0.912685    0.722276  0.700531   \n",
       "133      4              12              1     0.906017    0.721389  0.699287   \n",
       "68       2              12              1     0.652485    0.624645  0.690852   \n",
       "134      5              12              1     0.869582    0.713006  0.687802   \n",
       "119      2             122              3     0.814574    0.707328  0.684336   \n",
       "3        5               1              1     0.845134    0.723119  0.684109   \n",
       "140      3              24              1     0.779409    0.670955  0.670925   \n",
       "10       4              12              1     0.843308    0.710389  0.668898   \n",
       "25       3              48              1     0.797746    0.707993  0.665141   \n",
       "92       2             122              1     0.644864    0.704223  0.658227   \n",
       "132      3              12              1     0.798460    0.687012  0.649130   \n",
       "9        3              12              1     0.815923    0.688565  0.634988   \n",
       "173      5              24              3     0.822035    0.659599  0.633629   \n",
       "102      3              12              3     0.743610    0.663857  0.619425   \n",
       "8        2              12              1     0.771630    0.656893  0.587092   \n",
       "110      3              24              3     0.799016    0.626863  0.580491   \n",
       "131      2              12              1     0.672805    0.629569  0.566294   \n",
       "60       2               1              1     0.641292    0.620564  0.538121   \n",
       "109      2              24              3     0.759168    0.597853  0.536076   \n",
       "139      2              24              1     0.491586    0.393941  0.518993   \n",
       "154      3             122              1     0.797269    0.620963  0.518836   \n",
       "84       2              48              1     0.444991    0.464203  0.497274   \n",
       "153      2             122              1     0.753770    0.598696  0.485644   \n",
       "171      3              24              3     0.576361    0.545334  0.480065   \n",
       "101      2              12              3     0.559375    0.560814  0.475110   \n",
       "178      2              48              3     0.582077    0.569996  0.444788   \n",
       "24       2              48              1     0.605414    0.537083  0.366671   \n",
       "1        3               1              1     0.690903    0.551943  0.362029   \n",
       "170      2              24              3     0.483569    0.430092  0.335471   \n",
       "0        2               1              1     0.616368    0.525106  0.307324   \n",
       "161      2               1              3     0.533180    0.430758  0.000000   \n",
       "100      2               1              3     0.528973    0.430758  0.000000   \n",
       "39       2               1              3     0.531751    0.430758  0.000000   \n",
       "\n",
       "     test_score_20  f1_score_20  time_taken  \n",
       "168       0.830886     0.899950   13.064114  \n",
       "169       0.830633     0.899755   14.526398  \n",
       "167       0.830633     0.899835   11.607629  \n",
       "166       0.831055     0.900170   10.226502  \n",
       "164       0.831139     0.900304    8.766340  \n",
       "118       0.833924     0.902158    3.292134  \n",
       "186       0.820338     0.893171    2.023987  \n",
       "55        0.797553     0.875924    7.526736  \n",
       "165       0.826835     0.898627    5.834742  \n",
       "47        0.787679     0.871080   16.187101  \n",
       "54        0.781181     0.869718    5.672433  \n",
       "46        0.788101     0.871435   14.600832  \n",
       "44        0.788776     0.872015   11.358085  \n",
       "45        0.788523     0.871750   12.995717  \n",
       "42        0.788861     0.872151    9.727539  \n",
       "51        0.813840     0.881830    7.621298  \n",
       "43        0.797384     0.878707    6.505927  \n",
       "50        0.829789     0.900889    4.893100  \n",
       "53        0.800169     0.884970    3.783092  \n",
       "163       0.816371     0.896074    2.967211  \n",
       "59        0.760591     0.837094    9.750888  \n",
       "58        0.757384     0.834875    7.226873  \n",
       "99        0.757215     0.831587   11.076009  \n",
       "98        0.756878     0.831254    9.937142  \n",
       "97        0.753671     0.828607    8.869919  \n",
       "49        0.850549     0.914803    3.299453  \n",
       "185       0.740506     0.831793   16.152194  \n",
       "75        0.741435     0.825314   11.106206  \n",
       "184       0.738987     0.830715   14.537344  \n",
       "74        0.741435     0.825393   10.044874  \n",
       "73        0.741181     0.825372    8.846274  \n",
       "72        0.741266     0.825518    7.764378  \n",
       "70        0.741350     0.825624    6.625149  \n",
       "183       0.737806     0.829837   13.012917  \n",
       "182       0.736456     0.828867   11.367363  \n",
       "122       0.737468     0.827099    7.816151  \n",
       "180       0.734599     0.827473    9.744935  \n",
       "96        0.742194     0.819049    7.718013  \n",
       "121       0.731983     0.823261    5.856151  \n",
       "71        0.736962     0.824325    4.412476  \n",
       "162       0.805654     0.891044    1.492567  \n",
       "94        0.731392     0.809981    6.638997  \n",
       "38        0.729789     0.809041   11.822337  \n",
       "37        0.729620     0.808876    9.744194  \n",
       "36        0.729620     0.808899    8.510845  \n",
       "34        0.729451     0.808734    7.348495  \n",
       "52        0.810802     0.892932    1.901208  \n",
       "35        0.725401     0.805963    4.894346  \n",
       "57        0.720422     0.805060    4.835380  \n",
       "181       0.705823     0.804859    6.584548  \n",
       "..             ...          ...         ...  \n",
       "93        0.556371     0.635209    2.240047  \n",
       "77        0.560253     0.646879    2.233437  \n",
       "85        0.535274     0.669507    2.155548  \n",
       "48        0.848270     0.914511    1.652404  \n",
       "56        0.536962     0.628881    2.462379  \n",
       "76        0.550380     0.643135    1.165165  \n",
       "117       0.807089     0.892686    1.648278  \n",
       "16        0.502532     0.623395    1.216665  \n",
       "15        0.514684     0.592965   12.032783  \n",
       "14        0.514177     0.592194   10.791958  \n",
       "13        0.513165     0.590822    9.633597  \n",
       "61        0.578312     0.681740    2.246917  \n",
       "12        0.512489     0.590138    8.413865  \n",
       "138       0.485316     0.586312   11.090278  \n",
       "11        0.510127     0.585860    5.016164  \n",
       "137       0.482363     0.582948   10.017707  \n",
       "136       0.480506     0.580825    8.869971  \n",
       "135       0.478903     0.578987    7.803448  \n",
       "133       0.477300     0.577144    6.662382  \n",
       "68        0.704304     0.809462    1.145454  \n",
       "134       0.463629     0.562018    4.476040  \n",
       "119       0.477722     0.575893    1.914489  \n",
       "3         0.490211     0.557403    4.899869  \n",
       "140       0.601772     0.727619    2.216461  \n",
       "10        0.471899     0.537472    3.817973  \n",
       "25        0.477131     0.544144    2.394279  \n",
       "92        0.507679     0.581972    1.176690  \n",
       "132       0.421435     0.508812    2.266125  \n",
       "9         0.434346     0.487499    2.518926  \n",
       "173       0.570886     0.682842    5.851403  \n",
       "102       0.510295     0.610929    2.978911  \n",
       "8         0.381603     0.416375    1.325962  \n",
       "110       0.487004     0.606002    2.907090  \n",
       "131       0.381435     0.469225    1.196492  \n",
       "60        0.533333     0.634356    1.184738  \n",
       "109       0.441097     0.556129    1.521174  \n",
       "139       0.584810     0.715377    1.142040  \n",
       "154       0.460169     0.525762    2.296757  \n",
       "84        0.509030     0.647180    1.092584  \n",
       "153       0.432489     0.493485    1.191725  \n",
       "171       0.512911     0.611992    2.920725  \n",
       "101       0.475105     0.581989    1.501227  \n",
       "178       0.441181     0.515369    1.660787  \n",
       "24        0.395696     0.430627    1.272138  \n",
       "1         0.332574     0.322570    2.525971  \n",
       "170       0.393586     0.466677    1.525701  \n",
       "0         0.308439     0.286585    1.392984  \n",
       "161       0.181603     0.000000    1.293707  \n",
       "100       0.181603     0.000000    1.279630  \n",
       "39        0.181603     0.000000    1.537956  \n",
       "\n",
       "[187 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:04.564393Z",
     "start_time": "2017-07-20T20:14:04.544985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.857755</td>\n",
       "      <td>0.894606</td>\n",
       "      <td>0.910878</td>\n",
       "      <td>0.830633</td>\n",
       "      <td>0.899755</td>\n",
       "      <td>14.526398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.825687</td>\n",
       "      <td>0.890126</td>\n",
       "      <td>0.907881</td>\n",
       "      <td>0.833924</td>\n",
       "      <td>0.902158</td>\n",
       "      <td>3.292134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.855056</td>\n",
       "      <td>0.887287</td>\n",
       "      <td>0.904484</td>\n",
       "      <td>0.820338</td>\n",
       "      <td>0.893171</td>\n",
       "      <td>2.023987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863312</td>\n",
       "      <td>0.874823</td>\n",
       "      <td>0.889498</td>\n",
       "      <td>0.813840</td>\n",
       "      <td>0.881830</td>\n",
       "      <td>7.621298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.924988</td>\n",
       "      <td>0.869855</td>\n",
       "      <td>0.874390</td>\n",
       "      <td>0.757215</td>\n",
       "      <td>0.831587</td>\n",
       "      <td>11.076009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.919114</td>\n",
       "      <td>0.861293</td>\n",
       "      <td>0.868949</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>0.825314</td>\n",
       "      <td>11.106206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.930386</td>\n",
       "      <td>0.834102</td>\n",
       "      <td>0.843828</td>\n",
       "      <td>0.695190</td>\n",
       "      <td>0.794235</td>\n",
       "      <td>11.245759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.909986</td>\n",
       "      <td>0.832816</td>\n",
       "      <td>0.839172</td>\n",
       "      <td>0.689873</td>\n",
       "      <td>0.785452</td>\n",
       "      <td>10.797683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.914590</td>\n",
       "      <td>0.821726</td>\n",
       "      <td>0.824398</td>\n",
       "      <td>0.667004</td>\n",
       "      <td>0.761657</td>\n",
       "      <td>11.076122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.528973</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.279630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "12             3                 12     0.857755    0.894606  0.910878   \n",
       "48             3                  3     0.825687    0.890126  0.907881   \n",
       "122            3                  2     0.855056    0.887287  0.904484   \n",
       "24             3                  6     0.863312    0.874823  0.889498   \n",
       "122            1                 12     0.924988    0.869855  0.874390   \n",
       "12             1                 12     0.919114    0.861293  0.868949   \n",
       "24             1                 12     0.930386    0.834102  0.843828   \n",
       "48             1                 12     0.909986    0.832816  0.839172   \n",
       "1              1                 12     0.914590    0.821726  0.824398   \n",
       "               3                  2     0.528973    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "12             3                   0.830633     0.899755   14.526398  \n",
       "48             3                   0.833924     0.902158    3.292134  \n",
       "122            3                   0.820338     0.893171    2.023987  \n",
       "24             3                   0.813840     0.881830    7.621298  \n",
       "122            1                   0.757215     0.831587   11.076009  \n",
       "12             1                   0.741435     0.825314   11.106206  \n",
       "24             1                   0.695190     0.794235   11.245759  \n",
       "48             1                   0.689873     0.785452   10.797683  \n",
       "1              1                   0.667004     0.761657   11.076122  \n",
       "               3                   0.181603     0.000000    1.279630  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:16.612249Z",
     "start_time": "2017-07-20T20:14:16.594961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.895389</td>\n",
       "      <td>0.829371</td>\n",
       "      <td>0.832344</td>\n",
       "      <td>0.692368</td>\n",
       "      <td>0.779285</td>\n",
       "      <td>5.084863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.822525</td>\n",
       "      <td>0.825593</td>\n",
       "      <td>0.838426</td>\n",
       "      <td>0.742057</td>\n",
       "      <td>0.828262</td>\n",
       "      <td>8.878852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>4.928571</td>\n",
       "      <td>0.807702</td>\n",
       "      <td>0.809404</td>\n",
       "      <td>0.824254</td>\n",
       "      <td>0.729717</td>\n",
       "      <td>0.816798</td>\n",
       "      <td>7.155954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>6.086957</td>\n",
       "      <td>0.873620</td>\n",
       "      <td>0.800608</td>\n",
       "      <td>0.788811</td>\n",
       "      <td>0.655799</td>\n",
       "      <td>0.736685</td>\n",
       "      <td>6.602675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.869139</td>\n",
       "      <td>0.772242</td>\n",
       "      <td>0.776736</td>\n",
       "      <td>0.620840</td>\n",
       "      <td>0.723665</td>\n",
       "      <td>6.771927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.856225</td>\n",
       "      <td>0.769009</td>\n",
       "      <td>0.759046</td>\n",
       "      <td>0.610660</td>\n",
       "      <td>0.703376</td>\n",
       "      <td>6.251669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.845352</td>\n",
       "      <td>0.761455</td>\n",
       "      <td>0.737676</td>\n",
       "      <td>0.588270</td>\n",
       "      <td>0.676262</td>\n",
       "      <td>6.768404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.865167</td>\n",
       "      <td>0.749634</td>\n",
       "      <td>0.732707</td>\n",
       "      <td>0.558087</td>\n",
       "      <td>0.642448</td>\n",
       "      <td>6.587167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.807374</td>\n",
       "      <td>0.724998</td>\n",
       "      <td>0.722034</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.711898</td>\n",
       "      <td>7.749273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.531301</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.370431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "122            3              3.333333     0.895389    0.829371  0.832344   \n",
       "12             3              6.250000     0.822525    0.825593  0.838426   \n",
       "48             3              4.928571     0.807702    0.809404  0.824254   \n",
       "122            1              6.086957     0.873620    0.800608  0.788811   \n",
       "24             1              6.250000     0.869139    0.772242  0.776736   \n",
       "48             1              6.000000     0.856225    0.769009  0.759046   \n",
       "1              1              6.250000     0.845352    0.761455  0.737676   \n",
       "12             1              6.250000     0.865167    0.749634  0.732707   \n",
       "24             3              5.750000     0.807374    0.724998  0.722034   \n",
       "1              3              2.000000     0.531301    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "122            3                   0.692368     0.779285    5.084863  \n",
       "12             3                   0.742057     0.828262    8.878852  \n",
       "48             3                   0.729717     0.816798    7.155954  \n",
       "122            1                   0.655799     0.736685    6.602675  \n",
       "24             1                   0.620840     0.723665    6.771927  \n",
       "48             1                   0.610660     0.703376    6.251669  \n",
       "1              1                   0.588270     0.676262    6.768404  \n",
       "12             1                   0.558087     0.642448    6.587167  \n",
       "24             3                   0.612245     0.711898    7.749273  \n",
       "1              3                   0.181603     0.000000    1.370431  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:47.664386Z",
     "start_time": "2017-07-20T20:14:47.199776Z"
    }
   },
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:48.302604Z",
     "start_time": "2017-07-20T20:14:48.274089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230107</td>\n",
       "      <td>0.769893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235818</td>\n",
       "      <td>0.764182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362311</td>\n",
       "      <td>0.637689</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282255</td>\n",
       "      <td>0.717745</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.197159</td>\n",
       "      <td>0.802841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.696406</td>\n",
       "      <td>0.303594</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478545</td>\n",
       "      <td>0.521455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.156524</td>\n",
       "      <td>0.843476</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.689525</td>\n",
       "      <td>0.310475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031989</td>\n",
       "      <td>0.968011</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329121</td>\n",
       "      <td>0.670879</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569964</td>\n",
       "      <td>0.430036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.237382</td>\n",
       "      <td>0.762618</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268989</td>\n",
       "      <td>0.731011</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482669</td>\n",
       "      <td>0.517331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716138</td>\n",
       "      <td>0.283862</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661418</td>\n",
       "      <td>0.338582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720062</td>\n",
       "      <td>0.279938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426814</td>\n",
       "      <td>0.573186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233329</td>\n",
       "      <td>0.766671</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142049</td>\n",
       "      <td>0.857951</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>0.986748</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.662964</td>\n",
       "      <td>0.337036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713569</td>\n",
       "      <td>0.286431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.970961</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054994</td>\n",
       "      <td>0.945006</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>0.319224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665089</td>\n",
       "      <td>0.334911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094858</td>\n",
       "      <td>0.905142</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433243</td>\n",
       "      <td>0.566757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205089</td>\n",
       "      <td>0.794911</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571781</td>\n",
       "      <td>0.428219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720927</td>\n",
       "      <td>0.279073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369710</td>\n",
       "      <td>0.630290</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104306</td>\n",
       "      <td>0.895694</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.988276</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664342</td>\n",
       "      <td>0.335658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100961</td>\n",
       "      <td>0.899039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528224</td>\n",
       "      <td>0.471776</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399177</td>\n",
       "      <td>0.600823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.273403</td>\n",
       "      <td>0.726597</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717704</td>\n",
       "      <td>0.282296</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708948</td>\n",
       "      <td>0.291052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655782</td>\n",
       "      <td>0.344218</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018439</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.662563</td>\n",
       "      <td>0.337437</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045649</td>\n",
       "      <td>0.954351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.649020</td>\n",
       "      <td>0.350980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316514</td>\n",
       "      <td>0.683486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463015</td>\n",
       "      <td>0.536985</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.993107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715421</td>\n",
       "      <td>0.284579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032290</td>\n",
       "      <td>0.967710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.229748</td>\n",
       "      <td>0.770252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238547</td>\n",
       "      <td>0.761453</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.350070</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>0.768452</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039416</td>\n",
       "      <td>0.960584</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214232</td>\n",
       "      <td>0.785768</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206248</td>\n",
       "      <td>0.793752</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102242</td>\n",
       "      <td>0.897758</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052388</td>\n",
       "      <td>0.947612</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514110</td>\n",
       "      <td>0.485890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520999</td>\n",
       "      <td>0.479001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633006</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.267469</td>\n",
       "      <td>0.732531</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.143803</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640742</td>\n",
       "      <td>0.359258</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.174969</td>\n",
       "      <td>0.825031</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.902647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>0.544816</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365732</td>\n",
       "      <td>0.634268</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.703092</td>\n",
       "      <td>0.296908</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.385163</td>\n",
       "      <td>0.614837</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617858</td>\n",
       "      <td>0.382142</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.239171</td>\n",
       "      <td>0.760829</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.146076</td>\n",
       "      <td>0.853924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366466</td>\n",
       "      <td>0.633534</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.639328</td>\n",
       "      <td>0.360672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.703958</td>\n",
       "      <td>0.296042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22524</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.244849</td>\n",
       "      <td>0.755151</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22525</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193197</td>\n",
       "      <td>0.806803</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707727</td>\n",
       "      <td>0.292273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22527</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.704080</td>\n",
       "      <td>0.295920</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213464</td>\n",
       "      <td>0.786536</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284326</td>\n",
       "      <td>0.715674</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257704</td>\n",
       "      <td>0.742296</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.247290</td>\n",
       "      <td>0.752710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666347</td>\n",
       "      <td>0.333653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713121</td>\n",
       "      <td>0.286879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.229835</td>\n",
       "      <td>0.770165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709884</td>\n",
       "      <td>0.290116</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22536</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231734</td>\n",
       "      <td>0.768266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578593</td>\n",
       "      <td>0.421407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200660</td>\n",
       "      <td>0.799340</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.570416</td>\n",
       "      <td>0.429584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677936</td>\n",
       "      <td>0.322064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.634504</td>\n",
       "      <td>0.365496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531635</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22543</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.993123</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22544 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.230107     0.769893         1.0\n",
       "1         1.0     0.235818     0.764182         1.0\n",
       "2         0.0     0.362311     0.637689         1.0\n",
       "3         1.0     0.282255     0.717745         1.0\n",
       "4         1.0     0.197159     0.802841         1.0\n",
       "5         0.0     0.696406     0.303594         0.0\n",
       "6         0.0     0.478545     0.521455         1.0\n",
       "7         1.0     0.156524     0.843476         1.0\n",
       "8         0.0     0.689525     0.310475         0.0\n",
       "9         1.0     0.031989     0.968011         1.0\n",
       "10        1.0     0.329121     0.670879         1.0\n",
       "11        0.0     0.569964     0.430036         0.0\n",
       "12        1.0     0.237382     0.762618         1.0\n",
       "13        1.0     0.268989     0.731011         1.0\n",
       "14        0.0     0.482669     0.517331         1.0\n",
       "15        0.0     0.716138     0.283862         0.0\n",
       "16        0.0     0.661418     0.338582         0.0\n",
       "17        0.0     0.720062     0.279938         0.0\n",
       "18        0.0     0.426814     0.573186         1.0\n",
       "19        1.0     0.233329     0.766671         1.0\n",
       "20        1.0     0.142049     0.857951         1.0\n",
       "21        1.0     0.013252     0.986748         1.0\n",
       "22        0.0     0.662964     0.337036         0.0\n",
       "23        0.0     0.713569     0.286431         0.0\n",
       "24        1.0     0.029039     0.970961         1.0\n",
       "25        1.0     0.054994     0.945006         1.0\n",
       "26        0.0     0.680776     0.319224         0.0\n",
       "27        0.0     0.665089     0.334911         0.0\n",
       "28        1.0     0.094858     0.905142         1.0\n",
       "29        0.0     0.433243     0.566757         1.0\n",
       "30        1.0     0.205089     0.794911         1.0\n",
       "31        0.0     0.571781     0.428219         0.0\n",
       "32        0.0     0.720927     0.279073         0.0\n",
       "33        0.0     0.369710     0.630290         1.0\n",
       "34        1.0     0.104306     0.895694         1.0\n",
       "35        1.0     0.011724     0.988276         1.0\n",
       "36        0.0     0.664342     0.335658         0.0\n",
       "37        1.0     0.100961     0.899039         1.0\n",
       "38        0.0     0.528224     0.471776         0.0\n",
       "39        0.0     0.399177     0.600823         1.0\n",
       "40        1.0     0.273403     0.726597         1.0\n",
       "41        0.0     0.717704     0.282296         0.0\n",
       "42        0.0     0.708948     0.291052         0.0\n",
       "43        0.0     0.655782     0.344218         0.0\n",
       "44        1.0     0.018439     0.981560         1.0\n",
       "45        0.0     0.662563     0.337437         0.0\n",
       "46        1.0     0.045649     0.954351         1.0\n",
       "47        1.0     0.649020     0.350980         0.0\n",
       "48        1.0     0.316514     0.683486         1.0\n",
       "49        0.0     0.463015     0.536985         1.0\n",
       "...       ...          ...          ...         ...\n",
       "22494     1.0     0.006893     0.993107         1.0\n",
       "22495     0.0     0.715421     0.284579         0.0\n",
       "22496     1.0     0.032290     0.967710         1.0\n",
       "22497     1.0     0.229748     0.770252         1.0\n",
       "22498     1.0     0.238547     0.761453         1.0\n",
       "22499     0.0     0.649930     0.350070         0.0\n",
       "22500     1.0     0.231548     0.768452         1.0\n",
       "22501     1.0     0.039416     0.960584         1.0\n",
       "22502     1.0     0.214232     0.785768         1.0\n",
       "22503     1.0     0.206248     0.793752         1.0\n",
       "22504     1.0     0.102242     0.897758         1.0\n",
       "22505     1.0     0.052388     0.947612         1.0\n",
       "22506     0.0     0.514110     0.485890         0.0\n",
       "22507     0.0     0.520999     0.479001         0.0\n",
       "22508     0.0     0.633006     0.366994         0.0\n",
       "22509     1.0     0.267469     0.732531         1.0\n",
       "22510     1.0     0.143803     0.856197         1.0\n",
       "22511     0.0     0.640742     0.359258         0.0\n",
       "22512     1.0     0.174969     0.825031         1.0\n",
       "22513     1.0     0.097353     0.902647         1.0\n",
       "22514     0.0     0.455184     0.544816         1.0\n",
       "22515     1.0     0.365732     0.634268         1.0\n",
       "22516     0.0     0.703092     0.296908         0.0\n",
       "22517     1.0     0.385163     0.614837         1.0\n",
       "22518     0.0     0.617858     0.382142         0.0\n",
       "22519     1.0     0.239171     0.760829         1.0\n",
       "22520     1.0     0.146076     0.853924         1.0\n",
       "22521     1.0     0.366466     0.633534         1.0\n",
       "22522     1.0     0.639328     0.360672         0.0\n",
       "22523     0.0     0.703958     0.296042         0.0\n",
       "22524     1.0     0.244849     0.755151         1.0\n",
       "22525     1.0     0.193197     0.806803         1.0\n",
       "22526     0.0     0.707727     0.292273         0.0\n",
       "22527     0.0     0.704080     0.295920         0.0\n",
       "22528     1.0     0.213464     0.786536         1.0\n",
       "22529     0.0     0.284326     0.715674         1.0\n",
       "22530     1.0     0.257704     0.742296         1.0\n",
       "22531     1.0     0.247290     0.752710         1.0\n",
       "22532     0.0     0.666347     0.333653         0.0\n",
       "22533     0.0     0.713121     0.286879         0.0\n",
       "22534     1.0     0.229835     0.770165         1.0\n",
       "22535     0.0     0.709884     0.290116         0.0\n",
       "22536     1.0     0.231734     0.768266         1.0\n",
       "22537     1.0     0.578593     0.421407         0.0\n",
       "22538     1.0     0.200660     0.799340         1.0\n",
       "22539     0.0     0.570416     0.429584         0.0\n",
       "22540     0.0     0.677936     0.322064         0.0\n",
       "22541     1.0     0.634504     0.365496         0.0\n",
       "22542     0.0     0.531635     0.468365         0.0\n",
       "22543     1.0     0.006877     0.993123         1.0\n",
       "\n",
       "[22544 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoch_nof_hidden\n",
    "Train.predictions[\"12_12_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:14:49.147230Z",
     "start_time": "2017-07-20T20:14:49.117994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.081985e-01</td>\n",
       "      <td>0.591802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.129235e-01</td>\n",
       "      <td>0.487077</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.105986e-03</td>\n",
       "      <td>0.993894</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.317109e-01</td>\n",
       "      <td>0.568289</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.202209e-02</td>\n",
       "      <td>0.987978</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.364901e-01</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.391150e-02</td>\n",
       "      <td>0.986089</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.760841e-01</td>\n",
       "      <td>0.623916</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.293991e-01</td>\n",
       "      <td>0.570601</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.886536e-01</td>\n",
       "      <td>0.611346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.988538e-01</td>\n",
       "      <td>0.801146</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000973e-01</td>\n",
       "      <td>0.799903</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.835148e-01</td>\n",
       "      <td>0.716485</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.729564e-01</td>\n",
       "      <td>0.527044</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.831362e-03</td>\n",
       "      <td>0.993169</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.992346e-02</td>\n",
       "      <td>0.980076</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.855242e-01</td>\n",
       "      <td>0.614476</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.429226e-02</td>\n",
       "      <td>0.965708</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.288761e-03</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.392506e-01</td>\n",
       "      <td>0.660749</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.406323e-03</td>\n",
       "      <td>0.992594</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.592387e-01</td>\n",
       "      <td>0.740761</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.979247e-01</td>\n",
       "      <td>0.502075</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.075400e-02</td>\n",
       "      <td>0.959246</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.748659e-01</td>\n",
       "      <td>0.625134</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.726591e-01</td>\n",
       "      <td>0.527341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.483587e-01</td>\n",
       "      <td>0.351641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.432151e-01</td>\n",
       "      <td>0.756785</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.346250e-01</td>\n",
       "      <td>0.465375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.798146e-02</td>\n",
       "      <td>0.982019</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.724594e-01</td>\n",
       "      <td>0.727541</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.044514e-02</td>\n",
       "      <td>0.959555</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.369030e-01</td>\n",
       "      <td>0.463097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.083138e-01</td>\n",
       "      <td>0.491686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.593124e-02</td>\n",
       "      <td>0.904069</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.859200e-01</td>\n",
       "      <td>0.614080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.160897e-01</td>\n",
       "      <td>0.883910</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.603595e-01</td>\n",
       "      <td>0.639641</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.858745e-02</td>\n",
       "      <td>0.971413</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.169941e-02</td>\n",
       "      <td>0.948301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.835469e-01</td>\n",
       "      <td>0.416453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.063395e-01</td>\n",
       "      <td>0.393661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.246368e-02</td>\n",
       "      <td>0.907536</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.537343e-01</td>\n",
       "      <td>0.446266</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.429297e-02</td>\n",
       "      <td>0.925707</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.744259e-02</td>\n",
       "      <td>0.972557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.287006e-02</td>\n",
       "      <td>0.957130</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.731596e-01</td>\n",
       "      <td>0.526840</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.697889e-02</td>\n",
       "      <td>0.953021</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.989902e-01</td>\n",
       "      <td>0.801010</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11800</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.466564e-01</td>\n",
       "      <td>0.653344</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.151391e-01</td>\n",
       "      <td>0.884861</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.331784e-01</td>\n",
       "      <td>0.566822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.086163e-02</td>\n",
       "      <td>0.959138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.953891e-02</td>\n",
       "      <td>0.960461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11805</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.969820e-01</td>\n",
       "      <td>0.803018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.321574e-01</td>\n",
       "      <td>0.467843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.146728e-02</td>\n",
       "      <td>0.958533</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.097026e-02</td>\n",
       "      <td>0.989030</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.095432e-01</td>\n",
       "      <td>0.790457</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.022149e-02</td>\n",
       "      <td>0.969778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.718697e-01</td>\n",
       "      <td>0.628130</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.395334e-01</td>\n",
       "      <td>0.560467</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11813</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.057214e-03</td>\n",
       "      <td>0.996943</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.450555e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11815</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.405504e-01</td>\n",
       "      <td>0.659450</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.065494e-02</td>\n",
       "      <td>0.959345</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.395990e-01</td>\n",
       "      <td>0.460401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.598344e-01</td>\n",
       "      <td>0.440166</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.901265e-02</td>\n",
       "      <td>0.950987</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.740000e-01</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.024514e-01</td>\n",
       "      <td>0.897549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.425482e-03</td>\n",
       "      <td>0.994574</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.095693e-02</td>\n",
       "      <td>0.989043</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.407867e-01</td>\n",
       "      <td>0.459213</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.459347e-01</td>\n",
       "      <td>0.354065</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.967331e-02</td>\n",
       "      <td>0.970327</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.283084e-02</td>\n",
       "      <td>0.987169</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.843716e-02</td>\n",
       "      <td>0.961563</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.645008e-02</td>\n",
       "      <td>0.923550</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.258283e-01</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.784254e-01</td>\n",
       "      <td>0.721575</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.595799e-02</td>\n",
       "      <td>0.954042</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.711934e-01</td>\n",
       "      <td>0.628807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.726274e-01</td>\n",
       "      <td>0.727373</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.330516e-01</td>\n",
       "      <td>0.466948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.084852e-02</td>\n",
       "      <td>0.909151</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.318088e-03</td>\n",
       "      <td>0.993682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.420576e-01</td>\n",
       "      <td>0.757942</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.776491e-03</td>\n",
       "      <td>0.996223</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.638072e-01</td>\n",
       "      <td>0.736193</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.059437e-01</td>\n",
       "      <td>0.494056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.108737e-01</td>\n",
       "      <td>0.789126</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.124814e-01</td>\n",
       "      <td>0.787519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.129376e-02</td>\n",
       "      <td>0.958706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.350591e-01</td>\n",
       "      <td>0.464941</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.433154e-01</td>\n",
       "      <td>0.356685</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.195882e-01</td>\n",
       "      <td>0.780412</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.866083e-03</td>\n",
       "      <td>0.991134</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.740001e-01</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11850 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual   Attack_prob  Normal_prob  Prediction\n",
       "0         1.0  4.081985e-01     0.591802         1.0\n",
       "1         1.0  5.129235e-01     0.487077         0.0\n",
       "2         1.0  6.105986e-03     0.993894         1.0\n",
       "3         0.0  4.317109e-01     0.568289         1.0\n",
       "4         1.0  1.202209e-02     0.987978         1.0\n",
       "5         1.0  6.364901e-01     0.363510         0.0\n",
       "6         1.0  1.391150e-02     0.986089         1.0\n",
       "7         1.0  3.760841e-01     0.623916         1.0\n",
       "8         0.0  4.293991e-01     0.570601         1.0\n",
       "9         0.0  3.886536e-01     0.611346         1.0\n",
       "10        1.0  1.988538e-01     0.801146         1.0\n",
       "11        1.0  2.000973e-01     0.799903         1.0\n",
       "12        1.0  2.835148e-01     0.716485         1.0\n",
       "13        1.0  4.729564e-01     0.527044         1.0\n",
       "14        1.0  6.831362e-03     0.993169         1.0\n",
       "15        1.0  1.992346e-02     0.980076         1.0\n",
       "16        1.0  3.855242e-01     0.614476         1.0\n",
       "17        1.0  3.429226e-02     0.965708         1.0\n",
       "18        1.0  7.288761e-03     0.992711         1.0\n",
       "19        1.0  3.392506e-01     0.660749         1.0\n",
       "20        1.0  7.406323e-03     0.992594         1.0\n",
       "21        1.0  2.592387e-01     0.740761         1.0\n",
       "22        1.0  4.979247e-01     0.502075         1.0\n",
       "23        1.0  4.075400e-02     0.959246         1.0\n",
       "24        1.0  3.748659e-01     0.625134         1.0\n",
       "25        1.0  4.726591e-01     0.527341         1.0\n",
       "26        1.0  6.483587e-01     0.351641         0.0\n",
       "27        1.0  2.432151e-01     0.756785         1.0\n",
       "28        0.0  5.346250e-01     0.465375         0.0\n",
       "29        1.0  1.798146e-02     0.982019         1.0\n",
       "30        1.0  2.724594e-01     0.727541         1.0\n",
       "31        1.0  4.044514e-02     0.959555         1.0\n",
       "32        0.0  5.369030e-01     0.463097         0.0\n",
       "33        0.0  5.083138e-01     0.491686         0.0\n",
       "34        1.0  9.593124e-02     0.904069         1.0\n",
       "35        0.0  3.859200e-01     0.614080         1.0\n",
       "36        1.0  1.160897e-01     0.883910         1.0\n",
       "37        1.0  3.603595e-01     0.639641         1.0\n",
       "38        1.0  2.858745e-02     0.971413         1.0\n",
       "39        1.0  5.169941e-02     0.948301         1.0\n",
       "40        0.0  5.835469e-01     0.416453         0.0\n",
       "41        1.0  6.063395e-01     0.393661         0.0\n",
       "42        1.0  9.246368e-02     0.907536         1.0\n",
       "43        0.0  5.537343e-01     0.446266         0.0\n",
       "44        1.0  7.429297e-02     0.925707         1.0\n",
       "45        1.0  2.744259e-02     0.972557         1.0\n",
       "46        1.0  4.287006e-02     0.957130         1.0\n",
       "47        1.0  4.731596e-01     0.526840         1.0\n",
       "48        1.0  4.697889e-02     0.953021         1.0\n",
       "49        1.0  1.989902e-01     0.801010         1.0\n",
       "...       ...           ...          ...         ...\n",
       "11800     1.0  3.466564e-01     0.653344         1.0\n",
       "11801     1.0  1.151391e-01     0.884861         1.0\n",
       "11802     0.0  4.331784e-01     0.566822         1.0\n",
       "11803     1.0  4.086163e-02     0.959138         1.0\n",
       "11804     1.0  3.953891e-02     0.960461         1.0\n",
       "11805     1.0  1.969820e-01     0.803018         1.0\n",
       "11806     0.0  5.321574e-01     0.467843         0.0\n",
       "11807     1.0  4.146728e-02     0.958533         1.0\n",
       "11808     1.0  1.097026e-02     0.989030         1.0\n",
       "11809     1.0  2.095432e-01     0.790457         1.0\n",
       "11810     1.0  3.022149e-02     0.969778         1.0\n",
       "11811     1.0  3.718697e-01     0.628130         1.0\n",
       "11812     0.0  4.395334e-01     0.560467         1.0\n",
       "11813     1.0  3.057214e-03     0.996943         1.0\n",
       "11814     1.0  1.450555e-07     1.000000         1.0\n",
       "11815     1.0  3.405504e-01     0.659450         1.0\n",
       "11816     1.0  4.065494e-02     0.959345         1.0\n",
       "11817     0.0  5.395990e-01     0.460401         0.0\n",
       "11818     0.0  5.598344e-01     0.440166         0.0\n",
       "11819     1.0  4.901265e-02     0.950987         1.0\n",
       "11820     1.0  4.740000e-01     0.526000         1.0\n",
       "11821     1.0  1.024514e-01     0.897549         1.0\n",
       "11822     1.0  5.425482e-03     0.994574         1.0\n",
       "11823     1.0  1.095693e-02     0.989043         1.0\n",
       "11824     1.0  5.407867e-01     0.459213         0.0\n",
       "11825     0.0  6.459347e-01     0.354065         0.0\n",
       "11826     1.0  2.967331e-02     0.970327         1.0\n",
       "11827     1.0  1.283084e-02     0.987169         1.0\n",
       "11828     1.0  3.843716e-02     0.961563         1.0\n",
       "11829     1.0  7.645008e-02     0.923550         1.0\n",
       "11830     1.0  1.258283e-01     0.874172         1.0\n",
       "11831     1.0  2.784254e-01     0.721575         1.0\n",
       "11832     1.0  4.595799e-02     0.954042         1.0\n",
       "11833     0.0  3.711934e-01     0.628807         1.0\n",
       "11834     1.0  2.726274e-01     0.727373         1.0\n",
       "11835     0.0  5.330516e-01     0.466948         0.0\n",
       "11836     1.0  9.084852e-02     0.909151         1.0\n",
       "11837     1.0  6.318088e-03     0.993682         1.0\n",
       "11838     1.0  2.420576e-01     0.757942         1.0\n",
       "11839     1.0  3.776491e-03     0.996223         1.0\n",
       "11840     0.0  2.638072e-01     0.736193         1.0\n",
       "11841     0.0  5.059437e-01     0.494056         0.0\n",
       "11842     1.0  2.108737e-01     0.789126         1.0\n",
       "11843     1.0  2.124814e-01     0.787519         1.0\n",
       "11844     1.0  4.129376e-02     0.958706         1.0\n",
       "11845     0.0  5.350591e-01     0.464941         0.0\n",
       "11846     0.0  6.433154e-01     0.356685         0.0\n",
       "11847     1.0  2.195882e-01     0.780412         1.0\n",
       "11848     1.0  8.866083e-03     0.991134         1.0\n",
       "11849     1.0  4.740001e-01     0.526000         1.0\n",
       "\n",
       "[11850 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"12_12_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:15:36.473208Z",
     "start_time": "2017-07-20T20:15:36.465686Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions[\"12_12_3\"].dropna()\n",
    "df_ = Train.predictions_[\"12_12_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:22:49.969820Z",
     "start_time": "2017-07-20T20:22:49.962998Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:27:17.040895Z",
     "start_time": "2017-07-20T20:27:16.997484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894606</td>\n",
       "      <td>0.910878</td>\n",
       "      <td>0.878137</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.830633</td>\n",
       "      <td>0.899755</td>\n",
       "      <td>0.872518</td>\n",
       "      <td>0.928748</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision    Recall      Scenario\n",
       "0  0.894606  0.910878   0.878137  0.946154  Train+/Test+\n",
       "1  0.830633  0.899755   0.872518  0.928748  Train+/Test-"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:41.063815Z",
     "start_time": "2017-07-19T18:59:41.057296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:41.392674Z",
     "start_time": "2017-07-19T18:59:41.065208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGhCAYAAAAJL0FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feada299ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:41.400022Z",
     "start_time": "2017-07-19T18:59:41.394211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:41.683665Z",
     "start_time": "2017-07-19T18:59:41.401342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGhCAYAAAAJL0FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEKoCRHQQggh1IT/Apd/NHGsDflzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fead7f01978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T18:59:41.708794Z",
     "start_time": "2017-07-19T18:59:41.685146Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                  (0.46458786874, 1.01076358441)\n",
       "                3                                      (nan, nan)\n",
       "12              1                (0.549081633853, 0.916331648684)\n",
       "                3                 (0.639433709421, 1.03741828315)\n",
       "24              1                (0.638652754621, 0.914818438628)\n",
       "                3                (0.444861154322, 0.999207435556)\n",
       "48              1                (0.533239339446, 0.984852258506)\n",
       "                3                 (0.585151939604, 1.06335639119)\n",
       "122             1                (0.581870630479, 0.995752331818)\n",
       "                3                (0.687123021932, 0.977564237438)\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
