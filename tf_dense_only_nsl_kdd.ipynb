{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:29.756718Z",
     "start_time": "2017-07-11T15:55:29.327375Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:29.763559Z",
     "start_time": "2017-07-11T15:55:29.758344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:29.836297Z",
     "start_time": "2017-07-11T15:55:29.765343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140122': {'y': 'dataset/Kyoto2016/2014/01/20140122_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140122_x.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20150128': {'y': 'dataset/Kyoto2016/2015/01/20150128_y.csv', 'x': 'dataset/Kyoto2016/2015/01/20150128_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/01\")\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "test_paths = test_paths.popitem()\n",
    "test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:32.337644Z",
     "start_time": "2017-07-11T15:55:29.837948Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = pd.read_csv('dataset/Kyoto2016/2014/01/20140122_x.csv')\n",
    "y_data = pd.read_csv('dataset/Kyoto2016/2014/01/20140122_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:34.561894Z",
     "start_time": "2017-07-11T15:55:32.339341Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data_test = pd.read_csv('dataset/Kyoto2016/2015/01/20150128_x.csv')\n",
    "y_data_test = pd.read_csv('dataset/Kyoto2016/2015/01/20150128_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:34.625760Z",
     "start_time": "2017-07-11T15:55:34.563534Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = x_data[y_data.is_Attack== 1].sample(500)\n",
    "x2 = x_data[y_data.is_Normal== 1]\n",
    "\n",
    "x_data_temp = pd.concat((x1, x2))\n",
    "x_data_temp = x_data_temp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:34.631616Z",
     "start_time": "2017-07-11T15:55:34.627430Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data_temp = y_data.iloc[x_data_temp.index.values,:]\n",
    "y_data_temp = y_data_temp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:34.638045Z",
     "start_time": "2017-07-11T15:55:34.633078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_Attack     500\n",
       "is_Normal    1282\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_temp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:34.652169Z",
     "start_time": "2017-07-11T15:55:34.639430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_Attack    237077\n",
       "is_Normal      7418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:35.805257Z",
     "start_time": "2017-07-11T15:55:34.653637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:35.992840Z",
     "start_time": "2017-07-11T15:55:35.806960Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "            #loss = tf.losses.mean_squared_error(labels = self.y_, predictions = self.y)\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "            \n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:36.558993Z",
     "start_time": "2017-07-11T15:55:35.994507Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    \n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_data_temp.values, \n",
    "                                                                                  y_data_temp.values, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                                   net.regularized_loss, \n",
    "                                                                   ], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                                             net.y_: y_train[i,:], \n",
    "                                                                             net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                            train_batch()\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            #print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                                print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                             .format(epochs,h,f)))\n",
    "                                train_batch()\n",
    "\n",
    "\n",
    "                        valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                              feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "                    \n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(key, train_loss, valid_accuracy))\n",
    "                    \n",
    "\n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                               net.pred, \n",
    "                                                                               net.actual, net.y], \n",
    "                                                                              feed_dict={net.x: x_test, \n",
    "                                                                                         net.y_: y_test, \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        \n",
    "                        \n",
    "                        f1_score = me.f1_score(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        print(\"Key {} Test Accuracy: {} F1 score: {}, recall {}, precision {}\".format(key, accuracy, f1_score, recall, prec))\n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = accuracy\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        if accuracy > Train.best_acc:\n",
    "                            Train.best_acc = accuracy\n",
    "\n",
    "                            if not (np.isnan(train_loss)):\n",
    "                                net.saver.save(sess, \n",
    "                                           \"dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                            global_step = epochs)\n",
    "                            curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                            Train.predictions.update({\"{}_{}_{}\".format(key,f,h):(curr_pred, \n",
    "                                                       Train.result(key, f, h, valid_accuracy, accuracy, f1_score, time.perf_counter() - start_time))})\n",
    "\n",
    "                            #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:55:36.619204Z",
     "start_time": "2017-07-11T15:55:36.560726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 8, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [2]\n",
    "        lrs = [1e-4, 1e-5]\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - hidden layers:{} features count:{}\".format(h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:38.986572Z",
     "start_time": "2017-07-11T15:55:36.620748Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20140122 | Training Loss: 0.557267 | Validation Accuracy: 0.687151\n",
      "Key 20150128 Test Accuracy: 0.22528068721294403 F1 score: 0.04409723799286411, recall 0.5889727689404152, precision 0.022906126300888667\n",
      "Key 20140122 | Training Loss: 0.451608 | Validation Accuracy: 0.865922\n",
      "Key 20150128 Test Accuracy: 0.21761590242385864 F1 score: 0.05586184090381872, recall 0.7628740900512267, precision 0.028992412482260782\n",
      "Key 20140122 | Training Loss: 0.564077 | Validation Accuracy: 0.865922\n",
      "Key 20150128 Test Accuracy: 0.21646659076213837 F1 score: 0.05624963052003074, recall 0.7696144513345915, precision 0.02919159380273048\n",
      "Key 20140122 | Training Loss: 0.540143 | Validation Accuracy: 0.843575\n",
      "Key 20150128 Test Accuracy: 0.21433566510677338 F1 score: 0.05654140656080706, recall 0.7759503909409544, precision 0.029339653898106378\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20140122 | Training Loss: 0.874269 | Validation Accuracy: 0.703911\n",
      "Key 20150128 Test Accuracy: 0.16720178723335266 F1 score: 0.04912788661358488, recall 0.7090860070099757, precision 0.02544541571326983\n",
      "Key 20140122 | Training Loss: 0.424902 | Validation Accuracy: 0.810056\n",
      "Key 20150128 Test Accuracy: 0.1598723828792572 F1 score: 0.05146131858084239, recall 0.751145861418172, precision 0.026643332233554723\n",
      "Key 20140122 | Training Loss: 0.480070 | Validation Accuracy: 0.798883\n",
      "Key 20150128 Test Accuracy: 0.16002781689167023 F1 score: 0.05138272369083527, recall 0.7497977891614991, precision 0.026602893698433577\n",
      "Key 20140122 | Training Loss: 0.413235 | Validation Accuracy: 0.821229\n",
      "Key 20150128 Test Accuracy: 0.16049817204475403 F1 score: 0.051296036089335896, recall 0.7480452952278243, precision 0.026558626169029455\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20140122 | Training Loss: 0.788504 | Validation Accuracy: 0.368715\n",
      "Key 20150128 Test Accuracy: 0.2862471640110016 F1 score: 0.0555696867034317, recall 0.6921002965758964, precision 0.028946938131135157\n",
      "Key 20140122 | Training Loss: 0.483907 | Validation Accuracy: 0.715084\n",
      "Key 20150128 Test Accuracy: 0.2332276701927185 F1 score: 0.06649537410494656, recall 0.9001078457805338, precision 0.03452287392455328\n",
      "Key 20140122 | Training Loss: 0.627743 | Validation Accuracy: 0.810056\n",
      "Key 20150128 Test Accuracy: 0.2298574596643448 F1 score: 0.06744455560288043, recall 0.9179023995686169, precision 0.035008432050016455\n",
      "Key 20140122 | Training Loss: 0.706632 | Validation Accuracy: 0.737430\n",
      "Key 20150128 Test Accuracy: 0.22850364446640015 F1 score: 0.06815431052795384, recall 0.9299002426530062, precision 0.03537345196277018\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20140122 | Training Loss: 0.647507 | Validation Accuracy: 0.670391\n",
      "Key 20150128 Test Accuracy: 0.03034008853137493 F1 score: 0.05889334810033623, recall 1.0, precision 0.030340088754371254\n",
      "Key 20140122 | Training Loss: 0.683112 | Validation Accuracy: 0.648045\n",
      "Key 20150128 Test Accuracy: 0.03034008853137493 F1 score: 0.05889334810033623, recall 1.0, precision 0.030340088754371254\n",
      "Key 20140122 | Training Loss: 0.681081 | Validation Accuracy: 0.726257\n",
      "Key 20150128 Test Accuracy: 0.03034008853137493 F1 score: 0.05889334810033623, recall 1.0, precision 0.030340088754371254\n",
      "Key 20140122 | Training Loss: 0.634188 | Validation Accuracy: 0.698324\n",
      "Key 20150128 Test Accuracy: 0.03034008853137493 F1 score: 0.05889334810033623, recall 1.0, precision 0.030340088754371254\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20140122 | Training Loss: 0.645960 | Validation Accuracy: 0.675978\n",
      "Key 20150128 Test Accuracy: 0.11156465113162994 F1 score: 0.06264887630752236, recall 0.9785656511188999, precision 0.03236030991717116\n",
      "Key 20140122 | Training Loss: 0.625993 | Validation Accuracy: 0.709497\n",
      "Key 20150128 Test Accuracy: 0.03790261596441269 F1 score: 0.059140688121465196, recall 0.9966298193583176, precision 0.030474533792807797\n",
      "Key 20140122 | Training Loss: 0.639316 | Validation Accuracy: 0.698324\n",
      "Key 20150128 Test Accuracy: 0.03737499564886093 F1 score: 0.05914027927132012, recall 0.9971690482609867, precision 0.0304738127901851\n",
      "Key 20140122 | Training Loss: 0.697805 | Validation Accuracy: 0.731844\n",
      "Key 20150128 Test Accuracy: 0.0368228405714035 F1 score: 0.05910838001326482, recall 0.9971690482609867, precision 0.030456873692705503\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20140122 | Training Loss: 0.564771 | Validation Accuracy: 0.826816\n",
      "Key 20150128 Test Accuracy: 0.6496983766555786 F1 score: 0.11388960736640628, recall 0.7419789700727959, precision 0.061678451763282044\n",
      "Key 20140122 | Training Loss: 0.528605 | Validation Accuracy: 0.793296\n",
      "Key 20150128 Test Accuracy: 0.500182032585144 F1 score: 0.09905705586151475, recall 0.905634942032893, precision 0.052393913633492176\n",
      "Key 20140122 | Training Loss: 0.785631 | Validation Accuracy: 0.743017\n",
      "Key 20150128 Test Accuracy: 0.48636987805366516 F1 score: 0.09710538803330312, recall 0.9103531949312483, precision 0.051288088221891424\n",
      "Key 20140122 | Training Loss: 0.586633 | Validation Accuracy: 0.804469\n",
      "Key 20150128 Test Accuracy: 0.48347002267837524 F1 score: 0.09665166915830359, recall 0.9107576166082502, precision 0.05103374300325571\n",
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20140122 | Training Loss: 0.476463 | Validation Accuracy: 0.770950\n",
      "Key 20150128 Test Accuracy: 0.7226855158805847 F1 score: 0.13932823884840945, recall 0.7398220544621191, precision 0.07690582959641255\n",
      "Key 20140122 | Training Loss: 0.587057 | Validation Accuracy: 0.765363\n",
      "Key 20150128 Test Accuracy: 0.6957319974899292 F1 score: 0.13898148148148148, recall 0.8093825829064438, precision 0.07601732040211694\n",
      "Key 20140122 | Training Loss: 0.729478 | Validation Accuracy: 0.770950\n",
      "Key 20150128 Test Accuracy: 0.6945418119430542 F1 score: 0.13921001371584008, recall 0.8141008358047991, precision 0.07611257451823097\n",
      "Key 20140122 | Training Loss: 0.442543 | Validation Accuracy: 0.821229\n",
      "Key 20150128 Test Accuracy: 0.6923331618309021 F1 score: 0.13872382326337604, recall 0.8166621730924778, precision 0.07579985235419977\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20140122 | Training Loss: 0.748260 | Validation Accuracy: 0.234637\n",
      "Key 20150128 Test Accuracy: 0.3069715201854706 F1 score: 0.006193621039542986, recall 0.07117821515233216, precision 0.003237674760853569\n",
      "Key 20140122 | Training Loss: 0.580241 | Validation Accuracy: 0.625698\n",
      "Key 20150128 Test Accuracy: 0.28225526213645935 F1 score: 0.015042236129430583, recall 0.18064168239417633, precision 0.007847868483780095\n",
      "Key 20140122 | Training Loss: 0.610693 | Validation Accuracy: 0.698324\n",
      "Key 20150128 Test Accuracy: 0.2821366488933563 F1 score: 0.01587924596010003, recall 0.1908870315448908, precision 0.008284189834316204\n",
      "Key 20140122 | Training Loss: 0.794643 | Validation Accuracy: 0.798883\n",
      "Key 20150128 Test Accuracy: 0.2819525897502899 F1 score: 0.016217155217340143, recall 0.195066055540577, precision 0.008460256672610869\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20140122 | Training Loss: 0.626249 | Validation Accuracy: 0.703911\n",
      "Key 20150128 Test Accuracy: 0.08486063033342361 F1 score: 0.0598589034131256, recall 0.9602318684281478, precision 0.030892334381437712\n",
      "Key 20140122 | Training Loss: 0.545565 | Validation Accuracy: 0.737430\n",
      "Key 20150128 Test Accuracy: 0.07333074510097504 F1 score: 0.060157300014933544, recall 0.9774871933135616, precision 0.03103359726086026\n",
      "Key 20140122 | Training Loss: 0.606567 | Validation Accuracy: 0.787709\n",
      "Key 20150128 Test Accuracy: 0.07311397045850754 F1 score: 0.060728392554430494, recall 0.9875977352386088, precision 0.03132737232363921\n",
      "Key 20140122 | Training Loss: 0.666092 | Validation Accuracy: 0.782123\n",
      "Key 20150128 Test Accuracy: 0.07304035127162933 F1 score: 0.060739432971532524, recall 0.9878673496899434, precision 0.031332977017637625\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20140122 | Training Loss: 0.769708 | Validation Accuracy: 0.240223\n",
      "Key 20150128 Test Accuracy: 0.6956338286399841 F1 score: 0.07610557942045539, recall 0.41318414667026154, precision 0.04191281041461547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20140122 | Training Loss: 0.592643 | Validation Accuracy: 0.413408\n",
      "Key 20150128 Test Accuracy: 0.6767459511756897 F1 score: 0.0846613545816733, recall 0.49272040981396603, precision 0.04630920102374376\n",
      "Key 20140122 | Training Loss: 0.685125 | Validation Accuracy: 0.351955\n",
      "Key 20150128 Test Accuracy: 0.6722591519355774 F1 score: 0.08422760882732769, recall 0.4967646265839849, precision 0.04601475968682492\n",
      "Key 20140122 | Training Loss: 0.684738 | Validation Accuracy: 0.396648\n",
      "Key 20150128 Test Accuracy: 0.6707417368888855 F1 score: 0.08717541671391313, recall 0.518198975465085, precision 0.04759074926954885\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20140122 | Training Loss: 0.772875 | Validation Accuracy: 0.703911\n",
      "Key 20150128 Test Accuracy: 0.0327695868909359 F1 score: 0.058920207410590994, recall 0.9979778916149905, precision 0.030356212915844852\n",
      "Key 20140122 | Training Loss: 0.446241 | Validation Accuracy: 0.726257\n",
      "Key 20150128 Test Accuracy: 0.03343217819929123 F1 score: 0.058725827972835716, recall 0.9937988676193044, precision 0.03025688803884308\n",
      "Key 20140122 | Training Loss: 0.787046 | Validation Accuracy: 0.670391\n",
      "Key 20150128 Test Accuracy: 0.033481258898973465 F1 score: 0.05871363757672805, recall 0.9935292531679698, precision 0.030250665966153733\n",
      "Key 20140122 | Training Loss: 0.711223 | Validation Accuracy: 0.687151\n",
      "Key 20150128 Test Accuracy: 0.0334935262799263 F1 score: 0.058714339204767256, recall 0.9935292531679698, precision 0.030251038468484738\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20140122 | Training Loss: 0.741702 | Validation Accuracy: 0.815642\n",
      "Key 20150128 Test Accuracy: 0.1717744767665863 F1 score: 0.05577756121216643, recall 0.806282016716096, precision 0.028887998029375823\n",
      "Key 20140122 | Training Loss: 0.531882 | Validation Accuracy: 0.664804\n",
      "Key 20150128 Test Accuracy: 0.03600073605775833 F1 score: 0.05909851215783024, recall 0.9978430843893232, precision 0.030451005640141682\n",
      "Key 20140122 | Training Loss: 0.480639 | Validation Accuracy: 0.743017\n",
      "Key 20150128 Test Accuracy: 0.03587394580245018 F1 score: 0.059143776292996775, recall 0.9987867349689943, precision 0.030474161333311395\n",
      "Key 20140122 | Training Loss: 0.556678 | Validation Accuracy: 0.709497\n",
      "Key 20150128 Test Accuracy: 0.03584940358996391 F1 score: 0.059149870285372184, recall 0.9989215421946617, precision 0.030477271605547605\n",
      "2min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.000425Z",
     "start_time": "2017-07-11T15:59:38.988068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.722686</td>\n",
       "      <td>0.139328</td>\n",
       "      <td>4.889701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.306972</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>4.985622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150128</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.084861</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>4.953707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.695634</td>\n",
       "      <td>0.076106</td>\n",
       "      <td>5.192066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150128</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.058714</td>\n",
       "      <td>20.652154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20150128</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.171774</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>5.189661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "0  20150128               1              1     0.770950    0.722686  0.139328   \n",
       "1  20150128               8              1     0.234637    0.306972  0.006194   \n",
       "2  20150128              42              1     0.703911    0.084861  0.059859   \n",
       "3  20150128               1              3     0.240223    0.695634  0.076106   \n",
       "4  20150128               8              3     0.687151    0.033494  0.058714   \n",
       "5  20150128              42              3     0.815642    0.171774  0.055778   \n",
       "\n",
       "   time_taken  \n",
       "0    4.889701  \n",
       "1    4.985622  \n",
       "2    4.953707  \n",
       "3    5.192066  \n",
       "4   20.652154  \n",
       "5    5.189661  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.026852Z",
     "start_time": "2017-07-11T15:59:39.001826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.722686</td>\n",
       "      <td>0.139328</td>\n",
       "      <td>4.889701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.306972</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>4.985622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20150128</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.171774</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>5.189661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "0  20150128               1              1     0.770950    0.722686  0.139328   \n",
       "1  20150128               8              1     0.234637    0.306972  0.006194   \n",
       "5  20150128              42              3     0.815642    0.171774  0.055778   \n",
       "\n",
       "   time_taken  \n",
       "0    4.889701  \n",
       "1    4.985622  \n",
       "5    5.189661  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.068950Z",
     "start_time": "2017-07-11T15:59:39.028221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.722686</td>\n",
       "      <td>0.139328</td>\n",
       "      <td>4.889701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.695634</td>\n",
       "      <td>0.076106</td>\n",
       "      <td>5.192066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.306972</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>4.985622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20150128</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.171774</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>5.189661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150128</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.084861</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>4.953707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150128</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.058714</td>\n",
       "      <td>20.652154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "0  20150128               1              1     0.770950    0.722686  0.139328   \n",
       "3  20150128               1              3     0.240223    0.695634  0.076106   \n",
       "1  20150128               8              1     0.234637    0.306972  0.006194   \n",
       "5  20150128              42              3     0.815642    0.171774  0.055778   \n",
       "2  20150128              42              1     0.703911    0.084861  0.059859   \n",
       "4  20150128               8              3     0.687151    0.033494  0.058714   \n",
       "\n",
       "   time_taken  \n",
       "0    4.889701  \n",
       "3    5.192066  \n",
       "1    4.985622  \n",
       "5    5.189661  \n",
       "2    4.953707  \n",
       "4   20.652154  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.168269Z",
     "start_time": "2017-07-11T15:59:39.070446Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.231147Z",
     "start_time": "2017-07-11T15:59:39.169760Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Attack', 'Normal'], normalize = False,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.824729Z",
     "start_time": "2017-07-11T15:59:39.232598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[171205  65872]\n",
      " [  1930   5488]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niMRZ360AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e6426ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.833968Z",
     "start_time": "2017-07-11T15:59:39.826251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.809641\n",
       "                3                0.764017\n",
       "8               1                0.711057\n",
       "                3                0.772862\n",
       "42              1                0.587086\n",
       "                3                0.819853\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.test_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.877785Z",
     "start_time": "2017-07-11T15:59:39.835318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792998</td>\n",
       "      <td>6.465645</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811154</td>\n",
       "      <td>78.618491</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811669</td>\n",
       "      <td>124.388594</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267464</td>\n",
       "      <td>6.469462</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.340111</td>\n",
       "      <td>10.298638</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.442166</td>\n",
       "      <td>23.231877</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485988</td>\n",
       "      <td>112.785330</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153650</td>\n",
       "      <td>6.365872</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225749</td>\n",
       "      <td>10.108851</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312944</td>\n",
       "      <td>22.972485</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314376</td>\n",
       "      <td>76.573661</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.365286</td>\n",
       "      <td>6.527218</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423455</td>\n",
       "      <td>10.482187</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626541</td>\n",
       "      <td>23.440308</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863958</td>\n",
       "      <td>6.728304</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879492</td>\n",
       "      <td>29.744492</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.904895</td>\n",
       "      <td>41.474186</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.913925</td>\n",
       "      <td>57.709625</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.935750</td>\n",
       "      <td>115.324801</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.892468</td>\n",
       "      <td>7.935754</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936377</td>\n",
       "      <td>40.383476</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939672</td>\n",
       "      <td>233.162456</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792998</td>\n",
       "      <td>6.465645</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811154</td>\n",
       "      <td>78.618491</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811669</td>\n",
       "      <td>124.388594</td>\n",
       "      <td>0.858807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267464</td>\n",
       "      <td>6.469462</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.340111</td>\n",
       "      <td>10.298638</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.442166</td>\n",
       "      <td>23.231877</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485988</td>\n",
       "      <td>112.785330</td>\n",
       "      <td>0.188872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153650</td>\n",
       "      <td>6.365872</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225749</td>\n",
       "      <td>10.108851</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312944</td>\n",
       "      <td>22.972485</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314376</td>\n",
       "      <td>76.573661</td>\n",
       "      <td>0.252216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.365286</td>\n",
       "      <td>6.527218</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423455</td>\n",
       "      <td>10.482187</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626541</td>\n",
       "      <td>23.440308</td>\n",
       "      <td>0.410230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863958</td>\n",
       "      <td>6.728304</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879492</td>\n",
       "      <td>29.744492</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.904895</td>\n",
       "      <td>41.474186</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.913925</td>\n",
       "      <td>57.709625</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.935750</td>\n",
       "      <td>115.324801</td>\n",
       "      <td>0.821872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.892468</td>\n",
       "      <td>7.935754</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936377</td>\n",
       "      <td>40.383476</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939672</td>\n",
       "      <td>233.162456</td>\n",
       "      <td>0.925334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.467957</td>\n",
       "      <td>11.481968</td>\n",
       "      <td>0.681945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.556311</td>\n",
       "      <td>18.310476</td>\n",
       "      <td>0.681945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703291</td>\n",
       "      <td>41.359956</td>\n",
       "      <td>0.681945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.704369</td>\n",
       "      <td>65.632673</td>\n",
       "      <td>0.681945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828677</td>\n",
       "      <td>125.094539</td>\n",
       "      <td>0.681945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679324</td>\n",
       "      <td>6.319008</td>\n",
       "      <td>0.675397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.100458</td>\n",
       "      <td>0.930853</td>\n",
       "      <td>22.784687</td>\n",
       "      <td>0.951199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.098178</td>\n",
       "      <td>0.995587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.963439</td>\n",
       "      <td>22.320666</td>\n",
       "      <td>0.994935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969656</td>\n",
       "      <td>16.586507</td>\n",
       "      <td>0.995659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.968175</td>\n",
       "      <td>22.104923</td>\n",
       "      <td>0.994646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.189439</td>\n",
       "      <td>0.952224</td>\n",
       "      <td>22.765218</td>\n",
       "      <td>0.991969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.744917</td>\n",
       "      <td>0.995116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.163772</td>\n",
       "      <td>0.994863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.136098</td>\n",
       "      <td>0.922632</td>\n",
       "      <td>21.844470</td>\n",
       "      <td>0.964367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.075668</td>\n",
       "      <td>0.946240</td>\n",
       "      <td>21.895465</td>\n",
       "      <td>0.992005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.969893</td>\n",
       "      <td>10.705214</td>\n",
       "      <td>0.994682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.583854</td>\n",
       "      <td>0.995116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.686297</td>\n",
       "      <td>0.995116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.249746</td>\n",
       "      <td>0.995442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.028092</td>\n",
       "      <td>0.937455</td>\n",
       "      <td>22.722771</td>\n",
       "      <td>0.978403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>11.548177</td>\n",
       "      <td>0.995116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.083153</td>\n",
       "      <td>0.965365</td>\n",
       "      <td>22.323356</td>\n",
       "      <td>0.986615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.870486</td>\n",
       "      <td>0.995370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.014318</td>\n",
       "      <td>0.995297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.317930</td>\n",
       "      <td>0.995442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>0.964997</td>\n",
       "      <td>22.293730</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968433</td>\n",
       "      <td>22.261898</td>\n",
       "      <td>0.993597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.109518</td>\n",
       "      <td>0.967476</td>\n",
       "      <td>22.346177</td>\n",
       "      <td>0.992331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.712249</td>\n",
       "      <td>0.994646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.797586</td>\n",
       "      <td>0.995370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>6.336789</td>\n",
       "      <td>0.995623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.101088</td>\n",
       "      <td>0.951336</td>\n",
       "      <td>10.228945</td>\n",
       "      <td>0.933570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>15.011552</td>\n",
       "      <td>0.922055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.060114</td>\n",
       "      <td>0.969689</td>\n",
       "      <td>15.143814</td>\n",
       "      <td>0.893711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.211880</td>\n",
       "      <td>0.883082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.129594</td>\n",
       "      <td>0.846770</td>\n",
       "      <td>20.956560</td>\n",
       "      <td>0.877768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.456428</td>\n",
       "      <td>0.876882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.123491</td>\n",
       "      <td>0.626381</td>\n",
       "      <td>20.549499</td>\n",
       "      <td>0.821012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.183602</td>\n",
       "      <td>0.848177</td>\n",
       "      <td>9.993738</td>\n",
       "      <td>0.754864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.043754</td>\n",
       "      <td>0.858230</td>\n",
       "      <td>5.142384</td>\n",
       "      <td>0.789883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>5.256545</td>\n",
       "      <td>0.498054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.907880</td>\n",
       "      <td>5.261520</td>\n",
       "      <td>0.513619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.133877</td>\n",
       "      <td>0.628913</td>\n",
       "      <td>16.104060</td>\n",
       "      <td>0.859922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.120671</td>\n",
       "      <td>0.830295</td>\n",
       "      <td>10.312157</td>\n",
       "      <td>0.875486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.074274</td>\n",
       "      <td>0.858488</td>\n",
       "      <td>5.154122</td>\n",
       "      <td>0.774319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.123551</td>\n",
       "      <td>0.675040</td>\n",
       "      <td>20.139783</td>\n",
       "      <td>0.754864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.036517</td>\n",
       "      <td>0.392229</td>\n",
       "      <td>5.238622</td>\n",
       "      <td>0.731517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.062712</td>\n",
       "      <td>0.177926</td>\n",
       "      <td>21.101317</td>\n",
       "      <td>0.645914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.157491</td>\n",
       "      <td>0.725405</td>\n",
       "      <td>22.244774</td>\n",
       "      <td>0.754864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.044097</td>\n",
       "      <td>0.225281</td>\n",
       "      <td>5.109460</td>\n",
       "      <td>0.687151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.049128</td>\n",
       "      <td>0.167202</td>\n",
       "      <td>4.969768</td>\n",
       "      <td>0.703911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.055570</td>\n",
       "      <td>0.286247</td>\n",
       "      <td>5.079539</td>\n",
       "      <td>0.368715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.058893</td>\n",
       "      <td>0.030340</td>\n",
       "      <td>5.250539</td>\n",
       "      <td>0.670391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.062649</td>\n",
       "      <td>0.111565</td>\n",
       "      <td>5.190997</td>\n",
       "      <td>0.675978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20150128</td>\n",
       "      <td>0.113890</td>\n",
       "      <td>0.649698</td>\n",
       "      <td>5.140449</td>\n",
       "      <td>0.826816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 epoch  f1_score  test_score  time_taken  \\\n",
       "no_of_features hidden_layers                                               \n",
       "1              1              20150125       NaN    0.792998    6.465645   \n",
       "               1              20150129       NaN    0.811154   78.618491   \n",
       "               1              20150128       NaN    0.811669  124.388594   \n",
       "8              1              20150125       NaN    0.267464    6.469462   \n",
       "               1              20150101       NaN    0.340111   10.298638   \n",
       "               1              20150105       NaN    0.442166   23.231877   \n",
       "               1              20150124       NaN    0.485988  112.785330   \n",
       "42             1              20150125       NaN    0.153650    6.365872   \n",
       "               1              20150101       NaN    0.225749   10.108851   \n",
       "               1              20150105       NaN    0.312944   22.972485   \n",
       "               1              20150123       NaN    0.314376   76.573661   \n",
       "1              3              20150125       NaN    0.365286    6.527218   \n",
       "               3              20150101       NaN    0.423455   10.482187   \n",
       "               3              20150105       NaN    0.626541   23.440308   \n",
       "8              3              20150125       NaN    0.863958    6.728304   \n",
       "               3              20150131       NaN    0.879492   29.744492   \n",
       "               3              20150127       NaN    0.904895   41.474186   \n",
       "               3              20150122       NaN    0.913925   57.709625   \n",
       "               3              20150124       NaN    0.935750  115.324801   \n",
       "42             3              20150125       NaN    0.892468    7.935754   \n",
       "               3              20150131       NaN    0.936377   40.383476   \n",
       "               3              20150128       NaN    0.939672  233.162456   \n",
       "1              1              20150125       NaN    0.792998    6.465645   \n",
       "               1              20150129       NaN    0.811154   78.618491   \n",
       "               1              20150128       NaN    0.811669  124.388594   \n",
       "8              1              20150125       NaN    0.267464    6.469462   \n",
       "               1              20150101       NaN    0.340111   10.298638   \n",
       "               1              20150105       NaN    0.442166   23.231877   \n",
       "               1              20150124       NaN    0.485988  112.785330   \n",
       "42             1              20150125       NaN    0.153650    6.365872   \n",
       "               1              20150101       NaN    0.225749   10.108851   \n",
       "               1              20150105       NaN    0.312944   22.972485   \n",
       "               1              20150123       NaN    0.314376   76.573661   \n",
       "1              3              20150125       NaN    0.365286    6.527218   \n",
       "               3              20150101       NaN    0.423455   10.482187   \n",
       "               3              20150105       NaN    0.626541   23.440308   \n",
       "8              3              20150125       NaN    0.863958    6.728304   \n",
       "               3              20150131       NaN    0.879492   29.744492   \n",
       "               3              20150127       NaN    0.904895   41.474186   \n",
       "               3              20150122       NaN    0.913925   57.709625   \n",
       "               3              20150124       NaN    0.935750  115.324801   \n",
       "42             3              20150125       NaN    0.892468    7.935754   \n",
       "               3              20150131       NaN    0.936377   40.383476   \n",
       "               3              20150128       NaN    0.939672  233.162456   \n",
       "1              1              20150125       NaN    0.467957   11.481968   \n",
       "               1              20150101       NaN    0.556311   18.310476   \n",
       "               1              20150105       NaN    0.703291   41.359956   \n",
       "               1              20150102       NaN    0.704369   65.632673   \n",
       "               1              20150104       NaN    0.828677  125.094539   \n",
       "8              1              20150125       NaN    0.679324    6.319008   \n",
       "...                                ...       ...         ...         ...   \n",
       "               3              20150128  0.100458    0.930853   22.784687   \n",
       "42             3              20150128  0.000000    0.969660    6.098178   \n",
       "1              1              20150128  0.000447    0.963439   22.320666   \n",
       "8              1              20150128  0.000000    0.969656   16.586507   \n",
       "42             1              20150128  0.000771    0.968175   22.104923   \n",
       "1              3              20150128  0.189439    0.952224   22.765218   \n",
       "8              3              20150128  0.000000    0.969660    5.744917   \n",
       "42             3              20150128  0.000000    0.969660    6.163772   \n",
       "1              1              20150128  0.136098    0.922632   21.844470   \n",
       "8              1              20150128  0.075668    0.946240   21.895465   \n",
       "42             1              20150128  0.021013    0.969893   10.705214   \n",
       "1              3              20150128  0.000000    0.969660    5.583854   \n",
       "8              3              20150128  0.000000    0.969660    5.686297   \n",
       "42             3              20150128  0.000000    0.969660    6.249746   \n",
       "1              1              20150128  0.028092    0.937455   22.722771   \n",
       "8              1              20150128  0.000000    0.969660   11.548177   \n",
       "42             1              20150128  0.083153    0.965365   22.323356   \n",
       "1              3              20150128  0.000000    0.969660    5.870486   \n",
       "8              3              20150128  0.000000    0.969660    6.014318   \n",
       "42             3              20150128  0.000000    0.969660    6.317930   \n",
       "1              1              20150128  0.044226    0.964997   22.293730   \n",
       "8              1              20150128  0.000000    0.968433   22.261898   \n",
       "42             1              20150128  0.109518    0.967476   22.346177   \n",
       "1              3              20150128  0.000000    0.969660    5.712249   \n",
       "8              3              20150128  0.000000    0.969660    5.797586   \n",
       "42             3              20150128  0.000000    0.969660    6.336789   \n",
       "1              1              20150128  0.101088    0.951336   10.228945   \n",
       "8              1              20150128  0.053020    0.953246   15.011552   \n",
       "42             1              20150128  0.060114    0.969689   15.143814   \n",
       "1              3              20150128  0.000000    0.969660    5.211880   \n",
       "8              3              20150128  0.129594    0.846770   20.956560   \n",
       "42             3              20150128  0.000000    0.969660    5.456428   \n",
       "1              1              20150128  0.123491    0.626381   20.549499   \n",
       "8              1              20150128  0.183602    0.848177    9.993738   \n",
       "42             1              20150128  0.043754    0.858230    5.142384   \n",
       "1              3              20150128  0.000000    0.969660    5.256545   \n",
       "8              3              20150128  0.040554    0.907880    5.261520   \n",
       "42             3              20150128  0.133877    0.628913   16.104060   \n",
       "1              1              20150128  0.120671    0.830295   10.312157   \n",
       "8              1              20150128  0.074274    0.858488    5.154122   \n",
       "42             1              20150128  0.123551    0.675040   20.139783   \n",
       "1              3              20150128  0.036517    0.392229    5.238622   \n",
       "8              3              20150128  0.062712    0.177926   21.101317   \n",
       "42             3              20150128  0.157491    0.725405   22.244774   \n",
       "1              1              20150128  0.044097    0.225281    5.109460   \n",
       "8              1              20150128  0.049128    0.167202    4.969768   \n",
       "42             1              20150128  0.055570    0.286247    5.079539   \n",
       "1              3              20150128  0.058893    0.030340    5.250539   \n",
       "8              3              20150128  0.062649    0.111565    5.190997   \n",
       "42             3              20150128  0.113890    0.649698    5.140449   \n",
       "\n",
       "                              train_score  \n",
       "no_of_features hidden_layers               \n",
       "1              1                 0.858807  \n",
       "               1                 0.858807  \n",
       "               1                 0.858807  \n",
       "8              1                 0.188872  \n",
       "               1                 0.188872  \n",
       "               1                 0.188872  \n",
       "               1                 0.188872  \n",
       "42             1                 0.252216  \n",
       "               1                 0.252216  \n",
       "               1                 0.252216  \n",
       "               1                 0.252216  \n",
       "1              3                 0.410230  \n",
       "               3                 0.410230  \n",
       "               3                 0.410230  \n",
       "8              3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "42             3                 0.925334  \n",
       "               3                 0.925334  \n",
       "               3                 0.925334  \n",
       "1              1                 0.858807  \n",
       "               1                 0.858807  \n",
       "               1                 0.858807  \n",
       "8              1                 0.188872  \n",
       "               1                 0.188872  \n",
       "               1                 0.188872  \n",
       "               1                 0.188872  \n",
       "42             1                 0.252216  \n",
       "               1                 0.252216  \n",
       "               1                 0.252216  \n",
       "               1                 0.252216  \n",
       "1              3                 0.410230  \n",
       "               3                 0.410230  \n",
       "               3                 0.410230  \n",
       "8              3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "               3                 0.821872  \n",
       "42             3                 0.925334  \n",
       "               3                 0.925334  \n",
       "               3                 0.925334  \n",
       "1              1                 0.681945  \n",
       "               1                 0.681945  \n",
       "               1                 0.681945  \n",
       "               1                 0.681945  \n",
       "               1                 0.681945  \n",
       "8              1                 0.675397  \n",
       "...                                   ...  \n",
       "               3                 0.951199  \n",
       "42             3                 0.995587  \n",
       "1              1                 0.994935  \n",
       "8              1                 0.995659  \n",
       "42             1                 0.994646  \n",
       "1              3                 0.991969  \n",
       "8              3                 0.995116  \n",
       "42             3                 0.994863  \n",
       "1              1                 0.964367  \n",
       "8              1                 0.992005  \n",
       "42             1                 0.994682  \n",
       "1              3                 0.995116  \n",
       "8              3                 0.995116  \n",
       "42             3                 0.995442  \n",
       "1              1                 0.978403  \n",
       "8              1                 0.995116  \n",
       "42             1                 0.986615  \n",
       "1              3                 0.995370  \n",
       "8              3                 0.995297  \n",
       "42             3                 0.995442  \n",
       "1              1                 0.994718  \n",
       "8              1                 0.993597  \n",
       "42             1                 0.992331  \n",
       "1              3                 0.994646  \n",
       "8              3                 0.995370  \n",
       "42             3                 0.995623  \n",
       "1              1                 0.933570  \n",
       "8              1                 0.922055  \n",
       "42             1                 0.893711  \n",
       "1              3                 0.883082  \n",
       "8              3                 0.877768  \n",
       "42             3                 0.876882  \n",
       "1              1                 0.821012  \n",
       "8              1                 0.754864  \n",
       "42             1                 0.789883  \n",
       "1              3                 0.498054  \n",
       "8              3                 0.513619  \n",
       "42             3                 0.859922  \n",
       "1              1                 0.875486  \n",
       "8              1                 0.774319  \n",
       "42             1                 0.754864  \n",
       "1              3                 0.731517  \n",
       "8              3                 0.645914  \n",
       "42             3                 0.754864  \n",
       "1              1                 0.687151  \n",
       "8              1                 0.703911  \n",
       "42             1                 0.368715  \n",
       "1              3                 0.670391  \n",
       "8              3                 0.675978  \n",
       "42             3                 0.826816  \n",
       "\n",
       "[151 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T15:59:39.887720Z",
     "start_time": "2017-07-11T15:59:39.879439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.809641\n",
       "                3                0.764017\n",
       "8               1                0.711057\n",
       "                3                0.772862\n",
       "42              1                0.587086\n",
       "                3                0.819853\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.mean().test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
