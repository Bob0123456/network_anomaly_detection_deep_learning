{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:40.289012Z",
     "start_time": "2017-07-23T22:08:39.891075Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:40.300949Z",
     "start_time": "2017-07-23T22:08:40.290535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/tf_vae_only_nsl_kdd_all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_only_nsl_kdd_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:40.391608Z",
     "start_time": "2017-07-23T22:08:40.302848Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:40.398624Z",
     "start_time": "2017-07-23T22:08:40.393216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:40.405065Z",
     "start_time": "2017-07-23T22:08:40.400427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:41.285723Z",
     "start_time": "2017-07-23T22:08:40.406740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    x_test_ = np.hstack((x_test_, np.random.normal(size = (x_test_.shape[0], y_train.shape[1]))))\n",
    "\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:42.366494Z",
     "start_time": "2017-07-23T22:08:41.287139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:42.795217Z",
     "start_time": "2017-07-23T22:08:42.368107Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            \n",
    "            loss = tf.clip_by_value(loss, -1e-4, 1e-4)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-4, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-4), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-4), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate= self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:43.162105Z",
     "start_time": "2017-07-23T22:08:42.796853Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h, f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                      net.regularized_loss, \n",
    "                                                      ], #net.summary_op\n",
    "                                                      feed_dict={net.x: x_train[i,:], \n",
    "                                                                 net.y_: y_train[i,:], \n",
    "                                                                 net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "\n",
    "                        count = 10\n",
    "                        if((train_loss > 1e9 or np.isnan(train_loss) ) and epoch > 1 and count > 1):\n",
    "                            print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "                            count -= 1\n",
    "\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        #if(train_loss > 1e9):\n",
    "\n",
    "                        #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "\n",
    "                    #print(\"\")\n",
    "                    valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "\n",
    "                    accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, test_loss_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                                           net.pred, \n",
    "                                                                                           net.actual, net.y], \n",
    "                                                                                          feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                                                     net.y_: preprocess.y_test_, \n",
    "                                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    #print(\"*************** \\n\")\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "                    #print(\"*************** \\n\")\n",
    "                    #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "\n",
    "                        #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                        #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                        #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "                        Train.best_acc = accuracy\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(epochs,h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:43.250074Z",
     "start_time": "2017-07-23T22:08:43.163686Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "\n",
    "\n",
    "    def start_training():\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [15]\n",
    "        lrs = [1e-2, 1e-2, 1e-3]\n",
    "\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_only_nsl_kdd_all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "            \n",
    "        past_scores.to_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all.pkl\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:35:25.984372Z",
     "start_time": "2017-07-23T22:08:43.251544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.418217 | Validation Accuracy: 0.467455\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: 0.178212 | Validation Accuracy: 0.464042\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: 1.480006 | Validation Accuracy: 0.466106\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: 1.251007 | Validation Accuracy: 0.465153\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: -0.643187 | Validation Accuracy: 0.467931\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: -0.237010 | Validation Accuracy: 0.461422\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: 1.048596 | Validation Accuracy: 0.462454\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: 0.812701 | Validation Accuracy: 0.466661\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: -1.140688 | Validation Accuracy: 0.471424\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: -0.338736 | Validation Accuracy: 0.461740\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -0.885049 | Validation Accuracy: 0.459994\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: 1.282180 | Validation Accuracy: 0.459756\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: 0.797158 | Validation Accuracy: 0.459517\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: -1.403800 | Validation Accuracy: 0.464359\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 15 | Training Loss: -0.443617 | Validation Accuracy: 0.466582\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 1 | Training Loss: 1.040990 | Validation Accuracy: 0.461105\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: -0.274009 | Validation Accuracy: 0.465629\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: -1.070887 | Validation Accuracy: 0.464042\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: 0.655549 | Validation Accuracy: 0.463724\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: -1.839745 | Validation Accuracy: 0.469519\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: -0.441599 | Validation Accuracy: 0.452056\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: 0.916663 | Validation Accuracy: 0.470154\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: 0.622863 | Validation Accuracy: 0.468328\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: -0.791769 | Validation Accuracy: 0.462851\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: -0.011622 | Validation Accuracy: 0.458565\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -0.212138 | Validation Accuracy: 0.460867\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: -1.022322 | Validation Accuracy: 0.461422\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: -1.171892 | Validation Accuracy: 0.466026\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: 0.370288 | Validation Accuracy: 0.470233\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 15 | Training Loss: -0.382149 | Validation Accuracy: 0.468090\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 1 | Training Loss: 2.924260 | Validation Accuracy: 0.466423\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: -0.216336 | Validation Accuracy: 0.459438\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: 0.580174 | Validation Accuracy: 0.455945\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: -1.100542 | Validation Accuracy: 0.462296\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: 0.388642 | Validation Accuracy: 0.460946\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: -0.671529 | Validation Accuracy: 0.464201\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: -0.842099 | Validation Accuracy: 0.464121\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: 1.490514 | Validation Accuracy: 0.463566\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: 1.226799 | Validation Accuracy: 0.468725\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: -1.227068 | Validation Accuracy: 0.464756\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -0.394940 | Validation Accuracy: 0.468328\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: -0.388331 | Validation Accuracy: 0.461819\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: 0.168941 | Validation Accuracy: 0.465074\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: 0.061555 | Validation Accuracy: 0.469360\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 15 | Training Loss: -0.799397 | Validation Accuracy: 0.460152\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: -0.432083 | Validation Accuracy: 0.531116\n",
      "Accuracy on Test data: 0.4310237765312195, 0.18210969865322113\n",
      "Step 2 | Training Loss: 0.449668 | Validation Accuracy: 0.532624\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18168775737285614\n",
      "Step 3 | Training Loss: 0.940231 | Validation Accuracy: 0.534291\n",
      "Accuracy on Test data: 0.4308907091617584, 0.1817721575498581\n",
      "Step 4 | Training Loss: -1.971056 | Validation Accuracy: 0.535482\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 5 | Training Loss: -1.433115 | Validation Accuracy: 0.532465\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18202531337738037\n",
      "Step 6 | Training Loss: 1.106279 | Validation Accuracy: 0.543420\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18185654282569885\n",
      "Step 7 | Training Loss: 1.837642 | Validation Accuracy: 0.535641\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18202531337738037\n",
      "Step 8 | Training Loss: -0.232630 | Validation Accuracy: 0.536672\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1819409281015396\n",
      "Step 9 | Training Loss: -1.578243 | Validation Accuracy: 0.547388\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18185654282569885\n",
      "Step 10 | Training Loss: 0.348642 | Validation Accuracy: 0.535323\n",
      "Accuracy on Test data: 0.43111249804496765, 0.18185654282569885\n",
      "Step 11 | Training Loss: -0.013454 | Validation Accuracy: 0.536276\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 12 | Training Loss: 0.071708 | Validation Accuracy: 0.535244\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18168775737285614\n",
      "Step 13 | Training Loss: -0.654956 | Validation Accuracy: 0.532783\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18227848410606384\n",
      "Step 14 | Training Loss: 0.319839 | Validation Accuracy: 0.533815\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 15 | Training Loss: 1.679183 | Validation Accuracy: 0.536196\n",
      "Accuracy on Test data: 0.4312012195587158, 0.18185654282569885\n",
      "Step 1 | Training Loss: -0.186095 | Validation Accuracy: 0.534053\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18185654282569885\n",
      "Step 2 | Training Loss: -1.513198 | Validation Accuracy: 0.528100\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 3 | Training Loss: 0.180680 | Validation Accuracy: 0.532465\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18202531337738037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: -0.561221 | Validation Accuracy: 0.532545\n",
      "Accuracy on Test data: 0.4310681223869324, 0.1817721575498581\n",
      "Step 5 | Training Loss: -0.193438 | Validation Accuracy: 0.541276\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18185654282569885\n",
      "Step 6 | Training Loss: -0.449760 | Validation Accuracy: 0.536593\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18210969865322113\n",
      "Step 7 | Training Loss: 0.380731 | Validation Accuracy: 0.540403\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18168775737285614\n",
      "Step 8 | Training Loss: 0.871456 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1817721575498581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.885348 | Validation Accuracy: 0.530322\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.045026 | Validation Accuracy: 0.536593\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18160337209701538\n",
      "Step 11 | Training Loss: -0.913828 | Validation Accuracy: 0.536831\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18151898682117462\n",
      "Step 12 | Training Loss: -0.857155 | Validation Accuracy: 0.536276\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18210969865322113\n",
      "Step 13 | Training Loss: -1.798858 | Validation Accuracy: 0.531830\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18151898682117462\n",
      "Step 14 | Training Loss: -1.745927 | Validation Accuracy: 0.530640\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18185654282569885\n",
      "Step 15 | Training Loss: -0.013035 | Validation Accuracy: 0.541832\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18168775737285614\n",
      "Step 1 | Training Loss: 0.774752 | Validation Accuracy: 0.538260\n",
      "Accuracy on Test data: 0.4310237765312195, 0.1817721575498581\n",
      "Step 2 | Training Loss: -0.683542 | Validation Accuracy: 0.528973\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18210969865322113\n",
      "Step 3 | Training Loss: -0.379713 | Validation Accuracy: 0.538895\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1817721575498581\n",
      "Step 4 | Training Loss: 0.325528 | Validation Accuracy: 0.527147\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1817721575498581\n",
      "Step 5 | Training Loss: 0.428161 | Validation Accuracy: 0.537387\n",
      "Accuracy on Test data: 0.4309350550174713, 0.18185654282569885\n",
      "Step 6 | Training Loss: -1.312698 | Validation Accuracy: 0.537546\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1819409281015396\n",
      "Step 7 | Training Loss: -0.817668 | Validation Accuracy: 0.532624\n",
      "Accuracy on Test data: 0.4309794306755066, 0.18185654282569885\n",
      "Step 8 | Training Loss: -1.258087 | Validation Accuracy: 0.536355\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 9 | Training Loss: -1.340716 | Validation Accuracy: 0.529052\n",
      "Accuracy on Test data: 0.4309350550174713, 0.1819409281015396\n",
      "Step 10 | Training Loss: 0.756468 | Validation Accuracy: 0.538736\n",
      "Accuracy on Test data: 0.4309794306755066, 0.18202531337738037\n",
      "Step 11 | Training Loss: -1.450755 | Validation Accuracy: 0.541118\n",
      "Accuracy on Test data: 0.4310237765312195, 0.18185654282569885\n",
      "Step 12 | Training Loss: -1.277534 | Validation Accuracy: 0.527862\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18143460154533386\n",
      "Step 13 | Training Loss: -2.465485 | Validation Accuracy: 0.539292\n",
      "Accuracy on Test data: 0.4309794306755066, 0.1819409281015396\n",
      "Step 14 | Training Loss: 1.172924 | Validation Accuracy: 0.541118\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18219409883022308\n",
      "Step 15 | Training Loss: -0.396893 | Validation Accuracy: 0.533259\n",
      "Accuracy on Test data: 0.4309794306755066, 0.18168775737285614\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.654530 | Validation Accuracy: 0.518654\n",
      "Accuracy on Test data: 0.5727022886276245, 0.6783122420310974\n",
      "Step 2 | Training Loss: 0.540756 | Validation Accuracy: 0.520162\n",
      "Accuracy on Test data: 0.5728353261947632, 0.6798312067985535\n",
      "Step 3 | Training Loss: 0.927570 | Validation Accuracy: 0.518654\n",
      "Accuracy on Test data: 0.5735894441604614, 0.6822784543037415\n",
      "Step 4 | Training Loss: -1.608803 | Validation Accuracy: 0.523416\n",
      "Accuracy on Test data: 0.5755411386489868, 0.6821097135543823\n",
      "Step 5 | Training Loss: -0.182881 | Validation Accuracy: 0.517622\n",
      "Accuracy on Test data: 0.5740330219268799, 0.6807594895362854\n",
      "Step 6 | Training Loss: -1.245601 | Validation Accuracy: 0.525718\n",
      "Accuracy on Test data: 0.5738999247550964, 0.6763713359832764\n",
      "Step 7 | Training Loss: 0.065866 | Validation Accuracy: 0.515955\n",
      "Accuracy on Test data: 0.5698633790016174, 0.6803375482559204\n",
      "Step 8 | Training Loss: 1.929854 | Validation Accuracy: 0.521194\n",
      "Accuracy on Test data: 0.5792672038078308, 0.6842194199562073\n",
      "Step 9 | Training Loss: 0.527028 | Validation Accuracy: 0.526671\n",
      "Accuracy on Test data: 0.5727909803390503, 0.6750211119651794\n",
      "Step 10 | Training Loss: -0.273436 | Validation Accuracy: 0.520559\n",
      "Accuracy on Test data: 0.5667139887809753, 0.6752742528915405\n",
      "Step 11 | Training Loss: 0.404422 | Validation Accuracy: 0.525163\n",
      "Accuracy on Test data: 0.5774042010307312, 0.6774683594703674\n",
      "Step 12 | Training Loss: 0.726112 | Validation Accuracy: 0.516114\n",
      "Accuracy on Test data: 0.5752306580543518, 0.6784810423851013\n",
      "Step 13 | Training Loss: -0.661201 | Validation Accuracy: 0.518971\n",
      "Accuracy on Test data: 0.5744321942329407, 0.6751898527145386\n",
      "Step 14 | Training Loss: -0.169606 | Validation Accuracy: 0.515558\n",
      "Accuracy on Test data: 0.5730571150779724, 0.6793248653411865\n",
      "Step 15 | Training Loss: 1.010002 | Validation Accuracy: 0.517542\n",
      "Accuracy on Test data: 0.573145866394043, 0.6784810423851013\n",
      "Step 1 | Training Loss: 0.447814 | Validation Accuracy: 0.517304\n",
      "Accuracy on Test data: 0.5765613913536072, 0.6827847957611084\n",
      "Step 2 | Training Loss: -0.789153 | Validation Accuracy: 0.517622\n",
      "Accuracy on Test data: 0.5764282941818237, 0.6843038201332092\n",
      "Step 3 | Training Loss: -0.445988 | Validation Accuracy: 0.520718\n",
      "Accuracy on Test data: 0.5789567232131958, 0.6834599375724792\n",
      "Step 4 | Training Loss: -0.605346 | Validation Accuracy: 0.520638\n",
      "Accuracy on Test data: 0.5713715553283691, 0.6824472546577454\n",
      "Step 5 | Training Loss: -0.607951 | Validation Accuracy: 0.518495\n",
      "Accuracy on Test data: 0.5737224817276001, 0.6790717244148254\n",
      "Step 6 | Training Loss: -0.138578 | Validation Accuracy: 0.514606\n",
      "Accuracy on Test data: 0.5717707872390747, 0.6700422167778015\n",
      "Step 7 | Training Loss: -0.632634 | Validation Accuracy: 0.517860\n",
      "Accuracy on Test data: 0.5777590274810791, 0.6784810423851013\n",
      "Step 8 | Training Loss: 0.537642 | Validation Accuracy: 0.522623\n",
      "Accuracy on Test data: 0.5707061886787415, 0.6795780658721924\n",
      "Step 9 | Training Loss: 0.676905 | Validation Accuracy: 0.527385\n",
      "Accuracy on Test data: 0.5724361538887024, 0.6749367117881775\n",
      "Step 10 | Training Loss: -1.485056 | Validation Accuracy: 0.526274\n",
      "Accuracy on Test data: 0.5742548108100891, 0.6777215003967285\n",
      "Step 11 | Training Loss: -2.254299 | Validation Accuracy: 0.521432\n",
      "Accuracy on Test data: 0.5758960247039795, 0.6803375482559204\n",
      "Step 12 | Training Loss: 0.005333 | Validation Accuracy: 0.524925\n",
      "Accuracy on Test data: 0.5766057372093201, 0.6776371598243713\n",
      "Step 13 | Training Loss: -1.395011 | Validation Accuracy: 0.519051\n",
      "Accuracy on Test data: 0.5750975608825684, 0.6800844073295593\n",
      "Step 14 | Training Loss: 0.981533 | Validation Accuracy: 0.513970\n",
      "Accuracy on Test data: 0.5724361538887024, 0.6752742528915405\n",
      "Step 15 | Training Loss: 0.198280 | Validation Accuracy: 0.519527\n",
      "Accuracy on Test data: 0.5747870802879333, 0.6756117939949036\n",
      "Step 1 | Training Loss: 1.182623 | Validation Accuracy: 0.528020\n",
      "Accuracy on Test data: 0.574299156665802, 0.6816877722740173\n",
      "Step 2 | Training Loss: 0.040263 | Validation Accuracy: 0.520718\n",
      "Accuracy on Test data: 0.5769162774085999, 0.6764556765556335\n",
      "Step 3 | Training Loss: -0.324438 | Validation Accuracy: 0.522861\n",
      "Accuracy on Test data: 0.574609637260437, 0.6824472546577454\n",
      "Step 4 | Training Loss: -1.380126 | Validation Accuracy: 0.518892\n",
      "Accuracy on Test data: 0.5729240775108337, 0.6760337352752686\n",
      "Step 5 | Training Loss: 1.600832 | Validation Accuracy: 0.524290\n",
      "Accuracy on Test data: 0.5727466344833374, 0.6721519231796265\n",
      "Step 6 | Training Loss: 0.109454 | Validation Accuracy: 0.534053\n",
      "Accuracy on Test data: 0.572303056716919, 0.6804219484329224\n",
      "Step 7 | Training Loss: 1.702545 | Validation Accuracy: 0.520241\n",
      "Accuracy on Test data: 0.5677785873413086, 0.6813502311706543\n",
      "Step 8 | Training Loss: -1.202075 | Validation Accuracy: 0.517622\n",
      "Accuracy on Test data: 0.5698633790016174, 0.6772152185440063\n",
      "Step 9 | Training Loss: -1.400505 | Validation Accuracy: 0.527782\n",
      "Accuracy on Test data: 0.5748314261436462, 0.6769620180130005\n",
      "Step 10 | Training Loss: -1.584774 | Validation Accuracy: 0.521114\n",
      "Accuracy on Test data: 0.5739442706108093, 0.6761181354522705\n",
      "Step 11 | Training Loss: 0.164386 | Validation Accuracy: 0.516749\n",
      "Accuracy on Test data: 0.571415901184082, 0.6786497831344604\n",
      "Step 12 | Training Loss: -3.274766 | Validation Accuracy: 0.525560\n",
      "Accuracy on Test data: 0.5704400539398193, 0.6813502311706543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 | Training Loss: 0.843715 | Validation Accuracy: 0.518733\n",
      "Accuracy on Test data: 0.5799769163131714, 0.6806750893592834\n",
      "Step 14 | Training Loss: -0.086184 | Validation Accuracy: 0.523258\n",
      "Accuracy on Test data: 0.5715045928955078, 0.6782278418540955\n",
      "Step 15 | Training Loss: -0.412395 | Validation Accuracy: 0.516828\n",
      "Accuracy on Test data: 0.5706618428230286, 0.6814345717430115\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.263189 | Validation Accuracy: 0.457612\n",
      "Accuracy on Test data: 0.4368346333503723, 0.3520675003528595\n",
      "Step 2 | Training Loss: -1.027995 | Validation Accuracy: 0.539530\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.852167 | Validation Accuracy: 0.538736\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: -1.470325 | Validation Accuracy: 0.540562\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: -0.572491 | Validation Accuracy: 0.533577\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.088185 | Validation Accuracy: 0.536911\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.099524 | Validation Accuracy: 0.536831\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -0.320707 | Validation Accuracy: 0.542308\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.679813 | Validation Accuracy: 0.533894\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: -0.722976 | Validation Accuracy: 0.537546\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.401748 | Validation Accuracy: 0.530322\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: -0.441778 | Validation Accuracy: 0.538101\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -1.963437 | Validation Accuracy: 0.533815\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: -0.542742 | Validation Accuracy: 0.528417\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 1.041847 | Validation Accuracy: 0.528020\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.251742 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.071912 | Validation Accuracy: 0.533418\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.956717 | Validation Accuracy: 0.532862\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: -0.813523 | Validation Accuracy: 0.537625\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: -0.829825 | Validation Accuracy: 0.534688\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -1.349162 | Validation Accuracy: 0.526195\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.239458 | Validation Accuracy: 0.532069\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -0.792829 | Validation Accuracy: 0.534609\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 1.346607 | Validation Accuracy: 0.541753\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.573069 | Validation Accuracy: 0.529052\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.778049 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.729154 | Validation Accuracy: 0.534847\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -1.106356 | Validation Accuracy: 0.541515\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.556852 | Validation Accuracy: 0.538419\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: -0.304140 | Validation Accuracy: 0.545245\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: -1.035173 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 1.554144 | Validation Accuracy: 0.538419\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: -0.889722 | Validation Accuracy: 0.533815\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.187234 | Validation Accuracy: 0.540324\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.669020 | Validation Accuracy: 0.539927\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.622150 | Validation Accuracy: 0.540244\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 1.013988 | Validation Accuracy: 0.537228\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -1.110389 | Validation Accuracy: 0.534847\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.100799 | Validation Accuracy: 0.534609\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: -0.042125 | Validation Accuracy: 0.538736\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 1.994857 | Validation Accuracy: 0.536831\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 1.186738 | Validation Accuracy: 0.537228\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -2.413786 | Validation Accuracy: 0.532386\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 1.253651 | Validation Accuracy: 0.536593\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: -0.391763 | Validation Accuracy: 0.535958\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: -0.735813 | Validation Accuracy: 0.519448\n",
      "Accuracy on Test data: 0.47932931780815125, 0.36767932772636414\n",
      "Step 2 | Training Loss: 0.119449 | Validation Accuracy: 0.517146\n",
      "Accuracy on Test data: 0.47524839639663696, 0.36438819766044617\n",
      "Step 3 | Training Loss: 1.493907 | Validation Accuracy: 0.525004\n",
      "Accuracy on Test data: 0.4686391055583954, 0.36784809827804565\n",
      "Step 4 | Training Loss: -2.374441 | Validation Accuracy: 0.521035\n",
      "Accuracy on Test data: 0.47338539361953735, 0.36447256803512573\n",
      "Step 5 | Training Loss: -1.238810 | Validation Accuracy: 0.519289\n",
      "Accuracy on Test data: 0.4754701852798462, 0.3638818562030792\n",
      "Step 6 | Training Loss: 0.555806 | Validation Accuracy: 0.518495\n",
      "Accuracy on Test data: 0.476091206073761, 0.36548522114753723\n",
      "Step 7 | Training Loss: -1.415678 | Validation Accuracy: 0.513891\n",
      "Accuracy on Test data: 0.474228173494339, 0.36784809827804565\n",
      "Step 8 | Training Loss: -1.293781 | Validation Accuracy: 0.511430\n",
      "Accuracy on Test data: 0.4738289713859558, 0.36970463395118713\n",
      "Step 9 | Training Loss: 1.010111 | Validation Accuracy: 0.510716\n",
      "Accuracy on Test data: 0.47378459572792053, 0.3671729862689972\n",
      "Step 10 | Training Loss: -1.114636 | Validation Accuracy: 0.513891\n",
      "Accuracy on Test data: 0.48026081919670105, 0.3701265752315521\n",
      "Step 11 | Training Loss: 0.114186 | Validation Accuracy: 0.510478\n",
      "Accuracy on Test data: 0.47529277205467224, 0.3685232102870941\n",
      "Step 12 | Training Loss: -0.113792 | Validation Accuracy: 0.517384\n",
      "Accuracy on Test data: 0.47844216227531433, 0.3711392283439636\n",
      "Step 13 | Training Loss: -0.973666 | Validation Accuracy: 0.523813\n",
      "Accuracy on Test data: 0.4797728955745697, 0.37164556980133057\n",
      "Step 14 | Training Loss: 0.675407 | Validation Accuracy: 0.511351\n",
      "Accuracy on Test data: 0.47307488322257996, 0.3754430413246155\n",
      "Step 15 | Training Loss: 1.058347 | Validation Accuracy: 0.523178\n",
      "Accuracy on Test data: 0.4761355519294739, 0.37164556980133057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.367316 | Validation Accuracy: 0.521194\n",
      "Accuracy on Test data: 0.47901880741119385, 0.3647257387638092\n",
      "Step 2 | Training Loss: 0.086181 | Validation Accuracy: 0.508970\n",
      "Accuracy on Test data: 0.47254258394241333, 0.36438819766044617\n",
      "Step 3 | Training Loss: 1.136523 | Validation Accuracy: 0.513335\n",
      "Accuracy on Test data: 0.4781759977340698, 0.3712236285209656\n",
      "Step 4 | Training Loss: 1.141942 | Validation Accuracy: 0.517781\n",
      "Accuracy on Test data: 0.4814141094684601, 0.37105485796928406\n",
      "Step 5 | Training Loss: -0.440260 | Validation Accuracy: 0.508414\n",
      "Accuracy on Test data: 0.47396203875541687, 0.36405062675476074\n",
      "Step 6 | Training Loss: -0.790469 | Validation Accuracy: 0.517384\n",
      "Accuracy on Test data: 0.48114797472953796, 0.3659915626049042\n",
      "Step 7 | Training Loss: 0.784671 | Validation Accuracy: 0.517146\n",
      "Accuracy on Test data: 0.47560325264930725, 0.37308016419410706\n",
      "Step 8 | Training Loss: 0.853886 | Validation Accuracy: 0.523496\n",
      "Accuracy on Test data: 0.4742725193500519, 0.36902952194213867\n",
      "Step 9 | Training Loss: -1.660832 | Validation Accuracy: 0.521829\n",
      "Accuracy on Test data: 0.47551456093788147, 0.3689451515674591\n",
      "Step 10 | Training Loss: 0.026090 | Validation Accuracy: 0.518416\n",
      "Accuracy on Test data: 0.4752040505409241, 0.36320674419403076\n",
      "Step 11 | Training Loss: -0.804045 | Validation Accuracy: 0.524925\n",
      "Accuracy on Test data: 0.4756919741630554, 0.3679324984550476\n",
      "Step 12 | Training Loss: -0.739518 | Validation Accuracy: 0.514606\n",
      "Accuracy on Test data: 0.46872782707214355, 0.36869198083877563\n",
      "Step 13 | Training Loss: -0.978530 | Validation Accuracy: 0.508176\n",
      "Accuracy on Test data: 0.4730305075645447, 0.37029534578323364\n",
      "Step 14 | Training Loss: -0.188905 | Validation Accuracy: 0.521670\n",
      "Accuracy on Test data: 0.47311922907829285, 0.3683544397354126\n",
      "Step 15 | Training Loss: 1.505791 | Validation Accuracy: 0.518971\n",
      "Accuracy on Test data: 0.4757806956768036, 0.3625316321849823\n",
      "Step 1 | Training Loss: 1.024063 | Validation Accuracy: 0.519368\n",
      "Accuracy on Test data: 0.47555890679359436, 0.3724050521850586\n",
      "Step 2 | Training Loss: 1.031202 | Validation Accuracy: 0.516828\n",
      "Accuracy on Test data: 0.47657912969589233, 0.3675105571746826\n",
      "Step 3 | Training Loss: -3.243369 | Validation Accuracy: 0.520718\n",
      "Accuracy on Test data: 0.4733410179615021, 0.3641350269317627\n",
      "Step 4 | Training Loss: 0.006475 | Validation Accuracy: 0.520559\n",
      "Accuracy on Test data: 0.4792405962944031, 0.3702109754085541\n",
      "Step 5 | Training Loss: -0.235751 | Validation Accuracy: 0.521670\n",
      "Accuracy on Test data: 0.4825230538845062, 0.36970463395118713\n",
      "Step 6 | Training Loss: -0.068964 | Validation Accuracy: 0.521511\n",
      "Accuracy on Test data: 0.4800390303134918, 0.3687763810157776\n",
      "Step 7 | Training Loss: 0.250760 | Validation Accuracy: 0.519686\n",
      "Accuracy on Test data: 0.47937366366386414, 0.3671729862689972\n",
      "Step 8 | Training Loss: 1.378008 | Validation Accuracy: 0.516431\n",
      "Accuracy on Test data: 0.4744499623775482, 0.36725738644599915\n",
      "Step 9 | Training Loss: -0.531199 | Validation Accuracy: 0.514764\n",
      "Accuracy on Test data: 0.4815915524959564, 0.36683544516563416\n",
      "Step 10 | Training Loss: 0.923578 | Validation Accuracy: 0.512621\n",
      "Accuracy on Test data: 0.4688608944416046, 0.3659071624279022\n",
      "Step 11 | Training Loss: 0.327171 | Validation Accuracy: 0.521670\n",
      "Accuracy on Test data: 0.4776880741119385, 0.3665822744369507\n",
      "Step 12 | Training Loss: -0.212827 | Validation Accuracy: 0.525560\n",
      "Accuracy on Test data: 0.47586941719055176, 0.3669198453426361\n",
      "Step 13 | Training Loss: -1.033102 | Validation Accuracy: 0.519924\n",
      "Accuracy on Test data: 0.4770227074623108, 0.3732489347457886\n",
      "Step 14 | Training Loss: 0.401689 | Validation Accuracy: 0.518416\n",
      "Accuracy on Test data: 0.4823456406593323, 0.36523208022117615\n",
      "Step 15 | Training Loss: -0.895667 | Validation Accuracy: 0.519844\n",
      "Accuracy on Test data: 0.47844216227531433, 0.3680168688297272\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.308550 | Validation Accuracy: 0.536276\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: -2.429029 | Validation Accuracy: 0.535085\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.618246 | Validation Accuracy: 0.530005\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.913477 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.107350 | Validation Accuracy: 0.532624\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.703572 | Validation Accuracy: 0.534212\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: -0.187139 | Validation Accuracy: 0.531434\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 1.174764 | Validation Accuracy: 0.534132\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.081895 | Validation Accuracy: 0.526353\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.149576 | Validation Accuracy: 0.528973\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: -0.466034 | Validation Accuracy: 0.543737\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: -0.542230 | Validation Accuracy: 0.535244\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -1.577385 | Validation Accuracy: 0.536434\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: -0.874511 | Validation Accuracy: 0.534450\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: -0.528959 | Validation Accuracy: 0.544610\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.235869 | Validation Accuracy: 0.528100\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: -0.969271 | Validation Accuracy: 0.539213\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 1.256606 | Validation Accuracy: 0.539213\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: -1.095958 | Validation Accuracy: 0.534847\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.949382 | Validation Accuracy: 0.530243\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.331188 | Validation Accuracy: 0.532307\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: -1.327836 | Validation Accuracy: 0.539609\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.613792 | Validation Accuracy: 0.527227\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.136423 | Validation Accuracy: 0.541197\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: -0.347073 | Validation Accuracy: 0.528497\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 1.821337 | Validation Accuracy: 0.539451\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.403623 | Validation Accuracy: 0.539530\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -1.141932 | Validation Accuracy: 0.530640\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: -0.622043 | Validation Accuracy: 0.531830\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.115537 | Validation Accuracy: 0.535402\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: -0.846392 | Validation Accuracy: 0.526433\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.770768 | Validation Accuracy: 0.533656\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: -0.233537 | Validation Accuracy: 0.530799\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 1.650866 | Validation Accuracy: 0.536514\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.185169 | Validation Accuracy: 0.532783\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.791056 | Validation Accuracy: 0.533815\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: -0.816615 | Validation Accuracy: 0.534688\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.154703 | Validation Accuracy: 0.536355\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.658850 | Validation Accuracy: 0.532227\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.805506 | Validation Accuracy: 0.536117\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: -0.101717 | Validation Accuracy: 0.531037\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.314344 | Validation Accuracy: 0.529132\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: -1.968944 | Validation Accuracy: 0.523972\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.612434 | Validation Accuracy: 0.541594\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: -0.512000 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: -1.151544 | Validation Accuracy: 0.528814\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18168775737285614\n",
      "Step 2 | Training Loss: -1.502429 | Validation Accuracy: 0.529846\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18168775737285614\n",
      "Step 3 | Training Loss: 0.628312 | Validation Accuracy: 0.524925\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18168775737285614\n",
      "Step 4 | Training Loss: -0.743152 | Validation Accuracy: 0.531354\n",
      "Accuracy on Test data: 0.430580198764801, 0.18185654282569885\n",
      "Step 5 | Training Loss: 0.402881 | Validation Accuracy: 0.527147\n",
      "Accuracy on Test data: 0.4303584098815918, 0.1819409281015396\n",
      "Step 6 | Training Loss: -0.955033 | Validation Accuracy: 0.539848\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 7 | Training Loss: 0.333428 | Validation Accuracy: 0.529211\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18168775737285614\n",
      "Step 8 | Training Loss: 0.421752 | Validation Accuracy: 0.523099\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 9 | Training Loss: -0.104036 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 10 | Training Loss: 0.901571 | Validation Accuracy: 0.541991\n",
      "Accuracy on Test data: 0.430580198764801, 0.18168775737285614\n",
      "Step 11 | Training Loss: 1.050447 | Validation Accuracy: 0.535641\n",
      "Accuracy on Test data: 0.4306689202785492, 0.1817721575498581\n",
      "Step 12 | Training Loss: -0.972398 | Validation Accuracy: 0.530402\n",
      "Accuracy on Test data: 0.430580198764801, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.017771 | Validation Accuracy: 0.529846\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.045850 | Validation Accuracy: 0.544451\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18168775737285614\n",
      "Step 15 | Training Loss: -0.962863 | Validation Accuracy: 0.535402\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: -0.406030 | Validation Accuracy: 0.534767\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.431222 | Validation Accuracy: 0.535244\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18160337209701538\n",
      "Step 3 | Training Loss: -0.298627 | Validation Accuracy: 0.529211\n",
      "Accuracy on Test data: 0.430580198764801, 0.18185654282569885\n",
      "Step 4 | Training Loss: -1.099392 | Validation Accuracy: 0.534767\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 5 | Training Loss: 0.090438 | Validation Accuracy: 0.532465\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 6 | Training Loss: -0.781521 | Validation Accuracy: 0.528179\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18151898682117462\n",
      "Step 7 | Training Loss: 2.242390 | Validation Accuracy: 0.537149\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.976829 | Validation Accuracy: 0.531116\n",
      "Accuracy on Test data: 0.430580198764801, 0.18151898682117462\n",
      "Step 9 | Training Loss: -0.847600 | Validation Accuracy: 0.536672\n",
      "Accuracy on Test data: 0.4306689202785492, 0.1817721575498581\n",
      "Step 10 | Training Loss: -0.266309 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.123764 | Validation Accuracy: 0.533577\n",
      "Accuracy on Test data: 0.4306245446205139, 0.1819409281015396\n",
      "Step 12 | Training Loss: -0.481975 | Validation Accuracy: 0.532227\n",
      "Accuracy on Test data: 0.43049147725105286, 0.18160337209701538\n",
      "Step 13 | Training Loss: -0.047159 | Validation Accuracy: 0.532386\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1817721575498581\n",
      "Step 14 | Training Loss: -0.435347 | Validation Accuracy: 0.529370\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18168775737285614\n",
      "Step 15 | Training Loss: 0.012729 | Validation Accuracy: 0.540959\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1817721575498581\n",
      "Step 1 | Training Loss: -1.697810 | Validation Accuracy: 0.541118\n",
      "Accuracy on Test data: 0.4307132661342621, 0.1817721575498581\n",
      "Step 2 | Training Loss: 1.148968 | Validation Accuracy: 0.534053\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18160337209701538\n",
      "Step 3 | Training Loss: -1.119336 | Validation Accuracy: 0.530878\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1819409281015396\n",
      "Step 4 | Training Loss: 0.061735 | Validation Accuracy: 0.532783\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18160337209701538\n",
      "Step 5 | Training Loss: -0.449541 | Validation Accuracy: 0.537546\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18160337209701538\n",
      "Step 6 | Training Loss: -0.198322 | Validation Accuracy: 0.528179\n",
      "Accuracy on Test data: 0.4303584098815918, 0.1813502162694931\n",
      "Step 7 | Training Loss: -0.183612 | Validation Accuracy: 0.533577\n",
      "Accuracy on Test data: 0.4307132661342621, 0.1819409281015396\n",
      "Step 8 | Training Loss: 0.382044 | Validation Accuracy: 0.529370\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18143460154533386\n",
      "Step 9 | Training Loss: -0.558358 | Validation Accuracy: 0.528100\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18151898682117462\n",
      "Step 10 | Training Loss: -0.283020 | Validation Accuracy: 0.534688\n",
      "Accuracy on Test data: 0.4307132661342621, 0.1817721575498581\n",
      "Step 11 | Training Loss: 0.318088 | Validation Accuracy: 0.530164\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18160337209701538\n",
      "Step 12 | Training Loss: -1.320241 | Validation Accuracy: 0.534926\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18151898682117462\n",
      "Step 13 | Training Loss: 0.362095 | Validation Accuracy: 0.533339\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 14 | Training Loss: -0.120695 | Validation Accuracy: 0.532465\n",
      "Accuracy on Test data: 0.4306689202785492, 0.1817721575498581\n",
      "Step 15 | Training Loss: 2.223898 | Validation Accuracy: 0.530560\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18168775737285614\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.628503 | Validation Accuracy: 0.481426\n",
      "Accuracy on Test data: 0.4863821864128113, 0.5411814451217651\n",
      "Step 2 | Training Loss: -0.071358 | Validation Accuracy: 0.476822\n",
      "Accuracy on Test data: 0.48341020941734314, 0.5375527143478394\n",
      "Step 3 | Training Loss: -0.349341 | Validation Accuracy: 0.482061\n",
      "Accuracy on Test data: 0.4837650954723358, 0.5346835255622864\n",
      "Step 4 | Training Loss: -0.354979 | Validation Accuracy: 0.474520\n",
      "Accuracy on Test data: 0.48806777596473694, 0.5382278561592102\n",
      "Step 5 | Training Loss: 1.191660 | Validation Accuracy: 0.481505\n",
      "Accuracy on Test data: 0.4884670078754425, 0.5390717387199402\n",
      "Step 6 | Training Loss: 0.425650 | Validation Accuracy: 0.483331\n",
      "Accuracy on Test data: 0.4819020628929138, 0.5336709022521973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 | Training Loss: 1.146113 | Validation Accuracy: 0.479997\n",
      "Accuracy on Test data: 0.48336586356163025, 0.5422784686088562\n",
      "Step 8 | Training Loss: 0.046716 | Validation Accuracy: 0.479441\n",
      "Accuracy on Test data: 0.4803495407104492, 0.5417721271514893\n",
      "Step 9 | Training Loss: 1.740526 | Validation Accuracy: 0.475155\n",
      "Accuracy on Test data: 0.4854063093662262, 0.5362025499343872\n",
      "Step 10 | Training Loss: -1.543785 | Validation Accuracy: 0.473567\n",
      "Accuracy on Test data: 0.4854506850242615, 0.5389873385429382\n",
      "Step 11 | Training Loss: 0.052799 | Validation Accuracy: 0.480394\n",
      "Accuracy on Test data: 0.48429736495018005, 0.5348523259162903\n",
      "Step 12 | Training Loss: 0.347129 | Validation Accuracy: 0.483728\n",
      "Accuracy on Test data: 0.4851401746273041, 0.5430379509925842\n",
      "Step 13 | Training Loss: -0.409866 | Validation Accuracy: 0.479679\n",
      "Accuracy on Test data: 0.4854063093662262, 0.5358649492263794\n",
      "Step 14 | Training Loss: -0.722835 | Validation Accuracy: 0.479997\n",
      "Accuracy on Test data: 0.4800834059715271, 0.5400000214576721\n",
      "Step 15 | Training Loss: -0.048336 | Validation Accuracy: 0.476425\n",
      "Accuracy on Test data: 0.47626861929893494, 0.5400000214576721\n",
      "Step 1 | Training Loss: -0.176869 | Validation Accuracy: 0.478012\n",
      "Accuracy on Test data: 0.48930978775024414, 0.5277637243270874\n",
      "Step 2 | Training Loss: 1.622499 | Validation Accuracy: 0.492221\n",
      "Accuracy on Test data: 0.4807487726211548, 0.5365400910377502\n",
      "Step 3 | Training Loss: -0.326561 | Validation Accuracy: 0.480949\n",
      "Accuracy on Test data: 0.4850514531135559, 0.5398312211036682\n",
      "Step 4 | Training Loss: -0.570155 | Validation Accuracy: 0.478965\n",
      "Accuracy on Test data: 0.48775726556777954, 0.5350211262702942\n",
      "Step 5 | Training Loss: -0.773242 | Validation Accuracy: 0.477377\n",
      "Accuracy on Test data: 0.4902856647968292, 0.5337553024291992\n",
      "Step 6 | Training Loss: -0.339049 | Validation Accuracy: 0.474361\n",
      "Accuracy on Test data: 0.4844748079776764, 0.5375527143478394\n",
      "Step 7 | Training Loss: -0.456868 | Validation Accuracy: 0.481584\n",
      "Accuracy on Test data: 0.4801720976829529, 0.547426164150238\n",
      "Step 8 | Training Loss: 0.272654 | Validation Accuracy: 0.473012\n",
      "Accuracy on Test data: 0.4806600511074066, 0.5389029383659363\n",
      "Step 9 | Training Loss: 0.406743 | Validation Accuracy: 0.484283\n",
      "Accuracy on Test data: 0.4853619635105133, 0.5401687622070312\n",
      "Step 10 | Training Loss: 0.805919 | Validation Accuracy: 0.479362\n",
      "Accuracy on Test data: 0.47999468445777893, 0.5362868905067444\n",
      "Step 11 | Training Loss: 0.448870 | Validation Accuracy: 0.475393\n",
      "Accuracy on Test data: 0.4847409427165985, 0.5431223511695862\n",
      "Step 12 | Training Loss: 0.262525 | Validation Accuracy: 0.476028\n",
      "Accuracy on Test data: 0.4846965968608856, 0.5329957604408264\n",
      "Step 13 | Training Loss: -0.237260 | Validation Accuracy: 0.479838\n",
      "Accuracy on Test data: 0.485007107257843, 0.5408439040184021\n",
      "Step 14 | Training Loss: 0.112012 | Validation Accuracy: 0.477854\n",
      "Accuracy on Test data: 0.48341020941734314, 0.5435442924499512\n",
      "Step 15 | Training Loss: -0.357382 | Validation Accuracy: 0.477695\n",
      "Accuracy on Test data: 0.48243436217308044, 0.5432911515235901\n",
      "Step 1 | Training Loss: -0.302044 | Validation Accuracy: 0.482458\n",
      "Accuracy on Test data: 0.48057132959365845, 0.5358649492263794\n",
      "Step 2 | Training Loss: -0.741541 | Validation Accuracy: 0.472138\n",
      "Accuracy on Test data: 0.47999468445777893, 0.5416033864021301\n",
      "Step 3 | Training Loss: -0.556823 | Validation Accuracy: 0.481743\n",
      "Accuracy on Test data: 0.48270049691200256, 0.5392404794692993\n",
      "Step 4 | Training Loss: -1.918236 | Validation Accuracy: 0.478251\n",
      "Accuracy on Test data: 0.4844748079776764, 0.5427004098892212\n",
      "Step 5 | Training Loss: 1.154723 | Validation Accuracy: 0.483966\n",
      "Accuracy on Test data: 0.483144074678421, 0.5329114198684692\n",
      "Step 6 | Training Loss: 0.092892 | Validation Accuracy: 0.475155\n",
      "Accuracy on Test data: 0.4761355519294739, 0.5455695986747742\n",
      "Step 7 | Training Loss: -0.302312 | Validation Accuracy: 0.474679\n",
      "Accuracy on Test data: 0.4814141094684601, 0.5339240431785583\n",
      "Step 8 | Training Loss: 0.527508 | Validation Accuracy: 0.482537\n",
      "Accuracy on Test data: 0.47928494215011597, 0.5400000214576721\n",
      "Step 9 | Training Loss: -1.249346 | Validation Accuracy: 0.477298\n",
      "Accuracy on Test data: 0.4897977411746979, 0.5381434559822083\n",
      "Step 10 | Training Loss: -0.147448 | Validation Accuracy: 0.473170\n",
      "Accuracy on Test data: 0.4784865081310272, 0.5383965969085693\n",
      "Step 11 | Training Loss: 0.148262 | Validation Accuracy: 0.481426\n",
      "Accuracy on Test data: 0.4862934648990631, 0.5431223511695862\n",
      "Step 12 | Training Loss: 0.390944 | Validation Accuracy: 0.479441\n",
      "Accuracy on Test data: 0.48589426279067993, 0.5387341976165771\n",
      "Step 13 | Training Loss: 0.438049 | Validation Accuracy: 0.479521\n",
      "Accuracy on Test data: 0.48642653226852417, 0.54835444688797\n",
      "Step 14 | Training Loss: -0.950814 | Validation Accuracy: 0.487855\n",
      "Accuracy on Test data: 0.4872249960899353, 0.5357806086540222\n",
      "Step 15 | Training Loss: 0.474390 | Validation Accuracy: 0.484521\n",
      "Accuracy on Test data: 0.4826561510562897, 0.5402531623840332\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.355827 | Validation Accuracy: 0.464994\n",
      "Accuracy on Test data: 0.4393186569213867, 0.3983122408390045\n",
      "Step 2 | Training Loss: -0.612768 | Validation Accuracy: 0.461343\n",
      "Accuracy on Test data: 0.4431777894496918, 0.4031223654747009\n",
      "Step 3 | Training Loss: 1.287429 | Validation Accuracy: 0.468805\n",
      "Accuracy on Test data: 0.44504082202911377, 0.3979746699333191\n",
      "Step 4 | Training Loss: -1.885202 | Validation Accuracy: 0.469281\n",
      "Accuracy on Test data: 0.4456618130207062, 0.39527425169944763\n",
      "Step 5 | Training Loss: -0.929834 | Validation Accuracy: 0.458724\n",
      "Accuracy on Test data: 0.452492892742157, 0.39358648657798767\n",
      "Step 6 | Training Loss: -0.758178 | Validation Accuracy: 0.466264\n",
      "Accuracy on Test data: 0.4418470561504364, 0.3962869346141815\n",
      "Step 7 | Training Loss: 1.866337 | Validation Accuracy: 0.460391\n",
      "Accuracy on Test data: 0.4492547810077667, 0.3967088460922241\n",
      "Step 8 | Training Loss: -0.227326 | Validation Accuracy: 0.461502\n",
      "Accuracy on Test data: 0.4483232796192169, 0.40227848291397095\n",
      "Step 9 | Training Loss: 1.543357 | Validation Accuracy: 0.457215\n",
      "Accuracy on Test data: 0.45169445872306824, 0.3945147693157196\n",
      "Step 10 | Training Loss: 0.263776 | Validation Accuracy: 0.468011\n",
      "Accuracy on Test data: 0.44606104493141174, 0.3961181342601776\n",
      "Step 11 | Training Loss: 0.522586 | Validation Accuracy: 0.462931\n",
      "Accuracy on Test data: 0.4495652914047241, 0.39485231041908264\n",
      "Step 12 | Training Loss: -0.508999 | Validation Accuracy: 0.460787\n",
      "Accuracy on Test data: 0.44952094554901123, 0.3947679400444031\n",
      "Step 13 | Training Loss: 1.438060 | Validation Accuracy: 0.460787\n",
      "Accuracy on Test data: 0.4446415901184082, 0.39443036913871765\n",
      "Step 14 | Training Loss: 0.416675 | Validation Accuracy: 0.464359\n",
      "Accuracy on Test data: 0.44508516788482666, 0.40565401315689087\n",
      "Step 15 | Training Loss: 0.514535 | Validation Accuracy: 0.457136\n",
      "Accuracy on Test data: 0.4477466344833374, 0.39586499333381653\n",
      "Step 1 | Training Loss: -0.095714 | Validation Accuracy: 0.461978\n",
      "Accuracy on Test data: 0.4492547810077667, 0.39527425169944763\n",
      "Step 2 | Training Loss: 1.253232 | Validation Accuracy: 0.459994\n",
      "Accuracy on Test data: 0.4499645233154297, 0.4071730077266693\n",
      "Step 3 | Training Loss: 0.691871 | Validation Accuracy: 0.467376\n",
      "Accuracy on Test data: 0.4489443004131317, 0.3963713049888611\n",
      "Step 4 | Training Loss: 0.863186 | Validation Accuracy: 0.458168\n",
      "Accuracy on Test data: 0.4434439241886139, 0.38894516229629517\n",
      "Step 5 | Training Loss: 0.484380 | Validation Accuracy: 0.468328\n",
      "Accuracy on Test data: 0.45262598991394043, 0.397299587726593\n",
      "Step 6 | Training Loss: 0.871962 | Validation Accuracy: 0.458009\n",
      "Accuracy on Test data: 0.4421575665473938, 0.39940929412841797\n",
      "Step 7 | Training Loss: 1.248527 | Validation Accuracy: 0.460708\n",
      "Accuracy on Test data: 0.44353264570236206, 0.40185654163360596\n",
      "Step 8 | Training Loss: -0.325262 | Validation Accuracy: 0.453961\n",
      "Accuracy on Test data: 0.44455286860466003, 0.39620253443717957\n",
      "Step 9 | Training Loss: -2.771638 | Validation Accuracy: 0.461661\n",
      "Accuracy on Test data: 0.44632717967033386, 0.3983966112136841\n",
      "Step 10 | Training Loss: 0.689197 | Validation Accuracy: 0.461740\n",
      "Accuracy on Test data: 0.4406493902206421, 0.39299577474594116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 | Training Loss: 0.978175 | Validation Accuracy: 0.471662\n",
      "Accuracy on Test data: 0.44419801235198975, 0.39240506291389465\n",
      "Step 12 | Training Loss: -0.989394 | Validation Accuracy: 0.456739\n",
      "Accuracy on Test data: 0.4415809214115143, 0.3983966112136841\n",
      "Step 13 | Training Loss: 1.046186 | Validation Accuracy: 0.470472\n",
      "Accuracy on Test data: 0.4449521005153656, 0.3959493637084961\n",
      "Step 14 | Training Loss: 0.933261 | Validation Accuracy: 0.461740\n",
      "Accuracy on Test data: 0.44663769006729126, 0.3933333456516266\n",
      "Step 15 | Training Loss: 0.749106 | Validation Accuracy: 0.460787\n",
      "Accuracy on Test data: 0.4426898658275604, 0.40244725346565247\n",
      "Step 1 | Training Loss: 1.763738 | Validation Accuracy: 0.459676\n",
      "Accuracy on Test data: 0.4430447220802307, 0.39983123540878296\n",
      "Step 2 | Training Loss: -0.167393 | Validation Accuracy: 0.466900\n",
      "Accuracy on Test data: 0.4486337900161743, 0.401012659072876\n",
      "Step 3 | Training Loss: 1.090603 | Validation Accuracy: 0.463963\n",
      "Accuracy on Test data: 0.43710076808929443, 0.40565401315689087\n",
      "Step 4 | Training Loss: -1.861504 | Validation Accuracy: 0.459835\n",
      "Accuracy on Test data: 0.44916608929634094, 0.4061603248119354\n",
      "Step 5 | Training Loss: -1.008079 | Validation Accuracy: 0.461026\n",
      "Accuracy on Test data: 0.4458392560482025, 0.39864978194236755\n",
      "Step 6 | Training Loss: 1.175566 | Validation Accuracy: 0.468090\n",
      "Accuracy on Test data: 0.44078245759010315, 0.39502111077308655\n",
      "Step 7 | Training Loss: 0.259656 | Validation Accuracy: 0.462216\n",
      "Accuracy on Test data: 0.4461941123008728, 0.3956961929798126\n",
      "Step 8 | Training Loss: -1.959803 | Validation Accuracy: 0.462057\n",
      "Accuracy on Test data: 0.4418914020061493, 0.3856540024280548\n",
      "Step 9 | Training Loss: 0.221468 | Validation Accuracy: 0.464280\n",
      "Accuracy on Test data: 0.4481458365917206, 0.3959493637084961\n",
      "Step 10 | Training Loss: -0.863473 | Validation Accuracy: 0.456819\n",
      "Accuracy on Test data: 0.4412260353565216, 0.39949366450309753\n",
      "Step 11 | Training Loss: -1.559928 | Validation Accuracy: 0.463566\n",
      "Accuracy on Test data: 0.44796842336654663, 0.3860759437084198\n",
      "Step 12 | Training Loss: 0.440366 | Validation Accuracy: 0.460708\n",
      "Accuracy on Test data: 0.4468151032924652, 0.3947679400444031\n",
      "Step 13 | Training Loss: 1.369730 | Validation Accuracy: 0.457454\n",
      "Accuracy on Test data: 0.4462384581565857, 0.3936708867549896\n",
      "Step 14 | Training Loss: -0.180935 | Validation Accuracy: 0.465947\n",
      "Accuracy on Test data: 0.4496983587741852, 0.3971307873725891\n",
      "Step 15 | Training Loss: 0.193773 | Validation Accuracy: 0.454278\n",
      "Accuracy on Test data: 0.4424237012863159, 0.40109705924987793\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: -0.829039 | Validation Accuracy: 0.493650\n",
      "Accuracy on Test data: 0.5525638461112976, 0.6772995591163635\n",
      "Step 2 | Training Loss: 0.078848 | Validation Accuracy: 0.486903\n",
      "Accuracy on Test data: 0.5446681976318359, 0.6767932772636414\n",
      "Step 3 | Training Loss: 2.367665 | Validation Accuracy: 0.495396\n",
      "Accuracy on Test data: 0.5476401448249817, 0.6800000071525574\n",
      "Step 4 | Training Loss: -0.755721 | Validation Accuracy: 0.496110\n",
      "Accuracy on Test data: 0.548660397529602, 0.6754430532455444\n",
      "Step 5 | Training Loss: 0.242883 | Validation Accuracy: 0.488649\n",
      "Accuracy on Test data: 0.5506121516227722, 0.6806750893592834\n",
      "Step 6 | Training Loss: -0.349003 | Validation Accuracy: 0.498968\n",
      "Accuracy on Test data: 0.5520759224891663, 0.6783966422080994\n",
      "Step 7 | Training Loss: 0.429802 | Validation Accuracy: 0.497063\n",
      "Accuracy on Test data: 0.5547817349433899, 0.6855696439743042\n",
      "Step 8 | Training Loss: 0.915955 | Validation Accuracy: 0.489046\n",
      "Accuracy on Test data: 0.5489708781242371, 0.6798312067985535\n",
      "Step 9 | Training Loss: 0.757289 | Validation Accuracy: 0.502302\n",
      "Accuracy on Test data: 0.5479063391685486, 0.6732489466667175\n",
      "Step 10 | Training Loss: -0.722592 | Validation Accuracy: 0.496746\n",
      "Accuracy on Test data: 0.5491039752960205, 0.6831223368644714\n",
      "Step 11 | Training Loss: 0.897001 | Validation Accuracy: 0.491586\n",
      "Accuracy on Test data: 0.5486160516738892, 0.6681856513023376\n",
      "Step 12 | Training Loss: -0.967470 | Validation Accuracy: 0.490554\n",
      "Accuracy on Test data: 0.5452892184257507, 0.6766244769096375\n",
      "Step 13 | Training Loss: 0.050463 | Validation Accuracy: 0.483489\n",
      "Accuracy on Test data: 0.5496806502342224, 0.6833755373954773\n",
      "Step 14 | Training Loss: 0.193850 | Validation Accuracy: 0.500238\n",
      "Accuracy on Test data: 0.5504347085952759, 0.6693670749664307\n",
      "Step 15 | Training Loss: 0.529341 | Validation Accuracy: 0.491030\n",
      "Accuracy on Test data: 0.5460876226425171, 0.6703797578811646\n",
      "Step 1 | Training Loss: -0.639364 | Validation Accuracy: 0.499365\n",
      "Accuracy on Test data: 0.5499911308288574, 0.6841350197792053\n",
      "Step 2 | Training Loss: 0.207946 | Validation Accuracy: 0.488252\n",
      "Accuracy on Test data: 0.5547817349433899, 0.6765400767326355\n",
      "Step 3 | Training Loss: -0.605851 | Validation Accuracy: 0.488808\n",
      "Accuracy on Test data: 0.5490596294403076, 0.6813502311706543\n",
      "Step 4 | Training Loss: 0.335514 | Validation Accuracy: 0.494205\n",
      "Accuracy on Test data: 0.5516767501831055, 0.6758649945259094\n",
      "Step 5 | Training Loss: 1.805044 | Validation Accuracy: 0.493967\n",
      "Accuracy on Test data: 0.5467973947525024, 0.6783122420310974\n",
      "Step 6 | Training Loss: 0.286914 | Validation Accuracy: 0.489681\n",
      "Accuracy on Test data: 0.5431600213050842, 0.6723206639289856\n",
      "Step 7 | Training Loss: -0.176515 | Validation Accuracy: 0.494523\n",
      "Accuracy on Test data: 0.5502572655677795, 0.6756961941719055\n",
      "Step 8 | Training Loss: 0.159133 | Validation Accuracy: 0.491110\n",
      "Accuracy on Test data: 0.5474627614021301, 0.6799156069755554\n",
      "Step 9 | Training Loss: -0.239493 | Validation Accuracy: 0.489125\n",
      "Accuracy on Test data: 0.5484386086463928, 0.6766244769096375\n",
      "Step 10 | Training Loss: -0.528334 | Validation Accuracy: 0.493809\n",
      "Accuracy on Test data: 0.5472409725189209, 0.6671729683876038\n",
      "Step 11 | Training Loss: 0.446515 | Validation Accuracy: 0.485553\n",
      "Accuracy on Test data: 0.5454222559928894, 0.6785653829574585\n",
      "Step 12 | Training Loss: -3.182454 | Validation Accuracy: 0.492856\n",
      "Accuracy on Test data: 0.5486160516738892, 0.6783966422080994\n",
      "Step 13 | Training Loss: 1.434484 | Validation Accuracy: 0.486030\n",
      "Accuracy on Test data: 0.5517654418945312, 0.6677637100219727\n",
      "Step 14 | Training Loss: 0.058186 | Validation Accuracy: 0.494444\n",
      "Accuracy on Test data: 0.5496362447738647, 0.6774683594703674\n",
      "Step 15 | Training Loss: 0.565758 | Validation Accuracy: 0.497698\n",
      "Accuracy on Test data: 0.5519428849220276, 0.6790717244148254\n",
      "Step 1 | Training Loss: 0.392350 | Validation Accuracy: 0.499286\n",
      "Accuracy on Test data: 0.5429826378822327, 0.6691983342170715\n",
      "Step 2 | Training Loss: 0.160170 | Validation Accuracy: 0.495475\n",
      "Accuracy on Test data: 0.5499467849731445, 0.6796624660491943\n",
      "Step 3 | Training Loss: 0.030987 | Validation Accuracy: 0.495317\n",
      "Accuracy on Test data: 0.5525195002555847, 0.6769620180130005\n",
      "Step 4 | Training Loss: -1.288673 | Validation Accuracy: 0.490633\n",
      "Accuracy on Test data: 0.5479950308799744, 0.6789873242378235\n",
      "Step 5 | Training Loss: -0.235033 | Validation Accuracy: 0.493729\n",
      "Accuracy on Test data: 0.5548704862594604, 0.6694514751434326\n",
      "Step 6 | Training Loss: -1.358910 | Validation Accuracy: 0.492459\n",
      "Accuracy on Test data: 0.5467529892921448, 0.6786497831344604\n",
      "Step 7 | Training Loss: 0.432335 | Validation Accuracy: 0.490078\n",
      "Accuracy on Test data: 0.5499911308288574, 0.6816033720970154\n",
      "Step 8 | Training Loss: -1.253712 | Validation Accuracy: 0.501270\n",
      "Accuracy on Test data: 0.5542051196098328, 0.6744303703308105\n",
      "Step 9 | Training Loss: -0.852819 | Validation Accuracy: 0.494602\n",
      "Accuracy on Test data: 0.5513662099838257, 0.6799156069755554\n",
      "Step 10 | Training Loss: 0.891970 | Validation Accuracy: 0.496666\n",
      "Accuracy on Test data: 0.5480837225914001, 0.6783966422080994\n",
      "Step 11 | Training Loss: -0.040943 | Validation Accuracy: 0.489840\n",
      "Accuracy on Test data: 0.5461763739585876, 0.6790717244148254\n",
      "Step 12 | Training Loss: 0.193401 | Validation Accuracy: 0.487935\n",
      "Accuracy on Test data: 0.5513662099838257, 0.6818565130233765\n",
      "Step 13 | Training Loss: -0.577459 | Validation Accuracy: 0.493650\n",
      "Accuracy on Test data: 0.5548704862594604, 0.6719831228256226\n",
      "Step 14 | Training Loss: 2.068877 | Validation Accuracy: 0.488728\n",
      "Accuracy on Test data: 0.5483942627906799, 0.6716455817222595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 | Training Loss: -1.183802 | Validation Accuracy: 0.489840\n",
      "Accuracy on Test data: 0.5508782863616943, 0.6751898527145386\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 10\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:35:26.104885Z",
     "start_time": "2017-07-23T22:35:25.985921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:35:26.203375Z",
     "start_time": "2017-07-23T22:35:26.107314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:41:50.516177Z",
     "start_time": "2017-07-23T22:41:50.512443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:41:51.873464Z",
     "start_time": "2017-07-23T22:41:51.851296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467455</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>1.855416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518733</td>\n",
       "      <td>0.579977</td>\n",
       "      <td>0.677805</td>\n",
       "      <td>0.680675</td>\n",
       "      <td>0.792156</td>\n",
       "      <td>120.183660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521194</td>\n",
       "      <td>0.579267</td>\n",
       "      <td>0.677282</td>\n",
       "      <td>0.684219</td>\n",
       "      <td>0.794959</td>\n",
       "      <td>21.663580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523416</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.673446</td>\n",
       "      <td>0.682110</td>\n",
       "      <td>0.793351</td>\n",
       "      <td>11.017996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520162</td>\n",
       "      <td>0.572835</td>\n",
       "      <td>0.671690</td>\n",
       "      <td>0.679831</td>\n",
       "      <td>0.791790</td>\n",
       "      <td>5.531324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518654</td>\n",
       "      <td>0.572702</td>\n",
       "      <td>0.670475</td>\n",
       "      <td>0.678312</td>\n",
       "      <td>0.790388</td>\n",
       "      <td>2.791521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518654</td>\n",
       "      <td>0.573589</td>\n",
       "      <td>0.670031</td>\n",
       "      <td>0.682278</td>\n",
       "      <td>0.793098</td>\n",
       "      <td>8.297130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>0.554870</td>\n",
       "      <td>0.666711</td>\n",
       "      <td>0.669451</td>\n",
       "      <td>0.791527</td>\n",
       "      <td>158.816847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.497063</td>\n",
       "      <td>0.554782</td>\n",
       "      <td>0.666556</td>\n",
       "      <td>0.685570</td>\n",
       "      <td>0.802460</td>\n",
       "      <td>31.471321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493650</td>\n",
       "      <td>0.552564</td>\n",
       "      <td>0.663688</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.797586</td>\n",
       "      <td>4.560201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.477377</td>\n",
       "      <td>0.490286</td>\n",
       "      <td>0.560960</td>\n",
       "      <td>0.533755</td>\n",
       "      <td>0.663048</td>\n",
       "      <td>73.788872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.478012</td>\n",
       "      <td>0.489310</td>\n",
       "      <td>0.560556</td>\n",
       "      <td>0.527764</td>\n",
       "      <td>0.658988</td>\n",
       "      <td>59.012607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474520</td>\n",
       "      <td>0.488068</td>\n",
       "      <td>0.559823</td>\n",
       "      <td>0.538228</td>\n",
       "      <td>0.667638</td>\n",
       "      <td>14.792543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481426</td>\n",
       "      <td>0.486382</td>\n",
       "      <td>0.559114</td>\n",
       "      <td>0.541181</td>\n",
       "      <td>0.668536</td>\n",
       "      <td>3.736107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481505</td>\n",
       "      <td>0.488467</td>\n",
       "      <td>0.558837</td>\n",
       "      <td>0.539072</td>\n",
       "      <td>0.667681</td>\n",
       "      <td>18.492774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.458724</td>\n",
       "      <td>0.452493</td>\n",
       "      <td>0.442124</td>\n",
       "      <td>0.393586</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>19.537112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.468328</td>\n",
       "      <td>0.452626</td>\n",
       "      <td>0.437352</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.493978</td>\n",
       "      <td>77.722343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.468805</td>\n",
       "      <td>0.445041</td>\n",
       "      <td>0.432428</td>\n",
       "      <td>0.397975</td>\n",
       "      <td>0.492603</td>\n",
       "      <td>11.768580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469281</td>\n",
       "      <td>0.445662</td>\n",
       "      <td>0.430998</td>\n",
       "      <td>0.395274</td>\n",
       "      <td>0.489819</td>\n",
       "      <td>15.645103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461343</td>\n",
       "      <td>0.443178</td>\n",
       "      <td>0.428031</td>\n",
       "      <td>0.403122</td>\n",
       "      <td>0.499469</td>\n",
       "      <td>7.866155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.464994</td>\n",
       "      <td>0.439319</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.398312</td>\n",
       "      <td>0.493176</td>\n",
       "      <td>3.969309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521670</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>0.369705</td>\n",
       "      <td>0.427092</td>\n",
       "      <td>136.635171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513891</td>\n",
       "      <td>0.480261</td>\n",
       "      <td>0.376656</td>\n",
       "      <td>0.370127</td>\n",
       "      <td>0.427695</td>\n",
       "      <td>38.382321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517781</td>\n",
       "      <td>0.481414</td>\n",
       "      <td>0.375714</td>\n",
       "      <td>0.371055</td>\n",
       "      <td>0.427353</td>\n",
       "      <td>74.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519448</td>\n",
       "      <td>0.479329</td>\n",
       "      <td>0.372903</td>\n",
       "      <td>0.367679</td>\n",
       "      <td>0.424191</td>\n",
       "      <td>4.032818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.457612</td>\n",
       "      <td>0.436835</td>\n",
       "      <td>0.356969</td>\n",
       "      <td>0.352068</td>\n",
       "      <td>0.411828</td>\n",
       "      <td>3.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.536196</td>\n",
       "      <td>0.431201</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.181857</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>39.525672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.535323</td>\n",
       "      <td>0.431112</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.181857</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>26.152301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.531116</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.182110</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>2.690389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.529846</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>7.393695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.536276</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.865133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.528814</td>\n",
       "      <td>0.430625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>3.720555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "0       2               1              1     0.467455    0.569242  0.725500   \n",
       "9      42              24              1     0.518733    0.579977  0.677805   \n",
       "8       9              24              1     0.521194    0.579267  0.677282   \n",
       "7       5              24              1     0.523416    0.575541  0.673446   \n",
       "5       3              24              1     0.520162    0.572835  0.671690   \n",
       "4       2              24              1     0.518654    0.572702  0.670475   \n",
       "6       4              24              1     0.518654    0.573589  0.670031   \n",
       "31     18             122              3     0.493729    0.554870  0.666711   \n",
       "30      8             122              3     0.497063    0.554782  0.666556   \n",
       "29      2             122              3     0.493650    0.552564  0.663688   \n",
       "22     12              24              3     0.477377    0.490286  0.560960   \n",
       "21      4              24              3     0.478012    0.489310  0.560556   \n",
       "19      5              24              3     0.474520    0.488068  0.559823   \n",
       "18      2              24              3     0.481426    0.486382  0.559114   \n",
       "20      6              24              3     0.481505    0.488467  0.558837   \n",
       "27      6              48              3     0.458724    0.452493  0.442124   \n",
       "28     12              48              3     0.468328    0.452626  0.437352   \n",
       "25      4              48              3     0.468805    0.445041  0.432428   \n",
       "26      5              48              3     0.469281    0.445662  0.430998   \n",
       "24      3              48              3     0.461343    0.443178  0.428031   \n",
       "23      2              48              3     0.464994    0.439319  0.422884   \n",
       "14     18             122              1     0.521670    0.482523  0.381508   \n",
       "12     11             122              1     0.513891    0.480261  0.376656   \n",
       "13     10             122              1     0.517781    0.481414  0.375714   \n",
       "11      2             122              1     0.519448    0.479329  0.372903   \n",
       "10      2              48              1     0.457612    0.436835  0.356969   \n",
       "3      16              12              1     0.536196    0.431201  0.001713   \n",
       "2      11              12              1     0.535323    0.431112  0.001712   \n",
       "1       2              12              1     0.531116    0.431024  0.001246   \n",
       "17      3              12              3     0.529846    0.430846  0.000312   \n",
       "15      2               1              3     0.536276    0.430758  0.000000   \n",
       "16      2              12              3     0.528814    0.430625  0.000000   \n",
       "\n",
       "    test_score_20  f1_score_20  time_taken  \n",
       "0        0.818397     0.900130    1.855416  \n",
       "9        0.680675     0.792156  120.183660  \n",
       "8        0.684219     0.794959   21.663580  \n",
       "7        0.682110     0.793351   11.017996  \n",
       "5        0.679831     0.791790    5.531324  \n",
       "4        0.678312     0.790388    2.791521  \n",
       "6        0.682278     0.793098    8.297130  \n",
       "31       0.669451     0.791527  158.816847  \n",
       "30       0.685570     0.802460   31.471321  \n",
       "29       0.677300     0.797586    4.560201  \n",
       "22       0.533755     0.663048   73.788872  \n",
       "21       0.527764     0.658988   59.012607  \n",
       "19       0.538228     0.667638   14.792543  \n",
       "18       0.541181     0.668536    3.736107  \n",
       "20       0.539072     0.667681   18.492774  \n",
       "27       0.393586     0.486934   19.537112  \n",
       "28       0.397300     0.493978   77.722343  \n",
       "25       0.397975     0.492603   11.768580  \n",
       "26       0.395274     0.489819   15.645103  \n",
       "24       0.403122     0.499469    7.866155  \n",
       "23       0.398312     0.493176    3.969309  \n",
       "14       0.369705     0.427092  136.635171  \n",
       "12       0.370127     0.427695   38.382321  \n",
       "13       0.371055     0.427353   74.002575  \n",
       "11       0.367679     0.424191    4.032818  \n",
       "10       0.352068     0.411828    3.005940  \n",
       "3        0.181857     0.001030   39.525672  \n",
       "2        0.181857     0.000618   26.152301  \n",
       "1        0.182110     0.001442    2.690389  \n",
       "17       0.181688     0.000206    7.393695  \n",
       "15       0.181603     0.000000    4.865133  \n",
       "16       0.181688     0.000206    3.720555  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:41:52.670165Z",
     "start_time": "2017-07-23T22:41:52.650136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>0.518733</td>\n",
       "      <td>0.579977</td>\n",
       "      <td>0.677805</td>\n",
       "      <td>0.680675</td>\n",
       "      <td>0.792156</td>\n",
       "      <td>120.183660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467455</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>1.855416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>0.554870</td>\n",
       "      <td>0.666711</td>\n",
       "      <td>0.669451</td>\n",
       "      <td>0.791527</td>\n",
       "      <td>158.816847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.477377</td>\n",
       "      <td>0.490286</td>\n",
       "      <td>0.560960</td>\n",
       "      <td>0.533755</td>\n",
       "      <td>0.663048</td>\n",
       "      <td>73.788872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.521670</td>\n",
       "      <td>0.482523</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>0.369705</td>\n",
       "      <td>0.427092</td>\n",
       "      <td>136.635171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">48</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.468328</td>\n",
       "      <td>0.452626</td>\n",
       "      <td>0.437352</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.493978</td>\n",
       "      <td>77.722343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.457612</td>\n",
       "      <td>0.436835</td>\n",
       "      <td>0.356969</td>\n",
       "      <td>0.352068</td>\n",
       "      <td>0.411828</td>\n",
       "      <td>3.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">12</th>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.536196</td>\n",
       "      <td>0.431201</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.181857</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>39.525672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.529846</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>7.393695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.536276</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.865133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "24             1                 42     0.518733    0.579977  0.677805   \n",
       "1              1                  2     0.467455    0.569242  0.725500   \n",
       "122            3                 18     0.493729    0.554870  0.666711   \n",
       "24             3                 12     0.477377    0.490286  0.560960   \n",
       "122            1                 18     0.521670    0.482523  0.381508   \n",
       "48             3                 12     0.468328    0.452626  0.437352   \n",
       "               1                  2     0.457612    0.436835  0.356969   \n",
       "12             1                 16     0.536196    0.431201  0.001713   \n",
       "               3                  3     0.529846    0.430846  0.000312   \n",
       "1              3                  2     0.536276    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "24             1                   0.680675     0.792156  120.183660  \n",
       "1              1                   0.818397     0.900130    1.855416  \n",
       "122            3                   0.669451     0.791527  158.816847  \n",
       "24             3                   0.533755     0.663048   73.788872  \n",
       "122            1                   0.369705     0.427092  136.635171  \n",
       "48             3                   0.397300     0.493978   77.722343  \n",
       "               1                   0.352068     0.411828    3.005940  \n",
       "12             1                   0.181857     0.001030   39.525672  \n",
       "               3                   0.181688     0.000206    7.393695  \n",
       "1              3                   0.181603     0.000000    4.865133  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:41:55.217625Z",
     "start_time": "2017-07-23T22:41:54.988Z"
    }
   },
   "outputs": [],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:02.169862Z",
     "start_time": "2017-07-23T22:42:02.145529Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:19.309455Z",
     "start_time": "2017-07-23T22:42:19.299508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10336</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028711</td>\n",
       "      <td>0.01165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "10336     0.0    -0.028711      0.01165         1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoch_nof_hidden\n",
    "Train.predictions[\"2_1_1\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:34.143990Z",
     "start_time": "2017-07-23T22:42:34.133985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.019567</td>\n",
       "      <td>0.00794</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Attack_prob  Normal_prob  Prediction\n",
       "3929     1.0    -0.019567      0.00794         1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"2_1_1\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:42.788524Z",
     "start_time": "2017-07-23T22:42:42.781581Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions[\"2_1_1\"].dropna()\n",
    "df_ = Train.predictions_[\"2_1_1\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:44.287797Z",
     "start_time": "2017-07-23T22:42:44.281074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:44.983876Z",
     "start_time": "2017-07-23T22:42:44.941634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.72550</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.90013</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision  Recall      Scenario\n",
       "0  0.569242   0.72550   0.569242     1.0  Train+/Test+\n",
       "1  0.818397   0.90013   0.818397     1.0  Train+/Test-"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:45.901417Z",
     "start_time": "2017-07-23T22:42:45.895276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:49.218863Z",
     "start_time": "2017-07-23T22:42:48.932448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAGkCAYAAABdFwDgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naJQEtoiIaJT/A+ZydZ8V7PtXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f323744b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:49.339153Z",
     "start_time": "2017-07-23T22:42:49.333385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:49.359240Z",
     "start_time": "2017-07-23T22:42:49.495Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:42:49.975863Z",
     "start_time": "2017-07-23T22:42:50.287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
