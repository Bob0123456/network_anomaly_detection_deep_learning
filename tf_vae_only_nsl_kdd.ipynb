{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:30.261012Z",
     "start_time": "2017-05-15T18:01:29.675785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:30.358437Z",
     "start_time": "2017-05-15T18:01:30.263370Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:30.367379Z",
     "start_time": "2017-05-15T18:01:30.361048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:30.375543Z",
     "start_time": "2017-05-15T18:01:30.369380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:31.396542Z",
     "start_time": "2017-05-15T18:01:30.377934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:32.982917Z",
     "start_time": "2017-05-15T18:01:31.398762Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:33.547977Z",
     "start_time": "2017-05-15T18:01:32.985330Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.x, self.x_hat) #tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            loss = tf.where(tf.is_nan(self.regularized_loss), 1e-2, self.regularized_loss)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T18:01:33.878659Z",
     "start_time": "2017-05-15T18:01:33.550256Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    \n",
    "                    def train_batch():\n",
    "                        nonlocal train_loss\n",
    "                        _, train_loss = sess.run([net.train_op, \n",
    "                                                  net.regularized_loss, \n",
    "                                                  ], #net.summary_op\n",
    "                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                             net.y_: y_train[i,:], \n",
    "                                                             net.keep_prob:1})\n",
    "\n",
    "                    train_batch()\n",
    "                    \n",
    "                    count = 10\n",
    "                    if((train_loss > 1e4 or np.isnan(train_loss) ) and epoch > 1 and count < 1):\n",
    "                        print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                        net.saver.restore(sess, \n",
    "                                          tf.train.latest_checkpoint('dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                     .format(epochs,h,f)))\n",
    "                        train_batch()\n",
    "                        count -= 1\n",
    "                        \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    #if(train_loss > 1e9):\n",
    "                    \n",
    "                    #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "                    \n",
    "                #print(\"\")\n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                \n",
    "                accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                               net.pred, \n",
    "                                                               net.actual, net.y], \n",
    "                                                              feed_dict={net.x: preprocess.x_test, \n",
    "                                                                         net.y_: preprocess.y_test, \n",
    "                                                                         net.keep_prob:1})\n",
    "                #print(\"*************** \\n\")\n",
    "                print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, test_loss, accuracy))\n",
    "                #print(\"*************** \\n\")\n",
    "                #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "                \n",
    "                if accuracy > Train.best_acc:\n",
    "                    Train.best_acc = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    \n",
    "                    #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                    #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                    #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                    #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "                    \n",
    "                    if not (np.isnan(train_loss)):\n",
    "                        net.saver.save(sess, \n",
    "                                   \"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                   .format(epochs,h,f), \n",
    "                                   global_step = epoch, \n",
    "                                   write_meta_graph=False)\n",
    "                    \n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):\n",
    "                                              (curr_pred, \n",
    "                                               Train.result(epochs, f, h,valid_accuracy, accuracy))})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:04.182607Z",
     "start_time": "2017-05-15T18:01:33.880786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:100 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.384459 | Test Loss: 1.533378 | Test Accuracy: 0.803407\n",
      "Step 2 | Training Loss: 0.247638 | Test Loss: 1.271418 | Test Accuracy: 0.810282\n",
      "Step 3 | Training Loss: 0.523086 | Test Loss: 1.410251 | Test Accuracy: 0.794225\n",
      "Step 4 | Training Loss: 0.245564 | Test Loss: 1.355747 | Test Accuracy: 0.787881\n",
      "Step 5 | Training Loss: 0.177735 | Test Loss: 1.374732 | Test Accuracy: 0.762997\n",
      "Step 6 | Training Loss: 0.352172 | Test Loss: 1.338143 | Test Accuracy: 0.796221\n",
      "Step 7 | Training Loss: 0.704250 | Test Loss: 1.317026 | Test Accuracy: 0.772090\n",
      "Step 8 | Training Loss: 0.230779 | Test Loss: 1.357148 | Test Accuracy: 0.782071\n",
      "Step 9 | Training Loss: 0.392297 | Test Loss: 1.295085 | Test Accuracy: 0.770138\n",
      "Step 10 | Training Loss: 0.137825 | Test Loss: 1.332437 | Test Accuracy: 0.749068\n",
      "Step 11 | Training Loss: 0.193066 | Test Loss: 1.367272 | Test Accuracy: 0.777679\n",
      "Step 12 | Training Loss: 0.439224 | Test Loss: 1.320220 | Test Accuracy: 0.749335\n",
      "Step 13 | Training Loss: 0.257180 | Test Loss: 1.375121 | Test Accuracy: 0.701561\n",
      "Step 14 | Training Loss: 0.200376 | Test Loss: 1.416018 | Test Accuracy: 0.739753\n",
      "Step 15 | Training Loss: 0.239575 | Test Loss: 1.426324 | Test Accuracy: 0.775151\n",
      "Step 16 | Training Loss: 0.397112 | Test Loss: 1.400671 | Test Accuracy: 0.775994\n",
      "Step 17 | Training Loss: 0.241340 | Test Loss: 1.409978 | Test Accuracy: 0.778300\n",
      "Step 18 | Training Loss: 0.528640 | Test Loss: 1.403597 | Test Accuracy: 0.750577\n",
      "Step 19 | Training Loss: 1.086721 | Test Loss: 1.432332 | Test Accuracy: 0.699565\n",
      "Step 20 | Training Loss: 0.212772 | Test Loss: 1.455013 | Test Accuracy: 0.717752\n",
      "Step 21 | Training Loss: 0.145526 | Test Loss: 1.505932 | Test Accuracy: 0.692246\n",
      "Step 22 | Training Loss: 0.279898 | Test Loss: 1.578366 | Test Accuracy: 0.745121\n",
      "Step 23 | Training Loss: 0.206388 | Test Loss: 1.529489 | Test Accuracy: 0.749512\n",
      "Step 24 | Training Loss: 0.194341 | Test Loss: 1.506294 | Test Accuracy: 0.743701\n",
      "Step 25 | Training Loss: 0.436498 | Test Loss: 1.511102 | Test Accuracy: 0.767122\n",
      "Step 26 | Training Loss: 0.218523 | Test Loss: 1.547199 | Test Accuracy: 0.756609\n",
      "Step 27 | Training Loss: 0.410316 | Test Loss: 1.527651 | Test Accuracy: 0.761577\n",
      "Step 28 | Training Loss: 0.367147 | Test Loss: 1.555000 | Test Accuracy: 0.773199\n",
      "Step 29 | Training Loss: 0.219598 | Test Loss: 1.538772 | Test Accuracy: 0.780962\n",
      "Step 30 | Training Loss: 0.227500 | Test Loss: 1.546659 | Test Accuracy: 0.777058\n",
      "Step 31 | Training Loss: 0.207359 | Test Loss: 1.551868 | Test Accuracy: 0.781050\n",
      "Step 32 | Training Loss: 0.317830 | Test Loss: 1.538009 | Test Accuracy: 0.790587\n",
      "Step 33 | Training Loss: 0.291777 | Test Loss: 1.756492 | Test Accuracy: 0.811613\n",
      "Step 34 | Training Loss: 0.200285 | Test Loss: 1.846323 | Test Accuracy: 0.816315\n",
      "Step 35 | Training Loss: 0.246281 | Test Loss: 1.950601 | Test Accuracy: 0.814186\n",
      "Step 36 | Training Loss: 0.376335 | Test Loss: 2.044863 | Test Accuracy: 0.804205\n",
      "Step 37 | Training Loss: 0.312677 | Test Loss: 1.933220 | Test Accuracy: 0.809927\n",
      "Step 38 | Training Loss: 2.966917 | Test Loss: 2.009668 | Test Accuracy: 0.772622\n",
      "Step 39 | Training Loss: 0.189867 | Test Loss: 1.969894 | Test Accuracy: 0.812190\n",
      "Step 40 | Training Loss: 0.246196 | Test Loss: 2.026502 | Test Accuracy: 0.813343\n",
      "Step 41 | Training Loss: 0.409392 | Test Loss: 2.245652 | Test Accuracy: 0.814319\n",
      "Step 42 | Training Loss: 0.647992 | Test Loss: 2.155029 | Test Accuracy: 0.727289\n",
      "Step 43 | Training Loss: 0.388941 | Test Loss: 2.346709 | Test Accuracy: 0.704356\n",
      "Step 44 | Training Loss: 0.303677 | Test Loss: 2.497185 | Test Accuracy: 0.717087\n",
      "Step 45 | Training Loss: 2.189243 | Test Loss: 2.509620 | Test Accuracy: 0.732523\n",
      "Step 46 | Training Loss: 0.272193 | Test Loss: 2.539649 | Test Accuracy: 0.709280\n",
      "Step 47 | Training Loss: 0.257556 | Test Loss: 2.512760 | Test Accuracy: 0.720458\n",
      "Step 48 | Training Loss: 1.259992 | Test Loss: 2.616831 | Test Accuracy: 0.709147\n",
      "Step 49 | Training Loss: 1.142362 | Test Loss: 2.624584 | Test Accuracy: 0.706042\n",
      "Step 50 | Training Loss: 0.635536 | Test Loss: 2.575277 | Test Accuracy: 0.711542\n",
      "Step 51 | Training Loss: 0.267957 | Test Loss: 2.468896 | Test Accuracy: 0.722676\n",
      "Step 52 | Training Loss: 0.252079 | Test Loss: 2.368786 | Test Accuracy: 0.708215\n",
      "Step 53 | Training Loss: 0.451069 | Test Loss: 2.020759 | Test Accuracy: 0.767388\n",
      "Step 54 | Training Loss: 0.339097 | Test Loss: 2.492671 | Test Accuracy: 0.713627\n",
      "Step 55 | Training Loss: 0.379072 | Test Loss: 2.355090 | Test Accuracy: 0.817335\n",
      "Step 56 | Training Loss: 0.479294 | Test Loss: 2.566429 | Test Accuracy: 0.691625\n",
      "Step 57 | Training Loss: 0.241498 | Test Loss: 2.244321 | Test Accuracy: 0.779099\n",
      "Step 58 | Training Loss: 0.492115 | Test Loss: 2.375530 | Test Accuracy: 0.738156\n",
      "Step 59 | Training Loss: 0.266733 | Test Loss: 2.300627 | Test Accuracy: 0.698190\n",
      "Step 60 | Training Loss: 0.414177 | Test Loss: 2.204487 | Test Accuracy: 0.779365\n",
      "Step 61 | Training Loss: 0.247524 | Test Loss: 2.816700 | Test Accuracy: 0.774485\n",
      "Step 62 | Training Loss: 0.661346 | Test Loss: 3.180611 | Test Accuracy: 0.720502\n",
      "Step 63 | Training Loss: 0.213787 | Test Loss: 3.156249 | Test Accuracy: 0.741084\n",
      "Step 64 | Training Loss: 0.421264 | Test Loss: 3.026991 | Test Accuracy: 0.684794\n",
      "Step 65 | Training Loss: 0.228665 | Test Loss: 3.475573 | Test Accuracy: 0.758117\n",
      "Step 66 | Training Loss: 0.481829 | Test Loss: 3.405915 | Test Accuracy: 0.753105\n",
      "Step 67 | Training Loss: 0.393357 | Test Loss: 3.561350 | Test Accuracy: 0.750887\n",
      "Step 68 | Training Loss: 1.029522 | Test Loss: 2.567165 | Test Accuracy: 0.783534\n",
      "Step 69 | Training Loss: 0.341056 | Test Loss: 2.443208 | Test Accuracy: 0.790632\n",
      "Step 70 | Training Loss: 0.474827 | Test Loss: 2.899293 | Test Accuracy: 0.787083\n",
      "Step 71 | Training Loss: 1.011566 | Test Loss: 2.782902 | Test Accuracy: 0.791918\n",
      "Step 72 | Training Loss: 0.373570 | Test Loss: 2.850465 | Test Accuracy: 0.792716\n",
      "Step 73 | Training Loss: 0.263094 | Test Loss: 2.728971 | Test Accuracy: 0.787527\n",
      "Step 74 | Training Loss: 0.367069 | Test Loss: 3.105489 | Test Accuracy: 0.782204\n",
      "Step 75 | Training Loss: 0.826017 | Test Loss: 2.916640 | Test Accuracy: 0.796576\n",
      "Step 76 | Training Loss: 0.539326 | Test Loss: 3.014749 | Test Accuracy: 0.815206\n",
      "Step 77 | Training Loss: 0.468282 | Test Loss: 2.801545 | Test Accuracy: 0.791785\n",
      "Step 78 | Training Loss: 0.256014 | Test Loss: 2.664655 | Test Accuracy: 0.821017\n",
      "Step 79 | Training Loss: 0.293959 | Test Loss: 3.116082 | Test Accuracy: 0.805403\n",
      "Step 80 | Training Loss: 0.534723 | Test Loss: 2.328039 | Test Accuracy: 0.796620\n",
      "Step 81 | Training Loss: 0.322992 | Test Loss: 2.284159 | Test Accuracy: 0.786728\n",
      "Step 82 | Training Loss: 0.210522 | Test Loss: 2.495970 | Test Accuracy: 0.797551\n",
      "Step 83 | Training Loss: 0.307557 | Test Loss: 2.378784 | Test Accuracy: 0.809484\n",
      "Step 84 | Training Loss: 0.331703 | Test Loss: 2.314518 | Test Accuracy: 0.819819\n",
      "Step 85 | Training Loss: 1.304391 | Test Loss: 2.388201 | Test Accuracy: 0.803185\n",
      "Step 86 | Training Loss: 0.929427 | Test Loss: 2.419280 | Test Accuracy: 0.798927\n",
      "Step 87 | Training Loss: 0.368632 | Test Loss: 2.296512 | Test Accuracy: 0.810326\n",
      "Step 88 | Training Loss: 0.691076 | Test Loss: 1.982750 | Test Accuracy: 0.803318\n",
      "Step 89 | Training Loss: 0.289618 | Test Loss: 2.029815 | Test Accuracy: 0.809883\n",
      "Step 90 | Training Loss: 0.358069 | Test Loss: 2.064466 | Test Accuracy: 0.819553\n",
      "Step 91 | Training Loss: 0.355996 | Test Loss: 1.827048 | Test Accuracy: 0.810593\n",
      "Step 92 | Training Loss: 1.264367 | Test Loss: 2.004060 | Test Accuracy: 0.808597\n",
      "Step 93 | Training Loss: 0.654547 | Test Loss: 1.815244 | Test Accuracy: 0.768009\n",
      "Step 94 | Training Loss: 0.223280 | Test Loss: 1.901520 | Test Accuracy: 0.756121\n",
      "Step 95 | Training Loss: 0.375655 | Test Loss: 1.903273 | Test Accuracy: 0.758206\n",
      "Step 96 | Training Loss: 0.320799 | Test Loss: 1.897452 | Test Accuracy: 0.764239\n",
      "Step 97 | Training Loss: 0.464140 | Test Loss: 1.857306 | Test Accuracy: 0.779143\n",
      "Step 98 | Training Loss: 0.398419 | Test Loss: 1.902374 | Test Accuracy: 0.760690\n",
      "Step 99 | Training Loss: 0.261668 | Test Loss: 1.915019 | Test Accuracy: 0.760025\n",
      "Step 100 | Training Loss: 0.532760 | Test Loss: 1.918930 | Test Accuracy: 0.754879\n",
      "Current Layer Attributes - epochs:100 hidden layers:2 features count:8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.619020 | Test Loss: 1.248809 | Test Accuracy: 0.780518\n",
      "Step 2 | Training Loss: 0.267994 | Test Loss: 1.228233 | Test Accuracy: 0.780607\n",
      "Step 3 | Training Loss: 0.205088 | Test Loss: 1.149991 | Test Accuracy: 0.792229\n",
      "Step 4 | Training Loss: 1.280090 | Test Loss: 1.165473 | Test Accuracy: 0.798084\n",
      "Step 5 | Training Loss: 0.280400 | Test Loss: 2.286943 | Test Accuracy: 0.764594\n",
      "Step 6 | Training Loss: 0.331178 | Test Loss: 426850651788541952.000000 | Test Accuracy: 0.810326\n",
      "Step 7 | Training Loss: 0.376267 | Test Loss: 1.617893 | Test Accuracy: 0.832150\n",
      "Step 8 | Training Loss: 0.486785 | Test Loss: 1.269559 | Test Accuracy: 0.795999\n",
      "Step 9 | Training Loss: 0.188366 | Test Loss: 1.210629 | Test Accuracy: 0.802165\n",
      "Step 10 | Training Loss: 0.469673 | Test Loss: 1.214636 | Test Accuracy: 0.759005\n",
      "Step 11 | Training Loss: 0.223356 | Test Loss: 1.179432 | Test Accuracy: 0.763618\n",
      "Step 12 | Training Loss: 0.129866 | Test Loss: 1.181829 | Test Accuracy: 0.763440\n",
      "Step 13 | Training Loss: 0.194270 | Test Loss: 1.172687 | Test Accuracy: 0.806556\n",
      "Step 14 | Training Loss: 0.223832 | Test Loss: 1.169310 | Test Accuracy: 0.786373\n",
      "Step 15 | Training Loss: 0.222178 | Test Loss: 1.175965 | Test Accuracy: 0.786152\n",
      "Step 16 | Training Loss: 0.460736 | Test Loss: 1.169220 | Test Accuracy: 0.785220\n",
      "Step 17 | Training Loss: 0.173424 | Test Loss: 1.152591 | Test Accuracy: 0.775506\n",
      "Step 18 | Training Loss: 0.231978 | Test Loss: 1.172993 | Test Accuracy: 0.781583\n",
      "Step 19 | Training Loss: 0.185993 | Test Loss: 1.147800 | Test Accuracy: 0.777058\n",
      "Step 20 | Training Loss: 0.380408 | Test Loss: 1.379724 | Test Accuracy: 0.787394\n",
      "Step 21 | Training Loss: 0.385529 | Test Loss: 1.356285 | Test Accuracy: 0.818400\n",
      "Step 22 | Training Loss: 0.367889 | Test Loss: 1.351169 | Test Accuracy: 0.814540\n",
      "Step 23 | Training Loss: 0.236230 | Test Loss: 1.348207 | Test Accuracy: 0.784111\n",
      "Step 24 | Training Loss: 0.286121 | Test Loss: 1.373201 | Test Accuracy: 0.803806\n",
      "Step 25 | Training Loss: 0.198856 | Test Loss: 696825600.000000 | Test Accuracy: 0.825319\n",
      "Step 26 | Training Loss: 0.527033 | Test Loss: 1.742590 | Test Accuracy: 0.734430\n",
      "Step 27 | Training Loss: 1.003547 | Test Loss: 1.699735 | Test Accuracy: 0.731813\n",
      "Step 28 | Training Loss: 0.312537 | Test Loss: 1.683964 | Test Accuracy: 0.744189\n",
      "Step 29 | Training Loss: 0.310447 | Test Loss: 1.683861 | Test Accuracy: 0.747693\n",
      "Step 30 | Training Loss: 0.320129 | Test Loss: 1.674745 | Test Accuracy: 0.762331\n",
      "Step 31 | Training Loss: 0.505968 | Test Loss: 1.668132 | Test Accuracy: 0.762864\n",
      "Step 32 | Training Loss: 0.376475 | Test Loss: 1.665024 | Test Accuracy: 0.765392\n",
      "Step 33 | Training Loss: 0.301266 | Test Loss: 1.661473 | Test Accuracy: 0.767787\n",
      "Step 34 | Training Loss: 0.287928 | Test Loss: 1.690247 | Test Accuracy: 0.736338\n",
      "Step 35 | Training Loss: 0.532078 | Test Loss: 1.705821 | Test Accuracy: 0.756831\n",
      "Step 36 | Training Loss: 0.545246 | Test Loss: 1.692484 | Test Accuracy: 0.762952\n",
      "Step 37 | Training Loss: 0.277485 | Test Loss: 1.693043 | Test Accuracy: 0.754791\n",
      "Step 38 | Training Loss: 0.461288 | Test Loss: 1.687376 | Test Accuracy: 0.755633\n",
      "Step 39 | Training Loss: 0.567649 | Test Loss: 1.676947 | Test Accuracy: 0.760158\n",
      "Step 40 | Training Loss: 0.257300 | Test Loss: 1.705961 | Test Accuracy: 0.743923\n",
      "Step 41 | Training Loss: 0.356508 | Test Loss: 1.690174 | Test Accuracy: 0.752972\n",
      "Step 42 | Training Loss: 0.499802 | Test Loss: 1.687194 | Test Accuracy: 0.747339\n",
      "Step 43 | Training Loss: 0.285510 | Test Loss: 1.692161 | Test Accuracy: 0.744455\n",
      "Step 44 | Training Loss: 0.404372 | Test Loss: 1.690192 | Test Accuracy: 0.751863\n",
      "Step 45 | Training Loss: 0.333055 | Test Loss: 1.690085 | Test Accuracy: 0.748847\n",
      "Step 46 | Training Loss: 0.384398 | Test Loss: 1.692151 | Test Accuracy: 0.749823\n",
      "Step 47 | Training Loss: 1.277393 | Test Loss: 1.699734 | Test Accuracy: 0.695440\n",
      "Step 48 | Training Loss: 0.847162 | Test Loss: 1.705689 | Test Accuracy: 0.734475\n",
      "Step 49 | Training Loss: 0.847464 | Test Loss: 1.681459 | Test Accuracy: 0.748714\n",
      "Step 50 | Training Loss: 0.351420 | Test Loss: 1.683052 | Test Accuracy: 0.768009\n",
      "Step 51 | Training Loss: 0.353875 | Test Loss: 1.674233 | Test Accuracy: 0.748048\n",
      "Step 52 | Training Loss: 0.393745 | Test Loss: 1.683447 | Test Accuracy: 0.750976\n",
      "Step 53 | Training Loss: 1.203553 | Test Loss: 1.684171 | Test Accuracy: 0.747294\n",
      "Step 54 | Training Loss: 0.473677 | Test Loss: 1.675519 | Test Accuracy: 0.749246\n",
      "Step 55 | Training Loss: 0.331471 | Test Loss: 1.674251 | Test Accuracy: 0.751198\n",
      "Step 56 | Training Loss: 0.412052 | Test Loss: 1.672380 | Test Accuracy: 0.749867\n",
      "Step 57 | Training Loss: 1.022898 | Test Loss: 1.677210 | Test Accuracy: 0.754791\n",
      "Step 58 | Training Loss: 0.757994 | Test Loss: 1.706055 | Test Accuracy: 0.757585\n",
      "Step 59 | Training Loss: 0.364344 | Test Loss: 1.712691 | Test Accuracy: 0.751242\n",
      "Step 60 | Training Loss: 0.571296 | Test Loss: 1.716429 | Test Accuracy: 0.736205\n",
      "Step 61 | Training Loss: 0.348285 | Test Loss: 1.716314 | Test Accuracy: 0.752040\n",
      "Step 62 | Training Loss: 0.404003 | Test Loss: 1.684837 | Test Accuracy: 0.737048\n",
      "Step 63 | Training Loss: 0.306522 | Test Loss: 1.691451 | Test Accuracy: 0.748004\n",
      "Step 64 | Training Loss: 0.651188 | Test Loss: 1.688731 | Test Accuracy: 0.737269\n",
      "Step 65 | Training Loss: 0.625082 | Test Loss: 1.684625 | Test Accuracy: 0.737669\n",
      "Step 66 | Training Loss: 0.327994 | Test Loss: 1.679839 | Test Accuracy: 0.738600\n",
      "Step 67 | Training Loss: 0.433887 | Test Loss: 1.687310 | Test Accuracy: 0.747516\n",
      "Step 68 | Training Loss: 0.327403 | Test Loss: 1.689105 | Test Accuracy: 0.735140\n",
      "Step 69 | Training Loss: 0.280565 | Test Loss: 1.681758 | Test Accuracy: 0.733233\n",
      "Step 70 | Training Loss: 0.320013 | Test Loss: 1.676469 | Test Accuracy: 0.736027\n",
      "Step 71 | Training Loss: 0.554865 | Test Loss: 1.714000 | Test Accuracy: 0.725293\n",
      "Step 72 | Training Loss: 0.535294 | Test Loss: 1.681206 | Test Accuracy: 0.754391\n",
      "Step 73 | Training Loss: 0.397261 | Test Loss: 1.677337 | Test Accuracy: 0.754391\n",
      "Step 74 | Training Loss: 0.307909 | Test Loss: 1.679926 | Test Accuracy: 0.745342\n",
      "Step 75 | Training Loss: 0.362009 | Test Loss: 1.680669 | Test Accuracy: 0.748048\n",
      "Step 76 | Training Loss: 0.516325 | Test Loss: 1.689803 | Test Accuracy: 0.734652\n",
      "Step 77 | Training Loss: 0.298628 | Test Loss: 1.692403 | Test Accuracy: 0.728708\n",
      "Step 78 | Training Loss: 0.463043 | Test Loss: 1.683991 | Test Accuracy: 0.732656\n",
      "Step 79 | Training Loss: 0.370619 | Test Loss: 1.685606 | Test Accuracy: 0.748625\n",
      "Step 80 | Training Loss: 0.514207 | Test Loss: 1.683142 | Test Accuracy: 0.737890\n",
      "Step 81 | Training Loss: 0.649600 | Test Loss: 1.687507 | Test Accuracy: 0.732035\n",
      "Step 82 | Training Loss: 0.522959 | Test Loss: 1.687463 | Test Accuracy: 0.738245\n",
      "Step 83 | Training Loss: 0.367673 | Test Loss: 1.685060 | Test Accuracy: 0.731592\n",
      "Step 84 | Training Loss: 0.338277 | Test Loss: 1.687188 | Test Accuracy: 0.743790\n",
      "Step 85 | Training Loss: 0.520673 | Test Loss: 1.684849 | Test Accuracy: 0.741971\n",
      "Step 86 | Training Loss: 0.573750 | Test Loss: 1.691689 | Test Accuracy: 0.727422\n",
      "Step 87 | Training Loss: 0.310973 | Test Loss: 1.695468 | Test Accuracy: 0.731015\n",
      "Step 88 | Training Loss: 0.484383 | Test Loss: 1.697015 | Test Accuracy: 0.744233\n",
      "Step 89 | Training Loss: 0.352576 | Test Loss: 1.699625 | Test Accuracy: 0.740685\n",
      "Step 90 | Training Loss: 0.305692 | Test Loss: 1.702415 | Test Accuracy: 0.741883\n",
      "Step 91 | Training Loss: 0.417061 | Test Loss: 1.690844 | Test Accuracy: 0.756831\n",
      "Step 92 | Training Loss: 0.685901 | Test Loss: 1.693134 | Test Accuracy: 0.747915\n",
      "Step 93 | Training Loss: 0.579425 | Test Loss: 1.685589 | Test Accuracy: 0.750488\n",
      "Step 94 | Training Loss: 0.444982 | Test Loss: 1.687685 | Test Accuracy: 0.747117\n",
      "Step 95 | Training Loss: 0.460474 | Test Loss: 1.674707 | Test Accuracy: 0.759626\n",
      "Step 96 | Training Loss: 0.476809 | Test Loss: 1.670935 | Test Accuracy: 0.746984\n",
      "Step 97 | Training Loss: 0.342536 | Test Loss: 1.680017 | Test Accuracy: 0.761577\n",
      "Step 98 | Training Loss: 0.629913 | Test Loss: 1.679943 | Test Accuracy: 0.757363\n",
      "Step 99 | Training Loss: 0.332213 | Test Loss: 1.735091 | Test Accuracy: 0.742016\n",
      "Step 100 | Training Loss: 0.337512 | Test Loss: 1.712563 | Test Accuracy: 0.735850\n",
      "Current Layer Attributes - epochs:100 hidden layers:2 features count:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.626001 | Test Loss: 1.764891 | Test Accuracy: 0.795511\n",
      "Step 2 | Training Loss: 0.695514 | Test Loss: 1.750721 | Test Accuracy: 0.809705\n",
      "Step 3 | Training Loss: 0.479492 | Test Loss: 1.718943 | Test Accuracy: 0.752972\n",
      "Step 4 | Training Loss: 0.535962 | Test Loss: 1.689275 | Test Accuracy: 0.757496\n",
      "Step 5 | Training Loss: 0.533097 | Test Loss: 1.559718 | Test Accuracy: 0.769917\n",
      "Step 6 | Training Loss: 0.501231 | Test Loss: 1.562867 | Test Accuracy: 0.772756\n",
      "Step 7 | Training Loss: 0.535864 | Test Loss: 1.521429 | Test Accuracy: 0.745431\n",
      "Step 8 | Training Loss: 0.456684 | Test Loss: 1.548683 | Test Accuracy: 0.778123\n",
      "Step 9 | Training Loss: 0.455738 | Test Loss: 1.733833 | Test Accuracy: 0.775240\n",
      "Step 10 | Training Loss: 0.567605 | Test Loss: 1.717416 | Test Accuracy: 0.767965\n",
      "Step 11 | Training Loss: 0.445930 | Test Loss: 1.766691 | Test Accuracy: 0.745165\n",
      "Step 12 | Training Loss: 0.419813 | Test Loss: 1.732484 | Test Accuracy: 0.792140\n",
      "Step 13 | Training Loss: 0.474069 | Test Loss: 1.708883 | Test Accuracy: 0.782026\n",
      "Step 14 | Training Loss: 1.374429 | Test Loss: 1.707122 | Test Accuracy: 0.753416\n",
      "Step 15 | Training Loss: 0.419536 | Test Loss: 1.694822 | Test Accuracy: 0.754303\n",
      "Step 16 | Training Loss: 0.525171 | Test Loss: 1.692309 | Test Accuracy: 0.750399\n",
      "Step 17 | Training Loss: 0.389148 | Test Loss: 1.691438 | Test Accuracy: 0.764860\n",
      "Step 18 | Training Loss: 0.369448 | Test Loss: 1.686700 | Test Accuracy: 0.766057\n",
      "Step 19 | Training Loss: 0.501154 | Test Loss: 1.684516 | Test Accuracy: 0.780873\n",
      "Step 20 | Training Loss: 0.502812 | Test Loss: 1.683283 | Test Accuracy: 0.771292\n",
      "Step 21 | Training Loss: 0.495136 | Test Loss: 1.676733 | Test Accuracy: 0.756742\n",
      "Step 22 | Training Loss: 0.459053 | Test Loss: 1.791797 | Test Accuracy: 0.758694\n",
      "Step 23 | Training Loss: 0.598015 | Test Loss: 1.705904 | Test Accuracy: 0.731414\n",
      "Step 24 | Training Loss: 0.531710 | Test Loss: 1.691160 | Test Accuracy: 0.758206\n",
      "Step 25 | Training Loss: 0.557200 | Test Loss: 1.676685 | Test Accuracy: 0.764239\n",
      "Step 26 | Training Loss: 0.433865 | Test Loss: 1.718071 | Test Accuracy: 0.750532\n",
      "Step 27 | Training Loss: 0.500102 | Test Loss: 1.708718 | Test Accuracy: 0.768098\n",
      "Step 28 | Training Loss: 1.194939 | Test Loss: 1.703723 | Test Accuracy: 0.763307\n",
      "Step 29 | Training Loss: 0.565282 | Test Loss: 1.697336 | Test Accuracy: 0.773731\n",
      "Step 30 | Training Loss: 0.369302 | Test Loss: 1.693734 | Test Accuracy: 0.758117\n",
      "Step 31 | Training Loss: 0.425556 | Test Loss: 1.708408 | Test Accuracy: 0.767521\n",
      "Step 32 | Training Loss: 0.693854 | Test Loss: 1.691990 | Test Accuracy: 0.771114\n",
      "Step 33 | Training Loss: 0.402217 | Test Loss: 1.688063 | Test Accuracy: 0.771558\n",
      "Step 34 | Training Loss: 0.445448 | Test Loss: 1.684819 | Test Accuracy: 0.755988\n",
      "Step 35 | Training Loss: 0.224796 | Test Loss: 1.677907 | Test Accuracy: 0.756033\n",
      "Step 36 | Training Loss: 0.468373 | Test Loss: 1.679528 | Test Accuracy: 0.823235\n",
      "Step 37 | Training Loss: 0.398813 | Test Loss: 1.665879 | Test Accuracy: 0.837961\n",
      "Step 38 | Training Loss: 0.549993 | Test Loss: 1.678524 | Test Accuracy: 0.768231\n",
      "Step 39 | Training Loss: 0.388385 | Test Loss: 1.681698 | Test Accuracy: 0.760424\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 51 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 52 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 53 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 54 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 55 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 56 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 57 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 58 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 59 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 60 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 61 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 62 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 63 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 64 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 65 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 66 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 67 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.616111 | Test Loss: 1.454555 | Test Accuracy: 0.767610\n",
      "Step 2 | Training Loss: 0.522198 | Test Loss: 1.441723 | Test Accuracy: 0.776969\n",
      "Step 3 | Training Loss: 0.286177 | Test Loss: 1.367218 | Test Accuracy: 0.767787\n",
      "Step 4 | Training Loss: 0.712168 | Test Loss: 1.447486 | Test Accuracy: 0.789966\n",
      "Step 5 | Training Loss: 0.141177 | Test Loss: 1.408251 | Test Accuracy: 0.762864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Training Loss: 0.583164 | Test Loss: 1.393577 | Test Accuracy: 0.775905\n",
      "Step 7 | Training Loss: 0.174967 | Test Loss: 1.404928 | Test Accuracy: 0.759936\n",
      "Step 8 | Training Loss: 0.097741 | Test Loss: 1.199573 | Test Accuracy: 0.753371\n",
      "Step 9 | Training Loss: 0.130353 | Test Loss: 1.384366 | Test Accuracy: 0.750089\n",
      "Step 10 | Training Loss: 0.269215 | Test Loss: 1.399238 | Test Accuracy: 0.749911\n",
      "Step 11 | Training Loss: 0.051724 | Test Loss: 1.400853 | Test Accuracy: 0.789700\n",
      "Step 12 | Training Loss: 0.219505 | Test Loss: 1.420250 | Test Accuracy: 0.742370\n",
      "Step 13 | Training Loss: 0.310551 | Test Loss: 1.444661 | Test Accuracy: 0.731281\n",
      "Step 14 | Training Loss: 0.091303 | Test Loss: 1.454284 | Test Accuracy: 0.720724\n",
      "Step 15 | Training Loss: 0.348257 | Test Loss: 1.486715 | Test Accuracy: 0.721079\n",
      "Step 16 | Training Loss: 1.019738 | Test Loss: 1.522114 | Test Accuracy: 0.716377\n",
      "Step 17 | Training Loss: 0.140713 | Test Loss: 1.483016 | Test Accuracy: 0.744766\n",
      "Step 18 | Training Loss: 0.158278 | Test Loss: 1.505137 | Test Accuracy: 0.734874\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 21 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 22 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 23 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 24 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 25 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 27 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 28 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 30 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 31 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 51 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 52 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 53 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 54 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 55 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 56 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 57 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 58 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 59 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 60 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 61 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 62 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 63 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 64 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 65 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 66 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 67 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.375880 | Test Loss: 1.680661 | Test Accuracy: 0.788236\n",
      "Step 2 | Training Loss: 0.275414 | Test Loss: 1.566728 | Test Accuracy: 0.781361\n",
      "Step 3 | Training Loss: 0.212153 | Test Loss: 1.470513 | Test Accuracy: 0.792583\n",
      "Step 4 | Training Loss: 0.229666 | Test Loss: 1.808239 | Test Accuracy: 0.765170\n",
      "Step 5 | Training Loss: 0.297854 | Test Loss: 1.648711 | Test Accuracy: 0.741616\n",
      "Step 6 | Training Loss: 0.176246 | Test Loss: 1.497979 | Test Accuracy: 0.778478\n",
      "Step 7 | Training Loss: 0.157155 | Test Loss: 1.956396 | Test Accuracy: 0.766102\n",
      "Step 8 | Training Loss: 1.390977 | Test Loss: 1.711742 | Test Accuracy: 0.763263\n",
      "Step 9 | Training Loss: 0.217900 | Test Loss: 1.597254 | Test Accuracy: 0.763219\n",
      "Step 10 | Training Loss: 0.924353 | Test Loss: 1.758391 | Test Accuracy: 0.764194\n",
      "Step 11 | Training Loss: 0.331364 | Test Loss: 1.894327 | Test Accuracy: 0.709634\n",
      "Step 12 | Training Loss: 0.313715 | Test Loss: 2.053773 | Test Accuracy: 0.811702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 | Training Loss: 0.297938 | Test Loss: 2.395646 | Test Accuracy: 0.646026\n",
      "Step 14 | Training Loss: 1.437182 | Test Loss: 2.587249 | Test Accuracy: 0.720768\n",
      "Step 15 | Training Loss: 0.195186 | Test Loss: 1.563657 | Test Accuracy: 0.787305\n",
      "Step 16 | Training Loss: 0.543038 | Test Loss: 1.803699 | Test Accuracy: 0.839469\n",
      "Step 17 | Training Loss: 0.403719 | Test Loss: 1.741393 | Test Accuracy: 0.826162\n",
      "Step 18 | Training Loss: 0.337798 | Test Loss: 1.736630 | Test Accuracy: 0.830376\n",
      "Step 19 | Training Loss: 0.650283 | Test Loss: 1.726830 | Test Accuracy: 0.829489\n",
      "Step 20 | Training Loss: 0.446807 | Test Loss: 2.055157 | Test Accuracy: 0.839824\n",
      "Step 21 | Training Loss: 0.481162 | Test Loss: 2.898079 | Test Accuracy: 0.856458\n",
      "Step 22 | Training Loss: 1.948886 | Test Loss: 1.681496 | Test Accuracy: 0.854862\n",
      "Step 23 | Training Loss: 0.339468 | Test Loss: 1.713174 | Test Accuracy: 0.871318\n",
      "Step 24 | Training Loss: 0.443705 | Test Loss: 1.731594 | Test Accuracy: 0.856725\n",
      "Step 25 | Training Loss: 0.630076 | Test Loss: 1.886844 | Test Accuracy: 0.821416\n",
      "Step 26 | Training Loss: 1.575888 | Test Loss: 1.867438 | Test Accuracy: 0.822791\n",
      "Step 27 | Training Loss: 0.596252 | Test Loss: 1.855786 | Test Accuracy: 0.822214\n",
      "Step 28 | Training Loss: 0.521569 | Test Loss: 1.843178 | Test Accuracy: 0.843772\n",
      "Step 29 | Training Loss: 0.571610 | Test Loss: 1.844602 | Test Accuracy: 0.840711\n",
      "Step 30 | Training Loss: 0.598122 | Test Loss: 1.849483 | Test Accuracy: 0.836631\n",
      "Step 31 | Training Loss: 0.649209 | Test Loss: 1.875975 | Test Accuracy: 0.830554\n",
      "Step 32 | Training Loss: 0.597904 | Test Loss: 1.848003 | Test Accuracy: 0.839115\n",
      "Step 33 | Training Loss: 0.616570 | Test Loss: 1.875452 | Test Accuracy: 0.895582\n",
      "Step 34 | Training Loss: 0.712107 | Test Loss: 1.860313 | Test Accuracy: 0.837252\n",
      "Step 35 | Training Loss: 0.645664 | Test Loss: 1.898821 | Test Accuracy: 0.901792\n",
      "Step 36 | Training Loss: 2.099160 | Test Loss: 1.909235 | Test Accuracy: 0.894473\n",
      "Step 37 | Training Loss: 0.743536 | Test Loss: 1.883159 | Test Accuracy: 0.902014\n",
      "Step 38 | Training Loss: 0.542071 | Test Loss: 1.882905 | Test Accuracy: 0.900506\n",
      "Step 39 | Training Loss: 0.901898 | Test Loss: 1.886640 | Test Accuracy: 0.900195\n",
      "Step 40 | Training Loss: 0.760779 | Test Loss: 1.887700 | Test Accuracy: 0.898643\n",
      "Step 41 | Training Loss: 0.686553 | Test Loss: 1.875216 | Test Accuracy: 0.880589\n",
      "Step 42 | Training Loss: 0.820785 | Test Loss: 1.876937 | Test Accuracy: 0.879791\n",
      "Step 43 | Training Loss: 0.667765 | Test Loss: 1.875581 | Test Accuracy: 0.879924\n",
      "Step 44 | Training Loss: 0.622941 | Test Loss: 1.871249 | Test Accuracy: 0.879347\n",
      "Step 45 | Training Loss: 0.814987 | Test Loss: 1.874944 | Test Accuracy: 0.871984\n",
      "Step 46 | Training Loss: 0.783192 | Test Loss: 1.947321 | Test Accuracy: 0.816182\n",
      "Step 47 | Training Loss: 0.767459 | Test Loss: 1.950841 | Test Accuracy: 0.865774\n",
      "Step 48 | Training Loss: 0.726418 | Test Loss: 1.950648 | Test Accuracy: 0.865153\n",
      "Step 49 | Training Loss: 0.894804 | Test Loss: 1.949168 | Test Accuracy: 0.864931\n",
      "Step 50 | Training Loss: 0.823937 | Test Loss: 1.950490 | Test Accuracy: 0.862491\n",
      "Step 51 | Training Loss: 2.838499 | Test Loss: 1.950114 | Test Accuracy: 0.865374\n",
      "Step 52 | Training Loss: 1.702393 | Test Loss: 1.950479 | Test Accuracy: 0.865419\n",
      "Step 53 | Training Loss: 0.752167 | Test Loss: 1.949731 | Test Accuracy: 0.865286\n",
      "Step 54 | Training Loss: 0.713926 | Test Loss: 1.950005 | Test Accuracy: 0.865286\n",
      "Step 55 | Training Loss: 0.750558 | Test Loss: 1.950659 | Test Accuracy: 0.865951\n",
      "Step 56 | Training Loss: 0.745123 | Test Loss: 1.950577 | Test Accuracy: 0.865907\n",
      "Step 57 | Training Loss: 0.760942 | Test Loss: 1.948834 | Test Accuracy: 0.865995\n",
      "Step 58 | Training Loss: 0.673356 | Test Loss: 1.949775 | Test Accuracy: 0.865995\n",
      "Step 59 | Training Loss: 0.740831 | Test Loss: 1.948248 | Test Accuracy: 0.864886\n",
      "Step 60 | Training Loss: 1.123654 | Test Loss: 1.950691 | Test Accuracy: 0.864842\n",
      "Step 61 | Training Loss: 0.936436 | Test Loss: 1.949943 | Test Accuracy: 0.864842\n",
      "Step 62 | Training Loss: 1.032514 | Test Loss: 1.951108 | Test Accuracy: 0.864842\n",
      "Step 63 | Training Loss: 0.817696 | Test Loss: 1.950989 | Test Accuracy: 0.864798\n",
      "Step 64 | Training Loss: 0.778963 | Test Loss: 1.949986 | Test Accuracy: 0.864798\n",
      "Step 65 | Training Loss: 0.902277 | Test Loss: 1.948963 | Test Accuracy: 0.864798\n",
      "Step 66 | Training Loss: 1.368607 | Test Loss: 1.949634 | Test Accuracy: 0.864753\n",
      "Step 67 | Training Loss: 0.812063 | Test Loss: 1.950352 | Test Accuracy: 0.864753\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.317495 | Test Loss: 1.571876 | Test Accuracy: 0.827493\n",
      "Step 2 | Training Loss: 0.180280 | Test Loss: 1.472796 | Test Accuracy: 0.762908\n",
      "Step 3 | Training Loss: 0.274878 | Test Loss: 1.494933 | Test Accuracy: 0.734608\n",
      "Step 4 | Training Loss: 0.289043 | Test Loss: 1.453931 | Test Accuracy: 0.801455\n",
      "Step 5 | Training Loss: 0.460979 | Test Loss: 2.211323 | Test Accuracy: 0.721079\n",
      "Step 6 | Training Loss: 0.364538 | Test Loss: 1.521212 | Test Accuracy: 0.865286\n",
      "Step 7 | Training Loss: 0.395440 | Test Loss: 1.685481 | Test Accuracy: 0.858543\n",
      "Step 8 | Training Loss: 0.682025 | Test Loss: 1.667857 | Test Accuracy: 0.888086\n",
      "Step 9 | Training Loss: 0.549916 | Test Loss: 1.655263 | Test Accuracy: 0.884093\n",
      "Step 10 | Training Loss: 0.493568 | Test Loss: 1.663146 | Test Accuracy: 0.884626\n",
      "Step 11 | Training Loss: 0.331853 | Test Loss: 1.646838 | Test Accuracy: 0.881875\n",
      "Step 12 | Training Loss: 0.476961 | Test Loss: 1.636324 | Test Accuracy: 0.880057\n",
      "Step 13 | Training Loss: 0.471778 | Test Loss: 1.683839 | Test Accuracy: 0.857479\n",
      "Step 14 | Training Loss: 0.410593 | Test Loss: 1.670103 | Test Accuracy: 0.879569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 | Training Loss: 0.272696 | Test Loss: 1.656305 | Test Accuracy: 0.862491\n",
      "Step 16 | Training Loss: 2.167497 | Test Loss: 1.652878 | Test Accuracy: 0.859430\n",
      "Step 17 | Training Loss: 0.500568 | Test Loss: 1.649340 | Test Accuracy: 0.830332\n",
      "Step 18 | Training Loss: 0.346444 | Test Loss: 1.743773 | Test Accuracy: 0.800390\n",
      "Step 19 | Training Loss: 0.607198 | Test Loss: 1.718194 | Test Accuracy: 0.811036\n",
      "Step 20 | Training Loss: 0.350957 | Test Loss: 1.731444 | Test Accuracy: 0.818621\n",
      "Step 21 | Training Loss: 0.703021 | Test Loss: 1.809503 | Test Accuracy: 0.843417\n",
      "Step 22 | Training Loss: 0.383888 | Test Loss: 1.707352 | Test Accuracy: 0.868213\n",
      "Step 23 | Training Loss: 0.479404 | Test Loss: 1.759620 | Test Accuracy: 0.829045\n",
      "Step 24 | Training Loss: 0.294838 | Test Loss: 1.716016 | Test Accuracy: 0.851668\n",
      "Step 25 | Training Loss: 0.635159 | Test Loss: 1.847852 | Test Accuracy: 0.804959\n",
      "Step 26 | Training Loss: 0.592915 | Test Loss: 1.853323 | Test Accuracy: 0.767388\n",
      "Step 27 | Training Loss: 0.773303 | Test Loss: 1.893702 | Test Accuracy: 0.846256\n",
      "Step 28 | Training Loss: 0.813823 | Test Loss: 1.905192 | Test Accuracy: 0.823279\n",
      "Step 29 | Training Loss: 0.656497 | Test Loss: 1.866184 | Test Accuracy: 0.830110\n",
      "Step 30 | Training Loss: 0.839572 | Test Loss: 1.865500 | Test Accuracy: 0.853620\n",
      "Step 31 | Training Loss: 0.816776 | Test Loss:    nan | Test Accuracy: 0.819508\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 51 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 52 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 53 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 54 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 55 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 56 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 57 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 58 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 59 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 60 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 61 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 62 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 63 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 64 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 65 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 66 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 67 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 1.087870 | Test Loss: 1.677004 | Test Accuracy: 0.779276\n",
      "Step 2 | Training Loss: 0.428930 | Test Loss: 1.490407 | Test Accuracy: 0.787260\n",
      "Step 3 | Training Loss: 0.294060 | Test Loss: 1.510825 | Test Accuracy: 0.762997\n",
      "Step 4 | Training Loss: 0.617598 | Test Loss: 1.472119 | Test Accuracy: 0.823589\n",
      "Step 5 | Training Loss: 0.446993 | Test Loss: 1.482723 | Test Accuracy: 0.774175\n",
      "Step 6 | Training Loss: 0.355290 | Test Loss: 1.500146 | Test Accuracy: 0.749335\n",
      "Step 7 | Training Loss: 0.347332 | Test Loss: 1.556526 | Test Accuracy: 0.742370\n",
      "Step 8 | Training Loss: 0.366918 | Test Loss: 1.588051 | Test Accuracy: 0.780163\n",
      "Step 9 | Training Loss: 0.278372 | Test Loss: 1.511125 | Test Accuracy: 0.742681\n",
      "Step 10 | Training Loss: 0.350515 | Test Loss: 1.715433 | Test Accuracy: 0.788148\n",
      "Step 11 | Training Loss: 2.210899 | Test Loss: 1.756880 | Test Accuracy: 0.719393\n",
      "Step 12 | Training Loss: 0.591390 | Test Loss: 1.858552 | Test Accuracy: 0.710256\n",
      "Step 13 | Training Loss: 0.835294 | Test Loss: 1.797402 | Test Accuracy: 0.786817\n",
      "Step 14 | Training Loss: 0.813062 | Test Loss: 1.849547 | Test Accuracy: 0.724051\n",
      "Step 15 | Training Loss: 0.686833 | Test Loss: 1.837113 | Test Accuracy: 0.743657\n",
      "Step 16 | Training Loss: 0.599812 | Test Loss: 1.836725 | Test Accuracy: 0.750976\n",
      "Step 17 | Training Loss: 0.537876 | Test Loss: 833838626021310464.000000 | Test Accuracy: 0.719571\n",
      "Step 18 | Training Loss: 0.870213 | Test Loss: 1.912498 | Test Accuracy: 0.622960\n",
      "Step 19 | Training Loss: 0.821016 | Test Loss: 1.986986 | Test Accuracy: 0.480926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 | Training Loss: 0.885240 | Test Loss: 1.982711 | Test Accuracy: 0.430758\n",
      "Step 21 | Training Loss: 0.737717 | Test Loss: 1.983173 | Test Accuracy: 0.430758\n",
      "Step 22 | Training Loss: 0.805977 | Test Loss: 1.983631 | Test Accuracy: 0.430758\n",
      "Step 23 | Training Loss: 0.712358 | Test Loss: 1.983490 | Test Accuracy: 0.430758\n",
      "Step 24 | Training Loss: 0.821830 | Test Loss: 1.983486 | Test Accuracy: 0.430758\n",
      "Step 25 | Training Loss: 0.831557 | Test Loss: 1.983799 | Test Accuracy: 0.430758\n",
      "Step 26 | Training Loss: 0.857700 | Test Loss: 1.984073 | Test Accuracy: 0.430758\n",
      "Step 27 | Training Loss: 0.909120 | Test Loss: 1.983846 | Test Accuracy: 0.430758\n",
      "Step 28 | Training Loss: 0.876508 | Test Loss: 1.984031 | Test Accuracy: 0.430758\n",
      "Step 29 | Training Loss: 1.016787 | Test Loss: 1.983041 | Test Accuracy: 0.430758\n",
      "Step 30 | Training Loss: 0.999810 | Test Loss: 1.983847 | Test Accuracy: 0.430758\n",
      "Step 31 | Training Loss: 1.161029 | Test Loss: 1.983999 | Test Accuracy: 0.430758\n",
      "Step 32 | Training Loss: 1.083579 | Test Loss: 1.984510 | Test Accuracy: 0.430758\n",
      "Step 33 | Training Loss: 0.756324 | Test Loss: 1.982799 | Test Accuracy: 0.430758\n",
      "Step 34 | Training Loss: 0.890561 | Test Loss: 1.983175 | Test Accuracy: 0.430758\n",
      "Step 35 | Training Loss: 0.928767 | Test Loss: 1.983217 | Test Accuracy: 0.430758\n",
      "Step 36 | Training Loss: 0.751746 | Test Loss: 1.983816 | Test Accuracy: 0.430758\n",
      "Step 37 | Training Loss: 0.748418 | Test Loss: 1.983205 | Test Accuracy: 0.430758\n",
      "Step 38 | Training Loss: 0.827634 | Test Loss: 1.983040 | Test Accuracy: 0.430758\n",
      "Step 39 | Training Loss: 1.108883 | Test Loss: 1.982859 | Test Accuracy: 0.430758\n",
      "Step 40 | Training Loss: 0.904344 | Test Loss: 1.983207 | Test Accuracy: 0.430758\n",
      "Step 41 | Training Loss: 0.823750 | Test Loss: 1.983654 | Test Accuracy: 0.430758\n",
      "Step 42 | Training Loss: 1.157528 | Test Loss: 1.984389 | Test Accuracy: 0.430758\n",
      "Step 43 | Training Loss: 0.730893 | Test Loss: 1.983683 | Test Accuracy: 0.430758\n",
      "Step 44 | Training Loss: 1.364300 | Test Loss: 1.982662 | Test Accuracy: 0.430758\n",
      "Step 45 | Training Loss: 1.026465 | Test Loss: 1.984817 | Test Accuracy: 0.430758\n",
      "Step 46 | Training Loss: 0.855761 | Test Loss: 1.983657 | Test Accuracy: 0.430758\n",
      "Step 47 | Training Loss: 0.898197 | Test Loss: 1.982786 | Test Accuracy: 0.430758\n",
      "Step 48 | Training Loss: 0.916845 | Test Loss: 1.983891 | Test Accuracy: 0.430758\n",
      "Step 49 | Training Loss: 0.969581 | Test Loss: 1.983328 | Test Accuracy: 0.430758\n",
      "Step 50 | Training Loss: 1.117711 | Test Loss: 1.983747 | Test Accuracy: 0.430758\n",
      "Step 51 | Training Loss: 0.855994 | Test Loss: 1.983176 | Test Accuracy: 0.430758\n",
      "Step 52 | Training Loss: 1.045264 | Test Loss: 1.983696 | Test Accuracy: 0.430758\n",
      "Step 53 | Training Loss: 0.842482 | Test Loss: 1.984233 | Test Accuracy: 0.430758\n",
      "Step 54 | Training Loss: 0.862320 | Test Loss: 1.984810 | Test Accuracy: 0.430758\n",
      "Step 55 | Training Loss: 0.838422 | Test Loss: 1.982823 | Test Accuracy: 0.430758\n",
      "Step 56 | Training Loss: 1.698704 | Test Loss: 1.984707 | Test Accuracy: 0.430758\n",
      "Step 57 | Training Loss: 1.052155 | Test Loss: 1.984547 | Test Accuracy: 0.430758\n",
      "Step 58 | Training Loss: 0.837199 | Test Loss: 1.983218 | Test Accuracy: 0.430758\n",
      "Step 59 | Training Loss: 1.105175 | Test Loss: 1.984011 | Test Accuracy: 0.430758\n",
      "Step 60 | Training Loss: 0.888215 | Test Loss: 1.983455 | Test Accuracy: 0.430758\n",
      "Step 61 | Training Loss: 0.957554 | Test Loss: 1.983461 | Test Accuracy: 0.430758\n",
      "Step 62 | Training Loss: 0.765695 | Test Loss: 1.983072 | Test Accuracy: 0.430758\n",
      "Step 63 | Training Loss: 0.825608 | Test Loss: 1.984269 | Test Accuracy: 0.430758\n",
      "Step 64 | Training Loss: 0.811255 | Test Loss: 1.983040 | Test Accuracy: 0.430758\n",
      "Step 65 | Training Loss: 0.899763 | Test Loss: 1.984397 | Test Accuracy: 0.430758\n",
      "Step 66 | Training Loss: 0.792471 | Test Loss: 1.984345 | Test Accuracy: 0.430758\n",
      "Step 67 | Training Loss: 0.945402 | Test Loss: 1.983613 | Test Accuracy: 0.430758\n",
      "Step 68 | Training Loss: 0.794782 | Test Loss: 1.983859 | Test Accuracy: 0.430758\n",
      "Step 69 | Training Loss: 0.893319 | Test Loss: 1.984212 | Test Accuracy: 0.430758\n",
      "Step 70 | Training Loss: 0.768291 | Test Loss: 1.983869 | Test Accuracy: 0.430758\n",
      "Step 71 | Training Loss: 0.829740 | Test Loss: 1.982613 | Test Accuracy: 0.430758\n",
      "Step 72 | Training Loss: 0.863872 | Test Loss: 1.984364 | Test Accuracy: 0.430758\n",
      "Step 73 | Training Loss: 0.957833 | Test Loss: 1.984174 | Test Accuracy: 0.430758\n",
      "Step 74 | Training Loss: 0.853357 | Test Loss: 1.984827 | Test Accuracy: 0.430758\n",
      "Step 75 | Training Loss: 0.814644 | Test Loss: 1.985232 | Test Accuracy: 0.430758\n",
      "Step 76 | Training Loss: 0.799946 | Test Loss: 1.983913 | Test Accuracy: 0.430758\n",
      "Step 77 | Training Loss: 1.058791 | Test Loss: 1.981544 | Test Accuracy: 0.430758\n",
      "Step 78 | Training Loss: 0.777360 | Test Loss: 1.985635 | Test Accuracy: 0.430758\n",
      "Step 79 | Training Loss: 1.154317 | Test Loss: 1.983245 | Test Accuracy: 0.430758\n",
      "Step 80 | Training Loss: 0.782892 | Test Loss: 1.984067 | Test Accuracy: 0.430758\n",
      "Step 81 | Training Loss: 1.043894 | Test Loss: 1.984164 | Test Accuracy: 0.430758\n",
      "Step 82 | Training Loss: 0.830408 | Test Loss: 1.981769 | Test Accuracy: 0.430758\n",
      "Step 83 | Training Loss: 0.798182 | Test Loss: 1.983555 | Test Accuracy: 0.430758\n",
      "Step 84 | Training Loss: 0.854850 | Test Loss: 1.982735 | Test Accuracy: 0.430758\n",
      "Step 85 | Training Loss: 0.812284 | Test Loss: 1.983649 | Test Accuracy: 0.430758\n",
      "Step 86 | Training Loss: 0.855922 | Test Loss: 1.984080 | Test Accuracy: 0.430758\n",
      "Step 87 | Training Loss: 1.188520 | Test Loss: 1.983121 | Test Accuracy: 0.430758\n",
      "Step 88 | Training Loss: 0.930854 | Test Loss: 1.981977 | Test Accuracy: 0.430758\n",
      "Step 89 | Training Loss: 0.890848 | Test Loss: 1.983751 | Test Accuracy: 0.430758\n",
      "Step 90 | Training Loss: 0.789873 | Test Loss: 1.984820 | Test Accuracy: 0.430758\n",
      "Step 91 | Training Loss: 2.257466 | Test Loss: 1.985106 | Test Accuracy: 0.430758\n",
      "Step 92 | Training Loss: 1.769760 | Test Loss: 1.985371 | Test Accuracy: 0.430758\n",
      "Step 93 | Training Loss: 0.827218 | Test Loss: 1.983767 | Test Accuracy: 0.430758\n",
      "Step 94 | Training Loss: 0.805510 | Test Loss: 1.984429 | Test Accuracy: 0.430758\n",
      "Step 95 | Training Loss: 0.821274 | Test Loss: 1.982806 | Test Accuracy: 0.430758\n",
      "Step 96 | Training Loss: 1.031161 | Test Loss: 1.983141 | Test Accuracy: 0.430758\n",
      "Step 97 | Training Loss: 0.776622 | Test Loss: 1.982720 | Test Accuracy: 0.430758\n",
      "Step 98 | Training Loss: 0.911590 | Test Loss: 1.984725 | Test Accuracy: 0.430758\n",
      "Step 99 | Training Loss: 0.874782 | Test Loss: 1.982789 | Test Accuracy: 0.430758\n",
      "Step 100 | Training Loss: 1.494936 | Test Loss: 1.985235 | Test Accuracy: 0.430758\n",
      "Current Layer Attributes - epochs:100 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.648363 | Test Loss: 1.870690 | Test Accuracy: 0.760690\n",
      "Step 2 | Training Loss: 0.779031 | Test Loss: 1.939904 | Test Accuracy: 0.520626\n",
      "Step 3 | Training Loss: 0.649737 | Test Loss: 1.908890 | Test Accuracy: 0.790809\n",
      "Step 4 | Training Loss: 0.866741 | Test Loss: 1.832722 | Test Accuracy: 0.769163\n",
      "Step 5 | Training Loss: 1.531921 | Test Loss: 1.842294 | Test Accuracy: 0.731858\n",
      "Step 6 | Training Loss: 0.703534 | Test Loss: 1.832409 | Test Accuracy: 0.759670\n",
      "Step 7 | Training Loss: 0.829331 | Test Loss: 1.945977 | Test Accuracy: 0.549237\n",
      "Step 8 | Training Loss: 0.725038 | Test Loss: 1.928071 | Test Accuracy: 0.803096\n",
      "Step 9 | Training Loss: 0.656633 | Test Loss: 1.920381 | Test Accuracy: 0.744721\n",
      "Step 10 | Training Loss: 1.018545 | Test Loss: 1.912172 | Test Accuracy: 0.772977\n",
      "Step 11 | Training Loss: 1.055514 | Test Loss: 1.907706 | Test Accuracy: 0.766057\n",
      "Step 12 | Training Loss: 0.872529 | Test Loss: 1.903789 | Test Accuracy: 0.770626\n",
      "Step 13 | Training Loss: 0.734791 | Test Loss: 1.895562 | Test Accuracy: 0.734164\n",
      "Step 14 | Training Loss: 0.679560 | Test Loss: 1.888732 | Test Accuracy: 0.738689\n",
      "Step 15 | Training Loss: 0.649857 | Test Loss: 1.855948 | Test Accuracy: 0.741439\n",
      "Step 16 | Training Loss: 0.892192 | Test Loss: 1.891396 | Test Accuracy: 0.774885\n",
      "Step 17 | Training Loss: 1.092452 | Test Loss: 1.851514 | Test Accuracy: 0.734830\n",
      "Step 18 | Training Loss: 0.814912 | Test Loss: 1.861181 | Test Accuracy: 0.714869\n",
      "Step 19 | Training Loss: 0.766315 | Test Loss: 1.860206 | Test Accuracy: 0.726446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 | Training Loss: 0.642194 | Test Loss: 1.852695 | Test Accuracy: 0.780429\n",
      "Step 21 | Training Loss: 0.699400 | Test Loss: 1.852849 | Test Accuracy: 0.738733\n",
      "Step 22 | Training Loss: 0.692687 | Test Loss: 1.827217 | Test Accuracy: 0.750887\n",
      "Step 23 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 24 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 25 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 27 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 28 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 30 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 31 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 51 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 52 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 53 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 54 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 55 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 56 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 57 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 58 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 59 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 60 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 61 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 62 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 63 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 64 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 65 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 66 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 67 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.650933 | Test Loss: 1.821499 | Test Accuracy: 0.823323\n",
      "Step 2 | Training Loss: 0.576979 | Test Loss: 1.792397 | Test Accuracy: 0.766457\n",
      "Step 3 | Training Loss: 0.626769 | Test Loss: 1.728991 | Test Accuracy: 0.817202\n",
      "Step 4 | Training Loss: 0.575279 | Test Loss: 1.667240 | Test Accuracy: 0.784599\n",
      "Step 5 | Training Loss: 0.702765 | Test Loss: 1.631951 | Test Accuracy: 0.810060\n",
      "Step 6 | Training Loss: 0.801799 | Test Loss: 1.629656 | Test Accuracy: 0.843506\n",
      "Step 7 | Training Loss: 0.637035 | Test Loss: 1.623384 | Test Accuracy: 0.823368\n",
      "Step 8 | Training Loss: 0.566155 | Test Loss: 1.613286 | Test Accuracy: 0.822924\n",
      "Step 9 | Training Loss: 0.675936 | Test Loss: 1.645203 | Test Accuracy: 0.797418\n",
      "Step 10 | Training Loss: 0.742172 | Test Loss: 1.872295 | Test Accuracy: 0.824388\n",
      "Step 11 | Training Loss: 0.859575 | Test Loss: 1.983410 | Test Accuracy: 0.430758\n",
      "Step 12 | Training Loss: 0.904955 | Test Loss: 1.984770 | Test Accuracy: 0.430758\n",
      "Step 13 | Training Loss: 0.910095 | Test Loss: 1.984725 | Test Accuracy: 0.430758\n",
      "Step 14 | Training Loss: 0.769668 | Test Loss: 1.984851 | Test Accuracy: 0.430758\n",
      "Step 15 | Training Loss: 0.889651 | Test Loss: 1.984447 | Test Accuracy: 0.430758\n",
      "Step 16 | Training Loss: 0.822867 | Test Loss: 1.983862 | Test Accuracy: 0.430758\n",
      "Step 17 | Training Loss: 0.859776 | Test Loss: 1.983562 | Test Accuracy: 0.430758\n",
      "Step 18 | Training Loss: 0.914309 | Test Loss: 1.983015 | Test Accuracy: 0.430758\n",
      "Step 19 | Training Loss: 0.872960 | Test Loss: 1.983495 | Test Accuracy: 0.430758\n",
      "Step 20 | Training Loss: 1.731604 | Test Loss: 1.982808 | Test Accuracy: 0.430758\n",
      "Step 21 | Training Loss: 1.278109 | Test Loss: 1.983141 | Test Accuracy: 0.430758\n",
      "Step 22 | Training Loss: 1.790943 | Test Loss: 1.984432 | Test Accuracy: 0.430758\n",
      "Step 23 | Training Loss: 0.930936 | Test Loss: 1.984040 | Test Accuracy: 0.430758\n",
      "Step 24 | Training Loss: 0.769428 | Test Loss: 1.983582 | Test Accuracy: 0.430758\n",
      "Step 25 | Training Loss: 0.809590 | Test Loss: 1.983551 | Test Accuracy: 0.430758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26 | Training Loss: 0.791287 | Test Loss: 1.983923 | Test Accuracy: 0.430758\n",
      "Step 27 | Training Loss: 0.815244 | Test Loss: 1.984711 | Test Accuracy: 0.430758\n",
      "Step 28 | Training Loss: 0.924163 | Test Loss: 1.985973 | Test Accuracy: 0.430758\n",
      "Step 29 | Training Loss: 1.081409 | Test Loss: 1.983518 | Test Accuracy: 0.430758\n",
      "Step 30 | Training Loss: 0.860965 | Test Loss: 1.983073 | Test Accuracy: 0.430758\n",
      "Step 31 | Training Loss: 0.852655 | Test Loss: 1.983710 | Test Accuracy: 0.430758\n",
      "Step 32 | Training Loss: 1.029282 | Test Loss: 1.982590 | Test Accuracy: 0.430758\n",
      "Step 33 | Training Loss: 0.890896 | Test Loss: 1.983730 | Test Accuracy: 0.430758\n",
      "Step 34 | Training Loss: 0.866184 | Test Loss: 1.983332 | Test Accuracy: 0.430758\n",
      "Step 35 | Training Loss: 1.113015 | Test Loss: 1.983774 | Test Accuracy: 0.430758\n",
      "Step 36 | Training Loss: 0.900605 | Test Loss: 1.983039 | Test Accuracy: 0.430758\n",
      "Step 37 | Training Loss: 0.913290 | Test Loss: 1.983125 | Test Accuracy: 0.430758\n",
      "Step 38 | Training Loss: 0.906300 | Test Loss: 1.984516 | Test Accuracy: 0.430758\n",
      "Step 39 | Training Loss: 0.874146 | Test Loss: 1.983837 | Test Accuracy: 0.430758\n",
      "Step 40 | Training Loss: 0.922881 | Test Loss: 1.985412 | Test Accuracy: 0.430758\n",
      "Step 41 | Training Loss: 0.951739 | Test Loss: 1.984222 | Test Accuracy: 0.430758\n",
      "Step 42 | Training Loss: 0.939307 | Test Loss: 1.984057 | Test Accuracy: 0.430758\n",
      "Step 43 | Training Loss: 0.766694 | Test Loss: 1.982008 | Test Accuracy: 0.430758\n",
      "Step 44 | Training Loss: 1.118070 | Test Loss: 1.982944 | Test Accuracy: 0.430758\n",
      "Step 45 | Training Loss: 2.803575 | Test Loss: 1.982586 | Test Accuracy: 0.430758\n",
      "Step 46 | Training Loss: 0.813211 | Test Loss: 1.983096 | Test Accuracy: 0.430758\n",
      "Step 47 | Training Loss: 1.045414 | Test Loss: 1.984092 | Test Accuracy: 0.430758\n",
      "Step 48 | Training Loss: 0.711259 | Test Loss: 1.983938 | Test Accuracy: 0.430758\n",
      "Step 49 | Training Loss: 2.035059 | Test Loss: 1.982571 | Test Accuracy: 0.430758\n",
      "Step 50 | Training Loss: 0.789331 | Test Loss: 1.982812 | Test Accuracy: 0.430758\n",
      "Step 51 | Training Loss: 0.892068 | Test Loss: 1.983834 | Test Accuracy: 0.430758\n",
      "Step 52 | Training Loss: 0.890743 | Test Loss: 1.982741 | Test Accuracy: 0.430758\n",
      "Step 53 | Training Loss: 1.702801 | Test Loss: 1.983741 | Test Accuracy: 0.430758\n",
      "Step 54 | Training Loss: 0.757661 | Test Loss: 1.984163 | Test Accuracy: 0.430758\n",
      "Step 55 | Training Loss: 0.961959 | Test Loss: 1.982829 | Test Accuracy: 0.430758\n",
      "Step 56 | Training Loss: 0.832373 | Test Loss: 1.984641 | Test Accuracy: 0.430758\n",
      "Step 57 | Training Loss: 0.881488 | Test Loss: 1.983254 | Test Accuracy: 0.430758\n",
      "Step 58 | Training Loss: 0.825428 | Test Loss: 1.984760 | Test Accuracy: 0.430758\n",
      "Step 59 | Training Loss: 0.808562 | Test Loss: 1.983602 | Test Accuracy: 0.430758\n",
      "Step 60 | Training Loss: 0.821172 | Test Loss: 1.982932 | Test Accuracy: 0.430758\n",
      "Step 61 | Training Loss: 0.799600 | Test Loss: 1.982528 | Test Accuracy: 0.430758\n",
      "Step 62 | Training Loss: 0.872899 | Test Loss: 1.982870 | Test Accuracy: 0.430758\n",
      "Step 63 | Training Loss: 0.960827 | Test Loss: 1.983679 | Test Accuracy: 0.430758\n",
      "Step 64 | Training Loss: 0.876315 | Test Loss: 1.983240 | Test Accuracy: 0.430758\n",
      "Step 65 | Training Loss: 0.811372 | Test Loss: 1.983320 | Test Accuracy: 0.430758\n",
      "Step 66 | Training Loss: 0.799083 | Test Loss: 1.985190 | Test Accuracy: 0.430758\n",
      "Step 67 | Training Loss: 0.848014 | Test Loss: 1.983503 | Test Accuracy: 0.430758\n",
      "Step 68 | Training Loss: 0.972572 | Test Loss: 1.981798 | Test Accuracy: 0.430758\n",
      "Step 69 | Training Loss: 1.112768 | Test Loss: 1.984186 | Test Accuracy: 0.430758\n",
      "Step 70 | Training Loss: 1.484217 | Test Loss: 1.983780 | Test Accuracy: 0.430758\n",
      "Step 71 | Training Loss: 0.956165 | Test Loss: 1.984695 | Test Accuracy: 0.430758\n",
      "Step 72 | Training Loss: 0.994180 | Test Loss: 1.983828 | Test Accuracy: 0.430758\n",
      "Step 73 | Training Loss: 0.804280 | Test Loss: 1.984547 | Test Accuracy: 0.430758\n",
      "Step 74 | Training Loss: 0.863556 | Test Loss: 1.982715 | Test Accuracy: 0.430758\n",
      "Step 75 | Training Loss: 0.809307 | Test Loss: 1.983248 | Test Accuracy: 0.430758\n",
      "Step 76 | Training Loss: 0.968097 | Test Loss: 1.983039 | Test Accuracy: 0.430758\n",
      "Step 77 | Training Loss: 0.894797 | Test Loss: 1.984038 | Test Accuracy: 0.430758\n",
      "Step 78 | Training Loss: 0.894225 | Test Loss: 1.984050 | Test Accuracy: 0.430758\n",
      "Step 79 | Training Loss: 1.018550 | Test Loss: 1.984489 | Test Accuracy: 0.430758\n",
      "Step 80 | Training Loss: 3.653718 | Test Loss: 1.985651 | Test Accuracy: 0.430758\n",
      "Step 81 | Training Loss: 0.815353 | Test Loss: 1.982859 | Test Accuracy: 0.430758\n",
      "Step 82 | Training Loss: 0.915482 | Test Loss: 1.985301 | Test Accuracy: 0.430758\n",
      "Step 83 | Training Loss: 0.911021 | Test Loss: 1.984066 | Test Accuracy: 0.430758\n",
      "Step 84 | Training Loss: 0.931032 | Test Loss: 1.983811 | Test Accuracy: 0.430758\n",
      "Step 85 | Training Loss: 0.927423 | Test Loss: 1.983703 | Test Accuracy: 0.430758\n",
      "Step 86 | Training Loss: 0.847653 | Test Loss: 1.984161 | Test Accuracy: 0.430758\n",
      "Step 87 | Training Loss: 0.898706 | Test Loss: 1.984035 | Test Accuracy: 0.430758\n",
      "Step 88 | Training Loss: 0.962122 | Test Loss: 1.983199 | Test Accuracy: 0.430758\n",
      "Step 89 | Training Loss: 1.780561 | Test Loss: 1.984575 | Test Accuracy: 0.430758\n",
      "Step 90 | Training Loss: 0.785646 | Test Loss: 1.982674 | Test Accuracy: 0.430758\n",
      "Step 91 | Training Loss: 0.867357 | Test Loss: 1.984003 | Test Accuracy: 0.430758\n",
      "Step 92 | Training Loss: 0.956170 | Test Loss: 1.983301 | Test Accuracy: 0.430758\n",
      "Step 93 | Training Loss: 0.833790 | Test Loss: 1.983376 | Test Accuracy: 0.430758\n",
      "Step 94 | Training Loss: 0.848547 | Test Loss: 1.983163 | Test Accuracy: 0.430758\n",
      "Step 95 | Training Loss: 1.493990 | Test Loss: 1.982841 | Test Accuracy: 0.430758\n",
      "Step 96 | Training Loss: 1.155536 | Test Loss: 1.983957 | Test Accuracy: 0.430758\n",
      "Step 97 | Training Loss: 0.827868 | Test Loss: 1.983275 | Test Accuracy: 0.430758\n",
      "Step 98 | Training Loss: 0.843791 | Test Loss: 1.983216 | Test Accuracy: 0.430758\n",
      "Step 99 | Training Loss: 0.862006 | Test Loss: 1.983390 | Test Accuracy: 0.430758\n",
      "Step 100 | Training Loss: 1.066970 | Test Loss: 1.983621 | Test Accuracy: 0.430758\n",
      "Current Layer Attributes - epochs:100 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.750288 | Test Loss: 1.766303 | Test Accuracy: 0.772445\n",
      "Step 2 | Training Loss: 0.647062 | Test Loss: 1.726847 | Test Accuracy: 0.787039\n",
      "Step 3 | Training Loss: 0.868565 | Test Loss: 1.743950 | Test Accuracy: 0.775772\n",
      "Step 4 | Training Loss: 0.747712 | Test Loss: 1.680271 | Test Accuracy: 0.796930\n",
      "Step 5 | Training Loss: 0.596043 | Test Loss: 1.791929 | Test Accuracy: 0.840224\n",
      "Step 6 | Training Loss: 0.718768 | Test Loss: 1.783594 | Test Accuracy: 0.824477\n",
      "Step 7 | Training Loss: 0.757530 | Test Loss: 1.752795 | Test Accuracy: 0.822835\n",
      "Step 8 | Training Loss: 0.855662 | Test Loss: 1.768846 | Test Accuracy: 0.831308\n",
      "Step 9 | Training Loss: 0.695158 | Test Loss: 1.790681 | Test Accuracy: 0.827892\n",
      "Step 10 | Training Loss: 0.673303 | Test Loss: 1.738356 | Test Accuracy: 0.832816\n",
      "Step 11 | Training Loss: 0.711490 | Test Loss: 1.732086 | Test Accuracy: 0.801721\n",
      "Step 12 | Training Loss: 0.582767 | Test Loss: 1.778083 | Test Accuracy: 0.818133\n",
      "Step 13 | Training Loss: 0.659584 | Test Loss: 1.771021 | Test Accuracy: 0.836364\n",
      "Step 14 | Training Loss: 0.793441 | Test Loss: 1.788492 | Test Accuracy: 0.846212\n",
      "Step 15 | Training Loss: 0.470695 | Test Loss: 1.777084 | Test Accuracy: 0.854862\n",
      "Step 16 | Training Loss: 0.771854 | Test Loss: 1.771151 | Test Accuracy: 0.853886\n",
      "Step 17 | Training Loss: 0.567461 | Test Loss: 1.757107 | Test Accuracy: 0.843062\n",
      "Step 18 | Training Loss: 0.583098 | Test Loss: 1.754340 | Test Accuracy: 0.863112\n",
      "Step 19 | Training Loss: 0.652927 | Test Loss: 1.758193 | Test Accuracy: 0.851623\n",
      "Step 20 | Training Loss: 0.984955 | Test Loss: 2.231523 | Test Accuracy: 0.570174\n",
      "Step 21 | Training Loss: 0.745776 | Test Loss: 1.852486 | Test Accuracy: 0.771602\n",
      "Step 22 | Training Loss: 0.650092 | Test Loss: 1.838868 | Test Accuracy: 0.767699\n",
      "Step 23 | Training Loss: 0.631504 | Test Loss: 1.806394 | Test Accuracy: 0.753504\n",
      "Step 24 | Training Loss: 0.638225 | Test Loss: 1.799652 | Test Accuracy: 0.753016\n",
      "Step 25 | Training Loss: 0.835073 | Test Loss: 1.882248 | Test Accuracy: 0.702537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26 | Training Loss: 0.659133 | Test Loss: 1.822496 | Test Accuracy: 0.866749\n",
      "Step 27 | Training Loss: 0.638845 | Test Loss: 1.808234 | Test Accuracy: 0.813343\n",
      "Step 28 | Training Loss: 0.661396 | Test Loss: 1.794670 | Test Accuracy: 0.805802\n",
      "Step 29 | Training Loss: 0.725138 | Test Loss: 1.809027 | Test Accuracy: 0.785043\n",
      "Step 30 | Training Loss: 0.507504 | Test Loss: 1.801346 | Test Accuracy: 0.805137\n",
      "Step 31 | Training Loss: 0.612397 | Test Loss: 1.806494 | Test Accuracy: 0.780696\n",
      "Step 32 | Training Loss: 0.622804 | Test Loss: 1.789462 | Test Accuracy: 0.801677\n",
      "Step 33 | Training Loss: 0.670715 | Test Loss: 1.790710 | Test Accuracy: 0.793426\n",
      "Step 34 | Training Loss: 0.746469 | Test Loss: 1.791449 | Test Accuracy: 0.785397\n",
      "Step 35 | Training Loss: 0.698536 | Test Loss: 1.795750 | Test Accuracy: 0.815472\n",
      "Step 36 | Training Loss: 0.792834 | Test Loss: 1.826619 | Test Accuracy: 0.777812\n",
      "Step 37 | Training Loss: 0.690955 | Test Loss: 1.814235 | Test Accuracy: 0.774574\n",
      "Step 38 | Training Loss: 1.011750 | Test Loss: 1.796239 | Test Accuracy: 0.774574\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 51 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 52 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 53 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 54 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 55 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 56 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 57 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 58 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 59 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 60 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 61 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 62 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 63 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 64 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 65 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 66 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 67 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 68 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 69 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 70 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 71 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 72 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 73 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 74 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 75 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 76 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 77 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 78 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 79 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 80 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 81 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 82 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 83 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 84 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 85 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 86 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 87 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 88 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 89 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 90 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 91 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 92 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 93 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 94 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 95 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 96 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 97 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 98 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 99 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 100 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:100 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.651517 | Test Loss: 1.828917 | Test Accuracy: 0.769473\n",
      "Step 2 | Training Loss: 0.543260 | Test Loss: 1.777358 | Test Accuracy: 0.777945\n",
      "Step 3 | Training Loss: 1.021051 | Test Loss: 1.751956 | Test Accuracy: 0.782825\n",
      "Step 4 | Training Loss: 0.528356 | Test Loss: 1.732447 | Test Accuracy: 0.792229\n",
      "Step 5 | Training Loss: 0.673236 | Test Loss: 1.725337 | Test Accuracy: 0.788236\n",
      "Step 6 | Training Loss: 0.588898 | Test Loss: 1.739628 | Test Accuracy: 0.791386\n",
      "Step 7 | Training Loss: 0.548145 | Test Loss: 1.798652 | Test Accuracy: 0.784954\n",
      "Step 8 | Training Loss: 0.739091 | Test Loss: 1.801346 | Test Accuracy: 0.816359\n",
      "Step 9 | Training Loss: 0.433607 | Test Loss: 1.805637 | Test Accuracy: 0.765747\n",
      "Step 10 | Training Loss: 0.514098 | Test Loss: 1.754044 | Test Accuracy: 0.782780\n",
      "Step 11 | Training Loss: 0.414679 | Test Loss: 1.802883 | Test Accuracy: 0.774752\n",
      "Step 12 | Training Loss: 1.202862 | Test Loss: 1.888731 | Test Accuracy: 0.796664\n",
      "Step 13 | Training Loss: 0.584162 | Test Loss: 1.795165 | Test Accuracy: 0.801854\n",
      "Step 14 | Training Loss: 0.548455 | Test Loss: 1.762850 | Test Accuracy: 0.782958\n",
      "Step 15 | Training Loss: 0.463499 | Test Loss: 1.768627 | Test Accuracy: 0.798749\n",
      "Step 16 | Training Loss: 0.504712 | Test Loss: 1.787808 | Test Accuracy: 0.784289\n",
      "Step 17 | Training Loss: 0.654179 | Test Loss: 1.856070 | Test Accuracy: 0.756787\n",
      "Step 18 | Training Loss: 0.585698 | Test Loss: 1.849795 | Test Accuracy: 0.747960\n",
      "Step 19 | Training Loss: 0.759328 | Test Loss: 1.734832 | Test Accuracy: 0.798039\n",
      "Step 20 | Training Loss: 0.515283 | Test Loss: 1.682141 | Test Accuracy: 0.785841\n",
      "Step 21 | Training Loss: 1.082518 | Test Loss: 1.818275 | Test Accuracy: 0.818000\n",
      "Step 22 | Training Loss: 1.099517 | Test Loss: 1.817052 | Test Accuracy: 0.815250\n",
      "Step 23 | Training Loss: 0.532437 | Test Loss: 1.724688 | Test Accuracy: 0.774175\n",
      "Step 24 | Training Loss: 0.499546 | Test Loss: 1.867432 | Test Accuracy: 0.836631\n",
      "Step 25 | Training Loss: 0.869029 | Test Loss: 1.943500 | Test Accuracy: 0.569819\n",
      "Step 26 | Training Loss: 0.818126 | Test Loss: 1.942912 | Test Accuracy: 0.472232\n",
      "Step 27 | Training Loss: 0.813853 | Test Loss: 1.942155 | Test Accuracy: 0.472010\n",
      "Step 28 | Training Loss: 1.307128 | Test Loss: 2.277375 | Test Accuracy: 0.597498\n",
      "Step 29 | Training Loss: 2.130126 | Test Loss: 1.957754 | Test Accuracy: 0.471966\n",
      "Step 30 | Training Loss: 0.636517 | Test Loss: 1.940228 | Test Accuracy: 0.595990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31 | Training Loss: 1.022943 | Test Loss: 1.940057 | Test Accuracy: 0.472010\n",
      "Step 32 | Training Loss: 0.763033 | Test Loss: 1.938905 | Test Accuracy: 0.472099\n",
      "Step 33 | Training Loss: 0.923087 | Test Loss: 1.938682 | Test Accuracy: 0.471833\n",
      "Step 34 | Training Loss: 1.741883 | Test Loss: 1.941228 | Test Accuracy: 0.471833\n",
      "Step 35 | Training Loss: 0.988240 | Test Loss: 1.959172 | Test Accuracy: 0.440472\n",
      "Step 36 | Training Loss: 0.830016 | Test Loss: 1.958054 | Test Accuracy: 0.607656\n",
      "Step 37 | Training Loss: 0.851921 | Test Loss: 1.956193 | Test Accuracy: 0.440073\n",
      "Step 38 | Training Loss: 0.939446 | Test Loss: 1.953362 | Test Accuracy: 0.440161\n",
      "Step 39 | Training Loss: 0.716382 | Test Loss: 1.955995 | Test Accuracy: 0.439984\n",
      "Step 40 | Training Loss: 0.770276 | Test Loss: 1.955936 | Test Accuracy: 0.611338\n",
      "Step 41 | Training Loss: 0.678908 | Test Loss: 1.953713 | Test Accuracy: 0.440916\n",
      "Step 42 | Training Loss: 0.783103 | Test Loss: 1.954714 | Test Accuracy: 0.442912\n",
      "Step 43 | Training Loss: 0.740929 | Test Loss: 1.953985 | Test Accuracy: 0.612580\n",
      "Step 44 | Training Loss: 0.750503 | Test Loss: 1.959151 | Test Accuracy: 0.443843\n",
      "Step 45 | Training Loss: 0.986035 | Test Loss: 1.953306 | Test Accuracy: 0.443799\n",
      "Step 46 | Training Loss: 0.800605 | Test Loss: 1.951733 | Test Accuracy: 0.611693\n",
      "Step 47 | Training Loss: 0.807420 | Test Loss: 1.952117 | Test Accuracy: 0.440916\n",
      "Step 48 | Training Loss: 0.747912 | Test Loss: 1.954062 | Test Accuracy: 0.440250\n",
      "Step 49 | Training Loss: 2.072486 | Test Loss: 1.983191 | Test Accuracy: 0.430758\n",
      "Step 50 | Training Loss: 0.854437 | Test Loss: 1.985703 | Test Accuracy: 0.430758\n",
      "Step 51 | Training Loss: 1.016147 | Test Loss: 1.984310 | Test Accuracy: 0.430758\n",
      "Step 52 | Training Loss: 1.002670 | Test Loss: 1.984561 | Test Accuracy: 0.430758\n",
      "Step 53 | Training Loss: 0.783948 | Test Loss: 1.984753 | Test Accuracy: 0.430758\n",
      "Step 54 | Training Loss: 1.132125 | Test Loss: 1.983715 | Test Accuracy: 0.430758\n",
      "Step 55 | Training Loss: 1.299521 | Test Loss: 1.984279 | Test Accuracy: 0.430758\n",
      "Step 56 | Training Loss: 0.796239 | Test Loss: 1.986120 | Test Accuracy: 0.430758\n",
      "Step 57 | Training Loss: 1.212182 | Test Loss: 1.985583 | Test Accuracy: 0.430758\n",
      "Step 58 | Training Loss: 0.824144 | Test Loss: 1.984662 | Test Accuracy: 0.430758\n",
      "Step 59 | Training Loss: 0.956369 | Test Loss: 1.983013 | Test Accuracy: 0.430758\n",
      "Step 60 | Training Loss: 0.900129 | Test Loss: 1.984958 | Test Accuracy: 0.430758\n",
      "Step 61 | Training Loss: 0.794104 | Test Loss: 1.983640 | Test Accuracy: 0.430758\n",
      "Step 62 | Training Loss: 0.893369 | Test Loss: 1.981539 | Test Accuracy: 0.430758\n",
      "Step 63 | Training Loss: 1.025594 | Test Loss: 1.983228 | Test Accuracy: 0.430758\n",
      "Step 64 | Training Loss: 0.785849 | Test Loss: 1.983625 | Test Accuracy: 0.430758\n",
      "Step 65 | Training Loss: 0.958946 | Test Loss: 1.983260 | Test Accuracy: 0.430758\n",
      "Step 66 | Training Loss: 0.890386 | Test Loss: 1.983239 | Test Accuracy: 0.430758\n",
      "Step 67 | Training Loss: 0.881610 | Test Loss: 1.984528 | Test Accuracy: 0.430758\n",
      "Step 68 | Training Loss: 0.909239 | Test Loss: 1.986610 | Test Accuracy: 0.430758\n",
      "Step 69 | Training Loss: 0.740849 | Test Loss: 1.983127 | Test Accuracy: 0.430758\n",
      "Step 70 | Training Loss: 1.033432 | Test Loss: 1.985423 | Test Accuracy: 0.430758\n",
      "Step 71 | Training Loss: 0.812515 | Test Loss: 1.982952 | Test Accuracy: 0.430758\n",
      "Step 72 | Training Loss: 1.201918 | Test Loss: 1.981963 | Test Accuracy: 0.430758\n",
      "Step 73 | Training Loss: 1.189837 | Test Loss: 1.983733 | Test Accuracy: 0.430758\n",
      "Step 74 | Training Loss: 0.665072 | Test Loss: 1.983512 | Test Accuracy: 0.430758\n",
      "Step 75 | Training Loss: 1.083968 | Test Loss: 1.983643 | Test Accuracy: 0.430758\n",
      "Step 76 | Training Loss: 0.918380 | Test Loss: 1.983033 | Test Accuracy: 0.430758\n",
      "Step 77 | Training Loss: 0.778823 | Test Loss: 1.982499 | Test Accuracy: 0.430758\n",
      "Step 78 | Training Loss: 2.965478 | Test Loss: 1.983783 | Test Accuracy: 0.430758\n",
      "Step 79 | Training Loss: 0.835272 | Test Loss: 1.984875 | Test Accuracy: 0.430758\n",
      "Step 80 | Training Loss: 0.696145 | Test Loss: 1.984396 | Test Accuracy: 0.430758\n",
      "Step 81 | Training Loss: 0.873895 | Test Loss: 1.983926 | Test Accuracy: 0.430758\n",
      "Step 82 | Training Loss: 0.757105 | Test Loss: 1.985659 | Test Accuracy: 0.430758\n",
      "Step 83 | Training Loss: 0.838776 | Test Loss: 1.984767 | Test Accuracy: 0.430758\n",
      "Step 84 | Training Loss: 0.860749 | Test Loss: 1.983230 | Test Accuracy: 0.430758\n",
      "Step 85 | Training Loss: 0.739023 | Test Loss: 1.983811 | Test Accuracy: 0.430758\n",
      "Step 86 | Training Loss: 0.978107 | Test Loss: 1.982721 | Test Accuracy: 0.430758\n",
      "Step 87 | Training Loss: 0.906984 | Test Loss: 1.986158 | Test Accuracy: 0.430758\n",
      "Step 88 | Training Loss: 0.867661 | Test Loss: 1.983047 | Test Accuracy: 0.430758\n",
      "Step 89 | Training Loss: 0.821262 | Test Loss: 1.983583 | Test Accuracy: 0.430758\n",
      "Step 90 | Training Loss: 1.164664 | Test Loss: 1.982763 | Test Accuracy: 0.430758\n",
      "Step 91 | Training Loss: 0.930258 | Test Loss: 1.984375 | Test Accuracy: 0.430758\n",
      "Step 92 | Training Loss: 1.088535 | Test Loss: 1.985142 | Test Accuracy: 0.430758\n",
      "Step 93 | Training Loss: 0.901716 | Test Loss: 1.983507 | Test Accuracy: 0.430758\n",
      "Step 94 | Training Loss: 0.970030 | Test Loss: 1.984051 | Test Accuracy: 0.430758\n",
      "Step 95 | Training Loss: 0.914833 | Test Loss: 1.984576 | Test Accuracy: 0.430758\n",
      "Step 96 | Training Loss: 1.069104 | Test Loss: 1.983826 | Test Accuracy: 0.430758\n",
      "Step 97 | Training Loss: 0.748188 | Test Loss: 1.985223 | Test Accuracy: 0.430758\n",
      "Step 98 | Training Loss: 1.144984 | Test Loss: 1.983908 | Test Accuracy: 0.430758\n",
      "Step 99 | Training Loss: 0.924059 | Test Loss: 1.982747 | Test Accuracy: 0.430758\n",
      "Step 100 | Training Loss: 0.815257 | Test Loss: 1.983590 | Test Accuracy: 0.430758\n",
      "Current Layer Attributes - epochs:100 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.591807 | Test Loss: 1.829239 | Test Accuracy: 0.725071\n",
      "Step 2 | Training Loss: 0.501600 | Test Loss: 1.914927 | Test Accuracy: 0.758428\n",
      "Step 3 | Training Loss: 0.711217 | Test Loss: 1.771825 | Test Accuracy: 0.802963\n",
      "Step 4 | Training Loss: 0.779015 | Test Loss: 1.898986 | Test Accuracy: 0.721700\n",
      "Step 5 | Training Loss: 0.564623 | Test Loss: 1.862335 | Test Accuracy: 0.837917\n",
      "Step 6 | Training Loss: 0.984832 | Test Loss: 1.798104 | Test Accuracy: 0.762775\n",
      "Step 7 | Training Loss: 0.661963 | Test Loss: 1.797837 | Test Accuracy: 0.725559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-77cc9c2cea4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#    hidden_layers_arr = [2, 4, 6, 10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-77cc9c2cea4f>\u001b[0m in \u001b[0;36mHyperparameters\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a12595fa14d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, net, h, f)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                              net.keep_prob:1})\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a12595fa14d8>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                   feed_dict={net.x: x_train[i,:], \n\u001b[1;32m     40\u001b[0m                                                              \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                                              net.keep_prob:1})\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [100]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:07.377960Z",
     "start_time": "2017-05-15T19:22:07.371814Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:09.339353Z",
     "start_time": "2017-05-15T19:22:09.335786Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:11.523958Z",
     "start_time": "2017-05-15T19:22:11.519617Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:13.219644Z",
     "start_time": "2017-05-15T19:22:13.208227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.870694</td>\n",
       "      <td>0.902014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.943880</td>\n",
       "      <td>0.888086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.927766</td>\n",
       "      <td>0.866749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.959994</td>\n",
       "      <td>0.843506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943562</td>\n",
       "      <td>0.837961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.921337</td>\n",
       "      <td>0.837917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.914351</td>\n",
       "      <td>0.836631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.913161</td>\n",
       "      <td>0.832150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.959438</td>\n",
       "      <td>0.823589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954517</td>\n",
       "      <td>0.821017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.892364</td>\n",
       "      <td>0.803096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956501</td>\n",
       "      <td>0.789966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "4     100               4              4     0.870694    0.902014\n",
       "5     100               8              4     0.943880    0.888086\n",
       "9     100               8              6     0.927766    0.866749\n",
       "8     100               4              6     0.959994    0.843506\n",
       "2     100              16              2     0.943562    0.837961\n",
       "11    100              32              6     0.921337    0.837917\n",
       "10    100              16              6     0.914351    0.836631\n",
       "1     100               8              2     0.913161    0.832150\n",
       "6     100              16              4     0.959438    0.823589\n",
       "0     100               4              2     0.954517    0.821017\n",
       "7     100              32              4     0.892364    0.803096\n",
       "3     100              32              2     0.956501    0.789966"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:20.724364Z",
     "start_time": "2017-05-15T19:22:20.709858Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:23.020244Z",
     "start_time": "2017-05-15T19:22:22.958587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T19:22:26.226453Z",
     "start_time": "2017-05-15T19:22:25.948718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.7354  0.2646]\n",
      " [ 0.0266  0.9734]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57d4207dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
