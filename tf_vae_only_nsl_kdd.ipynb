{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:52.260580Z",
     "start_time": "2017-05-30T21:39:51.874037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:52.341983Z",
     "start_time": "2017-05-30T21:39:52.261945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:52.348034Z",
     "start_time": "2017-05-30T21:39:52.343537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:52.354045Z",
     "start_time": "2017-05-30T21:39:52.349385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:53.240442Z",
     "start_time": "2017-05-30T21:39:52.355483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:54.258061Z",
     "start_time": "2017-05-30T21:39:53.241975Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:54.625913Z",
     "start_time": "2017-05-30T21:39:54.259623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.x, self.x_hat) #tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            loss = tf.where(tf.is_nan(self.regularized_loss), 1e-2, self.regularized_loss)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T21:39:54.872296Z",
     "start_time": "2017-05-30T21:39:54.627459Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    \n",
    "                    def train_batch():\n",
    "                        nonlocal train_loss\n",
    "                        _, train_loss = sess.run([net.train_op, \n",
    "                                                  net.regularized_loss, \n",
    "                                                  ], #net.summary_op\n",
    "                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                             net.y_: y_train[i,:], \n",
    "                                                             net.keep_prob:1})\n",
    "\n",
    "                    train_batch()\n",
    "                    \n",
    "                    count = 10\n",
    "                    if((train_loss > 1e4 or np.isnan(train_loss) ) and epoch > 1 and count < 1):\n",
    "                        print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                        net.saver.restore(sess, \n",
    "                                          tf.train.latest_checkpoint('dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                     .format(epochs,h,f)))\n",
    "                        train_batch()\n",
    "                        count -= 1\n",
    "                        \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    #if(train_loss > 1e9):\n",
    "                    \n",
    "                    #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "                    \n",
    "                #print(\"\")\n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                \n",
    "                accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                               net.pred, \n",
    "                                                               net.actual, net.y], \n",
    "                                                              feed_dict={net.x: preprocess.x_test, \n",
    "                                                                         net.y_: preprocess.y_test, \n",
    "                                                                         net.keep_prob:1})\n",
    "                #print(\"*************** \\n\")\n",
    "                print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, test_loss, accuracy))\n",
    "                #print(\"*************** \\n\")\n",
    "                #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "                \n",
    "                if accuracy > Train.best_acc_global:\n",
    "                    Train.best_acc_global = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    \n",
    "                if accuracy > Train.best_acc:\n",
    "                   \n",
    "                    #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                    #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                    #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                    #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "                    \n",
    "                    Train.best_acc = accuracy\n",
    "                    if not (np.isnan(train_loss)):\n",
    "                        net.saver.save(sess, \n",
    "                                   \"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                   .format(epochs,h,f), \n",
    "                                   global_step = epoch, \n",
    "                                   write_meta_graph=False)\n",
    "                    \n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):\n",
    "                                              (curr_pred, \n",
    "                                               Train.result(epochs, f, h,valid_accuracy, accuracy, time.perf_counter() - start_time))})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.850940Z",
     "start_time": "2017-05-30T21:39:54.873791Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.580269 | Test Loss: 1.404408 | Test Accuracy: 0.788902\n",
      "Step 2 | Training Loss: 0.821346 | Test Loss: 1.267025 | Test Accuracy: 0.814807\n",
      "Step 3 | Training Loss: 0.265805 | Test Loss: 1.278444 | Test Accuracy: 0.787305\n",
      "Step 4 | Training Loss: 0.252674 | Test Loss: 1.161019 | Test Accuracy: 0.792273\n",
      "Step 5 | Training Loss: 0.193352 | Test Loss: 1.166501 | Test Accuracy: 0.795821\n",
      "Step 6 | Training Loss: 0.164578 | Test Loss: 1.360453 | Test Accuracy: 0.828691\n",
      "Step 7 | Training Loss: 0.522839 | Test Loss: 1.293808 | Test Accuracy: 0.776304\n",
      "Step 8 | Training Loss: 0.755994 | Test Loss: 1.192017 | Test Accuracy: 0.775328\n",
      "Step 9 | Training Loss: 0.143565 | Test Loss: 1.219596 | Test Accuracy: 0.780429\n",
      "Step 10 | Training Loss: 0.116609 | Test Loss: 1.183763 | Test Accuracy: 0.753637\n",
      "Step 11 | Training Loss: 2.908867 | Test Loss: 1.166580 | Test Accuracy: 0.761577\n",
      "Step 12 | Training Loss: 0.188159 | Test Loss: 1.157394 | Test Accuracy: 0.756831\n",
      "Step 13 | Training Loss: 0.158427 | Test Loss: 1.306363 | Test Accuracy: 0.711675\n",
      "Step 14 | Training Loss: 0.517419 | Test Loss: 1.236189 | Test Accuracy: 0.723474\n",
      "Step 15 | Training Loss: 0.158268 | Test Loss: 1.482440 | Test Accuracy: 0.677830\n",
      "Step 16 | Training Loss: 0.170549 | Test Loss: 1.476319 | Test Accuracy: 0.660131\n",
      "Step 17 | Training Loss: 0.246486 | Test Loss: 1.549528 | Test Accuracy: 0.689807\n",
      "Step 18 | Training Loss: 0.102702 | Test Loss: 1.450241 | Test Accuracy: 0.635690\n",
      "Step 19 | Training Loss: 0.241895 | Test Loss: 1.465413 | Test Accuracy: 0.616749\n",
      "Step 20 | Training Loss: 0.169590 | Test Loss: 1.446439 | Test Accuracy: 0.619322\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.465003 | Test Loss: 1.735448 | Test Accuracy: 0.743391\n",
      "Step 2 | Training Loss: 0.379481 | Test Loss: 1.608198 | Test Accuracy: 0.778655\n",
      "Step 3 | Training Loss: 0.596253 | Test Loss: 1.580246 | Test Accuracy: 0.775772\n",
      "Step 4 | Training Loss: 0.762012 | Test Loss: 1.562598 | Test Accuracy: 0.760646\n",
      "Step 5 | Training Loss: 0.291776 | Test Loss: 1.559338 | Test Accuracy: 0.758827\n",
      "Step 6 | Training Loss: 0.321678 | Test Loss: 1.546688 | Test Accuracy: 0.786817\n",
      "Step 7 | Training Loss: 0.260153 | Test Loss: 1.545335 | Test Accuracy: 0.759315\n",
      "Step 8 | Training Loss: 0.369456 | Test Loss: 1.534654 | Test Accuracy: 0.757896\n",
      "Step 9 | Training Loss: 0.590493 | Test Loss: 1.535841 | Test Accuracy: 0.759315\n",
      "Step 10 | Training Loss: 0.507048 | Test Loss: 1.523062 | Test Accuracy: 0.757363\n",
      "Step 11 | Training Loss: 0.473466 | Test Loss: 1.667273 | Test Accuracy: 0.733898\n",
      "Step 12 | Training Loss: 0.582708 | Test Loss: 1.703787 | Test Accuracy: 0.757807\n",
      "Step 13 | Training Loss: 0.310021 | Test Loss: 1.626973 | Test Accuracy: 0.763263\n",
      "Step 14 | Training Loss: 0.501268 | Test Loss: 1.600283 | Test Accuracy: 0.761666\n",
      "Step 15 | Training Loss: 0.440761 | Test Loss: 1.720572 | Test Accuracy: 0.802874\n",
      "Step 16 | Training Loss: 0.460725 | Test Loss: 1.649311 | Test Accuracy: 0.754125\n",
      "Step 17 | Training Loss: 0.335111 | Test Loss: 1.647113 | Test Accuracy: 0.754214\n",
      "Step 18 | Training Loss: 0.431378 | Test Loss: 1.646713 | Test Accuracy: 0.741572\n",
      "Step 19 | Training Loss: 0.586778 | Test Loss: 1.641384 | Test Accuracy: 0.733321\n",
      "Step 20 | Training Loss: 0.315248 | Test Loss: 1.631473 | Test Accuracy: 0.738600\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.433937 | Test Loss: 1.474709 | Test Accuracy: 0.790277\n",
      "Step 2 | Training Loss: 0.340418 | Test Loss: 1.470293 | Test Accuracy: 0.773776\n",
      "Step 3 | Training Loss: 0.653756 | Test Loss: 1.347320 | Test Accuracy: 0.794491\n",
      "Step 4 | Training Loss: 0.216052 | Test Loss: 1.373391 | Test Accuracy: 0.764638\n",
      "Step 5 | Training Loss: 0.116866 | Test Loss: 1.460027 | Test Accuracy: 0.714558\n",
      "Step 6 | Training Loss: 0.199852 | Test Loss: 1.422132 | Test Accuracy: 0.714647\n",
      "Step 7 | Training Loss: 0.196777 | Test Loss: 1.417169 | Test Accuracy: 0.719659\n",
      "Step 8 | Training Loss: 0.178443 | Test Loss: 1.423488 | Test Accuracy: 0.778034\n",
      "Step 9 | Training Loss: 0.569038 | Test Loss: 1.434487 | Test Accuracy: 0.751198\n",
      "Step 10 | Training Loss: 0.184334 | Test Loss: 1.447867 | Test Accuracy: 0.756742\n",
      "Step 11 | Training Loss: 0.073888 | Test Loss: 1.424771 | Test Accuracy: 0.751730\n",
      "Step 12 | Training Loss: 0.122018 | Test Loss: 1.405609 | Test Accuracy: 0.743036\n",
      "Step 13 | Training Loss: 0.445308 | Test Loss: 1.408231 | Test Accuracy: 0.762553\n",
      "Step 14 | Training Loss: 0.086717 | Test Loss: 1.405540 | Test Accuracy: 0.774885\n",
      "Step 15 | Training Loss: 0.077405 | Test Loss: 1.404110 | Test Accuracy: 0.760380\n",
      "Step 16 | Training Loss: 0.114672 | Test Loss: 1.424299 | Test Accuracy: 0.782381\n",
      "Step 17 | Training Loss: 196.674500 | Test Loss: 1.545494 | Test Accuracy: 0.757541\n",
      "Step 18 | Training Loss: 0.445224 | Test Loss: 1.559484 | Test Accuracy: 0.684351\n",
      "Step 19 | Training Loss: 0.236481 | Test Loss: 1.508173 | Test Accuracy: 0.783978\n",
      "Step 20 | Training Loss: 0.138481 | Test Loss: 1.472521 | Test Accuracy: 0.694730\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.245442 | Test Loss: 1.173038 | Test Accuracy: 0.799104\n",
      "Step 2 | Training Loss: 0.376273 | Test Loss: 1.117073 | Test Accuracy: 0.827049\n",
      "Step 3 | Training Loss: 0.210510 | Test Loss: 1.410861 | Test Accuracy: 0.768675\n",
      "Step 4 | Training Loss: 0.231752 | Test Loss: 1.318896 | Test Accuracy: 0.721345\n",
      "Step 5 | Training Loss: 0.135311 | Test Loss: 1.295265 | Test Accuracy: 0.750000\n",
      "Step 6 | Training Loss: 0.634335 | Test Loss: 1.365779 | Test Accuracy: 0.715800\n",
      "Step 7 | Training Loss: 0.054926 | Test Loss: 1.398737 | Test Accuracy: 0.760735\n",
      "Step 8 | Training Loss: 0.031981 | Test Loss: 1.383818 | Test Accuracy: 0.764638\n",
      "Step 9 | Training Loss: 0.122096 | Test Loss: 1.407067 | Test Accuracy: 0.752174\n",
      "Step 10 | Training Loss: 0.085292 | Test Loss: 1.404310 | Test Accuracy: 0.747072\n",
      "Step 11 | Training Loss: 0.120137 | Test Loss: 1.399515 | Test Accuracy: 0.754258\n",
      "Step 12 | Training Loss: 0.112811 | Test Loss: 1.395251 | Test Accuracy: 0.760380\n",
      "Step 13 | Training Loss: 0.334153 | Test Loss: 1.695637 | Test Accuracy: 0.829888\n",
      "Step 14 | Training Loss: 0.366769 | Test Loss: 1.663789 | Test Accuracy: 0.782026\n",
      "Step 15 | Training Loss: 0.208129 | Test Loss: 1.655320 | Test Accuracy: 0.821549\n",
      "Step 16 | Training Loss: 0.353454 | Test Loss: 1.646993 | Test Accuracy: 0.806822\n",
      "Step 17 | Training Loss: 0.423896 | Test Loss: 1.639420 | Test Accuracy: 0.827360\n",
      "Step 18 | Training Loss: 0.606779 | Test Loss: 1.655767 | Test Accuracy: 0.773731\n",
      "Step 19 | Training Loss: 0.207645 | Test Loss: 1.643543 | Test Accuracy: 0.797374\n",
      "Step 20 | Training Loss: 0.274727 | Test Loss: 1.634986 | Test Accuracy: 0.796487\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.447110 | Test Loss: 1.665224 | Test Accuracy: 0.810193\n",
      "Step 2 | Training Loss: 0.445768 | Test Loss: 1.470425 | Test Accuracy: 0.791120\n",
      "Step 3 | Training Loss: 0.358229 | Test Loss: 1.575881 | Test Accuracy: 0.799237\n",
      "Step 4 | Training Loss: 0.265410 | Test Loss: 1.579912 | Test Accuracy: 0.779276\n",
      "Step 5 | Training Loss: 0.406391 | Test Loss: 1.639806 | Test Accuracy: 0.768852\n",
      "Step 6 | Training Loss: 0.367902 | Test Loss: 1.644211 | Test Accuracy: 0.774086\n",
      "Step 7 | Training Loss: 0.251151 | Test Loss: 1.656250 | Test Accuracy: 0.822392\n",
      "Step 8 | Training Loss: 0.348368 | Test Loss: 1.797507 | Test Accuracy: 0.646957\n",
      "Step 9 | Training Loss: 0.298084 | Test Loss: 1.790403 | Test Accuracy: 0.770715\n",
      "Step 10 | Training Loss: 0.321189 | Test Loss: 1.553974 | Test Accuracy: 0.777058\n",
      "Step 11 | Training Loss: 0.395895 | Test Loss: 1.769783 | Test Accuracy: 0.757896\n",
      "Step 12 | Training Loss: 0.509730 | Test Loss: 1.595123 | Test Accuracy: 0.806068\n",
      "Step 13 | Training Loss: 0.582627 | Test Loss: 1.633072 | Test Accuracy: 0.772667\n",
      "Step 14 | Training Loss: 0.656428 | Test Loss: 1.747869 | Test Accuracy: 0.775106\n",
      "Step 15 | Training Loss: 0.485339 | Test Loss: 1.689280 | Test Accuracy: 0.701428\n",
      "Step 16 | Training Loss: 0.270314 | Test Loss: 1.634843 | Test Accuracy: 0.713937\n",
      "Step 17 | Training Loss: 0.463856 | Test Loss: 1.612256 | Test Accuracy: 0.727244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18 | Training Loss: 0.624205 | Test Loss: 1.608998 | Test Accuracy: 0.706529\n",
      "Step 19 | Training Loss: 0.540695 | Test Loss: 1.799898 | Test Accuracy: 0.764638\n",
      "Step 20 | Training Loss: 0.475173 | Test Loss: 1.778522 | Test Accuracy: 0.789345\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.304840 | Test Loss: 1.648718 | Test Accuracy: 0.823501\n",
      "Step 2 | Training Loss: 0.241734 | Test Loss: 1.516910 | Test Accuracy: 0.842486\n",
      "Step 3 | Training Loss: 0.509535 | Test Loss: 1.372849 | Test Accuracy: 0.771913\n",
      "Step 4 | Training Loss: 0.493071 | Test Loss: 1.598997 | Test Accuracy: 0.784510\n",
      "Step 5 | Training Loss: 0.282821 | Test Loss: 1.504609 | Test Accuracy: 0.793116\n",
      "Step 6 | Training Loss: 0.234035 | Test Loss: 1.575509 | Test Accuracy: 0.799681\n",
      "Step 7 | Training Loss: 0.315545 | Test Loss: 1.462222 | Test Accuracy: 0.809439\n",
      "Step 8 | Training Loss: 1.130453 | Test Loss: 1.648114 | Test Accuracy: 0.809351\n",
      "Step 9 | Training Loss: 0.575370 | Test Loss: 1.692291 | Test Accuracy: 0.831529\n",
      "Step 10 | Training Loss: 0.415829 | Test Loss: 1.771609 | Test Accuracy: 0.895848\n",
      "Step 11 | Training Loss: 0.539344 | Test Loss: 1.787859 | Test Accuracy: 0.888707\n",
      "Step 12 | Training Loss: 0.427444 | Test Loss: 1.734499 | Test Accuracy: 0.876375\n",
      "Step 13 | Training Loss: 0.457967 | Test Loss: 1.742271 | Test Accuracy: 0.863778\n",
      "Step 14 | Training Loss: 0.630082 | Test Loss: 1.697190 | Test Accuracy: 0.867637\n",
      "Step 15 | Training Loss: 0.603544 | Test Loss: 1.616237 | Test Accuracy: 0.759005\n",
      "Step 16 | Training Loss: 0.651942 | Test Loss: 1.759138 | Test Accuracy: 0.830154\n",
      "Step 17 | Training Loss: 0.536610 | Test Loss: 1.770857 | Test Accuracy: 0.830687\n",
      "Step 18 | Training Loss: 0.971134 | Test Loss: 1.859192 | Test Accuracy: 0.858277\n",
      "Step 19 | Training Loss: 0.729319 | Test Loss: 1.903088 | Test Accuracy: 0.879524\n",
      "Step 20 | Training Loss: 0.810143 | Test Loss: 1.914127 | Test Accuracy: 0.862535\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.563777 | Test Loss: 1.705304 | Test Accuracy: 0.797330\n",
      "Step 2 | Training Loss: 0.477667 | Test Loss: 1.691309 | Test Accuracy: 0.776393\n",
      "Step 3 | Training Loss: 0.539110 | Test Loss: 1.933967 | Test Accuracy: 0.774397\n",
      "Step 4 | Training Loss: 0.396342 | Test Loss: 1.569024 | Test Accuracy: 0.765525\n",
      "Step 5 | Training Loss: 1.339697 | Test Loss: 1.693759 | Test Accuracy: 0.786329\n",
      "Step 6 | Training Loss: 0.482631 | Test Loss: 1.715830 | Test Accuracy: 0.701694\n",
      "Step 7 | Training Loss: 0.770868 | Test Loss: 1.701391 | Test Accuracy: 0.697303\n",
      "Step 8 | Training Loss: 0.334199 | Test Loss: 1.591473 | Test Accuracy: 0.649042\n",
      "Step 9 | Training Loss: 0.365125 | Test Loss: 1.590411 | Test Accuracy: 0.645671\n",
      "Step 10 | Training Loss: 0.516601 | Test Loss: 1.574488 | Test Accuracy: 0.689851\n",
      "Step 11 | Training Loss: 0.340405 | Test Loss: 1.551895 | Test Accuracy: 0.733233\n",
      "Step 12 | Training Loss: 0.478476 | Test Loss: 1.551481 | Test Accuracy: 0.739088\n",
      "Step 13 | Training Loss: 0.441584 | Test Loss: 1.515807 | Test Accuracy: 0.746274\n",
      "Step 14 | Training Loss: 0.394126 | Test Loss: 1.533844 | Test Accuracy: 0.782514\n",
      "Step 15 | Training Loss: 0.396203 | Test Loss: 1.720682 | Test Accuracy: 0.746629\n",
      "Step 16 | Training Loss: 0.691698 | Test Loss: 1.933369 | Test Accuracy: 0.794402\n",
      "Step 17 | Training Loss: 0.788774 | Test Loss: 1.914809 | Test Accuracy: 0.827848\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.462608 | Test Loss: 1.710319 | Test Accuracy: 0.813831\n",
      "Step 2 | Training Loss: 0.530490 | Test Loss: 1.680461 | Test Accuracy: 0.784244\n",
      "Step 3 | Training Loss: 0.475528 | Test Loss: 1.904229 | Test Accuracy: 0.739886\n",
      "Step 4 | Training Loss: 0.466433 | Test Loss: 1.852019 | Test Accuracy: 0.777058\n",
      "Step 5 | Training Loss: 0.463895 | Test Loss: 1.749880 | Test Accuracy: 0.796576\n",
      "Step 6 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 11 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 12 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 13 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 14 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 15 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.722354 | Test Loss: 1.817114 | Test Accuracy: 0.791519\n",
      "Step 2 | Training Loss: 0.633199 | Test Loss: 1.809560 | Test Accuracy: 0.790632\n",
      "Step 3 | Training Loss: 0.608329 | Test Loss: 1.739721 | Test Accuracy: 0.803806\n",
      "Step 4 | Training Loss: 0.555763 | Test Loss: 1.650893 | Test Accuracy: 0.794180\n",
      "Step 5 | Training Loss: 0.640551 | Test Loss: 1.647851 | Test Accuracy: 0.791341\n",
      "Step 6 | Training Loss: 0.518221 | Test Loss: 1.657568 | Test Accuracy: 0.788502\n",
      "Step 7 | Training Loss: 0.702460 | Test Loss: 1.756663 | Test Accuracy: 0.802919\n",
      "Step 8 | Training Loss: 0.579335 | Test Loss: 1.636288 | Test Accuracy: 0.788148\n",
      "Step 9 | Training Loss: 0.540761 | Test Loss: 1.646082 | Test Accuracy: 0.796930\n",
      "Step 10 | Training Loss: 0.621299 | Test Loss: 1.775717 | Test Accuracy: 0.779365\n",
      "Step 11 | Training Loss: 0.822038 | Test Loss: 1.825802 | Test Accuracy: 0.755811\n",
      "Step 12 | Training Loss: 0.645189 | Test Loss: 1.793699 | Test Accuracy: 0.826340\n",
      "Step 13 | Training Loss: 0.542426 | Test Loss: 1.782974 | Test Accuracy: 0.853398\n",
      "Step 14 | Training Loss: 0.493501 | Test Loss: 1.764758 | Test Accuracy: 0.833393\n",
      "Step 15 | Training Loss: 0.575447 | Test Loss: 1.792099 | Test Accuracy: 0.867992\n",
      "Step 16 | Training Loss: 0.471552 | Test Loss: 1.786087 | Test Accuracy: 0.808597\n",
      "Step 17 | Training Loss: 0.647909 | Test Loss: 1.773505 | Test Accuracy: 0.850603\n",
      "Step 18 | Training Loss: 0.685997 | Test Loss: 1.760304 | Test Accuracy: 0.860406\n",
      "Step 19 | Training Loss: 1.154601 | Test Loss: 1.769601 | Test Accuracy: 0.877661\n",
      "Step 20 | Training Loss: 0.662634 | Test Loss: 1.770200 | Test Accuracy: 0.860185\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.676829 | Test Loss: 1.820012 | Test Accuracy: 0.736737\n",
      "Step 2 | Training Loss: 0.598587 | Test Loss: 1.812470 | Test Accuracy: 0.773820\n",
      "Step 3 | Training Loss: 0.501678 | Test Loss: 1.785696 | Test Accuracy: 0.765924\n",
      "Step 4 | Training Loss: 0.497546 | Test Loss: 1.769442 | Test Accuracy: 0.797197\n",
      "Step 5 | Training Loss: 0.696595 | Test Loss: 1.729902 | Test Accuracy: 0.794313\n",
      "Step 6 | Training Loss: 0.789691 | Test Loss: 1.884458 | Test Accuracy: 0.697303\n",
      "Step 7 | Training Loss: 0.609991 | Test Loss: 1.831060 | Test Accuracy: 0.725204\n",
      "Step 8 | Training Loss: 0.670290 | Test Loss: 1.815455 | Test Accuracy: 0.820573\n",
      "Step 9 | Training Loss: 0.875750 | Test Loss: 1.928188 | Test Accuracy: 0.734918\n",
      "Step 10 | Training Loss: 0.938098 | Test Loss: 1.840269 | Test Accuracy: 0.831396\n",
      "Step 11 | Training Loss: 0.768148 | Test Loss: 1.814388 | Test Accuracy: 0.827005\n",
      "Step 12 | Training Loss: 0.629830 | Test Loss: 1.707659 | Test Accuracy: 0.800612\n",
      "Step 13 | Training Loss: 0.814006 | Test Loss: 1.697232 | Test Accuracy: 0.815960\n",
      "Step 14 | Training Loss: 0.787267 | Test Loss: 1.680387 | Test Accuracy: 0.812456\n",
      "Step 15 | Training Loss: 0.870303 | Test Loss: 1.849847 | Test Accuracy: 0.830287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.838213 | Test Loss: 1.832120 | Test Accuracy: 0.777812\n",
      "Step 17 | Training Loss: 0.855038 | Test Loss: 1.831678 | Test Accuracy: 0.769650\n",
      "Step 18 | Training Loss: 0.626167 | Test Loss: 1.833996 | Test Accuracy: 0.771558\n",
      "Step 19 | Training Loss: 1.572298 | Test Loss: 1.823073 | Test Accuracy: 0.744633\n",
      "Step 20 | Training Loss: 0.621201 | Test Loss: 1.828681 | Test Accuracy: 0.753682\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.596528 | Test Loss: 1.804222 | Test Accuracy: 0.806246\n",
      "Step 2 | Training Loss: 0.850831 | Test Loss: 1.762337 | Test Accuracy: 0.769429\n",
      "Step 3 | Training Loss: 0.530447 | Test Loss: 1.807188 | Test Accuracy: 0.774219\n",
      "Step 4 | Training Loss: 0.557573 | Test Loss: 1.777008 | Test Accuracy: 0.778744\n",
      "Step 5 | Training Loss: 1.409683 | Test Loss: 1.783661 | Test Accuracy: 0.755323\n",
      "Step 6 | Training Loss: 0.583990 | Test Loss: 1.803169 | Test Accuracy: 0.766989\n",
      "Step 7 | Training Loss: 0.692630 | Test Loss: 1.766086 | Test Accuracy: 0.804294\n",
      "Step 8 | Training Loss: 0.599808 | Test Loss: 1.756309 | Test Accuracy: 0.798971\n",
      "Step 9 | Training Loss: 1.624646 | Test Loss: 1.869939 | Test Accuracy: 0.655607\n",
      "Step 10 | Training Loss: 1.112585 | Test Loss: 1.818276 | Test Accuracy: 0.770759\n",
      "Step 11 | Training Loss: 0.692213 | Test Loss: 1.827741 | Test Accuracy: 0.747072\n",
      "Step 12 | Training Loss: 0.685054 | Test Loss: 1.827564 | Test Accuracy: 0.730749\n",
      "Step 13 | Training Loss: 1.028536 | Test Loss: 1.855765 | Test Accuracy: 0.711498\n",
      "Step 14 | Training Loss: 0.809389 | Test Loss: 1.984170 | Test Accuracy: 0.430758\n",
      "Step 15 | Training Loss: 0.720552 | Test Loss: 1.983057 | Test Accuracy: 0.430758\n",
      "Step 16 | Training Loss: 0.788474 | Test Loss: 1.982053 | Test Accuracy: 0.430758\n",
      "Step 17 | Training Loss: 2.625085 | Test Loss: 1.983152 | Test Accuracy: 0.430758\n",
      "Step 18 | Training Loss: 0.914840 | Test Loss: 1.982378 | Test Accuracy: 0.430758\n",
      "Step 19 | Training Loss: 0.877172 | Test Loss: 1.982898 | Test Accuracy: 0.430758\n",
      "Step 20 | Training Loss: 0.952841 | Test Loss: 1.983255 | Test Accuracy: 0.430758\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.694481 | Test Loss: 1.802238 | Test Accuracy: 0.798483\n",
      "Step 2 | Training Loss: 1.574193 | Test Loss: 1.783768 | Test Accuracy: 0.750798\n",
      "Step 3 | Training Loss: 0.702055 | Test Loss: 1.754194 | Test Accuracy: 0.789123\n",
      "Step 4 | Training Loss: 0.508529 | Test Loss: 1.738200 | Test Accuracy: 0.786817\n",
      "Step 5 | Training Loss: 0.681560 | Test Loss: 1.779337 | Test Accuracy: 0.768453\n",
      "Step 6 | Training Loss: 0.614269 | Test Loss: 1.735457 | Test Accuracy: 0.761977\n",
      "Step 7 | Training Loss: 1.180254 | Test Loss: 1.812152 | Test Accuracy: 0.796576\n",
      "Step 8 | Training Loss: 0.597519 | Test Loss: 1.772696 | Test Accuracy: 0.821993\n",
      "Step 9 | Training Loss: 0.590208 | Test Loss: 1.807523 | Test Accuracy: 0.819508\n",
      "Step 10 | Training Loss: 0.714864 | Test Loss: 1.760364 | Test Accuracy: 0.768719\n",
      "Step 11 | Training Loss: 0.658034 | Test Loss: 1.783944 | Test Accuracy: 0.775994\n",
      "Step 12 | Training Loss: 0.559476 | Test Loss: 1.769005 | Test Accuracy: 0.743568\n",
      "Step 13 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 14 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 15 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [20]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.859884Z",
     "start_time": "2017-05-30T22:04:13.853477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.869692Z",
     "start_time": "2017-05-30T22:04:13.861572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.892154Z",
     "start_time": "2017-05-30T22:04:13.871631Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.908805Z",
     "start_time": "2017-05-30T22:04:13.893720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.942610</td>\n",
       "      <td>0.895848</td>\n",
       "      <td>58.912110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.909906</td>\n",
       "      <td>0.877661</td>\n",
       "      <td>155.164080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.927290</td>\n",
       "      <td>0.831396</td>\n",
       "      <td>83.473590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.949992</td>\n",
       "      <td>0.829888</td>\n",
       "      <td>48.012730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955945</td>\n",
       "      <td>0.828691</td>\n",
       "      <td>19.451182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.844975</td>\n",
       "      <td>0.827848</td>\n",
       "      <td>101.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.959200</td>\n",
       "      <td>0.822392</td>\n",
       "      <td>40.326999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.933323</td>\n",
       "      <td>0.821993</td>\n",
       "      <td>69.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>0.813831</td>\n",
       "      <td>6.107059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.947214</td>\n",
       "      <td>0.806246</td>\n",
       "      <td>8.385378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.938800</td>\n",
       "      <td>0.802874</td>\n",
       "      <td>52.740351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.794491</td>\n",
       "      <td>10.471723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  time_taken\n",
       "5      20               8              4     0.942610    0.895848   58.912110\n",
       "8      20               4              6     0.909906    0.877661  155.164080\n",
       "9      20               8              6     0.927290    0.831396   83.473590\n",
       "3      20              32              2     0.949992    0.829888   48.012730\n",
       "0      20               4              2     0.955945    0.828691   19.451182\n",
       "6      20              16              4     0.844975    0.827848  101.502000\n",
       "4      20               4              4     0.959200    0.822392   40.326999\n",
       "11     20              32              6     0.933323    0.821993   69.000163\n",
       "7      20              32              4     0.953008    0.813831    6.107059\n",
       "10     20              16              6     0.947214    0.806246    8.385378\n",
       "1      20               8              2     0.938800    0.802874   52.740351\n",
       "2      20              16              2     0.966423    0.794491   10.471723"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:13.985922Z",
     "start_time": "2017-05-30T22:04:13.911268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:14.053539Z",
     "start_time": "2017-05-30T22:04:13.987940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-30T22:04:15.537871Z",
     "start_time": "2017-05-30T22:04:14.055241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8751  0.1249]\n",
      " [ 0.0767  0.9233]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nljsObmZmljsObmZmljv/H4vmdJtsL1agAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0dad280d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
