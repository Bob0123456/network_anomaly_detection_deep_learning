{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:00.286371Z",
     "start_time": "2017-06-23T22:04:59.905785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:00.411907Z",
     "start_time": "2017-06-23T22:05:00.288050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:00.418754Z",
     "start_time": "2017-06-23T22:05:00.413463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:00.425293Z",
     "start_time": "2017-06-23T22:05:00.420145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:01.310478Z",
     "start_time": "2017-06-23T22:05:00.426944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    x_test_ = np.hstack((x_test_, np.random.normal(size = (x_test_.shape[0], y_train.shape[1]))))\n",
    "\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:02.317447Z",
     "start_time": "2017-06-23T22:05:01.312225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:02.619843Z",
     "start_time": "2017-06-23T22:05:02.318969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.x, self.x_hat) #tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            loss = tf.where(tf.is_nan(self.regularized_loss), 1e-2, self.regularized_loss)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate= self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-23T22:05:02.898138Z",
     "start_time": "2017-06-23T22:05:02.621576Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h, f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                      net.regularized_loss, \n",
    "                                                      ], #net.summary_op\n",
    "                                                      feed_dict={net.x: x_train[i,:], \n",
    "                                                                 net.y_: y_train[i,:], \n",
    "                                                                 net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "\n",
    "                        count = 10\n",
    "                        if((train_loss > 1e4 or np.isnan(train_loss) ) and epoch > 1 and count > 1):\n",
    "                            print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "                            count -= 1\n",
    "\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        #if(train_loss > 1e9):\n",
    "\n",
    "                        #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "\n",
    "                    #print(\"\")\n",
    "                    valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "\n",
    "                    accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    accuracy_, test_loss_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                                           net.pred, \n",
    "                                                                                           net.actual, net.y], \n",
    "                                                                                          feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                                                     net.y_: preprocess.y_test_, \n",
    "                                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #print(\"*************** \\n\")\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}, {:.6f}\".format(epoch, train_loss, test_loss, accuracy, accuracy_))\n",
    "                    #print(\"*************** \\n\")\n",
    "                    #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "\n",
    "                        #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                        #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                        #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "                        Train.best_acc = accuracy\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_vae_only_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(epochs,h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(epochs*len(lrs),f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(epochs*len(lrs), f, h,valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-23T22:05:00.487Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "\n",
    "\n",
    "    def start_training():\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "        \n",
    "        features_arr = [8, 32, 122]\n",
    "        hidden_layers_arr = [3, 5]\n",
    "\n",
    "        epochs = [15]\n",
    "        lrs = [1e-2, 1e-2, 1e-3]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "\n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "            \n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/tf_vae_only_nsl_kdd_all.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/tf_vae_only_nsl_kdd_all.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/tf_vae_only_nsl_kdd_all.pkl\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-23T22:05:00.497Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:8\n",
      "Step 1 | Training Loss: 0.466580 | Test Loss: 1.281702 | Test Accuracy: 0.777502, 0.584641\n",
      "Step 2 | Training Loss: 0.327729 | Test Loss: 1.212881 | Test Accuracy: 0.836498, 0.691983\n",
      "Step 3 | Training Loss: 0.258304 | Test Loss: 1.156412 | Test Accuracy: 0.751375, 0.623207\n",
      "Step 4 | Training Loss: 0.183690 | Test Loss: 1.244708 | Test Accuracy: 0.761932, 0.592743\n",
      "Step 5 | Training Loss: 0.310612 | Test Loss: 1.284593 | Test Accuracy: 0.797019, 0.640084\n",
      "Step 6 | Training Loss: 0.205378 | Test Loss: 1.148312 | Test Accuracy: 0.761977, 0.561435\n",
      "Step 7 | Training Loss: 0.138747 | Test Loss: 1.374767 | Test Accuracy: 0.786817, 0.611308\n",
      "Step 8 | Training Loss: 0.477039 | Test Loss: 1.456718 | Test Accuracy: 0.758827, 0.552658\n",
      "Step 9 | Training Loss: 0.817190 | Test Loss: 1.404510 | Test Accuracy: 0.773377, 0.606413\n",
      "Step 10 | Training Loss: 0.601043 | Test Loss: 1.744578 | Test Accuracy: 0.747427, 0.544641\n",
      "Step 11 | Training Loss: 0.465709 | Test Loss: 1.723312 | Test Accuracy: 0.815428, 0.738987\n",
      "Step 12 | Training Loss: 0.569337 | Test Loss: 1.767562 | Test Accuracy: 0.800213, 0.797384\n",
      "Step 13 | Training Loss: 0.434146 | Test Loss: 1.949355 | Test Accuracy: 0.771336, 0.747089\n",
      "Step 14 | Training Loss: 2.093661 | Test Loss: 1.887502 | Test Accuracy: 0.786285, 0.776456\n",
      "Step 15 | Training Loss: 0.366190 | Test Loss: 1.754913 | Test Accuracy: 0.775994, 0.746245\n",
      "Step 1 | Training Loss: 0.510946 | Test Loss: 2.141979 | Test Accuracy: 0.813831, 0.749367\n",
      "Step 2 | Training Loss: 0.248753 | Test Loss: 1.881378 | Test Accuracy: 0.733455, 0.715865\n",
      "Step 3 | Training Loss: 0.375091 | Test Loss: 1.861478 | Test Accuracy: 0.742681, 0.730717\n",
      "Step 4 | Training Loss: 1.386018 | Test Loss: 1.893756 | Test Accuracy: 0.770804, 0.751392\n",
      "Step 5 | Training Loss: 0.497552 | Test Loss: 1.878475 | Test Accuracy: 0.777945, 0.755696\n",
      "Step 6 | Training Loss: 0.344283 | Test Loss: 1.928534 | Test Accuracy: 0.790144, 0.740084\n",
      "Step 7 | Training Loss: 0.362550 | Test Loss: 1.910101 | Test Accuracy: 0.787039, 0.738312\n",
      "Step 8 | Training Loss: 0.427629 | Test Loss: 1.885852 | Test Accuracy: 0.780607, 0.747173\n",
      "Step 9 | Training Loss: 0.658517 | Test Loss: 2.093803 | Test Accuracy: 0.738511, 0.731561\n",
      "Step 10 | Training Loss: 0.431542 | Test Loss: 2.239663 | Test Accuracy: 0.693754, 0.738650\n",
      "Step 11 | Training Loss: 0.615431 | Test Loss: 2.126559 | Test Accuracy: 0.776393, 0.779578\n",
      "Step 12 | Training Loss: 1.905465 | Test Loss: 2.106302 | Test Accuracy: 0.780030, 0.790970\n",
      "Step 13 | Training Loss: 0.422247 | Test Loss: 2.142132 | Test Accuracy: 0.783401, 0.801266\n",
      "Step 14 | Training Loss: 0.408650 | Test Loss: 2.166371 | Test Accuracy: 0.772578, 0.799578\n",
      "Step 15 | Training Loss: 0.419967 | Test Loss: 2.174444 | Test Accuracy: 0.769118, 0.800759\n",
      "Step 1 | Training Loss: 0.534446 | Test Loss: 2.164477 | Test Accuracy: 0.768408, 0.795443\n",
      "Step 2 | Training Loss: 0.381560 | Test Loss: 2.167856 | Test Accuracy: 0.769429, 0.795190\n",
      "Step 3 | Training Loss: 0.386024 | Test Loss: 2.155625 | Test Accuracy: 0.769163, 0.794430\n",
      "Step 4 | Training Loss: 0.403655 | Test Loss: 2.145700 | Test Accuracy: 0.767300, 0.792996\n",
      "Step 5 | Training Loss: 0.737428 | Test Loss: 2.151630 | Test Accuracy: 0.767388, 0.790127\n",
      "Step 6 | Training Loss: 0.378216 | Test Loss: 2.147331 | Test Accuracy: 0.768763, 0.791477\n",
      "Step 7 | Training Loss: 0.512645 | Test Loss: 2.143640 | Test Accuracy: 0.764949, 0.792574\n",
      "Step 8 | Training Loss: 0.371181 | Test Loss: 2.136536 | Test Accuracy: 0.764372, 0.790633\n",
      "Step 9 | Training Loss: 0.438433 | Test Loss: 2.134202 | Test Accuracy: 0.761400, 0.789536\n",
      "Step 10 | Training Loss: 1.211945 | Test Loss: 2.129064 | Test Accuracy: 0.760690, 0.791646\n",
      "Step 11 | Training Loss: 0.684320 | Test Loss: 2.133195 | Test Accuracy: 0.761755, 0.788945\n",
      "Step 12 | Training Loss: 0.349455 | Test Loss: 2.122443 | Test Accuracy: 0.759892, 0.789283\n",
      "Step 13 | Training Loss: 0.475886 | Test Loss: 2.121171 | Test Accuracy: 0.760424, 0.789367\n",
      "Step 14 | Training Loss: 0.345084 | Test Loss: 2.130308 | Test Accuracy: 0.758650, 0.788439\n",
      "Step 15 | Training Loss: 0.359720 | Test Loss: 2.115753 | Test Accuracy: 0.760335, 0.789198\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:32\n",
      "Step 1 | Training Loss: 0.489146 | Test Loss: 1.660409 | Test Accuracy: 0.767211, 0.565907\n",
      "Step 2 | Training Loss: 0.316529 | Test Loss: 1.603432 | Test Accuracy: 0.768586, 0.587848\n",
      "Step 3 | Training Loss: 0.192978 | Test Loss: 1.456684 | Test Accuracy: 0.757541, 0.647004\n",
      "Step 4 | High Training Loss: 6135533499946881937047552.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_3/model-2\n",
      "Step 4 | Training Loss: 0.504038 | Test Loss: 1.591055 | Test Accuracy: 0.844438, 0.712658\n",
      "Step 5 | Training Loss: 0.353823 | Test Loss: 1.449204 | Test Accuracy: 0.808951, 0.647764\n",
      "Step 6 | Training Loss: 2.129357 | Test Loss: 1.428123 | Test Accuracy: 0.737402, 0.622869\n",
      "Step 7 | Training Loss: 0.333436 | Test Loss: 1.618623 | Test Accuracy: 0.758073, 0.577131\n",
      "Step 8 | High Training Loss: 14275518464.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_3/model-4\n",
      "Step 8 | Training Loss: 0.362167 | Test Loss: 1.473950 | Test Accuracy: 0.815073, 0.655190\n",
      "Step 9 | Training Loss: 0.229432 | Test Loss: 1.658623 | Test Accuracy: 0.798084, 0.626076\n",
      "Step 10 | Training Loss: 0.408633 | Test Loss: 1.732892 | Test Accuracy: 0.790853, 0.618987\n",
      "Step 11 | Training Loss: 0.552276 | Test Loss: 1.657668 | Test Accuracy: 0.780296, 0.592996\n",
      "Step 12 | Training Loss: 1.287282 | Test Loss: 1.654604 | Test Accuracy: 0.785087, 0.601941\n",
      "Step 13 | Training Loss: 0.348396 | Test Loss: 1.649375 | Test Accuracy: 0.857567, 0.772067\n",
      "Step 14 | Training Loss: 0.293674 | Test Loss: 1.647056 | Test Accuracy: 0.857612, 0.763291\n",
      "Step 15 | Training Loss: 0.407004 | Test Loss: 1.659032 | Test Accuracy: 0.818888, 0.740422\n",
      "Step 1 | Training Loss: 0.503746 | Test Loss: 1.640911 | Test Accuracy: 0.833082, 0.748270\n",
      "Step 2 | Training Loss: 0.296609 | Test Loss: 1.630675 | Test Accuracy: 0.794890, 0.689030\n",
      "Step 3 | Training Loss: 0.756593 | Test Loss: 1.628944 | Test Accuracy: 0.781671, 0.689283\n",
      "Step 4 | Training Loss: 0.489983 | Test Loss: 1.675866 | Test Accuracy: 0.832239, 0.704810\n",
      "Step 5 | Training Loss: 0.427728 | Test Loss: 1.679167 | Test Accuracy: 0.821194, 0.675106\n",
      "Step 6 | Training Loss: 0.393689 | Test Loss: 1.706628 | Test Accuracy: 0.816803, 0.672405\n",
      "Step 7 | Training Loss: 0.527774 | Test Loss: 1.732413 | Test Accuracy: 0.811480, 0.665316\n",
      "Step 8 | Training Loss: 0.837028 | Test Loss: 1.835611 | Test Accuracy: 0.837651, 0.718734\n",
      "Step 9 | Training Loss: 0.466291 | Test Loss: 1.781051 | Test Accuracy: 0.799548, 0.649367\n",
      "Step 10 | Training Loss: 0.502476 | Test Loss: 1.789404 | Test Accuracy: 0.812323, 0.666667\n",
      "Step 11 | Training Loss: 0.648485 | Test Loss: 1.702687 | Test Accuracy: 0.765880, 0.579072\n",
      "Step 12 | Training Loss: 0.452064 | Test Loss: 1.521816 | Test Accuracy: 0.731148, 0.524051\n",
      "Step 13 | High Training Loss: 4977641.500000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_3/model-14\n",
      "Step 13 | Training Loss: 0.345898 | Test Loss: 1.690334 | Test Accuracy: 0.848430, 0.731392\n",
      "Step 14 | Training Loss: 0.412084 | Test Loss: 1.687685 | Test Accuracy: 0.848385, 0.766076\n",
      "Step 15 | Training Loss: 0.523837 | Test Loss: 1.685710 | Test Accuracy: 0.815782, 0.741688\n",
      "Step 1 | Training Loss: 0.350072 | Test Loss: 1.678433 | Test Accuracy: 0.817202, 0.740506\n",
      "Step 2 | Training Loss: 0.373453 | Test Loss: 1.676690 | Test Accuracy: 0.812012, 0.735696\n",
      "Step 3 | Training Loss: 0.256205 | Test Loss: 1.675887 | Test Accuracy: 0.812278, 0.733924\n",
      "Step 4 | Training Loss: 0.391201 | Test Loss: 1.674341 | Test Accuracy: 0.810238, 0.734008\n",
      "Step 5 | Training Loss: 0.400272 | Test Loss: 1.674345 | Test Accuracy: 0.808774, 0.732911\n",
      "Step 6 | Training Loss: 0.372734 | Test Loss: 1.673977 | Test Accuracy: 0.810282, 0.730549\n",
      "Step 7 | Training Loss: 0.328692 | Test Loss: 1.676010 | Test Accuracy: 0.810238, 0.731899\n",
      "Step 8 | Training Loss: 0.349108 | Test Loss: 1.678218 | Test Accuracy: 0.806467, 0.723038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.595067 | Test Loss: 1.693841 | Test Accuracy: 0.802431, 0.710295\n",
      "Step 10 | Training Loss: 0.291139 | Test Loss: 1.702601 | Test Accuracy: 0.802431, 0.707089\n",
      "Step 11 | Training Loss: 0.429782 | Test Loss: 1.722091 | Test Accuracy: 0.803229, 0.699831\n",
      "Step 12 | Training Loss: 0.383797 | Test Loss: 1.722682 | Test Accuracy: 0.799858, 0.694937\n",
      "Step 13 | Training Loss: 0.340998 | Test Loss: 1.706284 | Test Accuracy: 0.810193, 0.706160\n",
      "Step 14 | Training Loss: 0.396882 | Test Loss: 1.742761 | Test Accuracy: 0.809705, 0.699831\n",
      "Step 15 | Training Loss: 0.434638 | Test Loss: 1.684222 | Test Accuracy: 0.780784, 0.727426\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.667167 | Test Loss: 1.817722 | Test Accuracy: 0.786107, 0.604388\n",
      "Step 2 | Training Loss: 0.579525 | Test Loss: 1.781187 | Test Accuracy: 0.786684, 0.597975\n",
      "Step 3 | Training Loss: 0.726052 | Test Loss: 1.771572 | Test Accuracy: 0.831751, 0.685823\n",
      "Step 4 | Training Loss: 1.265303 | Test Loss: 1.759200 | Test Accuracy: 0.816537, 0.665738\n",
      "Step 5 | Training Loss: 0.538072 | Test Loss: 1.752015 | Test Accuracy: 0.752085, 0.535359\n",
      "Step 6 | Training Loss: 0.920448 | Test Loss: 1.999781 | Test Accuracy: 0.569242, 0.818397\n",
      "Step 7 | Training Loss: 0.874789 | Test Loss: 1.983406 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 0.873277 | Test Loss: 1.983696 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 1.602883 | Test Loss: 1.983408 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 0.910653 | Test Loss: 1.983219 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.747335 | Test Loss: 1.983732 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.927940 | Test Loss: 1.983570 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.819480 | Test Loss: 1.984151 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 0.821762 | Test Loss: 1.983907 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.837472 | Test Loss: 1.983761 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 1 | Training Loss: 0.902159 | Test Loss: 1.983335 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 2 | Training Loss: 0.822892 | Test Loss: 1.983410 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 3 | Training Loss: 0.882166 | Test Loss: 1.982921 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 4 | Training Loss: 0.838803 | Test Loss: 1.984051 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 5 | Training Loss: 1.056573 | Test Loss: 1.982880 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 6 | Training Loss: 0.844582 | Test Loss: 1.983579 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 7 | Training Loss: 1.455242 | Test Loss: 1.983299 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 1.223576 | Test Loss: 1.983720 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 0.816708 | Test Loss: 1.983121 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 0.811901 | Test Loss: 1.983263 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.750036 | Test Loss: 1.984424 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.776555 | Test Loss: 1.982738 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 1.045106 | Test Loss: 1.983675 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 0.996699 | Test Loss: 1.984206 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.954868 | Test Loss: 1.983935 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 1 | Training Loss: 0.896116 | Test Loss: 1.983553 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 2 | Training Loss: 0.824585 | Test Loss: 1.983336 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 3 | Training Loss: 0.871113 | Test Loss: 1.983308 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 4 | Training Loss: 0.787256 | Test Loss: 1.983511 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 5 | Training Loss: 0.839678 | Test Loss: 1.983651 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 6 | Training Loss: 0.900030 | Test Loss: 1.983489 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 7 | Training Loss: 0.956953 | Test Loss: 1.983388 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 0.719396 | Test Loss: 1.983413 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 0.713499 | Test Loss: 1.983590 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 0.781042 | Test Loss: 1.983302 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 1.664594 | Test Loss: 1.983303 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.807166 | Test Loss: 1.983485 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.809210 | Test Loss: 1.983416 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 0.722812 | Test Loss: 1.983130 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 1.068260 | Test Loss: 1.983119 | Test Accuracy: 0.430758, 0.181603\n",
      "Current Layer Attributes - epochs:15 hidden layers:5 features count:8\n",
      "Step 1 | Training Loss: 0.597581 | Test Loss: 1.752566 | Test Accuracy: 0.808597, 0.643966\n",
      "Step 2 | Training Loss: 0.581372 | Test Loss: 1.722026 | Test Accuracy: 0.778478, 0.581519\n",
      "Step 3 | Training Loss: 0.422483 | Test Loss: 1.682107 | Test Accuracy: 0.802830, 0.630802\n",
      "Step 4 | Training Loss: 0.960513 | Test Loss: 1.679529 | Test Accuracy: 0.847099, 0.735190\n",
      "Step 5 | Training Loss: 0.503252 | Test Loss: 1.595562 | Test Accuracy: 0.781938, 0.597806\n",
      "Step 6 | Training Loss: 0.480759 | Test Loss: 1.658828 | Test Accuracy: 0.806334, 0.687511\n",
      "Step 7 | Training Loss: 0.546631 | Test Loss: 1.857155 | Test Accuracy: 0.749157, 0.590042\n",
      "Step 8 | Training Loss: 0.585724 | Test Loss: 1.836426 | Test Accuracy: 0.690383, 0.532405\n",
      "Step 9 | Training Loss: 1.432551 | Test Loss: 1.819623 | Test Accuracy: 0.715978, 0.578565\n",
      "Step 10 | Training Loss: 0.419457 | Test Loss: 1.777581 | Test Accuracy: 0.701739, 0.588354\n",
      "Step 11 | Training Loss: 0.616759 | Test Loss: 1.854815 | Test Accuracy: 0.750266, 0.583544\n",
      "Step 12 | Training Loss: 0.897155 | Test Loss: 1.823959 | Test Accuracy: 0.740286, 0.711224\n",
      "Step 13 | Training Loss: 0.446064 | Test Loss: 1.839080 | Test Accuracy: 0.650816, 0.592743\n",
      "Step 14 | Training Loss: 0.546508 | Test Loss: 1.817997 | Test Accuracy: 0.736338, 0.692743\n",
      "Step 15 | Training Loss: 0.368486 | Test Loss: 1.787312 | Test Accuracy: 0.736914, 0.575781\n",
      "Step 1 | Training Loss: 0.455602 | Test Loss: 1.755461 | Test Accuracy: 0.740330, 0.685654\n",
      "Step 2 | Training Loss: 0.482596 | Test Loss: 1.732645 | Test Accuracy: 0.777901, 0.697553\n",
      "Step 3 | Training Loss: 0.710553 | Test Loss: 1.947085 | Test Accuracy: 0.832150, 0.805823\n",
      "Step 4 | Training Loss: 0.826857 | Test Loss: 1.944703 | Test Accuracy: 0.831175, 0.803966\n",
      "Step 5 | Training Loss: 0.625548 | Test Loss: 1.929038 | Test Accuracy: 0.880988, 0.808776\n",
      "Step 6 | Training Loss: 0.983289 | Test Loss: 1.926581 | Test Accuracy: 0.885735, 0.820591\n",
      "Step 7 | Training Loss: 0.607520 | Test Loss: 1.924547 | Test Accuracy: 0.880145, 0.808354\n",
      "Step 8 | Training Loss: 0.823671 | Test Loss: 1.915980 | Test Accuracy: 0.845680, 0.731477\n",
      "Step 9 | Training Loss: 0.883561 | Test Loss: 1.921464 | Test Accuracy: 0.856148, 0.760506\n",
      "Step 10 | Training Loss: 0.807717 | Test Loss: 1.913435 | Test Accuracy: 0.880012, 0.798397\n",
      "Step 11 | Training Loss: 0.783978 | Test Loss: 1.915576 | Test Accuracy: 0.882452, 0.806835\n",
      "Step 12 | Training Loss: 1.746370 | Test Loss: 1.916999 | Test Accuracy: 0.882763, 0.806414\n",
      "Step 13 | Training Loss: 0.715918 | Test Loss: 1.919264 | Test Accuracy: 0.882585, 0.805232\n",
      "Step 14 | Training Loss: 0.637952 | Test Loss: 1.926377 | Test Accuracy: 0.882630, 0.804895\n",
      "Step 15 | Training Loss: 0.681519 | Test Loss: 1.925605 | Test Accuracy: 0.874113, 0.790127\n",
      "Step 1 | Training Loss: 0.965113 | Test Loss: 1.925056 | Test Accuracy: 0.875000, 0.792152\n",
      "Step 2 | Training Loss: 1.008325 | Test Loss: 1.924803 | Test Accuracy: 0.874556, 0.790380\n",
      "Step 3 | Training Loss: 0.689804 | Test Loss: 1.925299 | Test Accuracy: 0.872516, 0.787426\n",
      "Step 4 | Training Loss: 1.011064 | Test Loss: 1.928415 | Test Accuracy: 0.872649, 0.787426\n",
      "Step 5 | Training Loss: 0.672837 | Test Loss: 1.927723 | Test Accuracy: 0.870830, 0.785654\n",
      "Step 6 | Training Loss: 0.579737 | Test Loss: 1.927167 | Test Accuracy: 0.870830, 0.784979\n",
      "Step 7 | Training Loss: 0.918654 | Test Loss: 1.926154 | Test Accuracy: 0.870875, 0.782616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: 0.655106 | Test Loss: 1.925077 | Test Accuracy: 0.870520, 0.781603\n",
      "Step 9 | Training Loss: 0.832828 | Test Loss: 1.925123 | Test Accuracy: 0.870431, 0.781350\n",
      "Step 10 | Training Loss: 1.708123 | Test Loss: 1.924730 | Test Accuracy: 0.868346, 0.777300\n",
      "Step 11 | Training Loss: 0.839824 | Test Loss: 1.924186 | Test Accuracy: 0.865286, 0.770042\n",
      "Step 12 | Training Loss: 0.721813 | Test Loss: 1.923875 | Test Accuracy: 0.866439, 0.772911\n",
      "Step 13 | Training Loss: 1.203005 | Test Loss: 1.926491 | Test Accuracy: 0.857834, 0.753671\n",
      "Step 14 | Training Loss: 0.979997 | Test Loss: 1.928997 | Test Accuracy: 0.847764, 0.731730\n",
      "Step 15 | Training Loss: 0.821829 | Test Loss: 1.930167 | Test Accuracy: 0.835433, 0.711814\n",
      "Current Layer Attributes - epochs:15 hidden layers:5 features count:32\n",
      "Step 1 | Training Loss: 0.588801 | Test Loss: 1.713101 | Test Accuracy: 0.793870, 0.611899\n",
      "Step 2 | Training Loss: 0.831626 | Test Loss: 1.616760 | Test Accuracy: 0.764638, 0.559072\n",
      "Step 3 | Training Loss: 0.447000 | Test Loss: 1.691075 | Test Accuracy: 0.761755, 0.563207\n",
      "Step 4 | Training Loss: 0.555394 | Test Loss: 1.561300 | Test Accuracy: 0.770138, 0.570380\n",
      "Step 5 | Training Loss: 0.638233 | Test Loss: 1.824031 | Test Accuracy: 0.712385, 0.489705\n",
      "Step 6 | Training Loss: 0.629333 | Test Loss: 1.815880 | Test Accuracy: 0.854241, 0.734852\n",
      "Step 7 | Training Loss: 0.792525 | Test Loss: 1.846450 | Test Accuracy: 0.856813, 0.734008\n",
      "Step 8 | Training Loss: 1.619825 | Test Loss: 1.923492 | Test Accuracy: 0.845502, 0.715021\n",
      "Step 9 | Training Loss: 2.258904 | Test Loss: 1.842348 | Test Accuracy: 0.838804, 0.712236\n",
      "Step 10 | Training Loss: 0.706021 | Test Loss: 1.846802 | Test Accuracy: 0.818577, 0.683376\n",
      "Step 11 | Training Loss: 0.636541 | Test Loss: 1.837333 | Test Accuracy: 0.792850, 0.617215\n",
      "Step 12 | Training Loss: 0.803108 | Test Loss: 1.899476 | Test Accuracy: 0.799193, 0.627257\n",
      "Step 13 | Training Loss: 0.507278 | Test Loss: 1.854714 | Test Accuracy: 0.586409, 0.393924\n",
      "Step 14 | Training Loss: 0.633005 | Test Loss: 1.858504 | Test Accuracy: 0.728886, 0.550380\n",
      "Step 15 | Training Loss: 0.951898 | Test Loss: 1.955408 | Test Accuracy: 0.748758, 0.605232\n",
      "Step 1 | Training Loss: 0.947754 | Test Loss: 1.949866 | Test Accuracy: 0.753105, 0.617637\n",
      "Step 2 | Training Loss: 0.849581 | Test Loss: 1.957257 | Test Accuracy: 0.739886, 0.624388\n",
      "Step 3 | Training Loss: 0.810574 | Test Loss: 1.957831 | Test Accuracy: 0.739886, 0.624810\n",
      "Step 4 | Training Loss: 0.747367 | Test Loss: 1.955520 | Test Accuracy: 0.744810, 0.603882\n",
      "Step 5 | Training Loss: 1.770549 | Test Loss: 2.007723 | Test Accuracy: 0.549193, 0.781013\n",
      "Step 6 | Training Loss: 0.825863 | Test Loss: 1.983122 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 7 | Training Loss: 1.029502 | Test Loss: 1.983430 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 2.320221 | Test Loss: 1.983138 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 1.096682 | Test Loss: 1.983738 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 1.776344 | Test Loss: 1.982118 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.859030 | Test Loss: 1.982501 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.800086 | Test Loss: 1.983400 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.954566 | Test Loss: 1.983284 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 0.794736 | Test Loss: 1.986316 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.970710 | Test Loss: 1.985001 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 1 | Training Loss: 0.827774 | Test Loss: 1.983412 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 2 | Training Loss: 1.151106 | Test Loss: 1.983539 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 3 | Training Loss: 0.706264 | Test Loss: 1.983140 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 4 | Training Loss: 0.890802 | Test Loss: 1.983363 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 5 | Training Loss: 0.720096 | Test Loss: 1.983960 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 6 | Training Loss: 0.810214 | Test Loss: 1.983303 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 7 | Training Loss: 0.791258 | Test Loss: 1.983100 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 0.869610 | Test Loss: 1.983729 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 0.769598 | Test Loss: 1.983467 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 1.072386 | Test Loss: 1.983481 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.971201 | Test Loss: 1.983444 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.810560 | Test Loss: 1.983623 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.828347 | Test Loss: 1.983508 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 1.095855 | Test Loss: 1.983011 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.795052 | Test Loss: 1.983667 | Test Accuracy: 0.430758, 0.181603\n",
      "Current Layer Attributes - epochs:15 hidden layers:5 features count:122\n",
      "Step 1 | Training Loss: 0.716356 | Test Loss: 12.842004 | Test Accuracy: 0.771513, 0.568186\n",
      "Step 2 | Training Loss: 0.623244 | Test Loss: 1.802835 | Test Accuracy: 0.768763, 0.574177\n",
      "Step 3 | High Training Loss: 9795583082496.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_5/model-1\n",
      "Step 3 | Training Loss: 0.725009 | Test Loss: 1.773091 | Test Accuracy: 0.795511, 0.614768\n",
      "Step 4 | Training Loss: 0.537967 | Test Loss: 1.742539 | Test Accuracy: 0.787305, 0.600338\n",
      "Step 5 | Training Loss: 0.893103 | Test Loss: 1.715751 | Test Accuracy: 0.767211, 0.565992\n",
      "Step 6 | Training Loss: 0.505600 | Test Loss: 1.803452 | Test Accuracy: 0.780784, 0.587848\n",
      "Step 7 | Training Loss: 0.528070 | Test Loss: 1.738700 | Test Accuracy: 0.763840, 0.594599\n",
      "Step 8 | Training Loss: 0.815612 | Test Loss: 1.758621 | Test Accuracy: 0.762065, 0.554093\n",
      "Step 9 | Training Loss: 0.638845 | Test Loss: 1.776738 | Test Accuracy: 0.771824, 0.572405\n",
      "Step 10 | High Training Loss: 156686.453125 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_5/model-3\n",
      "Step 10 | Training Loss: 0.687578 | Test Loss: 1.742313 | Test Accuracy: 0.780474, 0.585823\n",
      "Step 11 | Training Loss: 0.806122 | Test Loss: 1.853154 | Test Accuracy: 0.864443, 0.755359\n",
      "Step 12 | Training Loss: 0.910764 | Test Loss: 1.769042 | Test Accuracy: 0.784377, 0.595527\n",
      "Step 13 | Training Loss: 0.653577 | Test Loss: 9927752.000000 | Test Accuracy: 0.663369, 0.452574\n",
      "Step 14 | Training Loss: 0.620214 | Test Loss: 1.835845 | Test Accuracy: 0.724273, 0.486582\n",
      "Step 15 | Training Loss: 0.763577 | Test Loss: 1.825719 | Test Accuracy: 0.710965, 0.464641\n",
      "Step 1 | Training Loss: 0.546556 | Test Loss: 1.831279 | Test Accuracy: 0.744721, 0.532911\n",
      "Step 2 | Training Loss: 0.652875 | Test Loss: 1.813322 | Test Accuracy: 0.725515, 0.488523\n",
      "Step 3 | Training Loss: 0.571472 | Test Loss: 1.810304 | Test Accuracy: 0.724894, 0.487511\n",
      "Step 4 | Training Loss: 0.596081 | Test Loss: 1.806437 | Test Accuracy: 0.727777, 0.495865\n",
      "Step 5 | High Training Loss: nan ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_5/model-11\n",
      "Step 5 | Training Loss: 0.658911 | Test Loss: 1.850120 | Test Accuracy: 0.848563, 0.729114\n",
      "Step 6 | Training Loss: 0.669143 | Test Loss: 1.762932 | Test Accuracy: 0.778699, 0.581603\n",
      "Step 7 | Training Loss: 0.479288 | Test Loss: 1.745828 | Test Accuracy: 0.773998, 0.572067\n",
      "Step 8 | High Training Loss: 54022656.000000 ... Restoring Net\n",
      "INFO:tensorflow:Restoring parameters from dataset/tf_vae_only_nsl_kdd/hidden layers_15_features count_5/model-11\n",
      "Step 8 | Training Loss: 0.710476 | Test Loss: 1.836755 | Test Accuracy: 0.713937, 0.538059\n",
      "Step 9 | Training Loss: 0.845252 | Test Loss: 1.984052 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 2.757974 | Test Loss: 1.983660 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.771396 | Test Loss: 1.983157 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.745920 | Test Loss: 1.984176 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.790529 | Test Loss: 1.983174 | Test Accuracy: 0.430758, 0.181603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14 | Training Loss: 0.908868 | Test Loss: 1.983569 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.802643 | Test Loss: 1.983806 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 1 | Training Loss: 0.890940 | Test Loss: 1.983346 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 2 | Training Loss: 0.790125 | Test Loss: 1.983743 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 3 | Training Loss: 0.829183 | Test Loss: 1.983646 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 4 | Training Loss: 0.816010 | Test Loss: 1.983248 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 5 | Training Loss: 0.899832 | Test Loss: 1.983193 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 6 | Training Loss: 1.250924 | Test Loss: 1.983675 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 7 | Training Loss: 0.908405 | Test Loss: 1.983618 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 8 | Training Loss: 0.959557 | Test Loss: 1.983244 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 9 | Training Loss: 1.025066 | Test Loss: 1.983758 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 10 | Training Loss: 0.849353 | Test Loss: 1.983299 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 11 | Training Loss: 0.974599 | Test Loss: 1.983995 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 12 | Training Loss: 0.756951 | Test Loss: 1.983653 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 13 | Training Loss: 0.946039 | Test Loss: 1.983136 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 14 | Training Loss: 0.886621 | Test Loss: 1.983756 | Test Accuracy: 0.430758, 0.181603\n",
      "Step 15 | Training Loss: 0.922911 | Test Loss: 1.983079 | Test Accuracy: 0.430758, 0.181603\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:8\n",
      "Step 1 | Training Loss: 0.368457 | Test Loss: 1.538155 | Test Accuracy: 0.786196, 0.599747\n",
      "Step 2 | Training Loss: 2.177150 | Test Loss: 1.549189 | Test Accuracy: 0.776570, 0.589620\n",
      "Step 3 | Training Loss: 0.304199 | Test Loss: 1.479052 | Test Accuracy: 0.762465, 0.640759\n",
      "Step 4 | Training Loss: 0.497949 | Test Loss: 1.479142 | Test Accuracy: 0.797773, 0.625823\n",
      "Step 5 | Training Loss: 0.286303 | Test Loss: 1.483156 | Test Accuracy: 0.835832, 0.696962\n",
      "Step 6 | Training Loss: 0.255799 | Test Loss: 1.636777 | Test Accuracy: 0.829267, 0.703460\n",
      "Step 7 | Training Loss: 0.342434 | Test Loss: 1.576993 | Test Accuracy: 0.860761, 0.755274\n",
      "Step 8 | Training Loss: 1.393941 | Test Loss: 1.550391 | Test Accuracy: 0.702582, 0.655190\n",
      "Step 9 | Training Loss: 0.503657 | Test Loss: 1.708516 | Test Accuracy: 0.786063, 0.728608\n",
      "Step 10 | Training Loss: 0.421382 | Test Loss: 1.812661 | Test Accuracy: 0.780740, 0.694768\n",
      "Step 11 | Training Loss: 0.411869 | Test Loss: 1.765049 | Test Accuracy: 0.741350, 0.637215\n",
      "Step 12 | Training Loss: 0.464469 | Test Loss: 1.735057 | Test Accuracy: 0.763174, 0.634599\n",
      "Step 13 | Training Loss: 0.457114 | Test Loss: 1.810040 | Test Accuracy: 0.775506, 0.661097\n",
      "Step 14 | Training Loss: 0.370474 | Test Loss: 1.857674 | Test Accuracy: 0.803628, 0.765485\n",
      "Step 15 | Training Loss: 0.504647 | Test Loss: 2.044036 | Test Accuracy: 0.839691, 0.749958\n",
      "Step 1 | Training Loss: 0.334737 | Test Loss: 1.866865 | Test Accuracy: 0.815782, 0.716962\n",
      "Step 2 | Training Loss: 0.374456 | Test Loss: 3.326184 | Test Accuracy: 0.823013, 0.726329\n",
      "Step 3 | Training Loss: 0.392899 | Test Loss: 1.947100 | Test Accuracy: 0.853797, 0.746667\n",
      "Step 4 | Training Loss: 0.451934 | Test Loss: 1.855133 | Test Accuracy: 0.835122, 0.746245\n",
      "Step 5 | Training Loss: 0.481672 | Test Loss: 1.930070 | Test Accuracy: 0.832284, 0.734093\n",
      "Step 6 | Training Loss: 0.406606 | Test Loss: 1.923351 | Test Accuracy: 0.829179, 0.731392\n",
      "Step 7 | Training Loss: 0.526898 | Test Loss: 1.924270 | Test Accuracy: 0.821105, 0.733840\n",
      "Step 8 | Training Loss: 0.433169 | Test Loss: 2.022808 | Test Accuracy: 0.839913, 0.758734\n",
      "Step 9 | Training Loss: 0.485217 | Test Loss: 1.984753 | Test Accuracy: 0.814008, 0.727848\n",
      "Step 10 | Training Loss: 0.826770 | Test Loss: 1.933126 | Test Accuracy: 0.804782, 0.743713\n",
      "Step 11 | Training Loss: 0.364303 | Test Loss: 1.842835 | Test Accuracy: 0.816625, 0.716878\n",
      "Step 12 | Training Loss: 0.398159 | Test Loss: 1.877141 | Test Accuracy: 0.822702, 0.719747\n",
      "Step 13 | Training Loss: 0.646360 | Test Loss: 1.817111 | Test Accuracy: 0.783446, 0.674515\n",
      "Step 14 | Training Loss: 0.653987 | Test Loss: 1.903381 | Test Accuracy: 0.757807, 0.653418\n",
      "Step 15 | Training Loss: 0.427503 | Test Loss: 1.922312 | Test Accuracy: 0.787926, 0.726751\n",
      "Step 1 | Training Loss: 0.608308 | Test Loss: 1.931868 | Test Accuracy: 0.757984, 0.686835\n",
      "Step 2 | Training Loss: 0.423060 | Test Loss: 1.933708 | Test Accuracy: 0.752129, 0.674177\n",
      "Step 3 | Training Loss: 0.495394 | Test Loss: 1.930297 | Test Accuracy: 0.726890, 0.683544\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:24.210106Z",
     "start_time": "2017-06-24T02:09:24.181735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.934593</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.760169</td>\n",
       "      <td>77.890941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.856547</td>\n",
       "      <td>0.789705</td>\n",
       "      <td>86.243578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944912</td>\n",
       "      <td>0.850381</td>\n",
       "      <td>0.732489</td>\n",
       "      <td>71.239707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "5     45             122              5     0.934593    0.869500   \n",
       "4     45              32              5     0.797904    0.856547   \n",
       "0     45               8              3     0.944912    0.850381   \n",
       "\n",
       "   test_score_20  time_taken  \n",
       "5       0.760169   77.890941  \n",
       "4       0.789705   86.243578  \n",
       "0       0.732489   71.239707  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-03T01:25:48.273401Z",
     "start_time": "2017-06-03T01:25:48.269113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:25.525382Z",
     "start_time": "2017-06-24T02:09:25.510729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.934593</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.760169</td>\n",
       "      <td>77.890941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.886252</td>\n",
       "      <td>0.868302</td>\n",
       "      <td>0.757131</td>\n",
       "      <td>95.354419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.856547</td>\n",
       "      <td>0.789705</td>\n",
       "      <td>86.243578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.944912</td>\n",
       "      <td>0.850381</td>\n",
       "      <td>0.732489</td>\n",
       "      <td>71.239707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.915304</td>\n",
       "      <td>0.837473</td>\n",
       "      <td>0.733418</td>\n",
       "      <td>78.966704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.959597</td>\n",
       "      <td>0.823811</td>\n",
       "      <td>0.680759</td>\n",
       "      <td>26.700803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "5     45             122              5     0.934593    0.869500   \n",
       "2     45             122              3     0.886252    0.868302   \n",
       "4     45              32              5     0.797904    0.856547   \n",
       "0     45               8              3     0.944912    0.850381   \n",
       "1     45              32              3     0.915304    0.837473   \n",
       "3     45               8              5     0.959597    0.823811   \n",
       "\n",
       "   test_score_20  time_taken  \n",
       "5       0.760169   77.890941  \n",
       "2       0.757131   95.354419  \n",
       "4       0.789705   86.243578  \n",
       "0       0.732489   71.239707  \n",
       "1       0.733418   78.966704  \n",
       "3       0.680759   26.700803  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:25.887300Z",
     "start_time": "2017-06-24T02:09:25.818190Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-523f9411a35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPanel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/tf_vae_only_nsl_kdd_scores.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.6/site-packages/pandas/core/panel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, items, major_axis, minor_axis, copy, dtype)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         self._init_data(data=data, items=items, major_axis=major_axis,\n\u001b[0;32m--> 148\u001b[0;31m                         minor_axis=minor_axis, copy=copy, dtype=dtype)\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.6/site-packages/pandas/core/panel.py\u001b[0m in \u001b[0;36m_init_data\u001b[0;34m(self, data, copy, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassed_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.6/site-packages/pandas/core/panel.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, axes, dtype)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# extract axis for remaining axes & create the slicemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         raxes = [self._extract_axis(self, data, axis=i) if a is None else a\n\u001b[0;32m--> 214\u001b[0;31m                  for i, a in enumerate(axes)]\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mraxes_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_axes_for_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.6/site-packages/pandas/core/panel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# extract axis for remaining axes & create the slicemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         raxes = [self._extract_axis(self, data, axis=i) if a is None else a\n\u001b[0;32m--> 214\u001b[0;31m                  for i, a in enumerate(axes)]\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mraxes_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_axes_for_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.6/site-packages/pandas/core/panel.py\u001b[0m in \u001b[0;36m_extract_axis\u001b[0;34m(self, data, axis, intersect)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[0mhave_raw_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m                 \u001b[0mraw_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:26.292326Z",
     "start_time": "2017-06-24T02:09:26.233798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:27.612530Z",
     "start_time": "2017-06-24T02:09:27.029521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.9258  0.0742]\n",
      " [ 0.1409  0.8591]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOz3HFwMzOz3Pn/AsshOxghjTYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9876ff128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:27.870600Z",
     "start_time": "2017-06-24T02:09:27.614098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.7505  0.2495]\n",
      " [ 0.1829  0.8171]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9876ff828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:28.086292Z",
     "start_time": "2017-06-24T02:09:28.061765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.774973</td>\n",
       "      <td>0.689114</td>\n",
       "      <td>16.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948801</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.676034</td>\n",
       "      <td>5.418028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.952215</td>\n",
       "      <td>0.811391</td>\n",
       "      <td>0.649367</td>\n",
       "      <td>6.201481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.533339</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>4.315096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.933243</td>\n",
       "      <td>0.833304</td>\n",
       "      <td>0.694684</td>\n",
       "      <td>33.188258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.961343</td>\n",
       "      <td>0.790188</td>\n",
       "      <td>0.609198</td>\n",
       "      <td>5.809257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.959914</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.749705</td>\n",
       "      <td>171.153311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.927449</td>\n",
       "      <td>0.809572</td>\n",
       "      <td>0.642869</td>\n",
       "      <td>51.082118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.865693</td>\n",
       "      <td>0.854152</td>\n",
       "      <td>0.740759</td>\n",
       "      <td>374.074049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948881</td>\n",
       "      <td>0.857567</td>\n",
       "      <td>0.774008</td>\n",
       "      <td>43.813142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.949595</td>\n",
       "      <td>0.849051</td>\n",
       "      <td>0.748017</td>\n",
       "      <td>211.216465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.920543</td>\n",
       "      <td>0.885468</td>\n",
       "      <td>0.789451</td>\n",
       "      <td>23.983338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.774973</td>\n",
       "      <td>0.689114</td>\n",
       "      <td>16.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948801</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.676034</td>\n",
       "      <td>5.418028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.952215</td>\n",
       "      <td>0.811391</td>\n",
       "      <td>0.649367</td>\n",
       "      <td>6.201481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.533339</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>4.315096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.933243</td>\n",
       "      <td>0.833304</td>\n",
       "      <td>0.694684</td>\n",
       "      <td>33.188258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.927449</td>\n",
       "      <td>0.809572</td>\n",
       "      <td>0.642869</td>\n",
       "      <td>51.082118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.865693</td>\n",
       "      <td>0.854152</td>\n",
       "      <td>0.740759</td>\n",
       "      <td>374.074049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948881</td>\n",
       "      <td>0.857567</td>\n",
       "      <td>0.774008</td>\n",
       "      <td>43.813142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.949595</td>\n",
       "      <td>0.849051</td>\n",
       "      <td>0.748017</td>\n",
       "      <td>211.216465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.920543</td>\n",
       "      <td>0.885468</td>\n",
       "      <td>0.789451</td>\n",
       "      <td>23.983338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.952532</td>\n",
       "      <td>0.836498</td>\n",
       "      <td>0.691983</td>\n",
       "      <td>5.763082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.955628</td>\n",
       "      <td>0.857612</td>\n",
       "      <td>0.763291</td>\n",
       "      <td>51.260095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.941737</td>\n",
       "      <td>0.831751</td>\n",
       "      <td>0.685823</td>\n",
       "      <td>12.752733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.881648</td>\n",
       "      <td>0.885735</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>109.836549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.928163</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>0.734008</td>\n",
       "      <td>37.905672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.919194</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.755359</td>\n",
       "      <td>63.165995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.923559</td>\n",
       "      <td>0.860761</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>23.127212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.954278</td>\n",
       "      <td>0.843772</td>\n",
       "      <td>0.756203</td>\n",
       "      <td>41.355281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.918162</td>\n",
       "      <td>0.871806</td>\n",
       "      <td>0.821772</td>\n",
       "      <td>117.810757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.923162</td>\n",
       "      <td>0.854551</td>\n",
       "      <td>0.730549</td>\n",
       "      <td>76.454872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>0.799769</td>\n",
       "      <td>0.623122</td>\n",
       "      <td>10.622074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.939276</td>\n",
       "      <td>0.835788</td>\n",
       "      <td>0.689705</td>\n",
       "      <td>29.703716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                 45     0.928639    0.774973       0.689114   \n",
       "8              1                 45     0.948801    0.810238       0.676034   \n",
       "32             1                 45     0.952215    0.811391       0.649367   \n",
       "122            1                 45     0.533339    0.430758       0.181603   \n",
       "1              3                 45     0.933243    0.833304       0.694684   \n",
       "8              3                 45     0.961343    0.790188       0.609198   \n",
       "32             3                 45     0.959914    0.860140       0.749705   \n",
       "122            3                 45     0.927449    0.809572       0.642869   \n",
       "1              5                 45     0.865693    0.854152       0.740759   \n",
       "8              5                 45     0.948881    0.857567       0.774008   \n",
       "32             5                 45     0.949595    0.849051       0.748017   \n",
       "122            5                 45     0.920543    0.885468       0.789451   \n",
       "1              1                 45     0.928639    0.774973       0.689114   \n",
       "8              1                 45     0.948801    0.810238       0.676034   \n",
       "32             1                 45     0.952215    0.811391       0.649367   \n",
       "122            1                 45     0.533339    0.430758       0.181603   \n",
       "1              3                 45     0.933243    0.833304       0.694684   \n",
       "...                             ...          ...         ...            ...   \n",
       "122            3                 45     0.927449    0.809572       0.642869   \n",
       "1              5                 45     0.865693    0.854152       0.740759   \n",
       "8              5                 45     0.948881    0.857567       0.774008   \n",
       "32             5                 45     0.949595    0.849051       0.748017   \n",
       "122            5                 45     0.920543    0.885468       0.789451   \n",
       "8              3                 45     0.952532    0.836498       0.691983   \n",
       "32             3                 45     0.955628    0.857612       0.763291   \n",
       "122            3                 45     0.941737    0.831751       0.685823   \n",
       "8              5                 45     0.881648    0.885735       0.820591   \n",
       "32             5                 45     0.928163    0.856813       0.734008   \n",
       "122            5                 45     0.919194    0.864443       0.755359   \n",
       "8              3                 45     0.923559    0.860761       0.755274   \n",
       "32             3                 45     0.954278    0.843772       0.756203   \n",
       "122            3                 45     0.918162    0.871806       0.821772   \n",
       "8              5                 45     0.923162    0.854551       0.730549   \n",
       "32             5                 45     0.965233    0.799769       0.623122   \n",
       "122            5                 45     0.939276    0.835788       0.689705   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1               16.006346  \n",
       "8              1                5.418028  \n",
       "32             1                6.201481  \n",
       "122            1                4.315096  \n",
       "1              3               33.188258  \n",
       "8              3                5.809257  \n",
       "32             3              171.153311  \n",
       "122            3               51.082118  \n",
       "1              5              374.074049  \n",
       "8              5               43.813142  \n",
       "32             5              211.216465  \n",
       "122            5               23.983338  \n",
       "1              1               16.006346  \n",
       "8              1                5.418028  \n",
       "32             1                6.201481  \n",
       "122            1                4.315096  \n",
       "1              3               33.188258  \n",
       "...                                  ...  \n",
       "122            3               51.082118  \n",
       "1              5              374.074049  \n",
       "8              5               43.813142  \n",
       "32             5              211.216465  \n",
       "122            5               23.983338  \n",
       "8              3                5.763082  \n",
       "32             3               51.260095  \n",
       "122            3               12.752733  \n",
       "8              5              109.836549  \n",
       "32             5               37.905672  \n",
       "122            5               63.165995  \n",
       "8              3               23.127212  \n",
       "32             3               41.355281  \n",
       "122            3              117.810757  \n",
       "8              5               76.454872  \n",
       "32             5               10.622074  \n",
       "122            5               29.703716  \n",
       "\n",
       "[36 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:29.028082Z",
     "start_time": "2017-06-24T02:09:29.011116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.774973</td>\n",
       "      <td>0.689114</td>\n",
       "      <td>16.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.933243</td>\n",
       "      <td>0.833304</td>\n",
       "      <td>0.694684</td>\n",
       "      <td>33.188258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.865693</td>\n",
       "      <td>0.854152</td>\n",
       "      <td>0.740759</td>\n",
       "      <td>374.074049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948801</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.676034</td>\n",
       "      <td>5.418028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.949694</td>\n",
       "      <td>0.819409</td>\n",
       "      <td>0.666414</td>\n",
       "      <td>10.127202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.925643</td>\n",
       "      <td>0.863855</td>\n",
       "      <td>0.774789</td>\n",
       "      <td>68.479426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.952215</td>\n",
       "      <td>0.811391</td>\n",
       "      <td>0.649367</td>\n",
       "      <td>6.201481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.957434</td>\n",
       "      <td>0.855416</td>\n",
       "      <td>0.754726</td>\n",
       "      <td>108.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.948147</td>\n",
       "      <td>0.838671</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>117.740169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.533339</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>4.315096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.928699</td>\n",
       "      <td>0.830676</td>\n",
       "      <td>0.698333</td>\n",
       "      <td>58.181932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.924889</td>\n",
       "      <td>0.867792</td>\n",
       "      <td>0.755992</td>\n",
       "      <td>35.209097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                 45     0.928639    0.774973       0.689114   \n",
       "               3                 45     0.933243    0.833304       0.694684   \n",
       "               5                 45     0.865693    0.854152       0.740759   \n",
       "8              1                 45     0.948801    0.810238       0.676034   \n",
       "               3                 45     0.949694    0.819409       0.666414   \n",
       "               5                 45     0.925643    0.863855       0.774789   \n",
       "32             1                 45     0.952215    0.811391       0.649367   \n",
       "               3                 45     0.957434    0.855416       0.754726   \n",
       "               5                 45     0.948147    0.838671       0.713291   \n",
       "122            1                 45     0.533339    0.430758       0.181603   \n",
       "               3                 45     0.928699    0.830676       0.698333   \n",
       "               5                 45     0.924889    0.867792       0.755992   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1               16.006346  \n",
       "               3               33.188258  \n",
       "               5              374.074049  \n",
       "8              1                5.418028  \n",
       "               3               10.127202  \n",
       "               5               68.479426  \n",
       "32             1                6.201481  \n",
       "               3              108.730500  \n",
       "               5              117.740169  \n",
       "122            1                4.315096  \n",
       "               3               58.181932  \n",
       "               5               35.209097  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T02:09:29.596307Z",
     "start_time": "2017-06-24T02:09:29.580106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>0.035165</td>\n",
       "      <td>0.070939</td>\n",
       "      <td>8.666701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031737</td>\n",
       "      <td>0.014655</td>\n",
       "      <td>0.036771</td>\n",
       "      <td>31.574623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>72.192987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>108.510321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.029346</td>\n",
       "      <td>0.084747</td>\n",
       "      <td>43.666248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>0.047023</td>\n",
       "      <td>18.832000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                0.0     0.000000    0.000000       0.000000   \n",
       "               3                0.0     0.000000    0.000000       0.000000   \n",
       "               5                0.0     0.000000    0.000000       0.000000   \n",
       "8              1                0.0     0.000000    0.000000       0.000000   \n",
       "               3                0.0     0.017912    0.035165       0.070939   \n",
       "               5                0.0     0.031737    0.014655       0.036771   \n",
       "32             1                0.0     0.000000    0.000000       0.000000   \n",
       "               3                0.0     0.002917    0.007854       0.006480   \n",
       "               5                0.0     0.015226    0.026191       0.060474   \n",
       "122            1                0.0     0.000000    0.000000       0.000000   \n",
       "               3                0.0     0.009732    0.029346       0.084747   \n",
       "               5                0.0     0.009613    0.023526       0.047023   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                0.000000  \n",
       "               3                0.000000  \n",
       "               5                0.000000  \n",
       "8              1                0.000000  \n",
       "               3                8.666701  \n",
       "               5               31.574623  \n",
       "32             1                0.000000  \n",
       "               3               72.192987  \n",
       "               5              108.510321  \n",
       "122            1                0.000000  \n",
       "               3               43.666248  \n",
       "               5               18.832000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgb.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
