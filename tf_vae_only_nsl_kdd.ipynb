{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:01.760663Z",
     "start_time": "2017-05-13T14:36:01.360159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:01.928481Z",
     "start_time": "2017-05-13T14:36:01.762114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:01.934454Z",
     "start_time": "2017-05-13T14:36:01.930012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:01.942426Z",
     "start_time": "2017-05-13T14:36:01.935737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:02.633991Z",
     "start_time": "2017-05-13T14:36:01.943863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_train_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "    y_train = y_output.values\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train_input = np.hstack([x_train_input, y_train])\n",
    "    x_test_input = np.hstack([x_test_input, y_test])\n",
    "    \n",
    "    x_train = ss.fit_transform(x_train_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    #x_train = np.hstack([x_train, y_train])\n",
    "    #x_test = np.hstack([x_test, y_test])\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:03.692495Z",
     "start_time": "2017-05-13T14:36:02.635382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:04.131490Z",
     "start_time": "2017-05-13T14:36:03.694093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            \n",
    "            loss = tf.reduce_mean((BCE + KLD) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            #gradients = [\n",
    "            #    None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "            #    for gradient in gradients]\n",
    "            #self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:36:04.263294Z",
     "start_time": "2017-05-13T14:36:04.132991Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 20\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e9):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "\n",
    "                valid_accuracy, y_pred = sess.run([net.tf_accuracy, net.x_hat], #net.summary_op \n",
    "                                                      feed_dict={net.x: preprocess.x_test, \n",
    "                                                                 net.y_: preprocess.y_test, \n",
    "                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "                acc = np.mean(np.equal(np.argmax(y_pred[:,-2:], axis = 1), np.argmax(preprocess.y_test, axis = 1)))\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f} | test: {}\".format(epoch, train_loss, valid_accuracy, acc))\n",
    "\n",
    "            accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual, net.y], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            \n",
    "            curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "            Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "            \n",
    "            if accuracy > Train.best_acc:\n",
    "                Train.best_acc = accuracy\n",
    "                Train.pred_value = pred_value\n",
    "                Train.actual_value = actual_value\n",
    "                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                #net.saver.save(sess, \"dataset/epochs_{}_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "            Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:24.007923Z",
     "start_time": "2017-05-13T14:36:04.264706Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.000164 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000326 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000045 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000162 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000046 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000086 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000156 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000022 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000053 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000005 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.002283 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000109 | Validation Accuracy: 0.558020 | test: 0.5580198722498225\n",
      "Step 3 | Training Loss: 0.000043 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000156 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000002 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000104 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000058 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000030 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000040 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000036 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.000092 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000210 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000065 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000550 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000025 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000028 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000072 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000170 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000012 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000012 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:4 features count:64\n",
      "Step 1 | Training Loss: 1702052396758753232289792.000000\n",
      "Step 1 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: nan | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.000537 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000151 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000018 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000109 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000102 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000009 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000020 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000027 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000001 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000088 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.000104 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000027 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000124 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000129 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000130 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000031 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000044 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000036 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000103 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000046 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.000007 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000018 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000083 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000097 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000050 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000056 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000029 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000022 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000167 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000103 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:6 features count:64\n",
      "Step 1 | Training Loss: 0.000040 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000027 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000424 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000021 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000072 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000043 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000192 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000024 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000050 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000008 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:4\n",
      "Step 1 | Training Loss: 0.000051 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000065 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000003 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000056 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000027 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000093 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000004 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000096 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000048 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000038 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:16\n",
      "Step 1 | Training Loss: 0.000125 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000156 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000068 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000014 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000073 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000014 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000085 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000103 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000036 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000045 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:32\n",
      "Step 1 | Training Loss: 0.000327 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000226 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000215 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000093 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000018 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000087 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000012 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000015 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000010 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000039 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n",
      "Current Layer Attributes - epochs:10 hidden layers:10 features count:64\n",
      "Step 1 | Training Loss: 0.000182 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 2 | Training Loss: 0.000155 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 3 | Training Loss: 0.000154 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 4 | Training Loss: 0.000226 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 5 | Training Loss: 0.000163 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 6 | Training Loss: 0.000048 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 7 | Training Loss: 0.000052 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 8 | Training Loss: 0.000026 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 9 | Training Loss: 0.000014 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Step 10 | Training Loss: 0.000017 | Validation Accuracy: 0.569242 | test: 0.5692423704755145\n",
      "Accuracy on Test data: 0.5692423582077026\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 16, 32, 64]\n",
    "    hidden_layers_arr = [4, 6, 10]\n",
    "\n",
    "    epochs = [10]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:24.041719Z",
     "start_time": "2017-05-13T14:45:24.009514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:24.087001Z",
     "start_time": "2017-05-13T14:45:24.043147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0      10               4              4     0.569242    0.569242\n",
       "1      10              16              4     0.569242    0.569242\n",
       "2      10              32              4     0.569242    0.569242\n",
       "3      10              64              4     0.569242    0.569242\n",
       "4      10               4              6     0.569242    0.569242\n",
       "5      10              16              6     0.569242    0.569242\n",
       "6      10              32              6     0.569242    0.569242\n",
       "7      10              64              6     0.569242    0.569242\n",
       "8      10               4             10     0.569242    0.569242\n",
       "9      10              16             10     0.569242    0.569242\n",
       "10     10              32             10     0.569242    0.569242\n",
       "11     10              64             10     0.569242    0.569242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:24.172840Z",
     "start_time": "2017-05-13T14:45:24.088479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:24.238890Z",
     "start_time": "2017-05-13T14:45:24.174640Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-13T14:45:25.682398Z",
     "start_time": "2017-05-13T14:45:24.240776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBzczMyscBzczMyscBzczMyuc/w9yJk7J1/kxigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc43aff6ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
