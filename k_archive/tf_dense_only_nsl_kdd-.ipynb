{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:29.710522Z",
     "start_time": "2017-07-16T23:57:28.782211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:29.805044Z",
     "start_time": "2017-07-16T23:57:29.715837Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train__2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:29.820863Z",
     "start_time": "2017-07-16T23:57:29.808655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:29.840456Z",
     "start_time": "2017-07-16T23:57:29.828751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:30.444295Z",
     "start_time": "2017-07-16T23:57:29.842848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:32.313307Z",
     "start_time": "2017-07-16T23:57:30.447079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:32.615435Z",
     "start_time": "2017-07-16T23:57:32.315886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:33.007956Z",
     "start_time": "2017-07-16T23:57:32.621069Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd-/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred,\n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_,\n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:57:33.121622Z",
     "start_time": "2017-07-16T23:57:33.011787Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-5, 1e-6]\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "            \n",
    "        for k, (v1_, v2) in Train.predictions.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:24.379486Z",
     "start_time": "2017-07-16T23:57:33.129113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.696619 | Validation Accuracy: 0.621825\n",
      "Accuracy on Test data: 0.6084102392196655, 0.39240506291389465\n",
      "Step 2 | Training Loss: 0.630072 | Validation Accuracy: 0.742063\n",
      "Accuracy on Test data: 0.68461674451828, 0.42683544754981995\n",
      "Step 3 | Training Loss: 0.660342 | Validation Accuracy: 0.785714\n",
      "Accuracy on Test data: 0.7082594037055969, 0.46000000834465027\n",
      "Step 4 | Training Loss: 0.541829 | Validation Accuracy: 0.850000\n",
      "Accuracy on Test data: 0.7194375395774841, 0.4780590832233429\n",
      "Step 5 | Training Loss: 0.556526 | Validation Accuracy: 0.883333\n",
      "Accuracy on Test data: 0.737358033657074, 0.5112236142158508\n",
      "Step 6 | Training Loss: 0.485439 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.7456085681915283, 0.5263291001319885\n",
      "Step 7 | Training Loss: 0.474231 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.7519074082374573, 0.5383122563362122\n",
      "Step 8 | Training Loss: 0.492558 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.7621539831161499, 0.5562869310379028\n",
      "Step 9 | Training Loss: 0.448800 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.7729329466819763, 0.5764557123184204\n",
      "Step 10 | Training Loss: 0.475091 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.7781671285629272, 0.5857384204864502\n",
      "Step 1 | Training Loss: 0.440640 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.7784332633018494, 0.5859915614128113\n",
      "Step 2 | Training Loss: 0.418244 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.7790542840957642, 0.5870042443275452\n",
      "Step 3 | Training Loss: 0.485042 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7799414396286011, 0.5883544087409973\n",
      "Step 4 | Training Loss: 0.410913 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.7806068062782288, 0.5892826914787292\n",
      "Step 5 | Training Loss: 0.442045 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.7809616923332214, 0.5897046327590942\n",
      "Step 6 | Training Loss: 0.439277 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.7818044424057007, 0.5911392569541931\n",
      "Step 7 | Training Loss: 0.458564 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.782514214515686, 0.5919831395149231\n",
      "Step 8 | Training Loss: 0.442809 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.7835787534713745, 0.5934177041053772\n",
      "Step 9 | Training Loss: 0.441603 | Validation Accuracy: 0.948016\n",
      "Accuracy on Test data: 0.7843772172927856, 0.5948523283004761\n",
      "Step 10 | Training Loss: 0.407254 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.7854418158531189, 0.5967932343482971\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.676125 | Validation Accuracy: 0.682936\n",
      "Accuracy on Test data: 0.7106103897094727, 0.6418565511703491\n",
      "Step 2 | Training Loss: 0.623865 | Validation Accuracy: 0.829762\n",
      "Accuracy on Test data: 0.8160929679870605, 0.6786497831344604\n",
      "Step 3 | Training Loss: 0.598203 | Validation Accuracy: 0.848810\n",
      "Accuracy on Test data: 0.8289123773574829, 0.6926582455635071\n",
      "Step 4 | Training Loss: 0.557506 | Validation Accuracy: 0.859921\n",
      "Accuracy on Test data: 0.8297995328903198, 0.6933333277702332\n",
      "Step 5 | Training Loss: 0.540771 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.8296664357185364, 0.6922363042831421\n",
      "Step 6 | Training Loss: 0.503479 | Validation Accuracy: 0.892857\n",
      "Accuracy on Test data: 0.8282470107078552, 0.6882700324058533\n",
      "Step 7 | Training Loss: 0.502244 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8237224817276001, 0.6794936656951904\n",
      "Step 8 | Training Loss: 0.482900 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8249645233154297, 0.6810970306396484\n",
      "Step 9 | Training Loss: 0.464183 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8252306580543518, 0.6805063486099243\n",
      "Step 10 | Training Loss: 0.428869 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8239886164665222, 0.6772995591163635\n",
      "Step 1 | Training Loss: 0.460218 | Validation Accuracy: 0.916667\n",
      "Accuracy on Test data: 0.8240330219268799, 0.6772152185440063\n",
      "Step 2 | Training Loss: 0.470758 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8241660594940186, 0.6773839592933655\n",
      "Step 3 | Training Loss: 0.478213 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8243435025215149, 0.6774683594703674\n",
      "Step 4 | Training Loss: 0.414661 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8229240775108337, 0.6746835708618164\n",
      "Step 5 | Training Loss: 0.428124 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.8205287456512451, 0.6701265573501587\n",
      "Step 6 | Training Loss: 0.445458 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8193311095237732, 0.6677637100219727\n",
      "Step 7 | Training Loss: 0.454886 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.818222165107727, 0.6654008626937866\n",
      "Step 8 | Training Loss: 0.439139 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8166696429252625, 0.6624472737312317\n",
      "Step 9 | Training Loss: 0.415393 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.8117902874946594, 0.6530801653862\n",
      "Step 10 | Training Loss: 0.468507 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8106813430786133, 0.650970458984375\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.666625 | Validation Accuracy: 0.750000\n",
      "Accuracy on Test data: 0.7734208703041077, 0.6368776559829712\n",
      "Step 2 | Training Loss: 0.627066 | Validation Accuracy: 0.850397\n",
      "Accuracy on Test data: 0.8238555788993835, 0.6929113864898682\n",
      "Step 3 | Training Loss: 0.575635 | Validation Accuracy: 0.864286\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6821097135543823\n",
      "Step 4 | Training Loss: 0.523073 | Validation Accuracy: 0.881746\n",
      "Accuracy on Test data: 0.8329489231109619, 0.6978902816772461\n",
      "Step 5 | Training Loss: 0.526373 | Validation Accuracy: 0.895238\n",
      "Accuracy on Test data: 0.8386266827583313, 0.7066666483879089\n",
      "Step 6 | Training Loss: 0.458571 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8447480201721191, 0.7174683809280396\n",
      "Step 7 | Training Loss: 0.487480 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.8484297394752502, 0.7242193818092346\n",
      "Step 8 | Training Loss: 0.471518 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.8315737843513489, 0.6917299628257751\n",
      "Step 9 | Training Loss: 0.460414 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8122338652610779, 0.652658224105835\n",
      "Step 10 | Training Loss: 0.435614 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.8156493902206421, 0.6575527191162109\n",
      "Step 1 | Training Loss: 0.466321 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8156050443649292, 0.6572151780128479\n",
      "Step 2 | Training Loss: 0.417137 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8157824873924255, 0.6573839783668518\n",
      "Step 3 | Training Loss: 0.440400 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.8158711791038513, 0.6574683785438538\n",
      "Step 4 | Training Loss: 0.443070 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8158711791038513, 0.6574683785438538\n",
      "Step 5 | Training Loss: 0.415735 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8159155249595642, 0.6575527191162109\n",
      "Step 6 | Training Loss: 0.433120 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8158711791038513, 0.6574683785438538\n",
      "Step 7 | Training Loss: 0.461276 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8156493902206421, 0.6569620370864868\n",
      "Step 8 | Training Loss: 0.423820 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.8156050443649292, 0.6568776369094849\n",
      "Step 9 | Training Loss: 0.446898 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8151614665985107, 0.6559493541717529\n",
      "Step 10 | Training Loss: 0.444677 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8152501583099365, 0.6560337543487549\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.650872 | Validation Accuracy: 0.708730\n",
      "Accuracy on Test data: 0.7995918989181519, 0.751645565032959\n",
      "Step 2 | Training Loss: 0.600901 | Validation Accuracy: 0.745635\n",
      "Accuracy on Test data: 0.8135645985603333, 0.7403375506401062\n",
      "Step 3 | Training Loss: 0.616212 | Validation Accuracy: 0.819444\n",
      "Accuracy on Test data: 0.853397786617279, 0.7603375315666199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 0.493843 | Validation Accuracy: 0.878571\n",
      "Accuracy on Test data: 0.8600514531135559, 0.7566244602203369\n",
      "Step 5 | Training Loss: 0.476521 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.8636444211006165, 0.7553586363792419\n",
      "Step 6 | Training Loss: 0.471299 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8704311847686768, 0.7649788856506348\n",
      "Step 7 | Training Loss: 0.471719 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.8685237765312195, 0.7594936490058899\n",
      "Step 8 | Training Loss: 0.449003 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.8554826378822327, 0.7325738668441772\n",
      "Step 9 | Training Loss: 0.426383 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8494055867195129, 0.7194092869758606\n",
      "Step 10 | Training Loss: 0.425396 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.8455021381378174, 0.7108016610145569\n",
      "Step 1 | Training Loss: 0.454522 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.845014214515686, 0.7098734378814697\n",
      "Step 2 | Training Loss: 0.429308 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.8447036743164062, 0.7092826962471008\n",
      "Step 3 | Training Loss: 0.424828 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.8443931937217712, 0.7086076140403748\n",
      "Step 4 | Training Loss: 0.440937 | Validation Accuracy: 0.940476\n",
      "Accuracy on Test data: 0.844171404838562, 0.7081012725830078\n",
      "Step 5 | Training Loss: 0.425006 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8439496159553528, 0.7072573900222778\n",
      "Step 6 | Training Loss: 0.414437 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8429293632507324, 0.7052320837974548\n",
      "Step 7 | Training Loss: 0.428736 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8427519798278809, 0.7046413421630859\n",
      "Step 8 | Training Loss: 0.424546 | Validation Accuracy: 0.940873\n",
      "Accuracy on Test data: 0.8428850173950195, 0.7046413421630859\n",
      "Step 9 | Training Loss: 0.430987 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8428406715393066, 0.7044726014137268\n",
      "Step 10 | Training Loss: 0.408853 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8428406715393066, 0.7043882012367249\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.800004 | Validation Accuracy: 0.544444\n",
      "Accuracy on Test data: 0.47804293036460876, 0.27569618821144104\n",
      "Step 2 | Training Loss: 0.708409 | Validation Accuracy: 0.556349\n",
      "Accuracy on Test data: 0.4778655171394348, 0.2675105631351471\n",
      "Step 3 | Training Loss: 0.690698 | Validation Accuracy: 0.596032\n",
      "Accuracy on Test data: 0.4943222105503082, 0.2929113805294037\n",
      "Step 4 | Training Loss: 0.613808 | Validation Accuracy: 0.734127\n",
      "Accuracy on Test data: 0.5590844750404358, 0.33864977955818176\n",
      "Step 5 | Training Loss: 0.571005 | Validation Accuracy: 0.766667\n",
      "Accuracy on Test data: 0.661018431186676, 0.4011814296245575\n",
      "Step 6 | Training Loss: 0.557245 | Validation Accuracy: 0.806746\n",
      "Accuracy on Test data: 0.694419801235199, 0.4317299723625183\n",
      "Step 7 | Training Loss: 0.528039 | Validation Accuracy: 0.859127\n",
      "Accuracy on Test data: 0.7198367714881897, 0.47594937682151794\n",
      "Step 8 | Training Loss: 0.509090 | Validation Accuracy: 0.895238\n",
      "Accuracy on Test data: 0.7413058876991272, 0.5157805681228638\n",
      "Step 9 | Training Loss: 0.530673 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.7594925761222839, 0.5484388470649719\n",
      "Step 10 | Training Loss: 0.471612 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.7665454149246216, 0.5610970258712769\n",
      "Step 1 | Training Loss: 0.480721 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.7667672038078308, 0.5615189671516418\n",
      "Step 2 | Training Loss: 0.508951 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.7672107815742493, 0.5623628497123718\n",
      "Step 3 | Training Loss: 0.469541 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.7677430510520935, 0.5633755326271057\n",
      "Step 4 | Training Loss: 0.462666 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.7683640718460083, 0.5645569562911987\n",
      "Step 5 | Training Loss: 0.445665 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.7702271342277527, 0.5677636861801147\n",
      "Step 6 | Training Loss: 0.429909 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.7709811925888062, 0.5692827105522156\n",
      "Step 7 | Training Loss: 0.480571 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.7715578675270081, 0.5703797340393066\n",
      "Step 8 | Training Loss: 0.472183 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.7722232341766357, 0.5715611577033997\n",
      "Step 9 | Training Loss: 0.458403 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.77275550365448, 0.5725738406181335\n",
      "Step 10 | Training Loss: 0.476464 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.7734208703041077, 0.5738396644592285\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.698713 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.716827 | Validation Accuracy: 0.525000\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.706949 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.703631 | Validation Accuracy: 0.564683\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.683113 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.678657 | Validation Accuracy: 0.532143\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.682055 | Validation Accuracy: 0.542063\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.665721 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.668206 | Validation Accuracy: 0.536905\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.685350 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.643748 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.657927 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.655756 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.669921 | Validation Accuracy: 0.536508\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.656489 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.678307 | Validation Accuracy: 0.518651\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.666066 | Validation Accuracy: 0.521825\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.657714 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.665817 | Validation Accuracy: 0.549603\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.648753 | Validation Accuracy: 0.515873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.698084 | Validation Accuracy: 0.636111\n",
      "Accuracy on Test data: 0.4571061134338379, 0.163459911942482\n",
      "Step 2 | Training Loss: 0.675415 | Validation Accuracy: 0.645238\n",
      "Accuracy on Test data: 0.4626508057117462, 0.1692827045917511\n",
      "Step 3 | Training Loss: 0.725736 | Validation Accuracy: 0.689683\n",
      "Accuracy on Test data: 0.4676632285118103, 0.17729957401752472\n",
      "Step 4 | Training Loss: 0.682474 | Validation Accuracy: 0.712302\n",
      "Accuracy on Test data: 0.4754258394241333, 0.1908016949892044\n",
      "Step 5 | Training Loss: 0.682762 | Validation Accuracy: 0.712302\n",
      "Accuracy on Test data: 0.48398688435554504, 0.20514768362045288\n",
      "Step 6 | Training Loss: 0.703629 | Validation Accuracy: 0.742063\n",
      "Accuracy on Test data: 0.4913058876991272, 0.21763713657855988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 | Training Loss: 0.661561 | Validation Accuracy: 0.760714\n",
      "Accuracy on Test data: 0.4975603222846985, 0.2282700389623642\n",
      "Step 8 | Training Loss: 0.661222 | Validation Accuracy: 0.776190\n",
      "Accuracy on Test data: 0.5193843245506287, 0.26455697417259216\n",
      "Step 9 | Training Loss: 0.654404 | Validation Accuracy: 0.790873\n",
      "Accuracy on Test data: 0.5501685738563538, 0.3206751048564911\n",
      "Step 10 | Training Loss: 0.614166 | Validation Accuracy: 0.814286\n",
      "Accuracy on Test data: 0.5615241527557373, 0.33890295028686523\n",
      "Step 1 | Training Loss: 0.682278 | Validation Accuracy: 0.807540\n",
      "Accuracy on Test data: 0.5623225569725037, 0.34008437395095825\n",
      "Step 2 | Training Loss: 0.638704 | Validation Accuracy: 0.809921\n",
      "Accuracy on Test data: 0.5632984638214111, 0.3416033685207367\n",
      "Step 3 | Training Loss: 0.682334 | Validation Accuracy: 0.826984\n",
      "Accuracy on Test data: 0.5640968680381775, 0.34286919236183167\n",
      "Step 4 | Training Loss: 0.627990 | Validation Accuracy: 0.819048\n",
      "Accuracy on Test data: 0.5651171207427979, 0.3446413576602936\n",
      "Step 5 | Training Loss: 0.602297 | Validation Accuracy: 0.838889\n",
      "Accuracy on Test data: 0.5661373138427734, 0.346160352230072\n",
      "Step 6 | Training Loss: 0.663095 | Validation Accuracy: 0.815873\n",
      "Accuracy on Test data: 0.5672019124031067, 0.3478480875492096\n",
      "Step 7 | Training Loss: 0.654036 | Validation Accuracy: 0.842063\n",
      "Accuracy on Test data: 0.568222165107727, 0.34953585267066956\n",
      "Step 8 | Training Loss: 0.616350 | Validation Accuracy: 0.833333\n",
      "Accuracy on Test data: 0.5691980123519897, 0.351054847240448\n",
      "Step 9 | Training Loss: 0.636103 | Validation Accuracy: 0.832143\n",
      "Accuracy on Test data: 0.5703956484794617, 0.35316455364227295\n",
      "Step 10 | Training Loss: 0.633458 | Validation Accuracy: 0.834921\n",
      "Accuracy on Test data: 0.5716820359230042, 0.3551054894924164\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.733933 | Validation Accuracy: 0.470238\n",
      "Accuracy on Test data: 0.5637863874435425, 0.8085231781005859\n",
      "Step 2 | Training Loss: 0.677271 | Validation Accuracy: 0.471825\n",
      "Accuracy on Test data: 0.5630766749382019, 0.8059915900230408\n",
      "Step 3 | Training Loss: 0.683476 | Validation Accuracy: 0.568254\n",
      "Accuracy on Test data: 0.6612846255302429, 0.7949367165565491\n",
      "Step 4 | Training Loss: 0.666758 | Validation Accuracy: 0.738095\n",
      "Accuracy on Test data: 0.8442600965499878, 0.803966224193573\n",
      "Step 5 | Training Loss: 0.642595 | Validation Accuracy: 0.777778\n",
      "Accuracy on Test data: 0.8485628366470337, 0.7865822911262512\n",
      "Step 6 | Training Loss: 0.667581 | Validation Accuracy: 0.773810\n",
      "Accuracy on Test data: 0.8537526726722717, 0.7885231971740723\n",
      "Step 7 | Training Loss: 0.671569 | Validation Accuracy: 0.784524\n",
      "Accuracy on Test data: 0.854284942150116, 0.7882700562477112\n",
      "Step 8 | Training Loss: 0.634152 | Validation Accuracy: 0.780159\n",
      "Accuracy on Test data: 0.8581884503364563, 0.7875105738639832\n",
      "Step 9 | Training Loss: 0.625654 | Validation Accuracy: 0.776190\n",
      "Accuracy on Test data: 0.8580553531646729, 0.7859915494918823\n",
      "Step 10 | Training Loss: 0.642924 | Validation Accuracy: 0.794048\n",
      "Accuracy on Test data: 0.8595191836357117, 0.7804219126701355\n",
      "Step 1 | Training Loss: 0.647104 | Validation Accuracy: 0.784524\n",
      "Accuracy on Test data: 0.8599627614021301, 0.7794092893600464\n",
      "Step 2 | Training Loss: 0.619690 | Validation Accuracy: 0.796825\n",
      "Accuracy on Test data: 0.8607168197631836, 0.7789873480796814\n",
      "Step 3 | Training Loss: 0.620933 | Validation Accuracy: 0.801190\n",
      "Accuracy on Test data: 0.8609386086463928, 0.7779746651649475\n",
      "Step 4 | Training Loss: 0.598655 | Validation Accuracy: 0.798810\n",
      "Accuracy on Test data: 0.8616483211517334, 0.7774683833122253\n",
      "Step 5 | Training Loss: 0.636483 | Validation Accuracy: 0.800794\n",
      "Accuracy on Test data: 0.8613821864128113, 0.7753586769104004\n",
      "Step 6 | Training Loss: 0.632391 | Validation Accuracy: 0.782143\n",
      "Accuracy on Test data: 0.8527324199676514, 0.7578902840614319\n",
      "Step 7 | Training Loss: 0.636696 | Validation Accuracy: 0.798810\n",
      "Accuracy on Test data: 0.8496717810630798, 0.7514767646789551\n",
      "Step 8 | Training Loss: 0.629840 | Validation Accuracy: 0.801587\n",
      "Accuracy on Test data: 0.8498048186302185, 0.7513924241065979\n",
      "Step 9 | Training Loss: 0.624921 | Validation Accuracy: 0.813095\n",
      "Accuracy on Test data: 0.849937915802002, 0.7510548233985901\n",
      "Step 10 | Training Loss: 0.639450 | Validation Accuracy: 0.803175\n",
      "Accuracy on Test data: 0.8501153588294983, 0.750801682472229\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.716404 | Validation Accuracy: 0.585317\n",
      "Accuracy on Test data: 0.5695084929466248, 0.5223628878593445\n",
      "Step 2 | Training Loss: 0.704238 | Validation Accuracy: 0.685317\n",
      "Accuracy on Test data: 0.6870564222335815, 0.5399156212806702\n",
      "Step 3 | Training Loss: 0.682472 | Validation Accuracy: 0.824603\n",
      "Accuracy on Test data: 0.7586497664451599, 0.5691139101982117\n",
      "Step 4 | Training Loss: 0.697217 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.7928051948547363, 0.6210970282554626\n",
      "Step 5 | Training Loss: 0.641662 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.7986160516738892, 0.6294514536857605\n",
      "Step 6 | Training Loss: 0.606705 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8034066557884216, 0.6374683380126953\n",
      "Step 7 | Training Loss: 0.563823 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.8046930432319641, 0.6394936442375183\n",
      "Step 8 | Training Loss: 0.616304 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.8054027557373047, 0.6396624445915222\n",
      "Step 9 | Training Loss: 0.570046 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.8076649904251099, 0.6432911157608032\n",
      "Step 10 | Training Loss: 0.535518 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8116128444671631, 0.65054851770401\n",
      "Step 1 | Training Loss: 0.561643 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8118346333503723, 0.650970458984375\n",
      "Step 2 | Training Loss: 0.576047 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8120564222335815, 0.65139240026474\n",
      "Step 3 | Training Loss: 0.571387 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8122338652610779, 0.651729941368103\n",
      "Step 4 | Training Loss: 0.524196 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.8123669028282166, 0.6519831418991089\n",
      "Step 5 | Training Loss: 0.524919 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8124113082885742, 0.6520674824714661\n",
      "Step 6 | Training Loss: 0.518458 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8123225569725037, 0.6518987417221069\n",
      "Step 7 | Training Loss: 0.611675 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.8124113082885742, 0.6520674824714661\n",
      "Step 8 | Training Loss: 0.543882 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.8125443458557129, 0.65223628282547\n",
      "Step 9 | Training Loss: 0.589435 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.8127217888832092, 0.6524050831794739\n",
      "Step 10 | Training Loss: 0.538671 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8125443458557129, 0.6519831418991089\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.671870 | Validation Accuracy: 0.764683\n",
      "Accuracy on Test data: 0.6343151330947876, 0.5134177207946777\n",
      "Step 2 | Training Loss: 0.681299 | Validation Accuracy: 0.898016\n",
      "Accuracy on Test data: 0.7436568737030029, 0.6232911348342896\n",
      "Step 3 | Training Loss: 0.657457 | Validation Accuracy: 0.916270\n",
      "Accuracy on Test data: 0.8093949556350708, 0.647679328918457\n",
      "Step 4 | Training Loss: 0.621810 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.8328601717948914, 0.6902953386306763\n",
      "Step 5 | Training Loss: 0.585013 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8262952566146851, 0.6765400767326355\n",
      "Step 6 | Training Loss: 0.565141 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8289123773574829, 0.6807594895362854\n",
      "Step 7 | Training Loss: 0.526524 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.8190205693244934, 0.6609282493591309\n",
      "Step 8 | Training Loss: 0.545763 | Validation Accuracy: 0.953571\n",
      "Accuracy on Test data: 0.8147622346878052, 0.6520674824714661\n",
      "Step 9 | Training Loss: 0.503629 | Validation Accuracy: 0.951190\n",
      "Accuracy on Test data: 0.8148065805435181, 0.65139240026474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.512662 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.8140968680381775, 0.6498734354972839\n",
      "Step 1 | Training Loss: 0.469955 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.8139638304710388, 0.6496202349662781\n",
      "Step 2 | Training Loss: 0.475418 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.8139638304710388, 0.6496202349662781\n",
      "Step 3 | Training Loss: 0.475644 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.8140525221824646, 0.64970463514328\n",
      "Step 4 | Training Loss: 0.483259 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.8140525221824646, 0.6495358943939209\n",
      "Step 5 | Training Loss: 0.450222 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.8140968680381775, 0.6496202349662781\n",
      "Step 6 | Training Loss: 0.518096 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.8141412138938904, 0.64970463514328\n",
      "Step 7 | Training Loss: 0.478529 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.8140968680381775, 0.6496202349662781\n",
      "Step 8 | Training Loss: 0.453239 | Validation Accuracy: 0.954762\n",
      "Accuracy on Test data: 0.8139194250106812, 0.649282693862915\n",
      "Step 9 | Training Loss: 0.499037 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8139194250106812, 0.6491982936859131\n",
      "Step 10 | Training Loss: 0.451632 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.8139638304710388, 0.649282693862915\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.673826 | Validation Accuracy: 0.762698\n",
      "Accuracy on Test data: 0.6945972442626953, 0.4383966326713562\n",
      "Step 2 | Training Loss: 0.606951 | Validation Accuracy: 0.810317\n",
      "Accuracy on Test data: 0.7118523716926575, 0.4674261510372162\n",
      "Step 3 | Training Loss: 0.572831 | Validation Accuracy: 0.842460\n",
      "Accuracy on Test data: 0.7298616170883179, 0.49966245889663696\n",
      "Step 4 | Training Loss: 0.566432 | Validation Accuracy: 0.861905\n",
      "Accuracy on Test data: 0.75, 0.5341772437095642\n",
      "Step 5 | Training Loss: 0.494209 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.7689407467842102, 0.5679324865341187\n",
      "Step 6 | Training Loss: 0.504036 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.7766146063804626, 0.5808438658714294\n",
      "Step 7 | Training Loss: 0.444554 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.7779896855354309, 0.5815190076828003\n",
      "Step 8 | Training Loss: 0.480121 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.7821149826049805, 0.5886920094490051\n",
      "Step 9 | Training Loss: 0.439711 | Validation Accuracy: 0.943651\n",
      "Accuracy on Test data: 0.7765259146690369, 0.5770463943481445\n",
      "Step 10 | Training Loss: 0.424194 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.7757274508476257, 0.5752742886543274\n",
      "Step 1 | Training Loss: 0.440410 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.7754169702529907, 0.5746835470199585\n",
      "Step 2 | Training Loss: 0.436354 | Validation Accuracy: 0.957937\n",
      "Accuracy on Test data: 0.7744854688644409, 0.5729113817214966\n",
      "Step 3 | Training Loss: 0.441327 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7742193341255188, 0.5724050402641296\n",
      "Step 4 | Training Loss: 0.455289 | Validation Accuracy: 0.957937\n",
      "Accuracy on Test data: 0.7739975452423096, 0.5719830989837646\n",
      "Step 5 | Training Loss: 0.445293 | Validation Accuracy: 0.955556\n",
      "Accuracy on Test data: 0.7739531397819519, 0.5718987584114075\n",
      "Step 6 | Training Loss: 0.445345 | Validation Accuracy: 0.954762\n",
      "Accuracy on Test data: 0.7735095620155334, 0.5709704756736755\n",
      "Step 7 | Training Loss: 0.431692 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7735095620155334, 0.5709704756736755\n",
      "Step 8 | Training Loss: 0.444863 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.7733321785926819, 0.5706329345703125\n",
      "Step 9 | Training Loss: 0.426695 | Validation Accuracy: 0.959921\n",
      "Accuracy on Test data: 0.7731547355651855, 0.5702953338623047\n",
      "Step 10 | Training Loss: 0.421607 | Validation Accuracy: 0.964683\n",
      "Accuracy on Test data: 0.7730216383934021, 0.5700421929359436\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.582474 | Validation Accuracy: 0.788889\n",
      "Accuracy on Test data: 0.8517122268676758, 0.7603375315666199\n",
      "Step 2 | Training Loss: 0.550560 | Validation Accuracy: 0.861508\n",
      "Accuracy on Test data: 0.8772178888320923, 0.7900422215461731\n",
      "Step 3 | Training Loss: 0.518329 | Validation Accuracy: 0.869841\n",
      "Accuracy on Test data: 0.8737136125564575, 0.7822784781455994\n",
      "Step 4 | Training Loss: 0.488385 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8743789792060852, 0.7800843715667725\n",
      "Step 5 | Training Loss: 0.433795 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.8732256889343262, 0.7743459939956665\n",
      "Step 6 | Training Loss: 0.444807 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.8727377653121948, 0.7706329226493835\n",
      "Step 7 | Training Loss: 0.440816 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.8742902874946594, 0.7714768052101135\n",
      "Step 8 | Training Loss: 0.452431 | Validation Accuracy: 0.905952\n",
      "Accuracy on Test data: 0.8629790544509888, 0.748860776424408\n",
      "Step 9 | Training Loss: 0.446355 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.8596522212028503, 0.7421941161155701\n",
      "Step 10 | Training Loss: 0.437144 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8586763739585876, 0.7397468090057373\n",
      "Step 1 | Training Loss: 0.433527 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8586763739585876, 0.7396624684333801\n",
      "Step 2 | Training Loss: 0.391359 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8585432767868042, 0.7393248677253723\n",
      "Step 3 | Training Loss: 0.412083 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.8582771420478821, 0.7387341856956482\n",
      "Step 4 | Training Loss: 0.431073 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.8582327961921692, 0.7385653853416443\n",
      "Step 5 | Training Loss: 0.415573 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8580996990203857, 0.7383122444152832\n",
      "Step 6 | Training Loss: 0.411868 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8580553531646729, 0.7382278442382812\n",
      "Step 7 | Training Loss: 0.411536 | Validation Accuracy: 0.943651\n",
      "Accuracy on Test data: 0.8577448725700378, 0.7374683618545532\n",
      "Step 8 | Training Loss: 0.403431 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8574787378311157, 0.7369620203971863\n",
      "Step 9 | Training Loss: 0.410918 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.8573456406593323, 0.7367088794708252\n",
      "Step 10 | Training Loss: 0.423158 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.8568577170372009, 0.7357805967330933\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.682041 | Validation Accuracy: 0.555159\n",
      "Accuracy on Test data: 0.6694020628929138, 0.7772151827812195\n",
      "Step 2 | Training Loss: 0.656250 | Validation Accuracy: 0.706349\n",
      "Accuracy on Test data: 0.7875266075134277, 0.7659915685653687\n",
      "Step 3 | Training Loss: 0.557503 | Validation Accuracy: 0.815873\n",
      "Accuracy on Test data: 0.8626242280006409, 0.7927426099777222\n",
      "Step 4 | Training Loss: 0.519679 | Validation Accuracy: 0.857143\n",
      "Accuracy on Test data: 0.879036545753479, 0.8021097183227539\n",
      "Step 5 | Training Loss: 0.505367 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.8824520707130432, 0.8014345765113831\n",
      "Step 6 | Training Loss: 0.515720 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.8743346333503723, 0.7775527238845825\n",
      "Step 7 | Training Loss: 0.532101 | Validation Accuracy: 0.907143\n",
      "Accuracy on Test data: 0.8703424334526062, 0.7651476860046387\n",
      "Step 8 | Training Loss: 0.478462 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8605393767356873, 0.7445569634437561\n",
      "Step 9 | Training Loss: 0.464418 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8510468602180481, 0.7254008650779724\n",
      "Step 10 | Training Loss: 0.482828 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8459900617599487, 0.7153586745262146\n",
      "Step 1 | Training Loss: 0.479376 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8458126187324524, 0.7149367332458496\n",
      "Step 2 | Training Loss: 0.416988 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.8455908298492432, 0.7144303917884827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 0.440137 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8452360033988953, 0.7135021090507507\n",
      "Step 4 | Training Loss: 0.420628 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8447924256324768, 0.7125738263130188\n",
      "Step 5 | Training Loss: 0.433634 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.844481885433197, 0.7118143439292908\n",
      "Step 6 | Training Loss: 0.388081 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8442157506942749, 0.7113080024719238\n",
      "Step 7 | Training Loss: 0.419587 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8439939618110657, 0.7108860611915588\n",
      "Step 8 | Training Loss: 0.436847 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8438165187835693, 0.7104641199111938\n",
      "Step 9 | Training Loss: 0.415148 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8438608646392822, 0.7105485200881958\n",
      "Step 10 | Training Loss: 0.437735 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8434616923332214, 0.7097046375274658\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.705090 | Validation Accuracy: 0.416270\n",
      "Accuracy on Test data: 0.41022002696990967, 0.6125738620758057\n",
      "Step 2 | Training Loss: 0.658635 | Validation Accuracy: 0.730952\n",
      "Accuracy on Test data: 0.7484918236732483, 0.6111392378807068\n",
      "Step 3 | Training Loss: 0.592087 | Validation Accuracy: 0.862698\n",
      "Accuracy on Test data: 0.8206618428230286, 0.6880168914794922\n",
      "Step 4 | Training Loss: 0.576446 | Validation Accuracy: 0.861905\n",
      "Accuracy on Test data: 0.8245209455490112, 0.6913080215454102\n",
      "Step 5 | Training Loss: 0.543725 | Validation Accuracy: 0.879365\n",
      "Accuracy on Test data: 0.8317068815231323, 0.701603353023529\n",
      "Step 6 | Training Loss: 0.518714 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.8276259899139404, 0.6913080215454102\n",
      "Step 7 | Training Loss: 0.495479 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6814345717430115\n",
      "Step 8 | Training Loss: 0.492144 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.8037171959877014, 0.6409282684326172\n",
      "Step 9 | Training Loss: 0.523258 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8065560460090637, 0.6458227634429932\n",
      "Step 10 | Training Loss: 0.449513 | Validation Accuracy: 0.916270\n",
      "Accuracy on Test data: 0.816226065158844, 0.6622784733772278\n",
      "Step 1 | Training Loss: 0.456334 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.8178672790527344, 0.6650632619857788\n",
      "Step 2 | Training Loss: 0.455884 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6681856513023376\n",
      "Step 3 | Training Loss: 0.482686 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.8209723234176636, 0.6706328988075256\n",
      "Step 4 | Training Loss: 0.451554 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8216376900672913, 0.6718143224716187\n",
      "Step 5 | Training Loss: 0.469324 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8224361538887024, 0.6731645464897156\n",
      "Step 6 | Training Loss: 0.431252 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.823456346988678, 0.6750211119651794\n",
      "Step 7 | Training Loss: 0.459589 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8238555788993835, 0.6757805943489075\n",
      "Step 8 | Training Loss: 0.431281 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.8242548108100891, 0.6765400767326355\n",
      "Step 9 | Training Loss: 0.430161 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.8246983885765076, 0.6773839592933655\n",
      "Step 10 | Training Loss: 0.440858 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8251863121986389, 0.6783122420310974\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.656293 | Validation Accuracy: 0.726190\n",
      "Accuracy on Test data: 0.5212916731834412, 0.27789029479026794\n",
      "Step 2 | Training Loss: 0.655933 | Validation Accuracy: 0.760714\n",
      "Accuracy on Test data: 0.5469304323196411, 0.322447270154953\n",
      "Step 3 | Training Loss: 0.588864 | Validation Accuracy: 0.786508\n",
      "Accuracy on Test data: 0.561657190322876, 0.3471730053424835\n",
      "Step 4 | Training Loss: 0.535257 | Validation Accuracy: 0.850397\n",
      "Accuracy on Test data: 0.659022331237793, 0.3719831109046936\n",
      "Step 5 | Training Loss: 0.532160 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.6782292127609253, 0.40168777108192444\n",
      "Step 6 | Training Loss: 0.521929 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.6933108568191528, 0.4285232126712799\n",
      "Step 7 | Training Loss: 0.485261 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.7044003009796143, 0.4460759460926056\n",
      "Step 8 | Training Loss: 0.455235 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7103441953659058, 0.45586496591567993\n",
      "Step 9 | Training Loss: 0.472032 | Validation Accuracy: 0.943651\n",
      "Accuracy on Test data: 0.7152235507965088, 0.4631223678588867\n",
      "Step 10 | Training Loss: 0.442101 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7219215631484985, 0.474683552980423\n",
      "Step 1 | Training Loss: 0.442184 | Validation Accuracy: 0.951190\n",
      "Accuracy on Test data: 0.7227643728256226, 0.47620251774787903\n",
      "Step 2 | Training Loss: 0.430020 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7237402200698853, 0.4780590832233429\n",
      "Step 3 | Training Loss: 0.426315 | Validation Accuracy: 0.957540\n",
      "Accuracy on Test data: 0.7240064144134521, 0.4784810245037079\n",
      "Step 4 | Training Loss: 0.442783 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7247604727745056, 0.4799156188964844\n",
      "Step 5 | Training Loss: 0.423257 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.7251153588294983, 0.48059073090553284\n",
      "Step 6 | Training Loss: 0.447688 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.7256476283073425, 0.48160338401794434\n",
      "Step 7 | Training Loss: 0.421495 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.7260468602180481, 0.48236286640167236\n",
      "Step 8 | Training Loss: 0.430782 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7263129949569702, 0.4828692078590393\n",
      "Step 9 | Training Loss: 0.435432 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.7265791296958923, 0.48337551951408386\n",
      "Step 10 | Training Loss: 0.446765 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7268896102905273, 0.48396623134613037\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693100 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693095 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693102 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693256 | Validation Accuracy: 0.510317\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692654 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692748 | Validation Accuracy: 0.528968\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692829 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693034 | Validation Accuracy: 0.522619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693115 | Validation Accuracy: 0.513492\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692598 | Validation Accuracy: 0.533730\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692489 | Validation Accuracy: 0.536905\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692903 | Validation Accuracy: 0.551587\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692055 | Validation Accuracy: 0.543254\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692899 | Validation Accuracy: 0.532143\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692358 | Validation Accuracy: 0.528175\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Training Loss: 0.692460 | Validation Accuracy: 0.536508\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692892 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692780 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692554 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692773 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.692682 | Validation Accuracy: 0.722619\n",
      "Accuracy on Test data: 0.5186746120452881, 0.27468353509902954\n",
      "Step 2 | Training Loss: 0.719414 | Validation Accuracy: 0.743254\n",
      "Accuracy on Test data: 0.5526525974273682, 0.33544304966926575\n",
      "Step 3 | Training Loss: 0.667304 | Validation Accuracy: 0.794048\n",
      "Accuracy on Test data: 0.5964779853820801, 0.3584810197353363\n",
      "Step 4 | Training Loss: 0.632227 | Validation Accuracy: 0.813492\n",
      "Accuracy on Test data: 0.6771202683448792, 0.40691983699798584\n",
      "Step 5 | Training Loss: 0.650929 | Validation Accuracy: 0.833333\n",
      "Accuracy on Test data: 0.6903389096260071, 0.43088608980178833\n",
      "Step 6 | Training Loss: 0.685725 | Validation Accuracy: 0.860317\n",
      "Accuracy on Test data: 0.7136266827583313, 0.47333332896232605\n",
      "Step 7 | Training Loss: 0.646188 | Validation Accuracy: 0.853968\n",
      "Accuracy on Test data: 0.72724449634552, 0.4982278347015381\n",
      "Step 8 | Training Loss: 0.619929 | Validation Accuracy: 0.861508\n",
      "Accuracy on Test data: 0.7379347085952759, 0.5170463919639587\n",
      "Step 9 | Training Loss: 0.639527 | Validation Accuracy: 0.870238\n",
      "Accuracy on Test data: 0.7445883750915527, 0.5286920070648193\n",
      "Step 10 | Training Loss: 0.645159 | Validation Accuracy: 0.886905\n",
      "Accuracy on Test data: 0.7533268332481384, 0.544641375541687\n",
      "Step 1 | Training Loss: 0.638498 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.7541252374649048, 0.5459915399551392\n",
      "Step 2 | Training Loss: 0.580023 | Validation Accuracy: 0.881746\n",
      "Accuracy on Test data: 0.7550567984580994, 0.54751056432724\n",
      "Step 3 | Training Loss: 0.604954 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.7564762234687805, 0.5496202707290649\n",
      "Step 4 | Training Loss: 0.582423 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.7575851678848267, 0.5513923764228821\n",
      "Step 5 | Training Loss: 0.612737 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.7583392262458801, 0.5525738596916199\n",
      "Step 6 | Training Loss: 0.595497 | Validation Accuracy: 0.895238\n",
      "Accuracy on Test data: 0.758915901184082, 0.5536708831787109\n",
      "Step 7 | Training Loss: 0.611740 | Validation Accuracy: 0.900000\n",
      "Accuracy on Test data: 0.7600248456001282, 0.5557805895805359\n",
      "Step 8 | Training Loss: 0.595593 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.7606902122497559, 0.5570464134216309\n",
      "Step 9 | Training Loss: 0.590177 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.7616660594940186, 0.5589029788970947\n",
      "Step 10 | Training Loss: 0.591144 | Validation Accuracy: 0.889683\n",
      "Accuracy on Test data: 0.7624201774597168, 0.5601688027381897\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.668258 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.6314318776130676, 0.7915611863136292\n",
      "Step 2 | Training Loss: 0.721940 | Validation Accuracy: 0.734127\n",
      "Accuracy on Test data: 0.8381831049919128, 0.8340928554534912\n",
      "Step 3 | Training Loss: 0.649041 | Validation Accuracy: 0.773016\n",
      "Accuracy on Test data: 0.8446149826049805, 0.8150210976600647\n",
      "Step 4 | Training Loss: 0.653050 | Validation Accuracy: 0.799206\n",
      "Accuracy on Test data: 0.8553495407104492, 0.8211814165115356\n",
      "Step 5 | Training Loss: 0.677737 | Validation Accuracy: 0.804365\n",
      "Accuracy on Test data: 0.8654187321662903, 0.8267510533332825\n",
      "Step 6 | Training Loss: 0.650170 | Validation Accuracy: 0.805159\n",
      "Accuracy on Test data: 0.8742902874946594, 0.8289451599121094\n",
      "Step 7 | Training Loss: 0.657501 | Validation Accuracy: 0.828968\n",
      "Accuracy on Test data: 0.878726065158844, 0.8298734426498413\n",
      "Step 8 | Training Loss: 0.587645 | Validation Accuracy: 0.827381\n",
      "Accuracy on Test data: 0.8765081763267517, 0.8163713216781616\n",
      "Step 9 | Training Loss: 0.610045 | Validation Accuracy: 0.840079\n",
      "Accuracy on Test data: 0.8757097125053406, 0.8108860850334167\n",
      "Step 10 | Training Loss: 0.635093 | Validation Accuracy: 0.877778\n",
      "Accuracy on Test data: 0.8796575665473938, 0.8075105547904968\n",
      "Step 1 | Training Loss: 0.608923 | Validation Accuracy: 0.869841\n",
      "Accuracy on Test data: 0.8801454901695251, 0.8068354725837708\n",
      "Step 2 | Training Loss: 0.633844 | Validation Accuracy: 0.867857\n",
      "Accuracy on Test data: 0.8802341818809509, 0.8054008483886719\n",
      "Step 3 | Training Loss: 0.595726 | Validation Accuracy: 0.865476\n",
      "Accuracy on Test data: 0.8807665109634399, 0.8050633072853088\n",
      "Step 4 | Training Loss: 0.617605 | Validation Accuracy: 0.868254\n",
      "Accuracy on Test data: 0.8808995485305786, 0.8043038249015808\n",
      "Step 5 | Training Loss: 0.606337 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.8808108568191528, 0.80362868309021\n",
      "Step 6 | Training Loss: 0.623009 | Validation Accuracy: 0.878175\n",
      "Accuracy on Test data: 0.8808552026748657, 0.8032911419868469\n",
      "Step 7 | Training Loss: 0.602514 | Validation Accuracy: 0.875000\n",
      "Accuracy on Test data: 0.8808108568191528, 0.80278480052948\n",
      "Step 8 | Training Loss: 0.595059 | Validation Accuracy: 0.884127\n",
      "Accuracy on Test data: 0.8804559707641602, 0.801856517791748\n",
      "Step 9 | Training Loss: 0.577496 | Validation Accuracy: 0.868254\n",
      "Accuracy on Test data: 0.8801454901695251, 0.8010126352310181\n",
      "Step 10 | Training Loss: 0.584548 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.879036545753479, 0.7989029288291931\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.662488 | Validation Accuracy: 0.754762\n",
      "Accuracy on Test data: 0.7023154497146606, 0.4706329107284546\n",
      "Step 2 | Training Loss: 0.656126 | Validation Accuracy: 0.803175\n",
      "Accuracy on Test data: 0.7465400695800781, 0.5348523259162903\n",
      "Step 3 | Training Loss: 0.619524 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.7813165187835693, 0.598227858543396\n",
      "Step 4 | Training Loss: 0.622733 | Validation Accuracy: 0.886111\n",
      "Accuracy on Test data: 0.8132540583610535, 0.6584810018539429\n",
      "Step 5 | Training Loss: 0.637013 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.8260291218757629, 0.6816877722740173\n",
      "Step 6 | Training Loss: 0.638238 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.8329489231109619, 0.6935864686965942\n",
      "Step 7 | Training Loss: 0.596811 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.8377395272254944, 0.702447235584259\n",
      "Step 8 | Training Loss: 0.559466 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.8386266827583313, 0.7034599184989929\n",
      "Step 9 | Training Loss: 0.561425 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8386266827583313, 0.7032067775726318\n",
      "Step 10 | Training Loss: 0.543846 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.8393363952636719, 0.7038818597793579\n",
      "Step 1 | Training Loss: 0.534783 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.8395138382911682, 0.704135000705719\n",
      "Step 2 | Training Loss: 0.545325 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.839602530002594, 0.704219400882721\n",
      "Step 3 | Training Loss: 0.510395 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8395581841468811, 0.704135000705719\n",
      "Step 4 | Training Loss: 0.524454 | Validation Accuracy: 0.918254\n",
      "Accuracy on Test data: 0.8394251465797424, 0.7038818597793579\n",
      "Step 5 | Training Loss: 0.501285 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8394694924354553, 0.7039662599563599\n",
      "Step 6 | Training Loss: 0.572812 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.8395581841468811, 0.704135000705719\n",
      "Step 7 | Training Loss: 0.515774 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8399130702018738, 0.7047257423400879\n",
      "Step 8 | Training Loss: 0.543856 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8397356271743774, 0.7043038010597229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.515720 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8398687243461609, 0.7044726014137268\n",
      "Step 10 | Training Loss: 0.535957 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8398243188858032, 0.7043038010597229\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.695120 | Validation Accuracy: 0.610317\n",
      "Accuracy on Test data: 0.5602821111679077, 0.375189870595932\n",
      "Step 2 | Training Loss: 0.660552 | Validation Accuracy: 0.815873\n",
      "Accuracy on Test data: 0.7616660594940186, 0.5670042037963867\n",
      "Step 3 | Training Loss: 0.623373 | Validation Accuracy: 0.882540\n",
      "Accuracy on Test data: 0.8219038248062134, 0.6778903007507324\n",
      "Step 4 | Training Loss: 0.629889 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.8461675047874451, 0.7217721343040466\n",
      "Step 5 | Training Loss: 0.626747 | Validation Accuracy: 0.902381\n",
      "Accuracy on Test data: 0.8419091701507568, 0.7113924026489258\n",
      "Step 6 | Training Loss: 0.573192 | Validation Accuracy: 0.907540\n",
      "Accuracy on Test data: 0.8411107063293457, 0.7059071660041809\n",
      "Step 7 | Training Loss: 0.547752 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.8390259146690369, 0.69974684715271\n",
      "Step 8 | Training Loss: 0.523875 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8366305828094482, 0.695189893245697\n",
      "Step 9 | Training Loss: 0.486426 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.8345457911491394, 0.6909704804420471\n",
      "Step 10 | Training Loss: 0.462855 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8324609398841858, 0.6869198083877563\n",
      "Step 1 | Training Loss: 0.508775 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8322391510009766, 0.6864135265350342\n",
      "Step 2 | Training Loss: 0.483329 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8318843245506287, 0.6857383847236633\n",
      "Step 3 | Training Loss: 0.460773 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8317955732345581, 0.6855696439743042\n",
      "Step 4 | Training Loss: 0.472317 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8316181898117065, 0.6852320432662964\n",
      "Step 5 | Training Loss: 0.493245 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.8314850926399231, 0.6848945021629333\n",
      "Step 6 | Training Loss: 0.458536 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8313076496124268, 0.6845569610595703\n",
      "Step 7 | Training Loss: 0.443293 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8310858607292175, 0.6841350197792053\n",
      "Step 8 | Training Loss: 0.464267 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8308197259902954, 0.6836286783218384\n",
      "Step 9 | Training Loss: 0.469244 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.830686628818512, 0.6833755373954773\n",
      "Step 10 | Training Loss: 0.464370 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8305979371070862, 0.6832067370414734\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.683240 | Validation Accuracy: 0.483730\n",
      "Accuracy on Test data: 0.5551809668540955, 0.7886075973510742\n",
      "Step 2 | Training Loss: 0.628144 | Validation Accuracy: 0.606746\n",
      "Accuracy on Test data: 0.6861249208450317, 0.7836287021636963\n",
      "Step 3 | Training Loss: 0.619903 | Validation Accuracy: 0.778571\n",
      "Accuracy on Test data: 0.8605837225914001, 0.8537552952766418\n",
      "Step 4 | Training Loss: 0.596097 | Validation Accuracy: 0.840873\n",
      "Accuracy on Test data: 0.8915454149246216, 0.8456540107727051\n",
      "Step 5 | Training Loss: 0.545713 | Validation Accuracy: 0.869841\n",
      "Accuracy on Test data: 0.901481568813324, 0.8410970568656921\n",
      "Step 6 | Training Loss: 0.480834 | Validation Accuracy: 0.882937\n",
      "Accuracy on Test data: 0.9074698090553284, 0.8424472808837891\n",
      "Step 7 | Training Loss: 0.514278 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.8821415901184082, 0.7877637147903442\n",
      "Step 8 | Training Loss: 0.465134 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8681688904762268, 0.7591561079025269\n",
      "Step 9 | Training Loss: 0.454755 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.85512775182724, 0.7339240312576294\n",
      "Step 10 | Training Loss: 0.435497 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8470990061759949, 0.7175527215003967\n",
      "Step 1 | Training Loss: 0.463840 | Validation Accuracy: 0.907143\n",
      "Accuracy on Test data: 0.8467441201210022, 0.7167932391166687\n",
      "Step 2 | Training Loss: 0.455051 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8459900617599487, 0.7151054739952087\n",
      "Step 3 | Training Loss: 0.441595 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8443931937217712, 0.7120674848556519\n",
      "Step 4 | Training Loss: 0.407686 | Validation Accuracy: 0.907937\n",
      "Accuracy on Test data: 0.8436834812164307, 0.7107173204421997\n",
      "Step 5 | Training Loss: 0.411223 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.8431511521339417, 0.7096202373504639\n",
      "Step 6 | Training Loss: 0.451034 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8419535160064697, 0.7072573900222778\n",
      "Step 7 | Training Loss: 0.429666 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.8410220146179199, 0.7054852247238159\n",
      "Step 8 | Training Loss: 0.419797 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8408889174461365, 0.704978883266449\n",
      "Step 9 | Training Loss: 0.426789 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8401792049407959, 0.7036287188529968\n",
      "Step 10 | Training Loss: 0.425938 | Validation Accuracy: 0.918254\n",
      "Accuracy on Test data: 0.8394694924354553, 0.702109694480896\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.705905 | Validation Accuracy: 0.650794\n",
      "Accuracy on Test data: 0.474228173494339, 0.19763712584972382\n",
      "Step 2 | Training Loss: 0.653529 | Validation Accuracy: 0.696825\n",
      "Accuracy on Test data: 0.4853619635105133, 0.2111392468214035\n",
      "Step 3 | Training Loss: 0.663189 | Validation Accuracy: 0.692857\n",
      "Accuracy on Test data: 0.4980482757091522, 0.23282700777053833\n",
      "Step 4 | Training Loss: 0.648770 | Validation Accuracy: 0.735714\n",
      "Accuracy on Test data: 0.514150083065033, 0.25814345479011536\n",
      "Step 5 | Training Loss: 0.546386 | Validation Accuracy: 0.785317\n",
      "Accuracy on Test data: 0.5499467849731445, 0.3212658166885376\n",
      "Step 6 | Training Loss: 0.556205 | Validation Accuracy: 0.817460\n",
      "Accuracy on Test data: 0.6643896102905273, 0.3868354558944702\n",
      "Step 7 | Training Loss: 0.535881 | Validation Accuracy: 0.838095\n",
      "Accuracy on Test data: 0.683773934841156, 0.41392403841018677\n",
      "Step 8 | Training Loss: 0.583606 | Validation Accuracy: 0.872619\n",
      "Accuracy on Test data: 0.6973474025726318, 0.4362025260925293\n",
      "Step 9 | Training Loss: 0.496912 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.7149130702018738, 0.4658227860927582\n",
      "Step 10 | Training Loss: 0.477875 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.725248396396637, 0.4832911491394043\n",
      "Step 1 | Training Loss: 0.428195 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.726401686668396, 0.4854852259159088\n",
      "Step 2 | Training Loss: 0.424124 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.7277324199676514, 0.4879325032234192\n",
      "Step 3 | Training Loss: 0.457274 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.7288857102394104, 0.49004217982292175\n",
      "Step 4 | Training Loss: 0.479830 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.7306600213050842, 0.4934177100658417\n",
      "Step 5 | Training Loss: 0.438482 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.7324787378311157, 0.49687764048576355\n",
      "Step 6 | Training Loss: 0.445216 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.7335432767868042, 0.49890294671058655\n",
      "Step 7 | Training Loss: 0.440132 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7347853183746338, 0.501265823841095\n",
      "Step 8 | Training Loss: 0.434766 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.735317587852478, 0.5022785067558289\n",
      "Step 9 | Training Loss: 0.477610 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.7362047433853149, 0.5039662718772888\n",
      "Step 10 | Training Loss: 0.422839 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.7371806502342224, 0.5056540369987488\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.650577 | Validation Accuracy: 0.719444\n",
      "Accuracy on Test data: 0.4916163980960846, 0.2133333384990692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.648512 | Validation Accuracy: 0.778968\n",
      "Accuracy on Test data: 0.5994055867195129, 0.2605063319206238\n",
      "Step 3 | Training Loss: 0.591696 | Validation Accuracy: 0.848016\n",
      "Accuracy on Test data: 0.6419003009796143, 0.33763712644577026\n",
      "Step 4 | Training Loss: 0.553540 | Validation Accuracy: 0.860714\n",
      "Accuracy on Test data: 0.6633694171905518, 0.37586498260498047\n",
      "Step 5 | Training Loss: 0.539500 | Validation Accuracy: 0.888889\n",
      "Accuracy on Test data: 0.6876330971717834, 0.4174683690071106\n",
      "Step 6 | Training Loss: 0.489102 | Validation Accuracy: 0.900000\n",
      "Accuracy on Test data: 0.7014282941818237, 0.4421941041946411\n",
      "Step 7 | Training Loss: 0.502714 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.713981568813324, 0.46447256207466125\n",
      "Step 8 | Training Loss: 0.455328 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.7282203435897827, 0.4911392331123352\n",
      "Step 9 | Training Loss: 0.461497 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.7443665862083435, 0.5204219222068787\n",
      "Step 10 | Training Loss: 0.436993 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.7578069567680359, 0.5445569753646851\n",
      "Step 1 | Training Loss: 0.411039 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7596256136894226, 0.5477637052536011\n",
      "Step 2 | Training Loss: 0.464011 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.7603353261947632, 0.5489451289176941\n",
      "Step 3 | Training Loss: 0.444773 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.7610450387001038, 0.5501265525817871\n",
      "Step 4 | Training Loss: 0.424294 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.7621983885765076, 0.552320659160614\n",
      "Step 5 | Training Loss: 0.411484 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7639282941818237, 0.5554430484771729\n",
      "Step 6 | Training Loss: 0.409193 | Validation Accuracy: 0.953571\n",
      "Accuracy on Test data: 0.7655695676803589, 0.5584810376167297\n",
      "Step 7 | Training Loss: 0.452699 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.7665897607803345, 0.5603375434875488\n",
      "Step 8 | Training Loss: 0.433482 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.76698899269104, 0.5608438849449158\n",
      "Step 9 | Training Loss: 0.406326 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7675212621688843, 0.5616033673286438\n",
      "Step 10 | Training Loss: 0.422596 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5623628497123718\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.651358 | Validation Accuracy: 0.588095\n",
      "Accuracy on Test data: 0.7310149073600769, 0.749620258808136\n",
      "Step 2 | Training Loss: 0.607094 | Validation Accuracy: 0.735714\n",
      "Accuracy on Test data: 0.8121007680892944, 0.7389873266220093\n",
      "Step 3 | Training Loss: 0.600913 | Validation Accuracy: 0.793254\n",
      "Accuracy on Test data: 0.8378726243972778, 0.746329128742218\n",
      "Step 4 | Training Loss: 0.542735 | Validation Accuracy: 0.824603\n",
      "Accuracy on Test data: 0.8594304323196411, 0.7711392641067505\n",
      "Step 5 | Training Loss: 0.543790 | Validation Accuracy: 0.850397\n",
      "Accuracy on Test data: 0.8567246198654175, 0.7509704828262329\n",
      "Step 6 | Training Loss: 0.507520 | Validation Accuracy: 0.888492\n",
      "Accuracy on Test data: 0.8523775935173035, 0.7348523139953613\n",
      "Step 7 | Training Loss: 0.492590 | Validation Accuracy: 0.898016\n",
      "Accuracy on Test data: 0.8505145311355591, 0.7289451360702515\n",
      "Step 8 | Training Loss: 0.487839 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.8511798977851868, 0.7269198298454285\n",
      "Step 9 | Training Loss: 0.474790 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8506919741630554, 0.7243037819862366\n",
      "Step 10 | Training Loss: 0.417204 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8464336395263672, 0.7151054739952087\n",
      "Step 1 | Training Loss: 0.435691 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8459457159042358, 0.7140084505081177\n",
      "Step 2 | Training Loss: 0.431399 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8455021381378174, 0.7130801677703857\n",
      "Step 3 | Training Loss: 0.479658 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.845014214515686, 0.7121518850326538\n",
      "Step 4 | Training Loss: 0.457613 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8441270589828491, 0.7102953791618347\n",
      "Step 5 | Training Loss: 0.412875 | Validation Accuracy: 0.926587\n",
      "Accuracy on Test data: 0.8427519798278809, 0.7075949311256409\n",
      "Step 6 | Training Loss: 0.429037 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8412438035011292, 0.7046413421630859\n",
      "Step 7 | Training Loss: 0.439051 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8393363952636719, 0.7010126709938049\n",
      "Step 8 | Training Loss: 0.422567 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8386266827583313, 0.699662446975708\n",
      "Step 9 | Training Loss: 0.446597 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8377838730812073, 0.6978902816772461\n",
      "Step 10 | Training Loss: 0.422712 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8374290466308594, 0.6970463991165161\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.672192 | Validation Accuracy: 0.601587\n",
      "Accuracy on Test data: 0.6231369972229004, 0.5653164386749268\n",
      "Step 2 | Training Loss: 0.609515 | Validation Accuracy: 0.754762\n",
      "Accuracy on Test data: 0.6798261404037476, 0.5778902769088745\n",
      "Step 3 | Training Loss: 0.604544 | Validation Accuracy: 0.819444\n",
      "Accuracy on Test data: 0.7626863121986389, 0.5991561412811279\n",
      "Step 4 | Training Loss: 0.571957 | Validation Accuracy: 0.836508\n",
      "Accuracy on Test data: 0.790897786617279, 0.6202531456947327\n",
      "Step 5 | Training Loss: 0.516549 | Validation Accuracy: 0.853571\n",
      "Accuracy on Test data: 0.7973740100860596, 0.6304641366004944\n",
      "Step 6 | Training Loss: 0.466741 | Validation Accuracy: 0.849603\n",
      "Accuracy on Test data: 0.8026525974273682, 0.6394936442375183\n",
      "Step 7 | Training Loss: 0.543326 | Validation Accuracy: 0.867857\n",
      "Accuracy on Test data: 0.8159599304199219, 0.6637130975723267\n",
      "Step 8 | Training Loss: 0.478861 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8324609398841858, 0.6915611624717712\n",
      "Step 9 | Training Loss: 0.524538 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.8412438035011292, 0.7076793313026428\n",
      "Step 10 | Training Loss: 0.446406 | Validation Accuracy: 0.905159\n",
      "Accuracy on Test data: 0.844171404838562, 0.7136709094047546\n",
      "Step 1 | Training Loss: 0.453631 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8445262312889099, 0.7142615914344788\n",
      "Step 2 | Training Loss: 0.435481 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8449254631996155, 0.7150210738182068\n",
      "Step 3 | Training Loss: 0.433149 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.8453690409660339, 0.7158649563789368\n",
      "Step 4 | Training Loss: 0.451488 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8459457159042358, 0.7167932391166687\n",
      "Step 5 | Training Loss: 0.445983 | Validation Accuracy: 0.921825\n",
      "Accuracy on Test data: 0.8464779853820801, 0.7179746627807617\n",
      "Step 6 | Training Loss: 0.447492 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8469659090042114, 0.7189873456954956\n",
      "Step 7 | Training Loss: 0.439902 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.8470990061759949, 0.7192404866218567\n",
      "Step 8 | Training Loss: 0.444052 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8472764492034912, 0.7194936871528625\n",
      "Step 9 | Training Loss: 0.410377 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8474094867706299, 0.7196624279022217\n",
      "Step 10 | Training Loss: 0.462310 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.8474094867706299, 0.7194936871528625\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693173 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693039 | Validation Accuracy: 0.547619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692953 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692685 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.693126 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692760 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692908 | Validation Accuracy: 0.539286\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693278 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693475 | Validation Accuracy: 0.526984\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.691717 | Validation Accuracy: 0.525397\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692407 | Validation Accuracy: 0.543254\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693520 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692599 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693113 | Validation Accuracy: 0.541270\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.693008 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692901 | Validation Accuracy: 0.523016\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.693219 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693650 | Validation Accuracy: 0.549206\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.694524 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.693002 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.720491 | Validation Accuracy: 0.633333\n",
      "Accuracy on Test data: 0.6304116249084473, 0.36118143796920776\n",
      "Step 2 | Training Loss: 0.699399 | Validation Accuracy: 0.670635\n",
      "Accuracy on Test data: 0.6430535912513733, 0.37713080644607544\n",
      "Step 3 | Training Loss: 0.710373 | Validation Accuracy: 0.697619\n",
      "Accuracy on Test data: 0.6423438787460327, 0.35940927267074585\n",
      "Step 4 | Training Loss: 0.652065 | Validation Accuracy: 0.768651\n",
      "Accuracy on Test data: 0.6583126187324524, 0.38075950741767883\n",
      "Step 5 | Training Loss: 0.669078 | Validation Accuracy: 0.817857\n",
      "Accuracy on Test data: 0.6812455654144287, 0.41898733377456665\n",
      "Step 6 | Training Loss: 0.637139 | Validation Accuracy: 0.838492\n",
      "Accuracy on Test data: 0.7141589522361755, 0.47594937682151794\n",
      "Step 7 | Training Loss: 0.622535 | Validation Accuracy: 0.855556\n",
      "Accuracy on Test data: 0.7232522964477539, 0.4891139268875122\n",
      "Step 8 | Training Loss: 0.637866 | Validation Accuracy: 0.865079\n",
      "Accuracy on Test data: 0.7310592532157898, 0.502362847328186\n",
      "Step 9 | Training Loss: 0.615460 | Validation Accuracy: 0.886111\n",
      "Accuracy on Test data: 0.7392654418945312, 0.5158649682998657\n",
      "Step 10 | Training Loss: 0.663402 | Validation Accuracy: 0.884127\n",
      "Accuracy on Test data: 0.7493346333503723, 0.5335865020751953\n",
      "Step 1 | Training Loss: 0.595137 | Validation Accuracy: 0.878571\n",
      "Accuracy on Test data: 0.7503992319107056, 0.5352742671966553\n",
      "Step 2 | Training Loss: 0.634373 | Validation Accuracy: 0.875000\n",
      "Accuracy on Test data: 0.7504435777664185, 0.5352742671966553\n",
      "Step 3 | Training Loss: 0.600545 | Validation Accuracy: 0.871032\n",
      "Accuracy on Test data: 0.7508871555328369, 0.5357806086540222\n",
      "Step 4 | Training Loss: 0.637083 | Validation Accuracy: 0.882937\n",
      "Accuracy on Test data: 0.7510645985603333, 0.5360337495803833\n",
      "Step 5 | Training Loss: 0.591993 | Validation Accuracy: 0.865873\n",
      "Accuracy on Test data: 0.7508871555328369, 0.5356962084770203\n",
      "Step 6 | Training Loss: 0.640063 | Validation Accuracy: 0.874603\n",
      "Accuracy on Test data: 0.7505766749382019, 0.5350211262702942\n",
      "Step 7 | Training Loss: 0.567454 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.7502661347389221, 0.5341772437095642\n",
      "Step 8 | Training Loss: 0.612293 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.7498669028282166, 0.5332489609718323\n",
      "Step 9 | Training Loss: 0.609455 | Validation Accuracy: 0.888889\n",
      "Accuracy on Test data: 0.7491128444671631, 0.5317299365997314\n",
      "Step 10 | Training Loss: 0.630708 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7484031319618225, 0.5302953720092773\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.694996 | Validation Accuracy: 0.633333\n",
      "Accuracy on Test data: 0.7238289713859558, 0.797468364238739\n",
      "Step 2 | Training Loss: 0.685773 | Validation Accuracy: 0.750397\n",
      "Accuracy on Test data: 0.8178672790527344, 0.7685232162475586\n",
      "Step 3 | Training Loss: 0.692638 | Validation Accuracy: 0.782143\n",
      "Accuracy on Test data: 0.8352998495101929, 0.7637974619865417\n",
      "Step 4 | Training Loss: 0.661395 | Validation Accuracy: 0.807936\n",
      "Accuracy on Test data: 0.8437278270721436, 0.7614346146583557\n",
      "Step 5 | Training Loss: 0.641245 | Validation Accuracy: 0.828175\n",
      "Accuracy on Test data: 0.8449254631996155, 0.7578058838844299\n",
      "Step 6 | Training Loss: 0.706467 | Validation Accuracy: 0.828571\n",
      "Accuracy on Test data: 0.8436834812164307, 0.748016893863678\n",
      "Step 7 | Training Loss: 0.639771 | Validation Accuracy: 0.837302\n",
      "Accuracy on Test data: 0.8292228579521179, 0.7178903222084045\n",
      "Step 8 | Training Loss: 0.656786 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8513129949569702, 0.7452320456504822\n",
      "Step 9 | Training Loss: 0.574072 | Validation Accuracy: 0.890873\n",
      "Accuracy on Test data: 0.8554826378822327, 0.7443881630897522\n",
      "Step 10 | Training Loss: 0.584728 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.8451915979385376, 0.7190717458724976\n",
      "Step 1 | Training Loss: 0.603377 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8452360033988953, 0.7190717458724976\n",
      "Step 2 | Training Loss: 0.597298 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.8453690409660339, 0.7191561460494995\n",
      "Step 3 | Training Loss: 0.552120 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.845324695110321, 0.7190717458724976\n",
      "Step 4 | Training Loss: 0.594733 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.8455908298492432, 0.7192404866218567\n",
      "Step 5 | Training Loss: 0.623434 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8454577922821045, 0.7189873456954956\n",
      "Step 6 | Training Loss: 0.595293 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8454133868217468, 0.7189029455184937\n",
      "Step 7 | Training Loss: 0.553951 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.8454577922821045, 0.7189029455184937\n",
      "Step 8 | Training Loss: 0.601343 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.8455021381378174, 0.7189029455184937\n",
      "Step 9 | Training Loss: 0.587721 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8455021381378174, 0.7189029455184937\n",
      "Step 10 | Training Loss: 0.596083 | Validation Accuracy: 0.910317\n",
      "Accuracy on Test data: 0.8455464839935303, 0.7189873456954956\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.674197 | Validation Accuracy: 0.716667\n",
      "Accuracy on Test data: 0.8100159764289856, 0.8524894714355469\n",
      "Step 2 | Training Loss: 0.661934 | Validation Accuracy: 0.785317\n",
      "Accuracy on Test data: 0.862358033657074, 0.8448101282119751\n",
      "Step 3 | Training Loss: 0.632108 | Validation Accuracy: 0.814683\n",
      "Accuracy on Test data: 0.8843151330947876, 0.8426160216331482\n",
      "Step 4 | Training Loss: 0.670609 | Validation Accuracy: 0.833333\n",
      "Accuracy on Test data: 0.8809882998466492, 0.8152742385864258\n",
      "Step 5 | Training Loss: 0.611444 | Validation Accuracy: 0.843254\n",
      "Accuracy on Test data: 0.8857345581054688, 0.8173839449882507\n",
      "Step 6 | Training Loss: 0.611900 | Validation Accuracy: 0.861111\n",
      "Accuracy on Test data: 0.8892831802368164, 0.8174683451652527\n",
      "Step 7 | Training Loss: 0.594286 | Validation Accuracy: 0.873810\n",
      "Accuracy on Test data: 0.8896380662918091, 0.8158649802207947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: 0.637401 | Validation Accuracy: 0.890476\n",
      "Accuracy on Test data: 0.8824077248573303, 0.7992405295372009\n",
      "Step 9 | Training Loss: 0.524148 | Validation Accuracy: 0.898413\n",
      "Accuracy on Test data: 0.8739354014396667, 0.7776371240615845\n",
      "Step 10 | Training Loss: 0.566216 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.8694996237754822, 0.7644725441932678\n",
      "Step 1 | Training Loss: 0.540101 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8700762987136841, 0.7651476860046387\n",
      "Step 2 | Training Loss: 0.552633 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.8702980875968933, 0.7652320861816406\n",
      "Step 3 | Training Loss: 0.564915 | Validation Accuracy: 0.905952\n",
      "Accuracy on Test data: 0.8700319528579712, 0.7646413445472717\n",
      "Step 4 | Training Loss: 0.519248 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8696770668029785, 0.7637974619865417\n",
      "Step 5 | Training Loss: 0.550367 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8695883750915527, 0.7632911205291748\n",
      "Step 6 | Training Loss: 0.567420 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8693222403526306, 0.7627004384994507\n",
      "Step 7 | Training Loss: 0.504278 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8690560460090637, 0.7621096968650818\n",
      "Step 8 | Training Loss: 0.522890 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.8688342571258545, 0.7614346146583557\n",
      "Step 9 | Training Loss: 0.542984 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.8685681223869324, 0.7607594728469849\n",
      "Step 10 | Training Loss: 0.500592 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.8682132959365845, 0.7600843906402588\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.654347 | Validation Accuracy: 0.742857\n",
      "Accuracy on Test data: 0.8440826535224915, 0.794177234172821\n",
      "Step 2 | Training Loss: 0.613005 | Validation Accuracy: 0.786111\n",
      "Accuracy on Test data: 0.8592086434364319, 0.7869198322296143\n",
      "Step 3 | Training Loss: 0.583966 | Validation Accuracy: 0.836111\n",
      "Accuracy on Test data: 0.8601401448249817, 0.7705485224723816\n",
      "Step 4 | Training Loss: 0.544836 | Validation Accuracy: 0.887302\n",
      "Accuracy on Test data: 0.865196943283081, 0.7589029669761658\n",
      "Step 5 | Training Loss: 0.537357 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.8671930432319641, 0.7559493780136108\n",
      "Step 6 | Training Loss: 0.568722 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8676809668540955, 0.7551898956298828\n",
      "Step 7 | Training Loss: 0.531322 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8679915070533752, 0.754514753818512\n",
      "Step 8 | Training Loss: 0.481520 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8668825626373291, 0.7518143653869629\n",
      "Step 9 | Training Loss: 0.454613 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.862890362739563, 0.74388188123703\n",
      "Step 10 | Training Loss: 0.451661 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8347232341766357, 0.6900421977043152\n",
      "Step 1 | Training Loss: 0.442827 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8335698843002319, 0.6878480911254883\n",
      "Step 2 | Training Loss: 0.430812 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.833215057849884, 0.6871730089187622\n",
      "Step 3 | Training Loss: 0.463640 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8328601717948914, 0.6864978671073914\n",
      "Step 4 | Training Loss: 0.450704 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8323279023170471, 0.6854852437973022\n",
      "Step 5 | Training Loss: 0.465463 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8316181898117065, 0.6841350197792053\n",
      "Step 6 | Training Loss: 0.430366 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8310858607292175, 0.6831223368644714\n",
      "Step 7 | Training Loss: 0.452485 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8307310342788696, 0.6824472546577454\n",
      "Step 8 | Training Loss: 0.423325 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8302430510520935, 0.6815189719200134\n",
      "Step 9 | Training Loss: 0.440535 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.82948899269104, 0.6800844073295593\n",
      "Step 10 | Training Loss: 0.436593 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8290454149246216, 0.6791561245918274\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.677952 | Validation Accuracy: 0.496825\n",
      "Accuracy on Test data: 0.46469128131866455, 0.5757805705070496\n",
      "Step 2 | Training Loss: 0.672981 | Validation Accuracy: 0.599603\n",
      "Accuracy on Test data: 0.5747427344322205, 0.5865823030471802\n",
      "Step 3 | Training Loss: 0.655341 | Validation Accuracy: 0.846429\n",
      "Accuracy on Test data: 0.7873491644859314, 0.6145991683006287\n",
      "Step 4 | Training Loss: 0.564717 | Validation Accuracy: 0.862302\n",
      "Accuracy on Test data: 0.78974449634552, 0.6174683570861816\n",
      "Step 5 | Training Loss: 0.611295 | Validation Accuracy: 0.876190\n",
      "Accuracy on Test data: 0.7806511521339417, 0.598565399646759\n",
      "Step 6 | Training Loss: 0.485849 | Validation Accuracy: 0.876587\n",
      "Accuracy on Test data: 0.7829577922821045, 0.6020253300666809\n",
      "Step 7 | Training Loss: 0.472327 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.7849982380867004, 0.603459894657135\n",
      "Step 8 | Training Loss: 0.496154 | Validation Accuracy: 0.892460\n",
      "Accuracy on Test data: 0.7886355519294739, 0.6082700490951538\n",
      "Step 9 | Training Loss: 0.433144 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.7912970185279846, 0.6118143200874329\n",
      "Step 10 | Training Loss: 0.454923 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.7966199517250061, 0.6197468638420105\n",
      "Step 1 | Training Loss: 0.461170 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.7968417406082153, 0.6200000047683716\n",
      "Step 2 | Training Loss: 0.430804 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.7985273003578186, 0.6230379939079285\n",
      "Step 3 | Training Loss: 0.489416 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.7999911308288574, 0.6258227825164795\n",
      "Step 4 | Training Loss: 0.475861 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8021202683448792, 0.6297046542167664\n",
      "Step 5 | Training Loss: 0.445020 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8031405210494995, 0.6314768195152283\n",
      "Step 6 | Training Loss: 0.455097 | Validation Accuracy: 0.928175\n",
      "Accuracy on Test data: 0.8030074238777161, 0.6311392188072205\n",
      "Step 7 | Training Loss: 0.453361 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8030961751937866, 0.6313080191612244\n",
      "Step 8 | Training Loss: 0.450888 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8032292127609253, 0.6313080191612244\n",
      "Step 9 | Training Loss: 0.454033 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8033623099327087, 0.6314768195152283\n",
      "Step 10 | Training Loss: 0.459188 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8034510016441345, 0.6316455602645874\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.704588 | Validation Accuracy: 0.528175\n",
      "Accuracy on Test data: 0.4981813430786133, 0.5344303846359253\n",
      "Step 2 | Training Loss: 0.659759 | Validation Accuracy: 0.729365\n",
      "Accuracy on Test data: 0.6438519954681396, 0.5297890305519104\n",
      "Step 3 | Training Loss: 0.585712 | Validation Accuracy: 0.858333\n",
      "Accuracy on Test data: 0.7441447973251343, 0.5342615842819214\n",
      "Step 4 | Training Loss: 0.546930 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.748314380645752, 0.5368776321411133\n",
      "Step 5 | Training Loss: 0.530453 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.7548350095748901, 0.5448101162910461\n",
      "Step 6 | Training Loss: 0.506566 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.7598917484283447, 0.551476776599884\n",
      "Step 7 | Training Loss: 0.476792 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.7618435025215149, 0.554345965385437\n",
      "Step 8 | Training Loss: 0.500576 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7659243941307068, 0.5612658262252808\n",
      "Step 9 | Training Loss: 0.419683 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.7662792801856995, 0.5610126852989197\n",
      "Step 10 | Training Loss: 0.446631 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.7633960247039795, 0.5551055073738098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.438551 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.7633960247039795, 0.5551055073738098\n",
      "Step 2 | Training Loss: 0.441539 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.7633516788482666, 0.5551055073738098\n",
      "Step 3 | Training Loss: 0.420653 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.763262927532196, 0.554852306842804\n",
      "Step 4 | Training Loss: 0.466701 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.7633073329925537, 0.5549367070198059\n",
      "Step 5 | Training Loss: 0.443039 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7633960247039795, 0.5551055073738098\n",
      "Step 6 | Training Loss: 0.424539 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.7633073329925537, 0.5551055073738098\n",
      "Step 7 | Training Loss: 0.437550 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.7632185816764832, 0.554852306842804\n",
      "Step 8 | Training Loss: 0.429481 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7632185816764832, 0.5546835660934448\n",
      "Step 9 | Training Loss: 0.434610 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.7630855441093445, 0.5545147657394409\n",
      "Step 10 | Training Loss: 0.424302 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.7631298899650574, 0.554430365562439\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.661364 | Validation Accuracy: 0.730159\n",
      "Accuracy on Test data: 0.5217796564102173, 0.27122363448143005\n",
      "Step 2 | Training Loss: 0.635159 | Validation Accuracy: 0.755952\n",
      "Accuracy on Test data: 0.5389460325241089, 0.2977215051651001\n",
      "Step 3 | Training Loss: 0.617150 | Validation Accuracy: 0.816270\n",
      "Accuracy on Test data: 0.5770493149757385, 0.36565402150154114\n",
      "Step 4 | Training Loss: 0.556307 | Validation Accuracy: 0.850794\n",
      "Accuracy on Test data: 0.6061480045318604, 0.41434597969055176\n",
      "Step 5 | Training Loss: 0.535421 | Validation Accuracy: 0.879762\n",
      "Accuracy on Test data: 0.7077271342277527, 0.45485231280326843\n",
      "Step 6 | Training Loss: 0.487212 | Validation Accuracy: 0.899603\n",
      "Accuracy on Test data: 0.7189939618110657, 0.47493672370910645\n",
      "Step 7 | Training Loss: 0.500562 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.728397786617279, 0.49223628640174866\n",
      "Step 8 | Training Loss: 0.472547 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.7405518293380737, 0.5146835446357727\n",
      "Step 9 | Training Loss: 0.462272 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.7482256889343262, 0.5288607478141785\n",
      "Step 10 | Training Loss: 0.433209 | Validation Accuracy: 0.953571\n",
      "Accuracy on Test data: 0.7603353261947632, 0.550295352935791\n",
      "Step 1 | Training Loss: 0.426087 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.7610894441604614, 0.551476776599884\n",
      "Step 2 | Training Loss: 0.433798 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7617104053497314, 0.552320659160614\n",
      "Step 3 | Training Loss: 0.443154 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7629967927932739, 0.5541772246360779\n",
      "Step 4 | Training Loss: 0.429782 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.7643718719482422, 0.5562869310379028\n",
      "Step 5 | Training Loss: 0.473760 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7656582593917847, 0.5586497783660889\n",
      "Step 6 | Training Loss: 0.425294 | Validation Accuracy: 0.960714\n",
      "Accuracy on Test data: 0.7661018371582031, 0.5592405200004578\n",
      "Step 7 | Training Loss: 0.429852 | Validation Accuracy: 0.961111\n",
      "Accuracy on Test data: 0.7667672038078308, 0.5600000023841858\n",
      "Step 8 | Training Loss: 0.439671 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.7672551274299622, 0.5606750845909119\n",
      "Step 9 | Training Loss: 0.423632 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7676100134849548, 0.5612658262252808\n",
      "Step 10 | Training Loss: 0.435612 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5618565678596497\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.636687 | Validation Accuracy: 0.724206\n",
      "Accuracy on Test data: 0.7848207950592041, 0.7139240503311157\n",
      "Step 2 | Training Loss: 0.603518 | Validation Accuracy: 0.782937\n",
      "Accuracy on Test data: 0.8004790544509888, 0.6762025356292725\n",
      "Step 3 | Training Loss: 0.558731 | Validation Accuracy: 0.821032\n",
      "Accuracy on Test data: 0.8148065805435181, 0.6662447452545166\n",
      "Step 4 | Training Loss: 0.516378 | Validation Accuracy: 0.873016\n",
      "Accuracy on Test data: 0.8268718719482422, 0.6837974786758423\n",
      "Step 5 | Training Loss: 0.508401 | Validation Accuracy: 0.894841\n",
      "Accuracy on Test data: 0.822613537311554, 0.6737552881240845\n",
      "Step 6 | Training Loss: 0.488547 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8239886164665222, 0.6753586530685425\n",
      "Step 7 | Training Loss: 0.467604 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8247427344322205, 0.6756961941719055\n",
      "Step 8 | Training Loss: 0.461499 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6735864877700806\n",
      "Step 9 | Training Loss: 0.440867 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8238555788993835, 0.6721519231796265\n",
      "Step 10 | Training Loss: 0.455344 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8207948803901672, 0.6659071445465088\n",
      "Step 1 | Training Loss: 0.412263 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.820262610912323, 0.6648101210594177\n",
      "Step 2 | Training Loss: 0.415664 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8200408220291138, 0.6643881797790527\n",
      "Step 3 | Training Loss: 0.411972 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8199520707130432, 0.6640506386756897\n",
      "Step 4 | Training Loss: 0.425523 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8198190331459045, 0.6637130975723267\n",
      "Step 5 | Training Loss: 0.421360 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8194641470909119, 0.6630379557609558\n",
      "Step 6 | Training Loss: 0.433482 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8191536664962769, 0.6624472737312317\n",
      "Step 7 | Training Loss: 0.467874 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8190649151802063, 0.6621940732002258\n",
      "Step 8 | Training Loss: 0.434142 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8189318776130676, 0.6616877913475037\n",
      "Step 9 | Training Loss: 0.435005 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8186657428741455, 0.6611814498901367\n",
      "Step 10 | Training Loss: 0.416219 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8184882998466492, 0.6606751084327698\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.687147 | Validation Accuracy: 0.772222\n",
      "Accuracy on Test data: 0.7924946546554565, 0.6712236404418945\n",
      "Step 2 | Training Loss: 0.624546 | Validation Accuracy: 0.850000\n",
      "Accuracy on Test data: 0.8142299652099609, 0.6681012511253357\n",
      "Step 3 | Training Loss: 0.566469 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6531645655632019\n",
      "Step 4 | Training Loss: 0.540415 | Validation Accuracy: 0.899603\n",
      "Accuracy on Test data: 0.8095280528068542, 0.6508016586303711\n",
      "Step 5 | Training Loss: 0.497224 | Validation Accuracy: 0.898810\n",
      "Accuracy on Test data: 0.8131210207939148, 0.6564556956291199\n",
      "Step 6 | Training Loss: 0.465531 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8130766749382019, 0.6556118130683899\n",
      "Step 7 | Training Loss: 0.484654 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8134758472442627, 0.6557806134223938\n",
      "Step 8 | Training Loss: 0.448423 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.813342809677124, 0.6552742719650269\n",
      "Step 9 | Training Loss: 0.454694 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8139194250106812, 0.6563712954521179\n",
      "Step 10 | Training Loss: 0.424600 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8137420415878296, 0.6556962132453918\n",
      "Step 1 | Training Loss: 0.446985 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.8137420415878296, 0.6555274128913879\n",
      "Step 2 | Training Loss: 0.444470 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8136976361274719, 0.655443012714386\n",
      "Step 3 | Training Loss: 0.449211 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8135645985603333, 0.6551898717880249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 0.413689 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8135202527046204, 0.655105471611023\n",
      "Step 5 | Training Loss: 0.419892 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8134315013885498, 0.6549367308616638\n",
      "Step 6 | Training Loss: 0.439355 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.8133871555328369, 0.6547679305076599\n",
      "Step 7 | Training Loss: 0.380982 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.8132984638214111, 0.654599130153656\n",
      "Step 8 | Training Loss: 0.442472 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.8131210207939148, 0.654261589050293\n",
      "Step 9 | Training Loss: 0.425359 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.8130766749382019, 0.654177188873291\n",
      "Step 10 | Training Loss: 0.427421 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8134315013885498, 0.6548523306846619\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693063 | Validation Accuracy: 0.547222\n",
      "Accuracy on Test data: 0.4394073784351349, 0.19662447273731232\n",
      "Step 2 | Training Loss: 0.692987 | Validation Accuracy: 0.536508\n",
      "Accuracy on Test data: 0.4381210207939148, 0.1934177279472351\n",
      "Step 3 | Training Loss: 0.692907 | Validation Accuracy: 0.557936\n",
      "Accuracy on Test data: 0.43829843401908875, 0.19350211322307587\n",
      "Step 4 | Training Loss: 0.693034 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.43891945481300354, 0.19434599578380585\n",
      "Step 5 | Training Loss: 0.691914 | Validation Accuracy: 0.568254\n",
      "Accuracy on Test data: 0.4422019124031067, 0.19991561770439148\n",
      "Step 6 | Training Loss: 0.691438 | Validation Accuracy: 0.580159\n",
      "Accuracy on Test data: 0.4658889174461365, 0.24270042777061462\n",
      "Step 7 | Training Loss: 0.691204 | Validation Accuracy: 0.682936\n",
      "Accuracy on Test data: 0.5130411386489868, 0.26016879081726074\n",
      "Step 8 | Training Loss: 0.693265 | Validation Accuracy: 0.742857\n",
      "Accuracy on Test data: 0.5805092453956604, 0.2859915494918823\n",
      "Step 9 | Training Loss: 0.690320 | Validation Accuracy: 0.790476\n",
      "Accuracy on Test data: 0.6267743110656738, 0.3099578022956848\n",
      "Step 10 | Training Loss: 0.690682 | Validation Accuracy: 0.827778\n",
      "Accuracy on Test data: 0.646025538444519, 0.3423628807067871\n",
      "Step 1 | Training Loss: 0.688777 | Validation Accuracy: 0.829365\n",
      "Accuracy on Test data: 0.647445023059845, 0.34489452838897705\n",
      "Step 2 | Training Loss: 0.689789 | Validation Accuracy: 0.837302\n",
      "Accuracy on Test data: 0.6484208703041077, 0.3466666638851166\n",
      "Step 3 | Training Loss: 0.689146 | Validation Accuracy: 0.832540\n",
      "Accuracy on Test data: 0.6500177383422852, 0.34945148229599\n",
      "Step 4 | Training Loss: 0.690214 | Validation Accuracy: 0.834127\n",
      "Accuracy on Test data: 0.6513041257858276, 0.3516455590724945\n",
      "Step 5 | Training Loss: 0.691653 | Validation Accuracy: 0.846032\n",
      "Accuracy on Test data: 0.6527235507965088, 0.3540928363800049\n",
      "Step 6 | Training Loss: 0.689352 | Validation Accuracy: 0.837698\n",
      "Accuracy on Test data: 0.6539212465286255, 0.35637131333351135\n",
      "Step 7 | Training Loss: 0.688342 | Validation Accuracy: 0.833333\n",
      "Accuracy on Test data: 0.6547196507453918, 0.3577215075492859\n",
      "Step 8 | Training Loss: 0.692078 | Validation Accuracy: 0.839286\n",
      "Accuracy on Test data: 0.6562721729278564, 0.3605063259601593\n",
      "Step 9 | Training Loss: 0.689387 | Validation Accuracy: 0.851587\n",
      "Accuracy on Test data: 0.6576915979385376, 0.36303797364234924\n",
      "Step 10 | Training Loss: 0.689915 | Validation Accuracy: 0.838889\n",
      "Accuracy on Test data: 0.6588892936706543, 0.36464133858680725\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.673877 | Validation Accuracy: 0.614683\n",
      "Accuracy on Test data: 0.720901370048523, 0.797046422958374\n",
      "Step 2 | Training Loss: 0.673931 | Validation Accuracy: 0.724603\n",
      "Accuracy on Test data: 0.8453690409660339, 0.801181435585022\n",
      "Step 3 | Training Loss: 0.658810 | Validation Accuracy: 0.764683\n",
      "Accuracy on Test data: 0.8496717810630798, 0.796286940574646\n",
      "Step 4 | Training Loss: 0.692749 | Validation Accuracy: 0.804762\n",
      "Accuracy on Test data: 0.8358321785926819, 0.7518987059593201\n",
      "Step 5 | Training Loss: 0.689353 | Validation Accuracy: 0.846032\n",
      "Accuracy on Test data: 0.8491394519805908, 0.7618565559387207\n",
      "Step 6 | Training Loss: 0.632527 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.8662615418434143, 0.7772151827812195\n",
      "Step 7 | Training Loss: 0.673166 | Validation Accuracy: 0.891667\n",
      "Accuracy on Test data: 0.8703424334526062, 0.7794092893600464\n",
      "Step 8 | Training Loss: 0.621597 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8743346333503723, 0.7810970544815063\n",
      "Step 9 | Training Loss: 0.614134 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8750886917114258, 0.7791560888290405\n",
      "Step 10 | Training Loss: 0.615622 | Validation Accuracy: 0.916667\n",
      "Accuracy on Test data: 0.8757540583610535, 0.7787341475486755\n",
      "Step 1 | Training Loss: 0.593523 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.8717618584632874, 0.7710548639297485\n",
      "Step 2 | Training Loss: 0.635315 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.8682576417922974, 0.7641350030899048\n",
      "Step 3 | Training Loss: 0.607584 | Validation Accuracy: 0.912302\n",
      "Accuracy on Test data: 0.8638218641281128, 0.7555274367332458\n",
      "Step 4 | Training Loss: 0.670006 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8639549612998962, 0.755696177482605\n",
      "Step 5 | Training Loss: 0.626130 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8635557293891907, 0.754936695098877\n",
      "Step 6 | Training Loss: 0.610956 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.8636887669563293, 0.7551054954528809\n",
      "Step 7 | Training Loss: 0.633534 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.863200843334198, 0.754092812538147\n",
      "Step 8 | Training Loss: 0.605641 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8631564974784851, 0.7539240717887878\n",
      "Step 9 | Training Loss: 0.584796 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8634226322174072, 0.7543460130691528\n",
      "Step 10 | Training Loss: 0.644811 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.8634669780731201, 0.75443035364151\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.673438 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.44983142614364624, 0.21729958057403564\n",
      "Step 2 | Training Loss: 0.700032 | Validation Accuracy: 0.607540\n",
      "Accuracy on Test data: 0.4835876524448395, 0.2272573858499527\n",
      "Step 3 | Training Loss: 0.681356 | Validation Accuracy: 0.677381\n",
      "Accuracy on Test data: 0.5026614665985107, 0.23721519112586975\n",
      "Step 4 | Training Loss: 0.668578 | Validation Accuracy: 0.711508\n",
      "Accuracy on Test data: 0.5856990814208984, 0.24008439481258392\n",
      "Step 5 | Training Loss: 0.631740 | Validation Accuracy: 0.734127\n",
      "Accuracy on Test data: 0.6113378405570984, 0.2866666615009308\n",
      "Step 6 | Training Loss: 0.649678 | Validation Accuracy: 0.775794\n",
      "Accuracy on Test data: 0.6230925917625427, 0.30725738406181335\n",
      "Step 7 | Training Loss: 0.611974 | Validation Accuracy: 0.815873\n",
      "Accuracy on Test data: 0.6378637552261353, 0.3331645429134369\n",
      "Step 8 | Training Loss: 0.596150 | Validation Accuracy: 0.829365\n",
      "Accuracy on Test data: 0.6576029062271118, 0.36843881011009216\n",
      "Step 9 | Training Loss: 0.595785 | Validation Accuracy: 0.855556\n",
      "Accuracy on Test data: 0.6698899865150452, 0.38894516229629517\n",
      "Step 10 | Training Loss: 0.556482 | Validation Accuracy: 0.877778\n",
      "Accuracy on Test data: 0.6859474778175354, 0.4151054918766022\n",
      "Step 1 | Training Loss: 0.605460 | Validation Accuracy: 0.871032\n",
      "Accuracy on Test data: 0.6877217888832092, 0.41839662194252014\n",
      "Step 2 | Training Loss: 0.599030 | Validation Accuracy: 0.888095\n",
      "Accuracy on Test data: 0.6901171207427979, 0.4229535758495331\n",
      "Step 3 | Training Loss: 0.667021 | Validation Accuracy: 0.887302\n",
      "Accuracy on Test data: 0.6921132206916809, 0.426751047372818\n",
      "Step 4 | Training Loss: 0.558025 | Validation Accuracy: 0.878968\n",
      "Accuracy on Test data: 0.6934882998466492, 0.42919832468032837\n",
      "Step 5 | Training Loss: 0.607616 | Validation Accuracy: 0.884127\n",
      "Accuracy on Test data: 0.6946859359741211, 0.4313924014568329\n",
      "Step 6 | Training Loss: 0.569873 | Validation Accuracy: 0.887302\n",
      "Accuracy on Test data: 0.6965489983558655, 0.43468353152275085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 | Training Loss: 0.599622 | Validation Accuracy: 0.892857\n",
      "Accuracy on Test data: 0.6985006928443909, 0.4378059208393097\n",
      "Step 8 | Training Loss: 0.584390 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.7002306580543518, 0.44109705090522766\n",
      "Step 9 | Training Loss: 0.572836 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.7011178135871887, 0.4427848160266876\n",
      "Step 10 | Training Loss: 0.581638 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.7020049691200256, 0.44438818097114563\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.674120 | Validation Accuracy: 0.672619\n",
      "Accuracy on Test data: 0.5823279023170471, 0.6183122396469116\n",
      "Step 2 | Training Loss: 0.651275 | Validation Accuracy: 0.822222\n",
      "Accuracy on Test data: 0.7797196507453918, 0.6556118130683899\n",
      "Step 3 | Training Loss: 0.629393 | Validation Accuracy: 0.832937\n",
      "Accuracy on Test data: 0.8017654418945312, 0.6791561245918274\n",
      "Step 4 | Training Loss: 0.631256 | Validation Accuracy: 0.841270\n",
      "Accuracy on Test data: 0.8219925761222839, 0.702025294303894\n",
      "Step 5 | Training Loss: 0.618366 | Validation Accuracy: 0.852381\n",
      "Accuracy on Test data: 0.8307753801345825, 0.7063291072845459\n",
      "Step 6 | Training Loss: 0.597137 | Validation Accuracy: 0.871032\n",
      "Accuracy on Test data: 0.8310415148735046, 0.69974684715271\n",
      "Step 7 | Training Loss: 0.594264 | Validation Accuracy: 0.882937\n",
      "Accuracy on Test data: 0.8160486221313477, 0.6672573685646057\n",
      "Step 8 | Training Loss: 0.607783 | Validation Accuracy: 0.890476\n",
      "Accuracy on Test data: 0.8149396777153015, 0.6637130975723267\n",
      "Step 9 | Training Loss: 0.564399 | Validation Accuracy: 0.905556\n",
      "Accuracy on Test data: 0.813653290271759, 0.6599156260490417\n",
      "Step 10 | Training Loss: 0.558876 | Validation Accuracy: 0.912302\n",
      "Accuracy on Test data: 0.8137863874435425, 0.6589873433113098\n",
      "Step 1 | Training Loss: 0.536020 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.8125443458557129, 0.6565400958061218\n",
      "Step 2 | Training Loss: 0.571064 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8124113082885742, 0.656286895275116\n",
      "Step 3 | Training Loss: 0.525460 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8126774430274963, 0.6567932367324829\n",
      "Step 4 | Training Loss: 0.542171 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8123225569725037, 0.6561181545257568\n",
      "Step 5 | Training Loss: 0.549845 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8120564222335815, 0.6555274128913879\n",
      "Step 6 | Training Loss: 0.534080 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.812189519405365, 0.6556962132453918\n",
      "Step 7 | Training Loss: 0.566619 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8119677305221558, 0.6552742719650269\n",
      "Step 8 | Training Loss: 0.548004 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.8116128444671631, 0.6545147895812988\n",
      "Step 9 | Training Loss: 0.537644 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.8112579584121704, 0.653839647769928\n",
      "Step 10 | Training Loss: 0.563511 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8109474778175354, 0.6531645655632019\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.695275 | Validation Accuracy: 0.721032\n",
      "Accuracy on Test data: 0.7192157506942749, 0.6224472522735596\n",
      "Step 2 | Training Loss: 0.661552 | Validation Accuracy: 0.794841\n",
      "Accuracy on Test data: 0.8200851678848267, 0.7048101425170898\n",
      "Step 3 | Training Loss: 0.617277 | Validation Accuracy: 0.848810\n",
      "Accuracy on Test data: 0.8369410634040833, 0.7124894261360168\n",
      "Step 4 | Training Loss: 0.607751 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.8577892184257507, 0.7427847981452942\n",
      "Step 5 | Training Loss: 0.566211 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.8592086434364319, 0.744303822517395\n",
      "Step 6 | Training Loss: 0.535724 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.8574343323707581, 0.7391561269760132\n",
      "Step 7 | Training Loss: 0.567525 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8469659090042114, 0.7185654044151306\n",
      "Step 8 | Training Loss: 0.467065 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.8424414396286011, 0.7094514966011047\n",
      "Step 9 | Training Loss: 0.505844 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8427963256835938, 0.7101265788078308\n",
      "Step 10 | Training Loss: 0.458680 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8384935855865479, 0.701603353023529\n",
      "Step 1 | Training Loss: 0.467905 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8387597799301147, 0.702109694480896\n",
      "Step 2 | Training Loss: 0.450754 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8380056619644165, 0.7006751298904419\n",
      "Step 3 | Training Loss: 0.470326 | Validation Accuracy: 0.934127\n",
      "Accuracy on Test data: 0.8376951813697815, 0.700084388256073\n",
      "Step 4 | Training Loss: 0.446108 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8372072577476501, 0.6991561055183411\n",
      "Step 5 | Training Loss: 0.464782 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.8370741605758667, 0.698818564414978\n",
      "Step 6 | Training Loss: 0.470170 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.8369410634040833, 0.6985654234886169\n",
      "Step 7 | Training Loss: 0.438572 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8359652161598206, 0.6967088580131531\n",
      "Step 8 | Training Loss: 0.458166 | Validation Accuracy: 0.934127\n",
      "Accuracy on Test data: 0.8348562717437744, 0.6945147514343262\n",
      "Step 9 | Training Loss: 0.492045 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8336586356163025, 0.6921519041061401\n",
      "Step 10 | Training Loss: 0.459956 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8330819606781006, 0.6910548806190491\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.677522 | Validation Accuracy: 0.678571\n",
      "Accuracy on Test data: 0.6083658337593079, 0.551476776599884\n",
      "Step 2 | Training Loss: 0.621664 | Validation Accuracy: 0.811111\n",
      "Accuracy on Test data: 0.6935326457023621, 0.6108016967773438\n",
      "Step 3 | Training Loss: 0.607377 | Validation Accuracy: 0.826190\n",
      "Accuracy on Test data: 0.6969925761222839, 0.5958649516105652\n",
      "Step 4 | Training Loss: 0.565843 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.7904542088508606, 0.6112236380577087\n",
      "Step 5 | Training Loss: 0.523955 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8235450387001038, 0.6725738644599915\n",
      "Step 6 | Training Loss: 0.475074 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.821415901184082, 0.6671729683876038\n",
      "Step 7 | Training Loss: 0.474063 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8238555788993835, 0.6706328988075256\n",
      "Step 8 | Training Loss: 0.442432 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8272711038589478, 0.6768776178359985\n",
      "Step 9 | Training Loss: 0.458479 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.82948899269104, 0.6803375482559204\n",
      "Step 10 | Training Loss: 0.428880 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.8224804997444153, 0.6669198274612427\n",
      "Step 1 | Training Loss: 0.456763 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8187544345855713, 0.6598312258720398\n",
      "Step 2 | Training Loss: 0.477474 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.8136976361274719, 0.650210976600647\n",
      "Step 3 | Training Loss: 0.416786 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.8089957237243652, 0.6412658095359802\n",
      "Step 4 | Training Loss: 0.435863 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8046486973762512, 0.6327426433563232\n",
      "Step 5 | Training Loss: 0.432077 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.8036284446716309, 0.6308016777038574\n",
      "Step 6 | Training Loss: 0.421386 | Validation Accuracy: 0.958333\n",
      "Accuracy on Test data: 0.8031848669052124, 0.6299577951431274\n",
      "Step 7 | Training Loss: 0.424550 | Validation Accuracy: 0.954762\n",
      "Accuracy on Test data: 0.8021646738052368, 0.6280168890953064\n",
      "Step 8 | Training Loss: 0.420930 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.8017654418945312, 0.6272574067115784\n",
      "Step 9 | Training Loss: 0.418368 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.8014549612998962, 0.6266666650772095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.439268 | Validation Accuracy: 0.945635\n",
      "Accuracy on Test data: 0.801543653011322, 0.6267510652542114\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.700626 | Validation Accuracy: 0.542857\n",
      "Accuracy on Test data: 0.4536462128162384, 0.2205907106399536\n",
      "Step 2 | Training Loss: 0.683508 | Validation Accuracy: 0.682936\n",
      "Accuracy on Test data: 0.5147267580032349, 0.2600843906402588\n",
      "Step 3 | Training Loss: 0.686013 | Validation Accuracy: 0.715873\n",
      "Accuracy on Test data: 0.6344481706619263, 0.325654000043869\n",
      "Step 4 | Training Loss: 0.640443 | Validation Accuracy: 0.767063\n",
      "Accuracy on Test data: 0.6586231589317322, 0.3670042157173157\n",
      "Step 5 | Training Loss: 0.574762 | Validation Accuracy: 0.800000\n",
      "Accuracy on Test data: 0.6729950308799744, 0.3911392390727997\n",
      "Step 6 | Training Loss: 0.582021 | Validation Accuracy: 0.838492\n",
      "Accuracy on Test data: 0.7015613913536072, 0.4430379867553711\n",
      "Step 7 | Training Loss: 0.554168 | Validation Accuracy: 0.848810\n",
      "Accuracy on Test data: 0.718018114566803, 0.472573846578598\n",
      "Step 8 | Training Loss: 0.507814 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.7257363200187683, 0.48658227920532227\n",
      "Step 9 | Training Loss: 0.490023 | Validation Accuracy: 0.886111\n",
      "Accuracy on Test data: 0.7335876226425171, 0.5009282827377319\n",
      "Step 10 | Training Loss: 0.511685 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.7414833307266235, 0.5143460035324097\n",
      "Step 1 | Training Loss: 0.450501 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.7421043515205383, 0.5155274271965027\n",
      "Step 2 | Training Loss: 0.480977 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.7429915070533752, 0.5172995924949646\n",
      "Step 3 | Training Loss: 0.454192 | Validation Accuracy: 0.892460\n",
      "Accuracy on Test data: 0.7441891431808472, 0.5195780396461487\n",
      "Step 4 | Training Loss: 0.464956 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.7453424334526062, 0.5216033458709717\n",
      "Step 5 | Training Loss: 0.496506 | Validation Accuracy: 0.894048\n",
      "Accuracy on Test data: 0.7461852431297302, 0.5230379700660706\n",
      "Step 6 | Training Loss: 0.506985 | Validation Accuracy: 0.894444\n",
      "Accuracy on Test data: 0.7469836473464966, 0.5245569348335266\n",
      "Step 7 | Training Loss: 0.421883 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.7477821111679077, 0.5259915590286255\n",
      "Step 8 | Training Loss: 0.503288 | Validation Accuracy: 0.900397\n",
      "Accuracy on Test data: 0.7492015361785889, 0.5285232067108154\n",
      "Step 9 | Training Loss: 0.451988 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.7497338652610779, 0.5295358896255493\n",
      "Step 10 | Training Loss: 0.426996 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.7508871555328369, 0.5316455960273743\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.626393 | Validation Accuracy: 0.823016\n",
      "Accuracy on Test data: 0.7058197259902954, 0.498818576335907\n",
      "Step 2 | Training Loss: 0.556652 | Validation Accuracy: 0.855952\n",
      "Accuracy on Test data: 0.7300390601158142, 0.5091139078140259\n",
      "Step 3 | Training Loss: 0.522642 | Validation Accuracy: 0.881746\n",
      "Accuracy on Test data: 0.742237389087677, 0.5267510414123535\n",
      "Step 4 | Training Loss: 0.510134 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.7503992319107056, 0.5392404794692993\n",
      "Step 5 | Training Loss: 0.516486 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.7527501583099365, 0.5424472689628601\n",
      "Step 6 | Training Loss: 0.465994 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.7560326457023621, 0.5469198226928711\n",
      "Step 7 | Training Loss: 0.476885 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7594481706619263, 0.5522362589836121\n",
      "Step 8 | Training Loss: 0.472546 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.7650372385978699, 0.5620253086090088\n",
      "Step 9 | Training Loss: 0.456143 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.7779009938240051, 0.5854852199554443\n",
      "Step 10 | Training Loss: 0.438764 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.7924946546554565, 0.6113080382347107\n",
      "Step 1 | Training Loss: 0.417137 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.7927164435386658, 0.6116455793380737\n",
      "Step 2 | Training Loss: 0.439666 | Validation Accuracy: 0.951587\n",
      "Accuracy on Test data: 0.7923616170883179, 0.6108860969543457\n",
      "Step 3 | Training Loss: 0.433804 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.792317271232605, 0.6106328964233398\n",
      "Step 4 | Training Loss: 0.444950 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.7921841740608215, 0.6103797554969788\n",
      "Step 5 | Training Loss: 0.434940 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.7917405962944031, 0.6095358729362488\n",
      "Step 6 | Training Loss: 0.429753 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.7911195755004883, 0.6083544492721558\n",
      "Step 7 | Training Loss: 0.385985 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.7899662852287292, 0.6061603426933289\n",
      "Step 8 | Training Loss: 0.409584 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.7891678214073181, 0.6046413779258728\n",
      "Step 9 | Training Loss: 0.420274 | Validation Accuracy: 0.958333\n",
      "Accuracy on Test data: 0.788591206073761, 0.603544294834137\n",
      "Step 10 | Training Loss: 0.460217 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.7873491644859314, 0.6011814475059509\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.740275 | Validation Accuracy: 0.510714\n",
      "Accuracy on Test data: 0.5398775935173035, 0.5609282851219177\n",
      "Step 2 | Training Loss: 0.641448 | Validation Accuracy: 0.660317\n",
      "Accuracy on Test data: 0.6429204940795898, 0.596202552318573\n",
      "Step 3 | Training Loss: 0.576629 | Validation Accuracy: 0.840873\n",
      "Accuracy on Test data: 0.7777235507965088, 0.6228691935539246\n",
      "Step 4 | Training Loss: 0.581998 | Validation Accuracy: 0.878571\n",
      "Accuracy on Test data: 0.7980837225914001, 0.6319831013679504\n",
      "Step 5 | Training Loss: 0.497206 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.8006564974784851, 0.6335021257400513\n",
      "Step 6 | Training Loss: 0.504190 | Validation Accuracy: 0.898413\n",
      "Accuracy on Test data: 0.8008782863616943, 0.6312236189842224\n",
      "Step 7 | Training Loss: 0.496960 | Validation Accuracy: 0.894444\n",
      "Accuracy on Test data: 0.8011887669563293, 0.6308016777038574\n",
      "Step 8 | Training Loss: 0.494713 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.8010113835334778, 0.6302953362464905\n",
      "Step 9 | Training Loss: 0.454368 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7932487726211548, 0.6146835684776306\n",
      "Step 10 | Training Loss: 0.433201 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.7928938865661621, 0.6135020852088928\n",
      "Step 1 | Training Loss: 0.445527 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.7926720976829529, 0.6129958033561707\n",
      "Step 2 | Training Loss: 0.458253 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.7923616170883179, 0.6123206615447998\n",
      "Step 3 | Training Loss: 0.477346 | Validation Accuracy: 0.926587\n",
      "Accuracy on Test data: 0.7924059629440308, 0.6124050617218018\n",
      "Step 4 | Training Loss: 0.460963 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.7922285199165344, 0.6119831204414368\n",
      "Step 5 | Training Loss: 0.457205 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.7918736934661865, 0.6112236380577087\n",
      "Step 6 | Training Loss: 0.427011 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.7916519045829773, 0.6107172966003418\n",
      "Step 7 | Training Loss: 0.426900 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7913857102394104, 0.6102109551429749\n",
      "Step 8 | Training Loss: 0.437091 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.791474461555481, 0.6103797554969788\n",
      "Step 9 | Training Loss: 0.455769 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7911639213562012, 0.6096202731132507\n",
      "Step 10 | Training Loss: 0.439554 | Validation Accuracy: 0.940476\n",
      "Accuracy on Test data: 0.7909865379333496, 0.6089451313018799\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.659970 | Validation Accuracy: 0.642857\n",
      "Accuracy on Test data: 0.7087473273277283, 0.5641350150108337\n",
      "Step 2 | Training Loss: 0.586200 | Validation Accuracy: 0.794841\n",
      "Accuracy on Test data: 0.7916074991226196, 0.6279324889183044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 0.576643 | Validation Accuracy: 0.841667\n",
      "Accuracy on Test data: 0.8094836473464966, 0.6539240479469299\n",
      "Step 4 | Training Loss: 0.549837 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.8215045928955078, 0.6715611815452576\n",
      "Step 5 | Training Loss: 0.547593 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6755274534225464\n",
      "Step 6 | Training Loss: 0.455088 | Validation Accuracy: 0.916667\n",
      "Accuracy on Test data: 0.8275372385978699, 0.6789029240608215\n",
      "Step 7 | Training Loss: 0.501299 | Validation Accuracy: 0.919048\n",
      "Accuracy on Test data: 0.8270936608314514, 0.6775527596473694\n",
      "Step 8 | Training Loss: 0.488933 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8265170454978943, 0.6763713359832764\n",
      "Step 9 | Training Loss: 0.482712 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8267831802368164, 0.6766244769096375\n",
      "Step 10 | Training Loss: 0.431244 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8253637552261353, 0.6736708879470825\n",
      "Step 1 | Training Loss: 0.437554 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8254081010818481, 0.6736708879470825\n",
      "Step 2 | Training Loss: 0.456346 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8253637552261353, 0.6735864877700806\n",
      "Step 3 | Training Loss: 0.430146 | Validation Accuracy: 0.940476\n",
      "Accuracy on Test data: 0.8253193497657776, 0.6735020875930786\n",
      "Step 4 | Training Loss: 0.455064 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6730801463127136\n",
      "Step 5 | Training Loss: 0.431418 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8249201774597168, 0.6727426052093506\n",
      "Step 6 | Training Loss: 0.438076 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.8249201774597168, 0.6727426052093506\n",
      "Step 7 | Training Loss: 0.418243 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8249201774597168, 0.6727426052093506\n",
      "Step 8 | Training Loss: 0.422538 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6720675230026245\n",
      "Step 9 | Training Loss: 0.420511 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.824299156665802, 0.6715611815452576\n",
      "Step 10 | Training Loss: 0.419378 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.8241660594940186, 0.6713080406188965\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.707739 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.701140 | Validation Accuracy: 0.529762\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.689011 | Validation Accuracy: 0.559921\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.688159 | Validation Accuracy: 0.516667\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.672713 | Validation Accuracy: 0.539683\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.702308 | Validation Accuracy: 0.529365\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.671768 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.656738 | Validation Accuracy: 0.529762\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.670383 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.652311 | Validation Accuracy: 0.526984\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.661280 | Validation Accuracy: 0.544444\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.679913 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.668328 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.653142 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.651726 | Validation Accuracy: 0.539683\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.652429 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.672052 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.672441 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.647983 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.683216 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.648386 | Validation Accuracy: 0.797222\n",
      "Accuracy on Test data: 0.8128548860549927, 0.7190717458724976\n",
      "Step 2 | Training Loss: 0.676625 | Validation Accuracy: 0.805159\n",
      "Accuracy on Test data: 0.8208836317062378, 0.7151054739952087\n",
      "Step 3 | Training Loss: 0.656458 | Validation Accuracy: 0.830159\n",
      "Accuracy on Test data: 0.830686628818512, 0.7105485200881958\n",
      "Step 4 | Training Loss: 0.676221 | Validation Accuracy: 0.861111\n",
      "Accuracy on Test data: 0.8431511521339417, 0.7240506410598755\n",
      "Step 5 | Training Loss: 0.650915 | Validation Accuracy: 0.871825\n",
      "Accuracy on Test data: 0.8514460325241089, 0.7350211143493652\n",
      "Step 6 | Training Loss: 0.612846 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.85224449634552, 0.7350211143493652\n",
      "Step 7 | Training Loss: 0.653966 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.8531759977340698, 0.7343459725379944\n",
      "Step 8 | Training Loss: 0.626661 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.8455908298492432, 0.7187342047691345\n",
      "Step 9 | Training Loss: 0.634473 | Validation Accuracy: 0.905952\n",
      "Accuracy on Test data: 0.8353441953659058, 0.6973839402198792\n",
      "Step 10 | Training Loss: 0.636796 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.8278034329414368, 0.6822784543037415\n",
      "Step 1 | Training Loss: 0.607689 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.8279364705085754, 0.6824472546577454\n",
      "Step 2 | Training Loss: 0.615720 | Validation Accuracy: 0.895238\n",
      "Accuracy on Test data: 0.8279808163642883, 0.6824472546577454\n",
      "Step 3 | Training Loss: 0.645993 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.8277590274810791, 0.6819409132003784\n",
      "Step 4 | Training Loss: 0.630247 | Validation Accuracy: 0.907937\n",
      "Accuracy on Test data: 0.827492892742157, 0.6813502311706543\n",
      "Step 5 | Training Loss: 0.619886 | Validation Accuracy: 0.898413\n",
      "Accuracy on Test data: 0.8276259899139404, 0.6816033720970154\n",
      "Step 6 | Training Loss: 0.641881 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8269606232643127, 0.6801687479019165\n",
      "Step 7 | Training Loss: 0.616905 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.8258516788482666, 0.6779747009277344\n",
      "Step 8 | Training Loss: 0.624828 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.8255855441093445, 0.6773839592933655\n",
      "Step 9 | Training Loss: 0.581095 | Validation Accuracy: 0.904762\n",
      "Accuracy on Test data: 0.8253193497657776, 0.6768776178359985\n",
      "Step 10 | Training Loss: 0.649522 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6753586530685425\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.735964 | Validation Accuracy: 0.634127\n",
      "Accuracy on Test data: 0.5324698090553284, 0.38092827796936035\n",
      "Step 2 | Training Loss: 0.711714 | Validation Accuracy: 0.711111\n",
      "Accuracy on Test data: 0.5833925008773804, 0.4027848243713379\n",
      "Step 3 | Training Loss: 0.691288 | Validation Accuracy: 0.734524\n",
      "Accuracy on Test data: 0.5992282032966614, 0.4281012713909149\n",
      "Step 4 | Training Loss: 0.671583 | Validation Accuracy: 0.753175\n",
      "Accuracy on Test data: 0.6125798225402832, 0.448016881942749\n",
      "Step 5 | Training Loss: 0.664708 | Validation Accuracy: 0.816667\n",
      "Accuracy on Test data: 0.6326738595962524, 0.4792405068874359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Training Loss: 0.646630 | Validation Accuracy: 0.838492\n",
      "Accuracy on Test data: 0.728087306022644, 0.5083544254302979\n",
      "Step 7 | Training Loss: 0.636528 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.7571859359741211, 0.5584810376167297\n",
      "Step 8 | Training Loss: 0.579515 | Validation Accuracy: 0.884921\n",
      "Accuracy on Test data: 0.7667228579521179, 0.5735021233558655\n",
      "Step 9 | Training Loss: 0.686004 | Validation Accuracy: 0.890873\n",
      "Accuracy on Test data: 0.7826029062271118, 0.6016877889633179\n",
      "Step 10 | Training Loss: 0.598271 | Validation Accuracy: 0.899603\n",
      "Accuracy on Test data: 0.797817587852478, 0.6291139125823975\n",
      "Step 1 | Training Loss: 0.617785 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.7989708781242371, 0.6311392188072205\n",
      "Step 2 | Training Loss: 0.648395 | Validation Accuracy: 0.919048\n",
      "Accuracy on Test data: 0.7991039752960205, 0.6313924193382263\n",
      "Step 3 | Training Loss: 0.620021 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.7996362447738647, 0.6316455602645874\n",
      "Step 4 | Training Loss: 0.588161 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.7999024391174316, 0.6317299604415894\n",
      "Step 5 | Training Loss: 0.580504 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.8003016114234924, 0.6324894428253174\n",
      "Step 6 | Training Loss: 0.610891 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.8009226322174072, 0.6329957842826843\n",
      "Step 7 | Training Loss: 0.603797 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8013662099838257, 0.6336708664894104\n",
      "Step 8 | Training Loss: 0.579425 | Validation Accuracy: 0.905159\n",
      "Accuracy on Test data: 0.8021202683448792, 0.6351054906845093\n",
      "Step 9 | Training Loss: 0.601571 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8028300404548645, 0.6363713145256042\n",
      "Step 10 | Training Loss: 0.614538 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8040720224380493, 0.6383966207504272\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.688161 | Validation Accuracy: 0.746032\n",
      "Accuracy on Test data: 0.707061767578125, 0.5457383990287781\n",
      "Step 2 | Training Loss: 0.676604 | Validation Accuracy: 0.810714\n",
      "Accuracy on Test data: 0.7244055867195129, 0.5652320384979248\n",
      "Step 3 | Training Loss: 0.672206 | Validation Accuracy: 0.828571\n",
      "Accuracy on Test data: 0.7441891431808472, 0.5770463943481445\n",
      "Step 4 | Training Loss: 0.646096 | Validation Accuracy: 0.863492\n",
      "Accuracy on Test data: 0.7540808916091919, 0.5784810185432434\n",
      "Step 5 | Training Loss: 0.621007 | Validation Accuracy: 0.894444\n",
      "Accuracy on Test data: 0.7619765996932983, 0.5703797340393066\n",
      "Step 6 | Training Loss: 0.642897 | Validation Accuracy: 0.900397\n",
      "Accuracy on Test data: 0.7647711038589478, 0.5733333230018616\n",
      "Step 7 | Training Loss: 0.591927 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.7688076496124268, 0.5784810185432434\n",
      "Step 8 | Training Loss: 0.602874 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.7773687243461609, 0.5929113626480103\n",
      "Step 9 | Training Loss: 0.576862 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.7805624604225159, 0.5980590581893921\n",
      "Step 10 | Training Loss: 0.571652 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.775949239730835, 0.5866666436195374\n",
      "Step 1 | Training Loss: 0.556296 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.7743523716926575, 0.5832911133766174\n",
      "Step 2 | Training Loss: 0.614986 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.773598313331604, 0.5816877484321594\n",
      "Step 3 | Training Loss: 0.547522 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.7731103897094727, 0.5804219245910645\n",
      "Step 4 | Training Loss: 0.556650 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.7726668119430542, 0.5794936418533325\n",
      "Step 5 | Training Loss: 0.568067 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7726224064826965, 0.5793249011039734\n",
      "Step 6 | Training Loss: 0.565965 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.7718683481216431, 0.5777215361595154\n",
      "Step 7 | Training Loss: 0.569765 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.7711142897605896, 0.5762025117874146\n",
      "Step 8 | Training Loss: 0.581032 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.7706263065338135, 0.5752742886543274\n",
      "Step 9 | Training Loss: 0.551982 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7700940370559692, 0.5742616057395935\n",
      "Step 10 | Training Loss: 0.538583 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7692512273788452, 0.5724050402641296\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.677408 | Validation Accuracy: 0.757937\n",
      "Accuracy on Test data: 0.7114975452423096, 0.46793249249458313\n",
      "Step 2 | Training Loss: 0.625989 | Validation Accuracy: 0.809524\n",
      "Accuracy on Test data: 0.757230281829834, 0.550801694393158\n",
      "Step 3 | Training Loss: 0.616962 | Validation Accuracy: 0.854762\n",
      "Accuracy on Test data: 0.794313371181488, 0.6179746985435486\n",
      "Step 4 | Training Loss: 0.603850 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.8149840235710144, 0.6567932367324829\n",
      "Step 5 | Training Loss: 0.583449 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.8227909803390503, 0.6713080406188965\n",
      "Step 6 | Training Loss: 0.512971 | Validation Accuracy: 0.902381\n",
      "Accuracy on Test data: 0.8135202527046204, 0.6524050831794739\n",
      "Step 7 | Training Loss: 0.511530 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8105482459068298, 0.6466666460037231\n",
      "Step 8 | Training Loss: 0.539288 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.7923616170883179, 0.6119831204414368\n",
      "Step 9 | Training Loss: 0.483647 | Validation Accuracy: 0.926587\n",
      "Accuracy on Test data: 0.7743523716926575, 0.5776371359825134\n",
      "Step 10 | Training Loss: 0.500466 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.7773687243461609, 0.5829535722732544\n",
      "Step 1 | Training Loss: 0.510243 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.7751951813697815, 0.5787341594696045\n",
      "Step 2 | Training Loss: 0.463331 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.7716909050941467, 0.5719830989837646\n",
      "Step 3 | Training Loss: 0.500457 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.7721788287162781, 0.5728269815444946\n",
      "Step 4 | Training Loss: 0.492165 | Validation Accuracy: 0.940873\n",
      "Accuracy on Test data: 0.771025538444519, 0.5706329345703125\n",
      "Step 5 | Training Loss: 0.427308 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.7704045176506042, 0.5694514513015747\n",
      "Step 6 | Training Loss: 0.470360 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.770182728767395, 0.5689451694488525\n",
      "Step 7 | Training Loss: 0.451619 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.770182728767395, 0.5688607692718506\n",
      "Step 8 | Training Loss: 0.473216 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7693843245506287, 0.5672574043273926\n",
      "Step 9 | Training Loss: 0.455865 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.7689407467842102, 0.5664135217666626\n",
      "Step 10 | Training Loss: 0.458298 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.7684528231620789, 0.5654852390289307\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.634514 | Validation Accuracy: 0.690079\n",
      "Accuracy on Test data: 0.5093151330947876, 0.24312236905097961\n",
      "Step 2 | Training Loss: 0.628080 | Validation Accuracy: 0.764286\n",
      "Accuracy on Test data: 0.6037526726722717, 0.2729957699775696\n",
      "Step 3 | Training Loss: 0.620433 | Validation Accuracy: 0.807143\n",
      "Accuracy on Test data: 0.6375088691711426, 0.33282700181007385\n",
      "Step 4 | Training Loss: 0.519709 | Validation Accuracy: 0.868254\n",
      "Accuracy on Test data: 0.6713094115257263, 0.39485231041908264\n",
      "Step 5 | Training Loss: 0.559398 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.6900283694267273, 0.4297046363353729\n",
      "Step 6 | Training Loss: 0.513416 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.7037349343299866, 0.4524894654750824\n",
      "Step 7 | Training Loss: 0.479462 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.7176188826560974, 0.47476792335510254\n",
      "Step 8 | Training Loss: 0.507989 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.7328335642814636, 0.5010126829147339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.466108 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.7419712543487549, 0.5174683332443237\n",
      "Step 10 | Training Loss: 0.417791 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.7490684986114502, 0.5292826890945435\n",
      "Step 1 | Training Loss: 0.448633 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.7492902874946594, 0.5297046303749084\n",
      "Step 2 | Training Loss: 0.414845 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.7496007680892944, 0.5302109718322754\n",
      "Step 3 | Training Loss: 0.450315 | Validation Accuracy: 0.945635\n",
      "Accuracy on Test data: 0.7499556541442871, 0.5307173132896423\n",
      "Step 4 | Training Loss: 0.427583 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.7502217888832092, 0.5312236547470093\n",
      "Step 5 | Training Loss: 0.433311 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.7501774430274963, 0.5311392545700073\n",
      "Step 6 | Training Loss: 0.462555 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7503548860549927, 0.5313923954963684\n",
      "Step 7 | Training Loss: 0.474380 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.7505322694778442, 0.5316455960273743\n",
      "Step 8 | Training Loss: 0.487839 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7504879236221313, 0.5315611958503723\n",
      "Step 9 | Training Loss: 0.429149 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.7507097125053406, 0.5319831371307373\n",
      "Step 10 | Training Loss: 0.421329 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.7506653666496277, 0.5318987369537354\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.701053 | Validation Accuracy: 0.579365\n",
      "Accuracy on Test data: 0.5544712543487549, 0.47054851055145264\n",
      "Step 2 | Training Loss: 0.650388 | Validation Accuracy: 0.735317\n",
      "Accuracy on Test data: 0.6542317271232605, 0.5033755302429199\n",
      "Step 3 | Training Loss: 0.680797 | Validation Accuracy: 0.780556\n",
      "Accuracy on Test data: 0.7310149073600769, 0.5305485129356384\n",
      "Step 4 | Training Loss: 0.660234 | Validation Accuracy: 0.810714\n",
      "Accuracy on Test data: 0.760645866394043, 0.5602531433105469\n",
      "Step 5 | Training Loss: 0.590869 | Validation Accuracy: 0.861905\n",
      "Accuracy on Test data: 0.7802963256835938, 0.5945991277694702\n",
      "Step 6 | Training Loss: 0.577350 | Validation Accuracy: 0.876984\n",
      "Accuracy on Test data: 0.779675304889679, 0.5929113626480103\n",
      "Step 7 | Training Loss: 0.480116 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.7855748534202576, 0.6029536128044128\n",
      "Step 8 | Training Loss: 0.504290 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.7918736934661865, 0.6135864853858948\n",
      "Step 9 | Training Loss: 0.469817 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7968860864639282, 0.6228691935539246\n",
      "Step 10 | Training Loss: 0.455082 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8018097877502441, 0.6320675015449524\n",
      "Step 1 | Training Loss: 0.422245 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8020759224891663, 0.6325738430023193\n",
      "Step 2 | Training Loss: 0.460734 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8050035238265991, 0.6381434798240662\n",
      "Step 3 | Training Loss: 0.473784 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8062455654144287, 0.6405063271522522\n",
      "Step 4 | Training Loss: 0.462595 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.8075762987136841, 0.6430379748344421\n",
      "Step 5 | Training Loss: 0.478669 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8100159764289856, 0.647257387638092\n",
      "Step 6 | Training Loss: 0.437118 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8123225569725037, 0.6515612006187439\n",
      "Step 7 | Training Loss: 0.444263 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8148509860038757, 0.6563712954521179\n",
      "Step 8 | Training Loss: 0.436740 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8170244693756104, 0.6605063080787659\n",
      "Step 9 | Training Loss: 0.434395 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8183552026748657, 0.6631223559379578\n",
      "Step 10 | Training Loss: 0.443289 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8192867040634155, 0.6647257208824158\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.816600 | Validation Accuracy: 0.560714\n",
      "Accuracy on Test data: 0.5030163526535034, 0.2107173055410385\n",
      "Step 2 | Training Loss: 0.701832 | Validation Accuracy: 0.679762\n",
      "Accuracy on Test data: 0.5955021381378174, 0.2707172930240631\n",
      "Step 3 | Training Loss: 0.603577 | Validation Accuracy: 0.729365\n",
      "Accuracy on Test data: 0.6135113835334778, 0.29080167412757874\n",
      "Step 4 | Training Loss: 0.585662 | Validation Accuracy: 0.759127\n",
      "Accuracy on Test data: 0.632762610912323, 0.324894517660141\n",
      "Step 5 | Training Loss: 0.559770 | Validation Accuracy: 0.798016\n",
      "Accuracy on Test data: 0.6495298147201538, 0.35417720675468445\n",
      "Step 6 | Training Loss: 0.577556 | Validation Accuracy: 0.838889\n",
      "Accuracy on Test data: 0.6639460325241089, 0.3783966302871704\n",
      "Step 7 | Training Loss: 0.551867 | Validation Accuracy: 0.853175\n",
      "Accuracy on Test data: 0.676233172416687, 0.40067511796951294\n",
      "Step 8 | Training Loss: 0.474794 | Validation Accuracy: 0.863889\n",
      "Accuracy on Test data: 0.6908711791038513, 0.4276793301105499\n",
      "Step 9 | Training Loss: 0.534620 | Validation Accuracy: 0.887698\n",
      "Accuracy on Test data: 0.7023154497146606, 0.4462447166442871\n",
      "Step 10 | Training Loss: 0.489927 | Validation Accuracy: 0.879762\n",
      "Accuracy on Test data: 0.712562084197998, 0.46329113841056824\n",
      "Step 1 | Training Loss: 0.462227 | Validation Accuracy: 0.878571\n",
      "Accuracy on Test data: 0.713449239730835, 0.46489450335502625\n",
      "Step 2 | Training Loss: 0.486050 | Validation Accuracy: 0.874206\n",
      "Accuracy on Test data: 0.7141146063804626, 0.4659915566444397\n",
      "Step 3 | Training Loss: 0.451362 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.7156227827072144, 0.4687763750553131\n",
      "Step 4 | Training Loss: 0.475880 | Validation Accuracy: 0.890873\n",
      "Accuracy on Test data: 0.7165542840957642, 0.47054851055145264\n",
      "Step 5 | Training Loss: 0.441494 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.717485785484314, 0.47232067584991455\n",
      "Step 6 | Training Loss: 0.461842 | Validation Accuracy: 0.891667\n",
      "Accuracy on Test data: 0.7182842493057251, 0.473839670419693\n",
      "Step 7 | Training Loss: 0.486917 | Validation Accuracy: 0.888492\n",
      "Accuracy on Test data: 0.7189496159553528, 0.475021094083786\n",
      "Step 8 | Training Loss: 0.456529 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.7201029062271118, 0.4772152006626129\n",
      "Step 9 | Training Loss: 0.466395 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7210344076156616, 0.47898733615875244\n",
      "Step 10 | Training Loss: 0.462796 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.7218772172927856, 0.48033756017684937\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.672406 | Validation Accuracy: 0.706349\n",
      "Accuracy on Test data: 0.7454755306243896, 0.6048945188522339\n",
      "Step 2 | Training Loss: 0.568802 | Validation Accuracy: 0.818651\n",
      "Accuracy on Test data: 0.8162704110145569, 0.6673417687416077\n",
      "Step 3 | Training Loss: 0.581592 | Validation Accuracy: 0.867460\n",
      "Accuracy on Test data: 0.8311746120452881, 0.6892827153205872\n",
      "Step 4 | Training Loss: 0.522586 | Validation Accuracy: 0.900000\n",
      "Accuracy on Test data: 0.8377395272254944, 0.700843870639801\n",
      "Step 5 | Training Loss: 0.494834 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.8376064300537109, 0.7005907297134399\n",
      "Step 6 | Training Loss: 0.489196 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.8481192588806152, 0.7188185453414917\n",
      "Step 7 | Training Loss: 0.462393 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8506032824516296, 0.7232067584991455\n",
      "Step 8 | Training Loss: 0.472594 | Validation Accuracy: 0.928175\n",
      "Accuracy on Test data: 0.8525106310844421, 0.7261603474617004\n",
      "Step 9 | Training Loss: 0.437431 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8534865379333496, 0.7263290882110596\n",
      "Step 10 | Training Loss: 0.463362 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.8521113991737366, 0.7227004170417786\n",
      "Step 1 | Training Loss: 0.428322 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.8521558046340942, 0.7227004170417786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.425381 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.8520670533180237, 0.7224472761154175\n",
      "Step 3 | Training Loss: 0.426639 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.8518896102905273, 0.7221096754074097\n",
      "Step 4 | Training Loss: 0.435035 | Validation Accuracy: 0.944841\n",
      "Accuracy on Test data: 0.8516234755516052, 0.7216033935546875\n",
      "Step 5 | Training Loss: 0.413669 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.8517122268676758, 0.7216033935546875\n",
      "Step 6 | Training Loss: 0.416473 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8514904379844666, 0.7209282517433167\n",
      "Step 7 | Training Loss: 0.425263 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8513129949569702, 0.7204219698905945\n",
      "Step 8 | Training Loss: 0.417076 | Validation Accuracy: 0.955556\n",
      "Accuracy on Test data: 0.8506476283073425, 0.7190717458724976\n",
      "Step 9 | Training Loss: 0.394421 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.8500709533691406, 0.7178903222084045\n",
      "Step 10 | Training Loss: 0.416676 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.8499822616577148, 0.7177215218544006\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.780910 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.690368 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.4324875771999359, 0.1839662492275238\n",
      "Step 3 | Training Loss: 0.677957 | Validation Accuracy: 0.657540\n",
      "Accuracy on Test data: 0.4726756513118744, 0.18751055002212524\n",
      "Step 4 | Training Loss: 0.645787 | Validation Accuracy: 0.664286\n",
      "Accuracy on Test data: 0.4795067310333252, 0.19400843977928162\n",
      "Step 5 | Training Loss: 0.629358 | Validation Accuracy: 0.735714\n",
      "Accuracy on Test data: 0.5999822616577148, 0.26540085673332214\n",
      "Step 6 | Training Loss: 0.570299 | Validation Accuracy: 0.784524\n",
      "Accuracy on Test data: 0.6163502335548401, 0.2935021221637726\n",
      "Step 7 | Training Loss: 0.554354 | Validation Accuracy: 0.809127\n",
      "Accuracy on Test data: 0.6512597799301147, 0.3590717315673828\n",
      "Step 8 | Training Loss: 0.572835 | Validation Accuracy: 0.855952\n",
      "Accuracy on Test data: 0.6817334890365601, 0.4100421965122223\n",
      "Step 9 | Training Loss: 0.500140 | Validation Accuracy: 0.869048\n",
      "Accuracy on Test data: 0.6993435025215149, 0.4410126507282257\n",
      "Step 10 | Training Loss: 0.480009 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.709945023059845, 0.45898735523223877\n",
      "Step 1 | Training Loss: 0.439751 | Validation Accuracy: 0.904365\n",
      "Accuracy on Test data: 0.7108765244483948, 0.46042194962501526\n",
      "Step 2 | Training Loss: 0.468868 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.7122959494590759, 0.4629535973072052\n",
      "Step 3 | Training Loss: 0.460023 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.7146469354629517, 0.46708860993385315\n",
      "Step 4 | Training Loss: 0.477517 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.7166430354118347, 0.4706329107284546\n",
      "Step 5 | Training Loss: 0.498049 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.7175745368003845, 0.4722362756729126\n",
      "Step 6 | Training Loss: 0.475751 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.7186834812164307, 0.47417721152305603\n",
      "Step 7 | Training Loss: 0.464870 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.7200585603713989, 0.47670885920524597\n",
      "Step 8 | Training Loss: 0.437806 | Validation Accuracy: 0.899603\n",
      "Accuracy on Test data: 0.7220103144645691, 0.48042193055152893\n",
      "Step 9 | Training Loss: 0.443514 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.723518431186676, 0.4830379784107208\n",
      "Step 10 | Training Loss: 0.417529 | Validation Accuracy: 0.910317\n",
      "Accuracy on Test data: 0.7254258394241333, 0.4864978790283203\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693131 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693013 | Validation Accuracy: 0.547619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692917 | Validation Accuracy: 0.544444\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692593 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.693075 | Validation Accuracy: 0.545635\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.693244 | Validation Accuracy: 0.545238\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692484 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692064 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692390 | Validation Accuracy: 0.542460\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.691512 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.693720 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692499 | Validation Accuracy: 0.532143\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692906 | Validation Accuracy: 0.539286\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692904 | Validation Accuracy: 0.541667\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692377 | Validation Accuracy: 0.548810\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692794 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692042 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692788 | Validation Accuracy: 0.538889\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692349 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.693551 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.674602 | Validation Accuracy: 0.517460\n",
      "Accuracy on Test data: 0.44175833463668823, 0.22033755481243134\n",
      "Step 2 | Training Loss: 0.668052 | Validation Accuracy: 0.630159\n",
      "Accuracy on Test data: 0.4894428551197052, 0.23400843143463135\n",
      "Step 3 | Training Loss: 0.682673 | Validation Accuracy: 0.682936\n",
      "Accuracy on Test data: 0.5122870802879333, 0.2643038034439087\n",
      "Step 4 | Training Loss: 0.700573 | Validation Accuracy: 0.734127\n",
      "Accuracy on Test data: 0.5390347838401794, 0.3118143379688263\n",
      "Step 5 | Training Loss: 0.677936 | Validation Accuracy: 0.743651\n",
      "Accuracy on Test data: 0.5877395272254944, 0.33603376150131226\n",
      "Step 6 | Training Loss: 0.658254 | Validation Accuracy: 0.793254\n",
      "Accuracy on Test data: 0.6486870050430298, 0.35940927267074585\n",
      "Step 7 | Training Loss: 0.678377 | Validation Accuracy: 0.800397\n",
      "Accuracy on Test data: 0.654675304889679, 0.36928269267082214\n",
      "Step 8 | Training Loss: 0.672955 | Validation Accuracy: 0.825000\n",
      "Accuracy on Test data: 0.6629258394241333, 0.3830379843711853\n",
      "Step 9 | Training Loss: 0.656626 | Validation Accuracy: 0.840873\n",
      "Accuracy on Test data: 0.6721078753471375, 0.39848101139068604\n",
      "Step 10 | Training Loss: 0.640856 | Validation Accuracy: 0.844841\n",
      "Accuracy on Test data: 0.6795156002044678, 0.41071730852127075\n",
      "Step 1 | Training Loss: 0.630917 | Validation Accuracy: 0.855952\n",
      "Accuracy on Test data: 0.680580198764801, 0.41232067346572876\n",
      "Step 2 | Training Loss: 0.618076 | Validation Accuracy: 0.855556\n",
      "Accuracy on Test data: 0.6818665862083435, 0.41417720913887024\n",
      "Step 3 | Training Loss: 0.635209 | Validation Accuracy: 0.854365\n",
      "Accuracy on Test data: 0.6832860112190247, 0.41611814498901367\n",
      "Step 4 | Training Loss: 0.621957 | Validation Accuracy: 0.847222\n",
      "Accuracy on Test data: 0.6848828792572021, 0.4184810221195221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.649242 | Validation Accuracy: 0.868254\n",
      "Accuracy on Test data: 0.6862136125564575, 0.42059072852134705\n",
      "Step 6 | Training Loss: 0.676349 | Validation Accuracy: 0.870635\n",
      "Accuracy on Test data: 0.687189519405365, 0.4221096932888031\n",
      "Step 7 | Training Loss: 0.662504 | Validation Accuracy: 0.852381\n",
      "Accuracy on Test data: 0.6880766749382019, 0.4232911467552185\n",
      "Step 8 | Training Loss: 0.646080 | Validation Accuracy: 0.864286\n",
      "Accuracy on Test data: 0.6890968680381775, 0.42497891187667847\n",
      "Step 9 | Training Loss: 0.629912 | Validation Accuracy: 0.854365\n",
      "Accuracy on Test data: 0.6899840235710144, 0.4265822768211365\n",
      "Step 10 | Training Loss: 0.650375 | Validation Accuracy: 0.865873\n",
      "Accuracy on Test data: 0.691226065158844, 0.4287763833999634\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.725629 | Validation Accuracy: 0.722619\n",
      "Accuracy on Test data: 0.7485361695289612, 0.6205063462257385\n",
      "Step 2 | Training Loss: 0.728004 | Validation Accuracy: 0.737698\n",
      "Accuracy on Test data: 0.7706263065338135, 0.6437974572181702\n",
      "Step 3 | Training Loss: 0.665736 | Validation Accuracy: 0.777778\n",
      "Accuracy on Test data: 0.7813608646392822, 0.6504641175270081\n",
      "Step 4 | Training Loss: 0.675929 | Validation Accuracy: 0.815873\n",
      "Accuracy on Test data: 0.7920954823493958, 0.6600843667984009\n",
      "Step 5 | Training Loss: 0.700924 | Validation Accuracy: 0.825000\n",
      "Accuracy on Test data: 0.801543653011322, 0.6723206639289856\n",
      "Step 6 | Training Loss: 0.668931 | Validation Accuracy: 0.845635\n",
      "Accuracy on Test data: 0.8115241527557373, 0.6792405247688293\n",
      "Step 7 | Training Loss: 0.605888 | Validation Accuracy: 0.864286\n",
      "Accuracy on Test data: 0.8183995485305786, 0.6809282898902893\n",
      "Step 8 | Training Loss: 0.620740 | Validation Accuracy: 0.882937\n",
      "Accuracy on Test data: 0.8232789039611816, 0.6845569610595703\n",
      "Step 9 | Training Loss: 0.558930 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8263839483261108, 0.6868354678153992\n",
      "Step 10 | Training Loss: 0.604543 | Validation Accuracy: 0.907540\n",
      "Accuracy on Test data: 0.8283800482749939, 0.6886919736862183\n",
      "Step 1 | Training Loss: 0.610870 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.828646183013916, 0.6890295147895813\n",
      "Step 2 | Training Loss: 0.621719 | Validation Accuracy: 0.904365\n",
      "Accuracy on Test data: 0.8288679718971252, 0.6891139149665833\n",
      "Step 3 | Training Loss: 0.594200 | Validation Accuracy: 0.907937\n",
      "Accuracy on Test data: 0.8294003009796143, 0.6898733973503113\n",
      "Step 4 | Training Loss: 0.618861 | Validation Accuracy: 0.905556\n",
      "Accuracy on Test data: 0.8293559551239014, 0.6896202564239502\n",
      "Step 5 | Training Loss: 0.584291 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8295777440071106, 0.6898733973503113\n",
      "Step 6 | Training Loss: 0.600628 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.8301987051963806, 0.6905485391616821\n",
      "Step 7 | Training Loss: 0.580113 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.830376148223877, 0.6904641389846802\n",
      "Step 8 | Training Loss: 0.607300 | Validation Accuracy: 0.912698\n",
      "Accuracy on Test data: 0.8304204940795898, 0.6901265978813171\n",
      "Step 9 | Training Loss: 0.608019 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.8309528231620789, 0.6908860802650452\n",
      "Step 10 | Training Loss: 0.596796 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8312633037567139, 0.6913080215454102\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.666365 | Validation Accuracy: 0.749206\n",
      "Accuracy on Test data: 0.6994321942329407, 0.46725738048553467\n",
      "Step 2 | Training Loss: 0.721750 | Validation Accuracy: 0.805556\n",
      "Accuracy on Test data: 0.7362934947013855, 0.5205063223838806\n",
      "Step 3 | Training Loss: 0.656314 | Validation Accuracy: 0.839286\n",
      "Accuracy on Test data: 0.7583836317062378, 0.5583122372627258\n",
      "Step 4 | Training Loss: 0.700008 | Validation Accuracy: 0.859524\n",
      "Accuracy on Test data: 0.7751951813697815, 0.5875105261802673\n",
      "Step 5 | Training Loss: 0.631244 | Validation Accuracy: 0.888095\n",
      "Accuracy on Test data: 0.7917405962944031, 0.6166244745254517\n",
      "Step 6 | Training Loss: 0.639342 | Validation Accuracy: 0.898810\n",
      "Accuracy on Test data: 0.8046486973762512, 0.6384810209274292\n",
      "Step 7 | Training Loss: 0.593647 | Validation Accuracy: 0.910317\n",
      "Accuracy on Test data: 0.8129435777664185, 0.652995765209198\n",
      "Step 8 | Training Loss: 0.608210 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.818222165107727, 0.6621097326278687\n",
      "Step 9 | Training Loss: 0.613670 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8333037495613098, 0.6907172799110413\n",
      "Step 10 | Training Loss: 0.586045 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.843018114566803, 0.7087763547897339\n",
      "Step 1 | Training Loss: 0.532670 | Validation Accuracy: 0.924206\n",
      "Accuracy on Test data: 0.8436391353607178, 0.7097046375274658\n",
      "Step 2 | Training Loss: 0.594499 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8431068062782288, 0.7088607549667358\n",
      "Step 3 | Training Loss: 0.587000 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8431511521339417, 0.7089451551437378\n",
      "Step 4 | Training Loss: 0.561092 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8433729410171509, 0.7092826962471008\n",
      "Step 5 | Training Loss: 0.566076 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8434173464775085, 0.7092826962471008\n",
      "Step 6 | Training Loss: 0.555511 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8436834812164307, 0.7096202373504639\n",
      "Step 7 | Training Loss: 0.566734 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8438608646392822, 0.7099577784538269\n",
      "Step 8 | Training Loss: 0.528194 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8438608646392822, 0.7099577784538269\n",
      "Step 9 | Training Loss: 0.557023 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8441270589828491, 0.7103797197341919\n",
      "Step 10 | Training Loss: 0.535637 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.844481885433197, 0.7110548615455627\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.650653 | Validation Accuracy: 0.800397\n",
      "Accuracy on Test data: 0.7495564222335815, 0.595780611038208\n",
      "Step 2 | Training Loss: 0.608130 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.7938697934150696, 0.6302953362464905\n",
      "Step 3 | Training Loss: 0.590446 | Validation Accuracy: 0.917460\n",
      "Accuracy on Test data: 0.803584098815918, 0.6425316333770752\n",
      "Step 4 | Training Loss: 0.555322 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.7896558046340942, 0.6130801439285278\n",
      "Step 5 | Training Loss: 0.559482 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.7751064300537109, 0.5841349959373474\n",
      "Step 6 | Training Loss: 0.539721 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7791873812675476, 0.5911392569541931\n",
      "Step 7 | Training Loss: 0.524516 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.7879258394241333, 0.6044725775718689\n",
      "Step 8 | Training Loss: 0.522994 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.7935149073600769, 0.6130801439285278\n",
      "Step 9 | Training Loss: 0.463488 | Validation Accuracy: 0.944841\n",
      "Accuracy on Test data: 0.79551100730896, 0.6150211095809937\n",
      "Step 10 | Training Loss: 0.474918 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.7957771420478821, 0.6151055097579956\n",
      "Step 1 | Training Loss: 0.472751 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7960876226425171, 0.6156961917877197\n",
      "Step 2 | Training Loss: 0.507862 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.7964868545532227, 0.6164556741714478\n",
      "Step 3 | Training Loss: 0.465778 | Validation Accuracy: 0.957937\n",
      "Accuracy on Test data: 0.796974778175354, 0.6173839569091797\n",
      "Step 4 | Training Loss: 0.481921 | Validation Accuracy: 0.959127\n",
      "Accuracy on Test data: 0.7971522212028503, 0.6177214980125427\n",
      "Step 5 | Training Loss: 0.467142 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.7972853183746338, 0.6179746985435486\n",
      "Step 6 | Training Loss: 0.469813 | Validation Accuracy: 0.951587\n",
      "Accuracy on Test data: 0.7972853183746338, 0.6179746985435486\n",
      "Step 7 | Training Loss: 0.447416 | Validation Accuracy: 0.954762\n",
      "Accuracy on Test data: 0.7976401448249817, 0.6186497807502747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: 0.444935 | Validation Accuracy: 0.961905\n",
      "Accuracy on Test data: 0.7979506850242615, 0.6192405223846436\n",
      "Step 9 | Training Loss: 0.458315 | Validation Accuracy: 0.953571\n",
      "Accuracy on Test data: 0.7983942627906799, 0.6200844049453735\n",
      "Step 10 | Training Loss: 0.439899 | Validation Accuracy: 0.955556\n",
      "Accuracy on Test data: 0.7988821864128113, 0.6210126876831055\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.664869 | Validation Accuracy: 0.497222\n",
      "Accuracy on Test data: 0.4430890679359436, 0.5031223893165588\n",
      "Step 2 | Training Loss: 0.646337 | Validation Accuracy: 0.781746\n",
      "Accuracy on Test data: 0.7225425839424133, 0.5286920070648193\n",
      "Step 3 | Training Loss: 0.581629 | Validation Accuracy: 0.834921\n",
      "Accuracy on Test data: 0.7565649151802063, 0.550717294216156\n",
      "Step 4 | Training Loss: 0.533676 | Validation Accuracy: 0.861111\n",
      "Accuracy on Test data: 0.7666341662406921, 0.5647257566452026\n",
      "Step 5 | Training Loss: 0.532177 | Validation Accuracy: 0.886905\n",
      "Accuracy on Test data: 0.7656582593917847, 0.5610970258712769\n",
      "Step 6 | Training Loss: 0.468242 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.787748396396637, 0.602278470993042\n",
      "Step 7 | Training Loss: 0.484245 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.7882806658744812, 0.601940929889679\n",
      "Step 8 | Training Loss: 0.471693 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7862846255302429, 0.5975527167320251\n",
      "Step 9 | Training Loss: 0.441798 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.7859297394752502, 0.5960337519645691\n",
      "Step 10 | Training Loss: 0.444633 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.7836675047874451, 0.5914767980575562\n",
      "Step 1 | Training Loss: 0.432826 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.783401370048523, 0.5909704566001892\n",
      "Step 2 | Training Loss: 0.456161 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7831795811653137, 0.5905485153198242\n",
      "Step 3 | Training Loss: 0.431300 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7829577922821045, 0.5901265740394592\n",
      "Step 4 | Training Loss: 0.451154 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7826915979385376, 0.5896202325820923\n",
      "Step 5 | Training Loss: 0.444473 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7822924256324768, 0.5888607501983643\n",
      "Step 6 | Training Loss: 0.436841 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.7821149826049805, 0.5885232090950012\n",
      "Step 7 | Training Loss: 0.430338 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7818044424057007, 0.5879324674606323\n",
      "Step 8 | Training Loss: 0.468322 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.7817600965499878, 0.5878481268882751\n",
      "Step 9 | Training Loss: 0.432195 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7814939618110657, 0.5873417854309082\n",
      "Step 10 | Training Loss: 0.402866 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.7811834812164307, 0.5867510437965393\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.589425 | Validation Accuracy: 0.840476\n",
      "Accuracy on Test data: 0.8049148321151733, 0.6491982936859131\n",
      "Step 2 | Training Loss: 0.563550 | Validation Accuracy: 0.876190\n",
      "Accuracy on Test data: 0.8116128444671631, 0.6512235999107361\n",
      "Step 3 | Training Loss: 0.508408 | Validation Accuracy: 0.901587\n",
      "Accuracy on Test data: 0.814540445804596, 0.655105471611023\n",
      "Step 4 | Training Loss: 0.527197 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6647257208824158\n",
      "Step 5 | Training Loss: 0.477479 | Validation Accuracy: 0.916270\n",
      "Accuracy on Test data: 0.8239886164665222, 0.6724894642829895\n",
      "Step 6 | Training Loss: 0.469425 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8234120011329651, 0.6708860993385315\n",
      "Step 7 | Training Loss: 0.436845 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8253637552261353, 0.6735020875930786\n",
      "Step 8 | Training Loss: 0.445295 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.8257185816764832, 0.6734177470207214\n",
      "Step 9 | Training Loss: 0.429894 | Validation Accuracy: 0.951190\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6705484986305237\n",
      "Step 10 | Training Loss: 0.412607 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.8240773677825928, 0.6691983342170715\n",
      "Step 1 | Training Loss: 0.403627 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.8239442706108093, 0.6689451336860657\n",
      "Step 2 | Training Loss: 0.407254 | Validation Accuracy: 0.944841\n",
      "Accuracy on Test data: 0.8239442706108093, 0.6687763929367065\n",
      "Step 3 | Training Loss: 0.453395 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8238999247550964, 0.6686919927597046\n",
      "Step 4 | Training Loss: 0.459912 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.8237224817276001, 0.6683544516563416\n",
      "Step 5 | Training Loss: 0.401353 | Validation Accuracy: 0.962302\n",
      "Accuracy on Test data: 0.8236337900161743, 0.6681012511253357\n",
      "Step 6 | Training Loss: 0.435515 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.8235006928443909, 0.6677637100219727\n",
      "Step 7 | Training Loss: 0.395880 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.8235450387001038, 0.6677637100219727\n",
      "Step 8 | Training Loss: 0.434277 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.8235450387001038, 0.6675949096679688\n",
      "Step 9 | Training Loss: 0.420926 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.823456346988678, 0.6673417687416077\n",
      "Step 10 | Training Loss: 0.413906 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.8233232498168945, 0.6670886278152466\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.653998 | Validation Accuracy: 0.726984\n",
      "Accuracy on Test data: 0.7276880741119385, 0.5215190052986145\n",
      "Step 2 | Training Loss: 0.625685 | Validation Accuracy: 0.765873\n",
      "Accuracy on Test data: 0.7365596294403076, 0.5217721462249756\n",
      "Step 3 | Training Loss: 0.573115 | Validation Accuracy: 0.780556\n",
      "Accuracy on Test data: 0.7529719471931458, 0.548607587814331\n",
      "Step 4 | Training Loss: 0.559331 | Validation Accuracy: 0.828968\n",
      "Accuracy on Test data: 0.7681422829627991, 0.5758649706840515\n",
      "Step 5 | Training Loss: 0.550663 | Validation Accuracy: 0.838492\n",
      "Accuracy on Test data: 0.7833569645881653, 0.601772129535675\n",
      "Step 6 | Training Loss: 0.512026 | Validation Accuracy: 0.869841\n",
      "Accuracy on Test data: 0.7940915822982788, 0.6196624636650085\n",
      "Step 7 | Training Loss: 0.494505 | Validation Accuracy: 0.882540\n",
      "Accuracy on Test data: 0.8025638461112976, 0.6349366903305054\n",
      "Step 8 | Training Loss: 0.508502 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.8111692667007446, 0.6503797173500061\n",
      "Step 9 | Training Loss: 0.504787 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8171575665473938, 0.6607595086097717\n",
      "Step 10 | Training Loss: 0.436321 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8235006928443909, 0.6713923811912537\n",
      "Step 1 | Training Loss: 0.482871 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.8253193497657776, 0.6746835708618164\n",
      "Step 2 | Training Loss: 0.455188 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.8270049691200256, 0.6775527596473694\n",
      "Step 3 | Training Loss: 0.435345 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8279364705085754, 0.6791561245918274\n",
      "Step 4 | Training Loss: 0.438341 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8284687995910645, 0.6800000071525574\n",
      "Step 5 | Training Loss: 0.426743 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.8290454149246216, 0.6809282898902893\n",
      "Step 6 | Training Loss: 0.430071 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8294003009796143, 0.6815189719200134\n",
      "Step 7 | Training Loss: 0.436922 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8299325704574585, 0.6822784543037415\n",
      "Step 8 | Training Loss: 0.429799 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8305979371070862, 0.6832067370414734\n",
      "Step 9 | Training Loss: 0.430236 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8313964009284973, 0.6843038201332092\n",
      "Step 10 | Training Loss: 0.438966 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8318843245506287, 0.6848101019859314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.612803 | Validation Accuracy: 0.754762\n",
      "Accuracy on Test data: 0.7948012948036194, 0.6987341642379761\n",
      "Step 2 | Training Loss: 0.580267 | Validation Accuracy: 0.794444\n",
      "Accuracy on Test data: 0.8097941875457764, 0.7043038010597229\n",
      "Step 3 | Training Loss: 0.561358 | Validation Accuracy: 0.874603\n",
      "Accuracy on Test data: 0.839292049407959, 0.7158649563789368\n",
      "Step 4 | Training Loss: 0.496500 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.8457682728767395, 0.7212658524513245\n",
      "Step 5 | Training Loss: 0.496553 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8356547355651855, 0.6999155879020691\n",
      "Step 6 | Training Loss: 0.454943 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.8326383829116821, 0.6914768218994141\n",
      "Step 7 | Training Loss: 0.404323 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8317955732345581, 0.6894514560699463\n",
      "Step 8 | Training Loss: 0.418696 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8310858607292175, 0.6877636909484863\n",
      "Step 9 | Training Loss: 0.442707 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.8313519954681396, 0.6870886087417603\n",
      "Step 10 | Training Loss: 0.446168 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8309971690177917, 0.6852320432662964\n",
      "Step 1 | Training Loss: 0.404353 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8309084177017212, 0.6850633025169373\n",
      "Step 2 | Training Loss: 0.405492 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.8309528231620789, 0.6849789023399353\n",
      "Step 3 | Training Loss: 0.455058 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8309528231620789, 0.6848101019859314\n",
      "Step 4 | Training Loss: 0.404041 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8308197259902954, 0.6844725608825684\n",
      "Step 5 | Training Loss: 0.367971 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8308197259902954, 0.6843881607055664\n",
      "Step 6 | Training Loss: 0.424454 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8304648399353027, 0.6837130784988403\n",
      "Step 7 | Training Loss: 0.431875 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8304204940795898, 0.6832067370414734\n",
      "Step 8 | Training Loss: 0.414714 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8303318023681641, 0.6827847957611084\n",
      "Step 9 | Training Loss: 0.454564 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8303318023681641, 0.6825316548347473\n",
      "Step 10 | Training Loss: 0.420728 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8301543593406677, 0.6819409132003784\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.694126 | Validation Accuracy: 0.455556\n",
      "Accuracy on Test data: 0.5479063391685486, 0.7900422215461731\n",
      "Step 2 | Training Loss: 0.632025 | Validation Accuracy: 0.780952\n",
      "Accuracy on Test data: 0.841864824295044, 0.8167932629585266\n",
      "Step 3 | Training Loss: 0.608314 | Validation Accuracy: 0.845635\n",
      "Accuracy on Test data: 0.8438165187835693, 0.7656540274620056\n",
      "Step 4 | Training Loss: 0.572632 | Validation Accuracy: 0.845238\n",
      "Accuracy on Test data: 0.851401686668396, 0.7705485224723816\n",
      "Step 5 | Training Loss: 0.533772 | Validation Accuracy: 0.867857\n",
      "Accuracy on Test data: 0.8575230836868286, 0.7740928530693054\n",
      "Step 6 | Training Loss: 0.591408 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8700762987136841, 0.7729957699775696\n",
      "Step 7 | Training Loss: 0.526547 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8702093958854675, 0.7629535794258118\n",
      "Step 8 | Training Loss: 0.491442 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8631121516227722, 0.74725741147995\n",
      "Step 9 | Training Loss: 0.450870 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8561035990715027, 0.7336708903312683\n",
      "Step 10 | Training Loss: 0.451040 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.8521113991737366, 0.7255696058273315\n",
      "Step 1 | Training Loss: 0.469236 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.8517122268676758, 0.7248101234436035\n",
      "Step 2 | Training Loss: 0.462070 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8512242436408997, 0.7238818407058716\n",
      "Step 3 | Training Loss: 0.449469 | Validation Accuracy: 0.944841\n",
      "Accuracy on Test data: 0.8506032824516296, 0.7226160168647766\n",
      "Step 4 | Training Loss: 0.449389 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8500266075134277, 0.7215189933776855\n",
      "Step 5 | Training Loss: 0.416672 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.8495386838912964, 0.7205907106399536\n",
      "Step 6 | Training Loss: 0.463863 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.8492282032966614, 0.7200000286102295\n",
      "Step 7 | Training Loss: 0.444846 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.8482522964477539, 0.7180590629577637\n",
      "Step 8 | Training Loss: 0.454912 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.8475869297981262, 0.7167932391166687\n",
      "Step 9 | Training Loss: 0.453878 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.8466554284095764, 0.7150210738182068\n",
      "Step 10 | Training Loss: 0.430441 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.8461675047874451, 0.7140928506851196\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.692977 | Validation Accuracy: 0.526587\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693095 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692979 | Validation Accuracy: 0.547222\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692718 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692972 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692877 | Validation Accuracy: 0.528968\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692546 | Validation Accuracy: 0.521429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692872 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693115 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692506 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.693215 | Validation Accuracy: 0.544841\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693216 | Validation Accuracy: 0.525000\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692383 | Validation Accuracy: 0.525397\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693007 | Validation Accuracy: 0.533730\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692794 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692898 | Validation Accuracy: 0.521429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.693004 | Validation Accuracy: 0.544048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692893 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692671 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.691890 | Validation Accuracy: 0.538889\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.691931 | Validation Accuracy: 0.734127\n",
      "Accuracy on Test data: 0.813342809677124, 0.6985654234886169\n",
      "Step 2 | Training Loss: 0.668223 | Validation Accuracy: 0.773413\n",
      "Accuracy on Test data: 0.8168026804924011, 0.6777215003967285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 0.659383 | Validation Accuracy: 0.818651\n",
      "Accuracy on Test data: 0.8191536664962769, 0.6737552881240845\n",
      "Step 4 | Training Loss: 0.709997 | Validation Accuracy: 0.841270\n",
      "Accuracy on Test data: 0.8188875317573547, 0.6703797578811646\n",
      "Step 5 | Training Loss: 0.704000 | Validation Accuracy: 0.846825\n",
      "Accuracy on Test data: 0.8144960999488831, 0.6606751084327698\n",
      "Step 6 | Training Loss: 0.643223 | Validation Accuracy: 0.856746\n",
      "Accuracy on Test data: 0.8156050443649292, 0.6619409322738647\n",
      "Step 7 | Training Loss: 0.648264 | Validation Accuracy: 0.873016\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6655696034431458\n",
      "Step 8 | Training Loss: 0.660275 | Validation Accuracy: 0.882540\n",
      "Accuracy on Test data: 0.8220369219779968, 0.6727426052093506\n",
      "Step 9 | Training Loss: 0.628110 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8214602470397949, 0.6711392402648926\n",
      "Step 10 | Training Loss: 0.655740 | Validation Accuracy: 0.918254\n",
      "Accuracy on Test data: 0.819730281829834, 0.6675949096679688\n",
      "Step 1 | Training Loss: 0.652830 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.8196859359741211, 0.6675105690956116\n",
      "Step 2 | Training Loss: 0.653694 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8195528984069824, 0.6671729683876038\n",
      "Step 3 | Training Loss: 0.635673 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.8192867040634155, 0.6666666865348816\n",
      "Step 4 | Training Loss: 0.652892 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6670042276382446\n",
      "Step 5 | Training Loss: 0.644060 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.8198633790016174, 0.6670042276382446\n",
      "Step 6 | Training Loss: 0.680189 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.8198190331459045, 0.6669198274612427\n",
      "Step 7 | Training Loss: 0.651579 | Validation Accuracy: 0.912698\n",
      "Accuracy on Test data: 0.8196859359741211, 0.6665822863578796\n",
      "Step 8 | Training Loss: 0.680889 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8194641470909119, 0.6659915447235107\n",
      "Step 9 | Training Loss: 0.627540 | Validation Accuracy: 0.916270\n",
      "Accuracy on Test data: 0.8195528984069824, 0.6659915447235107\n",
      "Step 10 | Training Loss: 0.626697 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8192867040634155, 0.6654852032661438\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.702601 | Validation Accuracy: 0.496825\n",
      "Accuracy on Test data: 0.5121539831161499, 0.6954430341720581\n",
      "Step 2 | Training Loss: 0.676910 | Validation Accuracy: 0.644048\n",
      "Accuracy on Test data: 0.6612846255302429, 0.7364556789398193\n",
      "Step 3 | Training Loss: 0.721454 | Validation Accuracy: 0.787302\n",
      "Accuracy on Test data: 0.8196859359741211, 0.7393248677253723\n",
      "Step 4 | Training Loss: 0.686265 | Validation Accuracy: 0.830952\n",
      "Accuracy on Test data: 0.8471433520317078, 0.7570464015007019\n",
      "Step 5 | Training Loss: 0.637322 | Validation Accuracy: 0.863492\n",
      "Accuracy on Test data: 0.8555713295936584, 0.7592405080795288\n",
      "Step 6 | Training Loss: 0.581510 | Validation Accuracy: 0.865873\n",
      "Accuracy on Test data: 0.848207950592041, 0.7313923835754395\n",
      "Step 7 | Training Loss: 0.608428 | Validation Accuracy: 0.883333\n",
      "Accuracy on Test data: 0.8499822616577148, 0.7317299842834473\n",
      "Step 8 | Training Loss: 0.641269 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.849937915802002, 0.7297890186309814\n",
      "Step 9 | Training Loss: 0.592612 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.8502040505409241, 0.7288607358932495\n",
      "Step 10 | Training Loss: 0.644354 | Validation Accuracy: 0.889286\n",
      "Accuracy on Test data: 0.8524662852287292, 0.7318143248558044\n",
      "Step 1 | Training Loss: 0.619906 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.8523331880569458, 0.7315611839294434\n",
      "Step 2 | Training Loss: 0.623730 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8522888422012329, 0.7313923835754395\n",
      "Step 3 | Training Loss: 0.555214 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.8521558046340942, 0.7311392426490784\n",
      "Step 4 | Training Loss: 0.597120 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8523775935173035, 0.7314767837524414\n",
      "Step 5 | Training Loss: 0.574108 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8524662852287292, 0.7316455841064453\n",
      "Step 6 | Training Loss: 0.631343 | Validation Accuracy: 0.900397\n",
      "Accuracy on Test data: 0.8527324199676514, 0.7318987250328064\n",
      "Step 7 | Training Loss: 0.586399 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8526437282562256, 0.7316455841064453\n",
      "Step 8 | Training Loss: 0.598379 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8528211712837219, 0.7319831252098083\n",
      "Step 9 | Training Loss: 0.607378 | Validation Accuracy: 0.901587\n",
      "Accuracy on Test data: 0.8528211712837219, 0.7318987250328064\n",
      "Step 10 | Training Loss: 0.575046 | Validation Accuracy: 0.905159\n",
      "Accuracy on Test data: 0.8527324199676514, 0.7314767837524414\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.717214 | Validation Accuracy: 0.638889\n",
      "Accuracy on Test data: 0.47564762830734253, 0.19535864889621735\n",
      "Step 2 | Training Loss: 0.718317 | Validation Accuracy: 0.679762\n",
      "Accuracy on Test data: 0.5, 0.23721519112586975\n",
      "Step 3 | Training Loss: 0.660299 | Validation Accuracy: 0.784921\n",
      "Accuracy on Test data: 0.5591288208961487, 0.3466666638851166\n",
      "Step 4 | Training Loss: 0.670138 | Validation Accuracy: 0.826190\n",
      "Accuracy on Test data: 0.6038857102394104, 0.42784810066223145\n",
      "Step 5 | Training Loss: 0.634420 | Validation Accuracy: 0.854365\n",
      "Accuracy on Test data: 0.686346709728241, 0.5042194128036499\n",
      "Step 6 | Training Loss: 0.660137 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.76987224817276, 0.5744304060935974\n",
      "Step 7 | Training Loss: 0.621017 | Validation Accuracy: 0.910714\n",
      "Accuracy on Test data: 0.8012775182723999, 0.6306329369544983\n",
      "Step 8 | Training Loss: 0.623037 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8227466344833374, 0.6697046160697937\n",
      "Step 9 | Training Loss: 0.554928 | Validation Accuracy: 0.928175\n",
      "Accuracy on Test data: 0.8373402953147888, 0.6962025165557861\n",
      "Step 10 | Training Loss: 0.582059 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8451029062271118, 0.7122362852096558\n",
      "Step 1 | Training Loss: 0.556614 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.8455908298492432, 0.7131645679473877\n",
      "Step 2 | Training Loss: 0.530850 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8466997742652893, 0.7152742743492126\n",
      "Step 3 | Training Loss: 0.501216 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8476312756538391, 0.7169620394706726\n",
      "Step 4 | Training Loss: 0.582262 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8484740853309631, 0.7184810042381287\n",
      "Step 5 | Training Loss: 0.521577 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8492725491523743, 0.7200000286102295\n",
      "Step 6 | Training Loss: 0.562213 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.849937915802002, 0.7210970520973206\n",
      "Step 7 | Training Loss: 0.573537 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.8500266075134277, 0.7212658524513245\n",
      "Step 8 | Training Loss: 0.617855 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8507363200187683, 0.7226160168647766\n",
      "Step 9 | Training Loss: 0.565426 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.8506919741630554, 0.7224472761154175\n",
      "Step 10 | Training Loss: 0.557457 | Validation Accuracy: 0.934127\n",
      "Accuracy on Test data: 0.8510024547576904, 0.7230379581451416\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.686758 | Validation Accuracy: 0.607540\n",
      "Accuracy on Test data: 0.6889194250106812, 0.7514767646789551\n",
      "Step 2 | Training Loss: 0.665358 | Validation Accuracy: 0.763492\n",
      "Accuracy on Test data: 0.8183995485305786, 0.7335020899772644\n",
      "Step 3 | Training Loss: 0.632740 | Validation Accuracy: 0.853571\n",
      "Accuracy on Test data: 0.8425301909446716, 0.7399156093597412\n",
      "Step 4 | Training Loss: 0.590730 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.8431511521339417, 0.7227848172187805\n",
      "Step 5 | Training Loss: 0.574531 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8416873812675476, 0.7125738263130188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Training Loss: 0.519842 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8354772925376892, 0.699240505695343\n",
      "Step 7 | Training Loss: 0.522926 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.833215057849884, 0.6945147514343262\n",
      "Step 8 | Training Loss: 0.486926 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8254967927932739, 0.6791561245918274\n",
      "Step 9 | Training Loss: 0.508210 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.824609637260437, 0.6770464181900024\n",
      "Step 10 | Training Loss: 0.457895 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8247427344322205, 0.6769620180130005\n",
      "Step 1 | Training Loss: 0.468962 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8247427344322205, 0.6769620180130005\n",
      "Step 2 | Training Loss: 0.497946 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6763713359832764\n",
      "Step 3 | Training Loss: 0.462751 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6763713359832764\n",
      "Step 4 | Training Loss: 0.480183 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6760337352752686\n",
      "Step 5 | Training Loss: 0.482385 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8246539831161499, 0.6761181354522705\n",
      "Step 6 | Training Loss: 0.445129 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8246539831161499, 0.6761181354522705\n",
      "Step 7 | Training Loss: 0.454202 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8247427344322205, 0.6762025356292725\n",
      "Step 8 | Training Loss: 0.474418 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8247870802879333, 0.6761181354522705\n",
      "Step 9 | Training Loss: 0.481775 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8246983885765076, 0.6759493947029114\n",
      "Step 10 | Training Loss: 0.462712 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8248314261436462, 0.6762025356292725\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.597511 | Validation Accuracy: 0.910714\n",
      "Accuracy on Test data: 0.7952448725700378, 0.6205063462257385\n",
      "Step 2 | Training Loss: 0.555913 | Validation Accuracy: 0.907937\n",
      "Accuracy on Test data: 0.7946681976318359, 0.6181434392929077\n",
      "Step 3 | Training Loss: 0.511178 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.79551100730896, 0.6193249225616455\n",
      "Step 4 | Training Loss: 0.468651 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.7931156754493713, 0.6145991683006287\n",
      "Step 5 | Training Loss: 0.476205 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.7953335642814636, 0.6181434392929077\n",
      "Step 6 | Training Loss: 0.447017 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.7981281280517578, 0.6232067346572876\n",
      "Step 7 | Training Loss: 0.470718 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8005234003067017, 0.6274261474609375\n",
      "Step 8 | Training Loss: 0.447599 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.8040276765823364, 0.6336708664894104\n",
      "Step 9 | Training Loss: 0.425799 | Validation Accuracy: 0.955556\n",
      "Accuracy on Test data: 0.799547553062439, 0.6245569586753845\n",
      "Step 10 | Training Loss: 0.427697 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.7989708781242371, 0.6232067346572876\n",
      "Step 1 | Training Loss: 0.431990 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7991483211517334, 0.6235442757606506\n",
      "Step 2 | Training Loss: 0.405276 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.7992814183235168, 0.6237130761146545\n",
      "Step 3 | Training Loss: 0.423262 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7995032072067261, 0.6239662170410156\n",
      "Step 4 | Training Loss: 0.376019 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.8000354766845703, 0.6248100996017456\n",
      "Step 5 | Training Loss: 0.421670 | Validation Accuracy: 0.954762\n",
      "Accuracy on Test data: 0.8005234003067017, 0.6256539821624756\n",
      "Step 6 | Training Loss: 0.398255 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8007895946502686, 0.6259071826934814\n",
      "Step 7 | Training Loss: 0.403782 | Validation Accuracy: 0.957937\n",
      "Accuracy on Test data: 0.8009226322174072, 0.6260759234428406\n",
      "Step 8 | Training Loss: 0.395043 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.8010113835334778, 0.6262447237968445\n",
      "Step 9 | Training Loss: 0.396767 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.8014993071556091, 0.6270042061805725\n",
      "Step 10 | Training Loss: 0.391655 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.801854133605957, 0.6275949478149414\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.685276 | Validation Accuracy: 0.692064\n",
      "Accuracy on Test data: 0.709412693977356, 0.6356962323188782\n",
      "Step 2 | Training Loss: 0.644687 | Validation Accuracy: 0.754762\n",
      "Accuracy on Test data: 0.773065984249115, 0.6459915637969971\n",
      "Step 3 | Training Loss: 0.564244 | Validation Accuracy: 0.777381\n",
      "Accuracy on Test data: 0.7856192588806152, 0.652995765209198\n",
      "Step 4 | Training Loss: 0.551196 | Validation Accuracy: 0.866270\n",
      "Accuracy on Test data: 0.8131210207939148, 0.6854852437973022\n",
      "Step 5 | Training Loss: 0.488799 | Validation Accuracy: 0.890079\n",
      "Accuracy on Test data: 0.8348562717437744, 0.7053164839744568\n",
      "Step 6 | Training Loss: 0.473321 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8458126187324524, 0.7188185453414917\n",
      "Step 7 | Training Loss: 0.483861 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8504701852798462, 0.7230379581451416\n",
      "Step 8 | Training Loss: 0.450949 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8368523716926575, 0.6938396692276001\n",
      "Step 9 | Training Loss: 0.407456 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8366305828094482, 0.6924050450325012\n",
      "Step 10 | Training Loss: 0.456510 | Validation Accuracy: 0.948016\n",
      "Accuracy on Test data: 0.8238112330436707, 0.6675949096679688\n",
      "Step 1 | Training Loss: 0.427949 | Validation Accuracy: 0.955159\n",
      "Accuracy on Test data: 0.8233232498168945, 0.6666666865348816\n",
      "Step 2 | Training Loss: 0.447332 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.8228797316551208, 0.6658228039741516\n",
      "Step 3 | Training Loss: 0.443942 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.822613537311554, 0.6653164625167847\n",
      "Step 4 | Training Loss: 0.452127 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.8225691914558411, 0.6650632619857788\n",
      "Step 5 | Training Loss: 0.442442 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.8223917484283447, 0.6647257208824158\n",
      "Step 6 | Training Loss: 0.453039 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.8221256136894226, 0.6642194390296936\n",
      "Step 7 | Training Loss: 0.470756 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.8220369219779968, 0.6640506386756897\n",
      "Step 8 | Training Loss: 0.416580 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8211941123008728, 0.6624472737312317\n",
      "Step 9 | Training Loss: 0.438177 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.8206174373626709, 0.6613501906394958\n",
      "Step 10 | Training Loss: 0.434558 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.8199077248573303, 0.6600000262260437\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.711239 | Validation Accuracy: 0.657143\n",
      "Accuracy on Test data: 0.47706708312034607, 0.3231223523616791\n",
      "Step 2 | Training Loss: 0.668185 | Validation Accuracy: 0.758730\n",
      "Accuracy on Test data: 0.5452448725700378, 0.3381434679031372\n",
      "Step 3 | Training Loss: 0.622716 | Validation Accuracy: 0.803571\n",
      "Accuracy on Test data: 0.5687987804412842, 0.3647257387638092\n",
      "Step 4 | Training Loss: 0.572492 | Validation Accuracy: 0.841270\n",
      "Accuracy on Test data: 0.5935060381889343, 0.4035443067550659\n",
      "Step 5 | Training Loss: 0.582139 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.7048438787460327, 0.4529114067554474\n",
      "Step 6 | Training Loss: 0.526232 | Validation Accuracy: 0.912302\n",
      "Accuracy on Test data: 0.7365152835845947, 0.5115611553192139\n",
      "Step 7 | Training Loss: 0.515628 | Validation Accuracy: 0.914683\n",
      "Accuracy on Test data: 0.7444109320640564, 0.5250632762908936\n",
      "Step 8 | Training Loss: 0.473605 | Validation Accuracy: 0.915079\n",
      "Accuracy on Test data: 0.7569641470909119, 0.5481012463569641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.503093 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.7600248456001282, 0.552827000617981\n",
      "Step 10 | Training Loss: 0.475460 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7623314261436462, 0.5570464134216309\n",
      "Step 1 | Training Loss: 0.476315 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.7625532150268555, 0.5573839545249939\n",
      "Step 2 | Training Loss: 0.480512 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.7629967927932739, 0.5581434369087219\n",
      "Step 3 | Training Loss: 0.439609 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.7639282941818237, 0.5599156022071838\n",
      "Step 4 | Training Loss: 0.442043 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.7649042010307312, 0.5617721676826477\n",
      "Step 5 | Training Loss: 0.444520 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.7654364705085754, 0.5627847909927368\n",
      "Step 6 | Training Loss: 0.468808 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.7659687995910645, 0.5637130737304688\n",
      "Step 7 | Training Loss: 0.436801 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.7665897607803345, 0.5646413564682007\n",
      "Step 8 | Training Loss: 0.426007 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.7672551274299622, 0.5652320384979248\n",
      "Step 9 | Training Loss: 0.450346 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.7674325704574585, 0.5652320384979248\n",
      "Step 10 | Training Loss: 0.405150 | Validation Accuracy: 0.926587\n",
      "Accuracy on Test data: 0.7676987051963806, 0.5657383799552917\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.646812 | Validation Accuracy: 0.618651\n",
      "Accuracy on Test data: 0.5704400539398193, 0.5921518802642822\n",
      "Step 2 | Training Loss: 0.627499 | Validation Accuracy: 0.783730\n",
      "Accuracy on Test data: 0.7158889174461365, 0.6121519207954407\n",
      "Step 3 | Training Loss: 0.596540 | Validation Accuracy: 0.877778\n",
      "Accuracy on Test data: 0.7902324199676514, 0.6380590796470642\n",
      "Step 4 | Training Loss: 0.557724 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8307753801345825, 0.6924894452095032\n",
      "Step 5 | Training Loss: 0.509376 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8409332633018494, 0.7075949311256409\n",
      "Step 6 | Training Loss: 0.463679 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8423970937728882, 0.7090295553207397\n",
      "Step 7 | Training Loss: 0.463332 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.8265613913536072, 0.6773839592933655\n",
      "Step 8 | Training Loss: 0.444123 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.8050479292869568, 0.6360337734222412\n",
      "Step 9 | Training Loss: 0.455680 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.7940471768379211, 0.6142616271972656\n",
      "Step 10 | Training Loss: 0.428842 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.7929826378822327, 0.6124050617218018\n",
      "Step 1 | Training Loss: 0.392396 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.792938232421875, 0.6123206615447998\n",
      "Step 2 | Training Loss: 0.436978 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.7927608489990234, 0.6118987202644348\n",
      "Step 3 | Training Loss: 0.440402 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.7924946546554565, 0.6115611791610718\n",
      "Step 4 | Training Loss: 0.450673 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.7922728657722473, 0.6108860969543457\n",
      "Step 5 | Training Loss: 0.445966 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7922285199165344, 0.6103797554969788\n",
      "Step 6 | Training Loss: 0.430984 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7921841740608215, 0.6099578142166138\n",
      "Step 7 | Training Loss: 0.390841 | Validation Accuracy: 0.951587\n",
      "Accuracy on Test data: 0.7919623851776123, 0.6094514727592468\n",
      "Step 8 | Training Loss: 0.456879 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7916519045829773, 0.6087763905525208\n",
      "Step 9 | Training Loss: 0.418521 | Validation Accuracy: 0.940873\n",
      "Accuracy on Test data: 0.7912970185279846, 0.6081012487411499\n",
      "Step 10 | Training Loss: 0.470465 | Validation Accuracy: 0.951190\n",
      "Accuracy on Test data: 0.7907647490501404, 0.6069198250770569\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.625234 | Validation Accuracy: 0.765873\n",
      "Accuracy on Test data: 0.6851490139961243, 0.4228692054748535\n",
      "Step 2 | Training Loss: 0.616017 | Validation Accuracy: 0.788889\n",
      "Accuracy on Test data: 0.6930447220802307, 0.4311392307281494\n",
      "Step 3 | Training Loss: 0.595882 | Validation Accuracy: 0.831349\n",
      "Accuracy on Test data: 0.7341199517250061, 0.5066666603088379\n",
      "Step 4 | Training Loss: 0.538641 | Validation Accuracy: 0.853571\n",
      "Accuracy on Test data: 0.751685619354248, 0.5380590558052063\n",
      "Step 5 | Training Loss: 0.525605 | Validation Accuracy: 0.884921\n",
      "Accuracy on Test data: 0.7706263065338135, 0.5715611577033997\n",
      "Step 6 | Training Loss: 0.482921 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.7838449478149414, 0.5955274105072021\n",
      "Step 7 | Training Loss: 0.478436 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.7893009185791016, 0.6056540012359619\n",
      "Step 8 | Training Loss: 0.472196 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.7926720976829529, 0.6114767789840698\n",
      "Step 9 | Training Loss: 0.471774 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.7899219393730164, 0.605991542339325\n",
      "Step 10 | Training Loss: 0.449106 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.78974449634552, 0.6036286950111389\n",
      "Step 1 | Training Loss: 0.422229 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.7897888422012329, 0.6037130951881409\n",
      "Step 2 | Training Loss: 0.479440 | Validation Accuracy: 0.940873\n",
      "Accuracy on Test data: 0.790054976940155, 0.6041350364685059\n",
      "Step 3 | Training Loss: 0.440423 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.7903211712837219, 0.6044725775718689\n",
      "Step 4 | Training Loss: 0.459038 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.7904985547065735, 0.6044725775718689\n",
      "Step 5 | Training Loss: 0.451387 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.790587306022644, 0.6044725775718689\n",
      "Step 6 | Training Loss: 0.424390 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.7905429601669312, 0.604303777217865\n",
      "Step 7 | Training Loss: 0.437329 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.7905429601669312, 0.604303777217865\n",
      "Step 8 | Training Loss: 0.425404 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7908534407615662, 0.6048945188522339\n",
      "Step 9 | Training Loss: 0.421577 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.7909421324729919, 0.6050633192062378\n",
      "Step 10 | Training Loss: 0.447294 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.7908534407615662, 0.6048945188522339\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693194 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692996 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693222 | Validation Accuracy: 0.552381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693443 | Validation Accuracy: 0.528175\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692930 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692595 | Validation Accuracy: 0.541270\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692915 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693196 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692850 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692331 | Validation Accuracy: 0.542857\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.691828 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.692514 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692609 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692604 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692804 | Validation Accuracy: 0.559127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692801 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692379 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693112 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692899 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692897 | Validation Accuracy: 0.541270\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.711835 | Validation Accuracy: 0.453175\n",
      "Accuracy on Test data: 0.5666252374649048, 0.8110548257827759\n",
      "Step 2 | Training Loss: 0.703032 | Validation Accuracy: 0.694048\n",
      "Accuracy on Test data: 0.8105482459068298, 0.8005063533782959\n",
      "Step 3 | Training Loss: 0.690341 | Validation Accuracy: 0.729365\n",
      "Accuracy on Test data: 0.8415099382400513, 0.7953586578369141\n",
      "Step 4 | Training Loss: 0.631821 | Validation Accuracy: 0.729762\n",
      "Accuracy on Test data: 0.8477643728256226, 0.796624481678009\n",
      "Step 5 | Training Loss: 0.654214 | Validation Accuracy: 0.773810\n",
      "Accuracy on Test data: 0.8566802740097046, 0.797552764415741\n",
      "Step 6 | Training Loss: 0.632235 | Validation Accuracy: 0.803175\n",
      "Accuracy on Test data: 0.8562366962432861, 0.7839662432670593\n",
      "Step 7 | Training Loss: 0.653441 | Validation Accuracy: 0.810317\n",
      "Accuracy on Test data: 0.8555269837379456, 0.7722362875938416\n",
      "Step 8 | Training Loss: 0.636940 | Validation Accuracy: 0.811905\n",
      "Accuracy on Test data: 0.8471433520317078, 0.745147705078125\n",
      "Step 9 | Training Loss: 0.608553 | Validation Accuracy: 0.873413\n",
      "Accuracy on Test data: 0.8610717058181763, 0.7609282732009888\n",
      "Step 10 | Training Loss: 0.612059 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.8607611656188965, 0.752067506313324\n",
      "Step 1 | Training Loss: 0.599603 | Validation Accuracy: 0.894048\n",
      "Accuracy on Test data: 0.8612047433853149, 0.7521519064903259\n",
      "Step 2 | Training Loss: 0.588163 | Validation Accuracy: 0.898016\n",
      "Accuracy on Test data: 0.8613378405570984, 0.751983106136322\n",
      "Step 3 | Training Loss: 0.628237 | Validation Accuracy: 0.890476\n",
      "Accuracy on Test data: 0.8611160516738892, 0.751223623752594\n",
      "Step 4 | Training Loss: 0.608137 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.8607168197631836, 0.7502109408378601\n",
      "Step 5 | Training Loss: 0.621982 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.8605393767356873, 0.7497046589851379\n",
      "Step 6 | Training Loss: 0.609397 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8606724739074707, 0.7494514584541321\n",
      "Step 7 | Training Loss: 0.618595 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.8606281280517578, 0.749198317527771\n",
      "Step 8 | Training Loss: 0.622119 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.8600514531135559, 0.747594952583313\n",
      "Step 9 | Training Loss: 0.591745 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.8510468602180481, 0.7299578189849854\n",
      "Step 10 | Training Loss: 0.638768 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.8484740853309631, 0.7246413230895996\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.675512 | Validation Accuracy: 0.714286\n",
      "Accuracy on Test data: 0.7251153588294983, 0.6199156045913696\n",
      "Step 2 | Training Loss: 0.713261 | Validation Accuracy: 0.794841\n",
      "Accuracy on Test data: 0.7733765244483948, 0.6345991492271423\n",
      "Step 3 | Training Loss: 0.658268 | Validation Accuracy: 0.807936\n",
      "Accuracy on Test data: 0.7841997742652893, 0.6349366903305054\n",
      "Step 4 | Training Loss: 0.687683 | Validation Accuracy: 0.832143\n",
      "Accuracy on Test data: 0.7933818101882935, 0.6367932558059692\n",
      "Step 5 | Training Loss: 0.613650 | Validation Accuracy: 0.844841\n",
      "Accuracy on Test data: 0.7933374643325806, 0.6299577951431274\n",
      "Step 6 | Training Loss: 0.610066 | Validation Accuracy: 0.851190\n",
      "Accuracy on Test data: 0.7989265322685242, 0.6378058791160583\n",
      "Step 7 | Training Loss: 0.591063 | Validation Accuracy: 0.841270\n",
      "Accuracy on Test data: 0.8025195002555847, 0.6430379748344421\n",
      "Step 8 | Training Loss: 0.585287 | Validation Accuracy: 0.864683\n",
      "Accuracy on Test data: 0.8057132959365845, 0.647257387638092\n",
      "Step 9 | Training Loss: 0.593968 | Validation Accuracy: 0.873810\n",
      "Accuracy on Test data: 0.8095280528068542, 0.6524050831794739\n",
      "Step 10 | Training Loss: 0.569856 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.8138307332992554, 0.6600000262260437\n",
      "Step 1 | Training Loss: 0.587240 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.8141412138938904, 0.6605063080787659\n",
      "Step 2 | Training Loss: 0.595880 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.8144074082374573, 0.6608439087867737\n",
      "Step 3 | Training Loss: 0.585826 | Validation Accuracy: 0.889683\n",
      "Accuracy on Test data: 0.8144960999488831, 0.6609282493591309\n",
      "Step 4 | Training Loss: 0.564447 | Validation Accuracy: 0.887698\n",
      "Accuracy on Test data: 0.8147622346878052, 0.6612658500671387\n",
      "Step 5 | Training Loss: 0.573700 | Validation Accuracy: 0.885317\n",
      "Accuracy on Test data: 0.8149840235710144, 0.6616877913475037\n",
      "Step 6 | Training Loss: 0.541798 | Validation Accuracy: 0.889683\n",
      "Accuracy on Test data: 0.8153389096260071, 0.6621097326278687\n",
      "Step 7 | Training Loss: 0.585719 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.8157824873924255, 0.6628692150115967\n",
      "Step 8 | Training Loss: 0.551881 | Validation Accuracy: 0.888889\n",
      "Accuracy on Test data: 0.8158711791038513, 0.6630379557609558\n",
      "Step 9 | Training Loss: 0.582297 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.8159599304199219, 0.6630379557609558\n",
      "Step 10 | Training Loss: 0.570393 | Validation Accuracy: 0.877381\n",
      "Accuracy on Test data: 0.8159599304199219, 0.6629536151885986\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.678088 | Validation Accuracy: 0.604762\n",
      "Accuracy on Test data: 0.6869677305221558, 0.7235442996025085\n",
      "Step 2 | Training Loss: 0.657782 | Validation Accuracy: 0.778175\n",
      "Accuracy on Test data: 0.8247427344322205, 0.7498733997344971\n",
      "Step 3 | Training Loss: 0.664318 | Validation Accuracy: 0.803968\n",
      "Accuracy on Test data: 0.844171404838562, 0.7686076164245605\n",
      "Step 4 | Training Loss: 0.590671 | Validation Accuracy: 0.835714\n",
      "Accuracy on Test data: 0.8435947299003601, 0.7594936490058899\n",
      "Step 5 | Training Loss: 0.585796 | Validation Accuracy: 0.854762\n",
      "Accuracy on Test data: 0.8375177383422852, 0.7303797602653503\n",
      "Step 6 | Training Loss: 0.577828 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.8422196507453918, 0.7255696058273315\n",
      "Step 7 | Training Loss: 0.546672 | Validation Accuracy: 0.906349\n",
      "Accuracy on Test data: 0.842175304889679, 0.7194936871528625\n",
      "Step 8 | Training Loss: 0.551687 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.8404896855354309, 0.7118987441062927\n",
      "Step 9 | Training Loss: 0.518157 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8369410634040833, 0.7019409537315369\n",
      "Step 10 | Training Loss: 0.522850 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8301987051963806, 0.6886075735092163\n",
      "Step 1 | Training Loss: 0.504923 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8299325704574585, 0.6879324913024902\n",
      "Step 2 | Training Loss: 0.558597 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8298882246017456, 0.6878480911254883\n",
      "Step 3 | Training Loss: 0.569669 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8296664357185364, 0.6874261498451233\n",
      "Step 4 | Training Loss: 0.526993 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.8290010690689087, 0.6859915852546692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.559182 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8289567232131958, 0.6858227849006653\n",
      "Step 6 | Training Loss: 0.517693 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.828646183013916, 0.6852320432662964\n",
      "Step 7 | Training Loss: 0.510088 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8281582593917847, 0.6842194199562073\n",
      "Step 8 | Training Loss: 0.552179 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8279364705085754, 0.6837974786758423\n",
      "Step 9 | Training Loss: 0.494122 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8276703357696533, 0.6832911372184753\n",
      "Step 10 | Training Loss: 0.566281 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8273154497146606, 0.6825316548347473\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.725356 | Validation Accuracy: 0.722619\n",
      "Accuracy on Test data: 0.7666341662406921, 0.8214346170425415\n",
      "Step 2 | Training Loss: 0.667352 | Validation Accuracy: 0.857937\n",
      "Accuracy on Test data: 0.8697658181190491, 0.797130823135376\n",
      "Step 3 | Training Loss: 0.643155 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.859474778175354, 0.7669198513031006\n",
      "Step 4 | Training Loss: 0.588855 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8854240775108337, 0.8005906939506531\n",
      "Step 5 | Training Loss: 0.545068 | Validation Accuracy: 0.911111\n",
      "Accuracy on Test data: 0.8841820359230042, 0.7913923859596252\n",
      "Step 6 | Training Loss: 0.559044 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.873846709728241, 0.7704641222953796\n",
      "Step 7 | Training Loss: 0.499997 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8649308085441589, 0.752826988697052\n",
      "Step 8 | Training Loss: 0.480775 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8573012948036194, 0.7371308207511902\n",
      "Step 9 | Training Loss: 0.520776 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8545954823493958, 0.7315611839294434\n",
      "Step 10 | Training Loss: 0.475770 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8511798977851868, 0.7244725823402405\n",
      "Step 1 | Training Loss: 0.468499 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8511798977851868, 0.7244725823402405\n",
      "Step 2 | Training Loss: 0.437161 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8504701852798462, 0.7231223583221436\n",
      "Step 3 | Training Loss: 0.464455 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.8503814935684204, 0.7228692173957825\n",
      "Step 4 | Training Loss: 0.463175 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8501597046852112, 0.7224472761154175\n",
      "Step 5 | Training Loss: 0.451576 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8498048186302185, 0.7217721343040466\n",
      "Step 6 | Training Loss: 0.468416 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8494943380355835, 0.7211814522743225\n",
      "Step 7 | Training Loss: 0.443862 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.849050760269165, 0.7203375697135925\n",
      "Step 8 | Training Loss: 0.507671 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8488733172416687, 0.7200000286102295\n",
      "Step 9 | Training Loss: 0.464983 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8485628366470337, 0.7193248867988586\n",
      "Step 10 | Training Loss: 0.437347 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8483410477638245, 0.7189029455184937\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.597483 | Validation Accuracy: 0.844444\n",
      "Accuracy on Test data: 0.8098828792572021, 0.6648945212364197\n",
      "Step 2 | Training Loss: 0.541182 | Validation Accuracy: 0.868651\n",
      "Accuracy on Test data: 0.8223474025726318, 0.6800844073295593\n",
      "Step 3 | Training Loss: 0.533863 | Validation Accuracy: 0.900000\n",
      "Accuracy on Test data: 0.8332594037055969, 0.6953586339950562\n",
      "Step 4 | Training Loss: 0.481789 | Validation Accuracy: 0.909921\n",
      "Accuracy on Test data: 0.83525550365448, 0.696793258190155\n",
      "Step 5 | Training Loss: 0.497574 | Validation Accuracy: 0.910714\n",
      "Accuracy on Test data: 0.8270049691200256, 0.6800844073295593\n",
      "Step 6 | Training Loss: 0.464508 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.822613537311554, 0.6715611815452576\n",
      "Step 7 | Training Loss: 0.434082 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.8201295137405396, 0.6656540036201477\n",
      "Step 8 | Training Loss: 0.453369 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.8171132206916809, 0.6597468256950378\n",
      "Step 9 | Training Loss: 0.434051 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.8142743110656738, 0.6543459892272949\n",
      "Step 10 | Training Loss: 0.415164 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.8111692667007446, 0.648185670375824\n",
      "Step 1 | Training Loss: 0.453860 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.8109918236732483, 0.6478481292724609\n",
      "Step 2 | Training Loss: 0.447004 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8107700347900391, 0.647426187992096\n",
      "Step 3 | Training Loss: 0.420165 | Validation Accuracy: 0.943651\n",
      "Accuracy on Test data: 0.810459554195404, 0.646835446357727\n",
      "Step 4 | Training Loss: 0.441780 | Validation Accuracy: 0.945635\n",
      "Accuracy on Test data: 0.8101046681404114, 0.646160364151001\n",
      "Step 5 | Training Loss: 0.414090 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.8097941875457764, 0.6454852223396301\n",
      "Step 6 | Training Loss: 0.429659 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.8096610903739929, 0.6451476812362671\n",
      "Step 7 | Training Loss: 0.435729 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.8089513778686523, 0.6437974572181702\n",
      "Step 8 | Training Loss: 0.405102 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.8087295889854431, 0.6432911157608032\n",
      "Step 9 | Training Loss: 0.416760 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.8086408972740173, 0.6429535746574402\n",
      "Step 10 | Training Loss: 0.414547 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.8087295889854431, 0.6428691744804382\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.617006 | Validation Accuracy: 0.774603\n",
      "Accuracy on Test data: 0.7382451891899109, 0.5636286735534668\n",
      "Step 2 | Training Loss: 0.607943 | Validation Accuracy: 0.855159\n",
      "Accuracy on Test data: 0.7734208703041077, 0.5864979028701782\n",
      "Step 3 | Training Loss: 0.552410 | Validation Accuracy: 0.890079\n",
      "Accuracy on Test data: 0.7877040505409241, 0.6090295314788818\n",
      "Step 4 | Training Loss: 0.504479 | Validation Accuracy: 0.916270\n",
      "Accuracy on Test data: 0.7737313508987427, 0.5799155831336975\n",
      "Step 5 | Training Loss: 0.519247 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.7559882998466492, 0.543375551700592\n",
      "Step 6 | Training Loss: 0.495744 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7578069567680359, 0.5452320575714111\n",
      "Step 7 | Training Loss: 0.437229 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.7610450387001038, 0.549957811832428\n",
      "Step 8 | Training Loss: 0.455807 | Validation Accuracy: 0.955952\n",
      "Accuracy on Test data: 0.7619765996932983, 0.551223635673523\n",
      "Step 9 | Training Loss: 0.417626 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7688076496124268, 0.5632067322731018\n",
      "Step 10 | Training Loss: 0.433366 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7717796564102173, 0.5685232281684875\n",
      "Step 1 | Training Loss: 0.446512 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7718683481216431, 0.5686919689178467\n",
      "Step 2 | Training Loss: 0.441959 | Validation Accuracy: 0.961508\n",
      "Accuracy on Test data: 0.7720457911491394, 0.5688607692718506\n",
      "Step 3 | Training Loss: 0.457564 | Validation Accuracy: 0.959921\n",
      "Accuracy on Test data: 0.7722675800323486, 0.5691983103752136\n",
      "Step 4 | Training Loss: 0.408241 | Validation Accuracy: 0.959127\n",
      "Accuracy on Test data: 0.7723562717437744, 0.5693671107292175\n",
      "Step 5 | Training Loss: 0.410523 | Validation Accuracy: 0.963095\n",
      "Accuracy on Test data: 0.7725337147712708, 0.5697046518325806\n",
      "Step 6 | Training Loss: 0.421309 | Validation Accuracy: 0.961111\n",
      "Accuracy on Test data: 0.7727998495101929, 0.5702109932899475\n",
      "Step 7 | Training Loss: 0.412625 | Validation Accuracy: 0.957937\n",
      "Accuracy on Test data: 0.7729772925376892, 0.5705485343933105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: 0.429061 | Validation Accuracy: 0.964286\n",
      "Accuracy on Test data: 0.7731990814208984, 0.5709704756736755\n",
      "Step 9 | Training Loss: 0.422732 | Validation Accuracy: 0.964286\n",
      "Accuracy on Test data: 0.7732434272766113, 0.5710548758506775\n",
      "Step 10 | Training Loss: 0.401546 | Validation Accuracy: 0.968254\n",
      "Accuracy on Test data: 0.7732877731323242, 0.5711392164230347\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.658309 | Validation Accuracy: 0.735714\n",
      "Accuracy on Test data: 0.5419623851776123, 0.3677637279033661\n",
      "Step 2 | Training Loss: 0.594321 | Validation Accuracy: 0.802778\n",
      "Accuracy on Test data: 0.5953690409660339, 0.4212658107280731\n",
      "Step 3 | Training Loss: 0.553387 | Validation Accuracy: 0.832540\n",
      "Accuracy on Test data: 0.6125354766845703, 0.4359493553638458\n",
      "Step 4 | Training Loss: 0.570066 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.7151792049407959, 0.4710548520088196\n",
      "Step 5 | Training Loss: 0.517392 | Validation Accuracy: 0.912302\n",
      "Accuracy on Test data: 0.7519960999488831, 0.5393248796463013\n",
      "Step 6 | Training Loss: 0.527377 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.7570972442626953, 0.5477637052536011\n",
      "Step 7 | Training Loss: 0.445396 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.7598917484283447, 0.552405059337616\n",
      "Step 8 | Training Loss: 0.474668 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.7604684233665466, 0.553080141544342\n",
      "Step 9 | Training Loss: 0.441159 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.7629081010818481, 0.5570464134216309\n",
      "Step 10 | Training Loss: 0.451910 | Validation Accuracy: 0.944048\n",
      "Accuracy on Test data: 0.7672551274299622, 0.5639662742614746\n",
      "Step 1 | Training Loss: 0.444735 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.7684528231620789, 0.5661603212356567\n",
      "Step 2 | Training Loss: 0.420754 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.769029438495636, 0.5672574043273926\n",
      "Step 3 | Training Loss: 0.460344 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7689407467842102, 0.5670886039733887\n",
      "Step 4 | Training Loss: 0.453697 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.768718957901001, 0.5666666626930237\n",
      "Step 5 | Training Loss: 0.434750 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.7685858607292175, 0.5663291215896606\n",
      "Step 6 | Training Loss: 0.411500 | Validation Accuracy: 0.949603\n",
      "Accuracy on Test data: 0.7685858607292175, 0.5663291215896606\n",
      "Step 7 | Training Loss: 0.391820 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.7686302065849304, 0.5664135217666626\n",
      "Step 8 | Training Loss: 0.440449 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7688076496124268, 0.5667510628700256\n",
      "Step 9 | Training Loss: 0.443733 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.769029438495636, 0.5671730041503906\n",
      "Step 10 | Training Loss: 0.427463 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.7689407467842102, 0.5670042037963867\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.690714 | Validation Accuracy: 0.714683\n",
      "Accuracy on Test data: 0.7558108568191528, 0.6773839592933655\n",
      "Step 2 | Training Loss: 0.603602 | Validation Accuracy: 0.815476\n",
      "Accuracy on Test data: 0.8101490139961243, 0.7131645679473877\n",
      "Step 3 | Training Loss: 0.573154 | Validation Accuracy: 0.863095\n",
      "Accuracy on Test data: 0.8353886008262634, 0.7227848172187805\n",
      "Step 4 | Training Loss: 0.585312 | Validation Accuracy: 0.875000\n",
      "Accuracy on Test data: 0.8494499921798706, 0.7340928316116333\n",
      "Step 5 | Training Loss: 0.476392 | Validation Accuracy: 0.889286\n",
      "Accuracy on Test data: 0.8524219393730164, 0.7327426075935364\n",
      "Step 6 | Training Loss: 0.501306 | Validation Accuracy: 0.905556\n",
      "Accuracy on Test data: 0.8573012948036194, 0.7372995615005493\n",
      "Step 7 | Training Loss: 0.532143 | Validation Accuracy: 0.914286\n",
      "Accuracy on Test data: 0.8704311847686768, 0.7606751322746277\n",
      "Step 8 | Training Loss: 0.439931 | Validation Accuracy: 0.912302\n",
      "Accuracy on Test data: 0.8732700347900391, 0.7645569443702698\n",
      "Step 9 | Training Loss: 0.459460 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.8602288961410522, 0.7386497855186462\n",
      "Step 10 | Training Loss: 0.441300 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.842485785484314, 0.7040506601333618\n",
      "Step 1 | Training Loss: 0.438660 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.841598629951477, 0.7023628950119019\n",
      "Step 2 | Training Loss: 0.441843 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8408445715904236, 0.700928270816803\n",
      "Step 3 | Training Loss: 0.454313 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.8401792049407959, 0.699578046798706\n",
      "Step 4 | Training Loss: 0.427321 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.838981568813324, 0.697299599647522\n",
      "Step 5 | Training Loss: 0.424375 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8384048938751221, 0.6962025165557861\n",
      "Step 6 | Training Loss: 0.463034 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8374290466308594, 0.694346010684967\n",
      "Step 7 | Training Loss: 0.434071 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8367636799812317, 0.6930801868438721\n",
      "Step 8 | Training Loss: 0.425761 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8364531397819519, 0.6924050450325012\n",
      "Step 9 | Training Loss: 0.410511 | Validation Accuracy: 0.942064\n",
      "Accuracy on Test data: 0.8359208703041077, 0.6913924217224121\n",
      "Step 10 | Training Loss: 0.392254 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8354329466819763, 0.6903797388076782\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.643037 | Validation Accuracy: 0.684127\n",
      "Accuracy on Test data: 0.5672019124031067, 0.37763711810112\n",
      "Step 2 | Training Loss: 0.670753 | Validation Accuracy: 0.738889\n",
      "Accuracy on Test data: 0.6243789792060852, 0.39822784066200256\n",
      "Step 3 | Training Loss: 0.586194 | Validation Accuracy: 0.781349\n",
      "Accuracy on Test data: 0.6931777596473694, 0.4313080310821533\n",
      "Step 4 | Training Loss: 0.577397 | Validation Accuracy: 0.811111\n",
      "Accuracy on Test data: 0.7111426591873169, 0.46430379152297974\n",
      "Step 5 | Training Loss: 0.543930 | Validation Accuracy: 0.835714\n",
      "Accuracy on Test data: 0.7453867793083191, 0.5283544063568115\n",
      "Step 6 | Training Loss: 0.478987 | Validation Accuracy: 0.869048\n",
      "Accuracy on Test data: 0.7558552026748657, 0.546751081943512\n",
      "Step 7 | Training Loss: 0.463547 | Validation Accuracy: 0.909127\n",
      "Accuracy on Test data: 0.7758605480194092, 0.5818565487861633\n",
      "Step 8 | Training Loss: 0.443105 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.7815826535224915, 0.5918143391609192\n",
      "Step 9 | Training Loss: 0.444435 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.789434015750885, 0.6065822839736938\n",
      "Step 10 | Training Loss: 0.439014 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8097941875457764, 0.6438818573951721\n",
      "Step 1 | Training Loss: 0.460896 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8101934194564819, 0.6446413397789001\n",
      "Step 2 | Training Loss: 0.420504 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8107700347900391, 0.6454852223396301\n",
      "Step 3 | Training Loss: 0.441159 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8114797472953796, 0.646497905254364\n",
      "Step 4 | Training Loss: 0.458317 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.8115684986114502, 0.646582305431366\n",
      "Step 5 | Training Loss: 0.455393 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8123669028282166, 0.6478481292724609\n",
      "Step 6 | Training Loss: 0.447992 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8130766749382019, 0.64886075258255\n",
      "Step 7 | Training Loss: 0.425109 | Validation Accuracy: 0.948016\n",
      "Accuracy on Test data: 0.8136089444160461, 0.64970463514328\n",
      "Step 8 | Training Loss: 0.436681 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.8140968680381775, 0.6504641175270081\n",
      "Step 9 | Training Loss: 0.457797 | Validation Accuracy: 0.945238\n",
      "Accuracy on Test data: 0.8142743110656738, 0.6507173180580139\n",
      "Step 10 | Training Loss: 0.457768 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.8146291971206665, 0.6512235999107361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693343 | Validation Accuracy: 0.599603\n",
      "Accuracy on Test data: 0.4595457911491394, 0.23383966088294983\n",
      "Step 2 | Training Loss: 0.690998 | Validation Accuracy: 0.657143\n",
      "Accuracy on Test data: 0.4950762987136841, 0.29924049973487854\n",
      "Step 3 | Training Loss: 0.685526 | Validation Accuracy: 0.755556\n",
      "Accuracy on Test data: 0.5516323447227478, 0.3321518898010254\n",
      "Step 4 | Training Loss: 0.692359 | Validation Accuracy: 0.765079\n",
      "Accuracy on Test data: 0.5843240022659302, 0.36303797364234924\n",
      "Step 5 | Training Loss: 0.690848 | Validation Accuracy: 0.808730\n",
      "Accuracy on Test data: 0.6439850926399231, 0.40042194724082947\n",
      "Step 6 | Training Loss: 0.679884 | Validation Accuracy: 0.835317\n",
      "Accuracy on Test data: 0.6851934194564819, 0.42219409346580505\n",
      "Step 7 | Training Loss: 0.680988 | Validation Accuracy: 0.851587\n",
      "Accuracy on Test data: 0.6922906041145325, 0.43426159024238586\n",
      "Step 8 | Training Loss: 0.677236 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.7076383829116821, 0.46270042657852173\n",
      "Step 9 | Training Loss: 0.678792 | Validation Accuracy: 0.877778\n",
      "Accuracy on Test data: 0.7193488478660583, 0.4840506315231323\n",
      "Step 10 | Training Loss: 0.672187 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7306156754493713, 0.5029535889625549\n",
      "Step 1 | Training Loss: 0.678767 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.7313697934150696, 0.5043882131576538\n",
      "Step 2 | Training Loss: 0.668381 | Validation Accuracy: 0.895238\n",
      "Accuracy on Test data: 0.7327004671096802, 0.5067510604858398\n",
      "Step 3 | Training Loss: 0.668265 | Validation Accuracy: 0.894841\n",
      "Accuracy on Test data: 0.7335432767868042, 0.508185625076294\n",
      "Step 4 | Training Loss: 0.669761 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7345191836357117, 0.5097046494483948\n",
      "Step 5 | Training Loss: 0.672139 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.7354063391685486, 0.5111392140388489\n",
      "Step 6 | Training Loss: 0.653941 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.737047553062439, 0.5140084624290466\n",
      "Step 7 | Training Loss: 0.666023 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.7382451891899109, 0.5161181688308716\n",
      "Step 8 | Training Loss: 0.675088 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.7395315766334534, 0.5183966159820557\n",
      "Step 9 | Training Loss: 0.673380 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.7405074238777161, 0.5202531814575195\n",
      "Step 10 | Training Loss: 0.683343 | Validation Accuracy: 0.902381\n",
      "Accuracy on Test data: 0.7415720224380493, 0.5221096873283386\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.709006 | Validation Accuracy: 0.734524\n",
      "Accuracy on Test data: 0.7374467849731445, 0.6279324889183044\n",
      "Step 2 | Training Loss: 0.660652 | Validation Accuracy: 0.794444\n",
      "Accuracy on Test data: 0.7717796564102173, 0.6394093036651611\n",
      "Step 3 | Training Loss: 0.659662 | Validation Accuracy: 0.809921\n",
      "Accuracy on Test data: 0.777102530002594, 0.6285232305526733\n",
      "Step 4 | Training Loss: 0.640532 | Validation Accuracy: 0.846429\n",
      "Accuracy on Test data: 0.785397469997406, 0.6286075711250305\n",
      "Step 5 | Training Loss: 0.632545 | Validation Accuracy: 0.851190\n",
      "Accuracy on Test data: 0.7946681976318359, 0.6407594680786133\n",
      "Step 6 | Training Loss: 0.655284 | Validation Accuracy: 0.872619\n",
      "Accuracy on Test data: 0.8010113835334778, 0.6500421762466431\n",
      "Step 7 | Training Loss: 0.627954 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.8076649904251099, 0.6575527191162109\n",
      "Step 8 | Training Loss: 0.605021 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.8152501583099365, 0.6638818383216858\n",
      "Step 9 | Training Loss: 0.607644 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.8200851678848267, 0.6686075925827026\n",
      "Step 10 | Training Loss: 0.602791 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8235006928443909, 0.6725738644599915\n",
      "Step 1 | Training Loss: 0.605303 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.8238555788993835, 0.6731645464897156\n",
      "Step 2 | Training Loss: 0.618814 | Validation Accuracy: 0.921032\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6740928292274475\n",
      "Step 3 | Training Loss: 0.587425 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.8245652914047241, 0.6744303703308105\n",
      "Step 4 | Training Loss: 0.578001 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8248314261436462, 0.6748523116111755\n",
      "Step 5 | Training Loss: 0.601311 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6753586530685425\n",
      "Step 6 | Training Loss: 0.600273 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6754430532455444\n",
      "Step 7 | Training Loss: 0.591688 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6754430532455444\n",
      "Step 8 | Training Loss: 0.611174 | Validation Accuracy: 0.919841\n",
      "Accuracy on Test data: 0.8253193497657776, 0.6757805943489075\n",
      "Step 9 | Training Loss: 0.580932 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.8254081010818481, 0.6759493947029114\n",
      "Step 10 | Training Loss: 0.622751 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8253637552261353, 0.6758649945259094\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.676507 | Validation Accuracy: 0.455159\n",
      "Accuracy on Test data: 0.5344659090042114, 0.7489451766014099\n",
      "Step 2 | Training Loss: 0.701306 | Validation Accuracy: 0.602778\n",
      "Accuracy on Test data: 0.6641234755516052, 0.7371308207511902\n",
      "Step 3 | Training Loss: 0.666532 | Validation Accuracy: 0.679762\n",
      "Accuracy on Test data: 0.7548350095748901, 0.7457383871078491\n",
      "Step 4 | Training Loss: 0.699594 | Validation Accuracy: 0.761508\n",
      "Accuracy on Test data: 0.8166696429252625, 0.7270886301994324\n",
      "Step 5 | Training Loss: 0.647242 | Validation Accuracy: 0.817064\n",
      "Accuracy on Test data: 0.840977668762207, 0.753248929977417\n",
      "Step 6 | Training Loss: 0.628010 | Validation Accuracy: 0.831349\n",
      "Accuracy on Test data: 0.8597409725189209, 0.7681012749671936\n",
      "Step 7 | Training Loss: 0.624328 | Validation Accuracy: 0.859127\n",
      "Accuracy on Test data: 0.8639993071556091, 0.7691982984542847\n",
      "Step 8 | Training Loss: 0.644475 | Validation Accuracy: 0.882540\n",
      "Accuracy on Test data: 0.8686568737030029, 0.7720674872398376\n",
      "Step 9 | Training Loss: 0.636686 | Validation Accuracy: 0.899603\n",
      "Accuracy on Test data: 0.8718949556350708, 0.7738396525382996\n",
      "Step 10 | Training Loss: 0.624266 | Validation Accuracy: 0.892063\n",
      "Accuracy on Test data: 0.8744233250617981, 0.7749367356300354\n",
      "Step 1 | Training Loss: 0.682210 | Validation Accuracy: 0.892460\n",
      "Accuracy on Test data: 0.8743789792060852, 0.7745147943496704\n",
      "Step 2 | Training Loss: 0.652531 | Validation Accuracy: 0.893651\n",
      "Accuracy on Test data: 0.8744677305221558, 0.7742615938186646\n",
      "Step 3 | Training Loss: 0.603160 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.8746007680892944, 0.7741771936416626\n",
      "Step 4 | Training Loss: 0.571847 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8747338652610779, 0.7740928530693054\n",
      "Step 5 | Training Loss: 0.564304 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8750443458557129, 0.7742615938186646\n",
      "Step 6 | Training Loss: 0.618003 | Validation Accuracy: 0.903571\n",
      "Accuracy on Test data: 0.8751774430274963, 0.7741771936416626\n",
      "Step 7 | Training Loss: 0.644191 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.8752661347389221, 0.7740928530693054\n",
      "Step 8 | Training Loss: 0.613471 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.8749556541442871, 0.7731645703315735\n",
      "Step 9 | Training Loss: 0.616389 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.873846709728241, 0.7708860635757446\n",
      "Step 10 | Training Loss: 0.633085 | Validation Accuracy: 0.898810\n",
      "Accuracy on Test data: 0.8732256889343262, 0.7696202397346497\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.672199 | Validation Accuracy: 0.458333\n",
      "Accuracy on Test data: 0.5706618428230286, 0.8198312520980835\n",
      "Step 2 | Training Loss: 0.620560 | Validation Accuracy: 0.689683\n",
      "Accuracy on Test data: 0.789434015750885, 0.8070886135101318\n",
      "Step 3 | Training Loss: 0.614150 | Validation Accuracy: 0.801984\n",
      "Accuracy on Test data: 0.8420422077178955, 0.7878481149673462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 0.678400 | Validation Accuracy: 0.862698\n",
      "Accuracy on Test data: 0.8813875317573547, 0.8142616152763367\n",
      "Step 5 | Training Loss: 0.628679 | Validation Accuracy: 0.867063\n",
      "Accuracy on Test data: 0.8727377653121948, 0.7875949144363403\n",
      "Step 6 | Training Loss: 0.607436 | Validation Accuracy: 0.904762\n",
      "Accuracy on Test data: 0.8797906041145325, 0.7826160192489624\n",
      "Step 7 | Training Loss: 0.576137 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.8793026804924011, 0.7783122658729553\n",
      "Step 8 | Training Loss: 0.548365 | Validation Accuracy: 0.921825\n",
      "Accuracy on Test data: 0.8712295889854431, 0.7618565559387207\n",
      "Step 9 | Training Loss: 0.572935 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8602288961410522, 0.7408438920974731\n",
      "Step 10 | Training Loss: 0.567939 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.8518452644348145, 0.7247257232666016\n",
      "Step 1 | Training Loss: 0.564364 | Validation Accuracy: 0.936508\n",
      "Accuracy on Test data: 0.8513573408126831, 0.7237130999565125\n",
      "Step 2 | Training Loss: 0.536607 | Validation Accuracy: 0.929365\n",
      "Accuracy on Test data: 0.8501153588294983, 0.7213501930236816\n",
      "Step 3 | Training Loss: 0.542414 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.8488733172416687, 0.7189873456954956\n",
      "Step 4 | Training Loss: 0.552728 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.8476756811141968, 0.7166244983673096\n",
      "Step 5 | Training Loss: 0.515897 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8461231589317322, 0.7136709094047546\n",
      "Step 6 | Training Loss: 0.515423 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8448811173439026, 0.7113924026489258\n",
      "Step 7 | Training Loss: 0.521187 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8439052700996399, 0.7093670964241028\n",
      "Step 8 | Training Loss: 0.515792 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8433729410171509, 0.7081856727600098\n",
      "Step 9 | Training Loss: 0.562222 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8420422077178955, 0.7055696249008179\n",
      "Step 10 | Training Loss: 0.524896 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.8409332633018494, 0.7034599184989929\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.667404 | Validation Accuracy: 0.814683\n",
      "Accuracy on Test data: 0.6625266075134277, 0.5385653972625732\n",
      "Step 2 | Training Loss: 0.658525 | Validation Accuracy: 0.883333\n",
      "Accuracy on Test data: 0.7271113991737366, 0.647257387638092\n",
      "Step 3 | Training Loss: 0.607844 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.8595635294914246, 0.7481856346130371\n",
      "Step 4 | Training Loss: 0.619875 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.8801011443138123, 0.7847257256507874\n",
      "Step 5 | Training Loss: 0.590402 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.8831618428230286, 0.7884387969970703\n",
      "Step 6 | Training Loss: 0.528730 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.8835166692733765, 0.7875105738639832\n",
      "Step 7 | Training Loss: 0.534611 | Validation Accuracy: 0.907143\n",
      "Accuracy on Test data: 0.8777501583099365, 0.7759493589401245\n",
      "Step 8 | Training Loss: 0.535255 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.8754435777664185, 0.7713080048561096\n",
      "Step 9 | Training Loss: 0.528475 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8704755306243896, 0.7616877555847168\n",
      "Step 10 | Training Loss: 0.485998 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8679027557373047, 0.7564557194709778\n",
      "Step 1 | Training Loss: 0.461329 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.867769718170166, 0.7562025189399719\n",
      "Step 2 | Training Loss: 0.488839 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8673704862594604, 0.7554430365562439\n",
      "Step 3 | Training Loss: 0.457580 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.866926908493042, 0.7545991539955139\n",
      "Step 4 | Training Loss: 0.477877 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8665720224380493, 0.7539240717887878\n",
      "Step 5 | Training Loss: 0.511020 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.866394579410553, 0.75358647108078\n",
      "Step 6 | Training Loss: 0.450740 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8659510016441345, 0.75274258852005\n",
      "Step 7 | Training Loss: 0.449850 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8657292127609253, 0.7523206472396851\n",
      "Step 8 | Training Loss: 0.503275 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.865196943283081, 0.751223623752594\n",
      "Step 9 | Training Loss: 0.498929 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8643985390663147, 0.7497046589851379\n",
      "Step 10 | Training Loss: 0.479838 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8633782863616943, 0.7477636933326721\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.678274 | Validation Accuracy: 0.673810\n",
      "Accuracy on Test data: 0.6263750791549683, 0.5431223511695862\n",
      "Step 2 | Training Loss: 0.631988 | Validation Accuracy: 0.725000\n",
      "Accuracy on Test data: 0.6564496159553528, 0.5744304060935974\n",
      "Step 3 | Training Loss: 0.617910 | Validation Accuracy: 0.787698\n",
      "Accuracy on Test data: 0.7182842493057251, 0.5593249201774597\n",
      "Step 4 | Training Loss: 0.560697 | Validation Accuracy: 0.856349\n",
      "Accuracy on Test data: 0.766678512096405, 0.5829535722732544\n",
      "Step 5 | Training Loss: 0.553952 | Validation Accuracy: 0.879365\n",
      "Accuracy on Test data: 0.7817600965499878, 0.601097047328949\n",
      "Step 6 | Training Loss: 0.453717 | Validation Accuracy: 0.899206\n",
      "Accuracy on Test data: 0.8055358529090881, 0.6410126686096191\n",
      "Step 7 | Training Loss: 0.468382 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6760337352752686\n",
      "Step 8 | Training Loss: 0.479441 | Validation Accuracy: 0.932936\n",
      "Accuracy on Test data: 0.8397356271743774, 0.702953577041626\n",
      "Step 9 | Training Loss: 0.490064 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.8454577922821045, 0.7133333086967468\n",
      "Step 10 | Training Loss: 0.464183 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.8481636047363281, 0.7181434631347656\n",
      "Step 1 | Training Loss: 0.468252 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.8484740853309631, 0.7185654044151306\n",
      "Step 2 | Training Loss: 0.445503 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.8484740853309631, 0.7185654044151306\n",
      "Step 3 | Training Loss: 0.436421 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.8486958742141724, 0.7189029455184937\n",
      "Step 4 | Training Loss: 0.429886 | Validation Accuracy: 0.959127\n",
      "Accuracy on Test data: 0.8486958742141724, 0.7188185453414917\n",
      "Step 5 | Training Loss: 0.442861 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.8488289713859558, 0.7190717458724976\n",
      "Step 6 | Training Loss: 0.466741 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.8488733172416687, 0.7191561460494995\n",
      "Step 7 | Training Loss: 0.436502 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.8487846255302429, 0.7189029455184937\n",
      "Step 8 | Training Loss: 0.443686 | Validation Accuracy: 0.952381\n",
      "Accuracy on Test data: 0.8488289713859558, 0.7189873456954956\n",
      "Step 9 | Training Loss: 0.419290 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.8489176630973816, 0.7190717458724976\n",
      "Step 10 | Training Loss: 0.427250 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.8489620089530945, 0.7190717458724976\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.716544 | Validation Accuracy: 0.633730\n",
      "Accuracy on Test data: 0.4879347085952759, 0.2672573924064636\n",
      "Step 2 | Training Loss: 0.647118 | Validation Accuracy: 0.721032\n",
      "Accuracy on Test data: 0.529675304889679, 0.28869199752807617\n",
      "Step 3 | Training Loss: 0.593776 | Validation Accuracy: 0.794841\n",
      "Accuracy on Test data: 0.6372870802879333, 0.33299577236175537\n",
      "Step 4 | Training Loss: 0.583078 | Validation Accuracy: 0.830159\n",
      "Accuracy on Test data: 0.6619943380355835, 0.3770464062690735\n",
      "Step 5 | Training Loss: 0.512966 | Validation Accuracy: 0.863889\n",
      "Accuracy on Test data: 0.6813342571258545, 0.40835443139076233\n",
      "Step 6 | Training Loss: 0.483888 | Validation Accuracy: 0.880159\n",
      "Accuracy on Test data: 0.6955287456512451, 0.4315611720085144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 | Training Loss: 0.507726 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.7117193341255188, 0.4591561257839203\n",
      "Step 8 | Training Loss: 0.465859 | Validation Accuracy: 0.906349\n",
      "Accuracy on Test data: 0.7245830297470093, 0.4819409251213074\n",
      "Step 9 | Training Loss: 0.467561 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.7351401448249817, 0.5017721652984619\n",
      "Step 10 | Training Loss: 0.445854 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.7431245446205139, 0.5157805681228638\n",
      "Step 1 | Training Loss: 0.456274 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.7434350848197937, 0.5162869095802307\n",
      "Step 2 | Training Loss: 0.426828 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7437899112701416, 0.5169620513916016\n",
      "Step 3 | Training Loss: 0.475308 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.743923008441925, 0.5173839926719666\n",
      "Step 4 | Training Loss: 0.491916 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.7443665862083435, 0.5179746747016907\n",
      "Step 5 | Training Loss: 0.476422 | Validation Accuracy: 0.934524\n",
      "Accuracy on Test data: 0.7448988556861877, 0.5188185572624207\n",
      "Step 6 | Training Loss: 0.442293 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.7456973195075989, 0.5203375816345215\n",
      "Step 7 | Training Loss: 0.448656 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.7466288208961487, 0.5220252871513367\n",
      "Step 8 | Training Loss: 0.466332 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.7475159764289856, 0.5235443115234375\n",
      "Step 9 | Training Loss: 0.430548 | Validation Accuracy: 0.940873\n",
      "Accuracy on Test data: 0.7490241527557373, 0.5263291001319885\n",
      "Step 10 | Training Loss: 0.449695 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.7500443458557129, 0.5281856656074524\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.644231 | Validation Accuracy: 0.695635\n",
      "Accuracy on Test data: 0.6577803492546082, 0.5669198036193848\n",
      "Step 2 | Training Loss: 0.634722 | Validation Accuracy: 0.747222\n",
      "Accuracy on Test data: 0.6568931937217712, 0.5594936609268188\n",
      "Step 3 | Training Loss: 0.580348 | Validation Accuracy: 0.843651\n",
      "Accuracy on Test data: 0.7696948051452637, 0.5952742695808411\n",
      "Step 4 | Training Loss: 0.514642 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.7873935699462891, 0.6156117916107178\n",
      "Step 5 | Training Loss: 0.487536 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.8009669780731201, 0.6378058791160583\n",
      "Step 6 | Training Loss: 0.456041 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.8125443458557129, 0.656708836555481\n",
      "Step 7 | Training Loss: 0.484349 | Validation Accuracy: 0.908333\n",
      "Accuracy on Test data: 0.818222165107727, 0.6665822863578796\n",
      "Step 8 | Training Loss: 0.443672 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6767088770866394\n",
      "Step 9 | Training Loss: 0.438353 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8290454149246216, 0.6843038201332092\n",
      "Step 10 | Training Loss: 0.408771 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8322835564613342, 0.6896202564239502\n",
      "Step 1 | Training Loss: 0.420233 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8324165940284729, 0.6897890567779541\n",
      "Step 2 | Training Loss: 0.428030 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.8327271342277527, 0.6900421977043152\n",
      "Step 3 | Training Loss: 0.448563 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8330819606781006, 0.6905485391616821\n",
      "Step 4 | Training Loss: 0.438855 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8334368467330933, 0.6910548806190491\n",
      "Step 5 | Training Loss: 0.447411 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8337029814720154, 0.6914768218994141\n",
      "Step 6 | Training Loss: 0.415915 | Validation Accuracy: 0.947619\n",
      "Accuracy on Test data: 0.8340134620666504, 0.6917299628257751\n",
      "Step 7 | Training Loss: 0.442359 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8345457911491394, 0.6926582455635071\n",
      "Step 8 | Training Loss: 0.444521 | Validation Accuracy: 0.941270\n",
      "Accuracy on Test data: 0.8347675800323486, 0.6929957866668701\n",
      "Step 9 | Training Loss: 0.441745 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.834945023059845, 0.6933333277702332\n",
      "Step 10 | Training Loss: 0.431333 | Validation Accuracy: 0.946032\n",
      "Accuracy on Test data: 0.8350337147712708, 0.6934177279472351\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.671292 | Validation Accuracy: 0.708333\n",
      "Accuracy on Test data: 0.7333658337593079, 0.6110548377037048\n",
      "Step 2 | Training Loss: 0.647159 | Validation Accuracy: 0.804762\n",
      "Accuracy on Test data: 0.7873935699462891, 0.6341772079467773\n",
      "Step 3 | Training Loss: 0.628559 | Validation Accuracy: 0.818254\n",
      "Accuracy on Test data: 0.7954222559928894, 0.6427004337310791\n",
      "Step 4 | Training Loss: 0.545910 | Validation Accuracy: 0.852381\n",
      "Accuracy on Test data: 0.8018985390663147, 0.6464135050773621\n",
      "Step 5 | Training Loss: 0.498666 | Validation Accuracy: 0.851190\n",
      "Accuracy on Test data: 0.7941359281539917, 0.6254852414131165\n",
      "Step 6 | Training Loss: 0.489462 | Validation Accuracy: 0.874206\n",
      "Accuracy on Test data: 0.7976845502853394, 0.6288607716560364\n",
      "Step 7 | Training Loss: 0.509629 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7986160516738892, 0.6285232305526733\n",
      "Step 8 | Training Loss: 0.511992 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.803273618221283, 0.6351054906845093\n",
      "Step 9 | Training Loss: 0.438199 | Validation Accuracy: 0.930556\n",
      "Accuracy on Test data: 0.8037615418434143, 0.6357805728912354\n",
      "Step 10 | Training Loss: 0.419296 | Validation Accuracy: 0.926587\n",
      "Accuracy on Test data: 0.8048261404037476, 0.6372151970863342\n",
      "Step 1 | Training Loss: 0.449268 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8053140640258789, 0.6376371383666992\n",
      "Step 2 | Training Loss: 0.426267 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8056245446205139, 0.6379746794700623\n",
      "Step 3 | Training Loss: 0.404282 | Validation Accuracy: 0.927381\n",
      "Accuracy on Test data: 0.8053584098815918, 0.6373839378356934\n",
      "Step 4 | Training Loss: 0.458081 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.805269718170166, 0.6372151970863342\n",
      "Step 5 | Training Loss: 0.458500 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8054471015930176, 0.6375527381896973\n",
      "Step 6 | Training Loss: 0.448714 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8054915070533752, 0.6375527381896973\n",
      "Step 7 | Training Loss: 0.423007 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8056688904762268, 0.6377215385437012\n",
      "Step 8 | Training Loss: 0.465170 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8056245446205139, 0.6376371383666992\n",
      "Step 9 | Training Loss: 0.447695 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.8056688904762268, 0.6377215385437012\n",
      "Step 10 | Training Loss: 0.471186 | Validation Accuracy: 0.936905\n",
      "Accuracy on Test data: 0.8056688904762268, 0.6377215385437012\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.617134 | Validation Accuracy: 0.689683\n",
      "Accuracy on Test data: 0.6719304323196411, 0.46303796768188477\n",
      "Step 2 | Training Loss: 0.616653 | Validation Accuracy: 0.748016\n",
      "Accuracy on Test data: 0.6898953318595886, 0.4719831347465515\n",
      "Step 3 | Training Loss: 0.588957 | Validation Accuracy: 0.831746\n",
      "Accuracy on Test data: 0.7325674295425415, 0.5259071588516235\n",
      "Step 4 | Training Loss: 0.557049 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.7482256889343262, 0.5424472689628601\n",
      "Step 5 | Training Loss: 0.563769 | Validation Accuracy: 0.884921\n",
      "Accuracy on Test data: 0.7586497664451599, 0.556877613067627\n",
      "Step 6 | Training Loss: 0.512108 | Validation Accuracy: 0.904365\n",
      "Accuracy on Test data: 0.7630411386489868, 0.5629535913467407\n",
      "Step 7 | Training Loss: 0.484166 | Validation Accuracy: 0.925794\n",
      "Accuracy on Test data: 0.7672995328903198, 0.5697046518325806\n",
      "Step 8 | Training Loss: 0.473977 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.770715057849884, 0.5747679471969604\n",
      "Step 9 | Training Loss: 0.473300 | Validation Accuracy: 0.934127\n",
      "Accuracy on Test data: 0.7733765244483948, 0.5794093012809753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Training Loss: 0.432038 | Validation Accuracy: 0.943254\n",
      "Accuracy on Test data: 0.7737313508987427, 0.5794936418533325\n",
      "Step 1 | Training Loss: 0.444602 | Validation Accuracy: 0.935714\n",
      "Accuracy on Test data: 0.7736426591873169, 0.5793249011039734\n",
      "Step 2 | Training Loss: 0.447341 | Validation Accuracy: 0.940476\n",
      "Accuracy on Test data: 0.7731547355651855, 0.5783966183662415\n",
      "Step 3 | Training Loss: 0.457475 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.7716465592384338, 0.5755274295806885\n",
      "Step 4 | Training Loss: 0.438023 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.7693399786949158, 0.5711392164230347\n",
      "Step 5 | Training Loss: 0.438565 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7680092453956604, 0.5685232281684875\n",
      "Step 6 | Training Loss: 0.464510 | Validation Accuracy: 0.951984\n",
      "Accuracy on Test data: 0.7679648399353027, 0.5684388279914856\n",
      "Step 7 | Training Loss: 0.420339 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7679648399353027, 0.5681856274604797\n",
      "Step 8 | Training Loss: 0.438993 | Validation Accuracy: 0.946825\n",
      "Accuracy on Test data: 0.7673882246017456, 0.5668354630470276\n",
      "Step 9 | Training Loss: 0.414694 | Validation Accuracy: 0.950000\n",
      "Accuracy on Test data: 0.7677430510520935, 0.5675105452537537\n",
      "Step 10 | Training Loss: 0.442244 | Validation Accuracy: 0.955952\n",
      "Accuracy on Test data: 0.7684528231620789, 0.5687763690948486\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693162 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693138 | Validation Accuracy: 0.522619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692981 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692888 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.693026 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.693063 | Validation Accuracy: 0.541270\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692911 | Validation Accuracy: 0.524206\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692318 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692220 | Validation Accuracy: 0.525397\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692915 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692813 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692810 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692095 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692804 | Validation Accuracy: 0.528175\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692698 | Validation Accuracy: 0.538492\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.693217 | Validation Accuracy: 0.544841\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692795 | Validation Accuracy: 0.529365\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693006 | Validation Accuracy: 0.539683\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692467 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692352 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.712486 | Validation Accuracy: 0.401587\n",
      "Accuracy on Test data: 0.4126596748828888, 0.4844725728034973\n",
      "Step 2 | Training Loss: 0.685245 | Validation Accuracy: 0.460714\n",
      "Accuracy on Test data: 0.4623403251171112, 0.48177215456962585\n",
      "Step 3 | Training Loss: 0.681166 | Validation Accuracy: 0.580159\n",
      "Accuracy on Test data: 0.5932842493057251, 0.4913924038410187\n",
      "Step 4 | Training Loss: 0.670817 | Validation Accuracy: 0.680952\n",
      "Accuracy on Test data: 0.6941536664962769, 0.5066666603088379\n",
      "Step 5 | Training Loss: 0.675091 | Validation Accuracy: 0.716667\n",
      "Accuracy on Test data: 0.7132717967033386, 0.5146835446357727\n",
      "Step 6 | Training Loss: 0.681704 | Validation Accuracy: 0.736508\n",
      "Accuracy on Test data: 0.7226312756538391, 0.5594092607498169\n",
      "Step 7 | Training Loss: 0.713746 | Validation Accuracy: 0.750397\n",
      "Accuracy on Test data: 0.7232966423034668, 0.5809282660484314\n",
      "Step 8 | Training Loss: 0.678665 | Validation Accuracy: 0.795635\n",
      "Accuracy on Test data: 0.7322125434875488, 0.5900421738624573\n",
      "Step 9 | Training Loss: 0.648992 | Validation Accuracy: 0.807936\n",
      "Accuracy on Test data: 0.7445883750915527, 0.6059072017669678\n",
      "Step 10 | Training Loss: 0.697915 | Validation Accuracy: 0.796429\n",
      "Accuracy on Test data: 0.7562987804412842, 0.6139240264892578\n",
      "Step 1 | Training Loss: 0.644115 | Validation Accuracy: 0.819444\n",
      "Accuracy on Test data: 0.7579843997955322, 0.6147679090499878\n",
      "Step 2 | Training Loss: 0.670104 | Validation Accuracy: 0.811111\n",
      "Accuracy on Test data: 0.7594038248062134, 0.6155274510383606\n",
      "Step 3 | Training Loss: 0.666771 | Validation Accuracy: 0.797222\n",
      "Accuracy on Test data: 0.7611337900161743, 0.6167932748794556\n",
      "Step 4 | Training Loss: 0.638149 | Validation Accuracy: 0.813492\n",
      "Accuracy on Test data: 0.7618878483772278, 0.6178058981895447\n",
      "Step 5 | Training Loss: 0.665071 | Validation Accuracy: 0.801984\n",
      "Accuracy on Test data: 0.7630411386489868, 0.6189029812812805\n",
      "Step 6 | Training Loss: 0.675673 | Validation Accuracy: 0.803571\n",
      "Accuracy on Test data: 0.7651703357696533, 0.6205063462257385\n",
      "Step 7 | Training Loss: 0.661568 | Validation Accuracy: 0.818254\n",
      "Accuracy on Test data: 0.7670777440071106, 0.6216877698898315\n",
      "Step 8 | Training Loss: 0.682282 | Validation Accuracy: 0.805952\n",
      "Accuracy on Test data: 0.7689850926399231, 0.6227847933769226\n",
      "Step 9 | Training Loss: 0.657124 | Validation Accuracy: 0.826190\n",
      "Accuracy on Test data: 0.7704932689666748, 0.6236286759376526\n",
      "Step 10 | Training Loss: 0.657591 | Validation Accuracy: 0.807540\n",
      "Accuracy on Test data: 0.7720014452934265, 0.6243038177490234\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.672524 | Validation Accuracy: 0.748810\n",
      "Accuracy on Test data: 0.7123402953147888, 0.4799156188964844\n",
      "Step 2 | Training Loss: 0.708504 | Validation Accuracy: 0.782540\n",
      "Accuracy on Test data: 0.7359386086463928, 0.5173839926719666\n",
      "Step 3 | Training Loss: 0.711852 | Validation Accuracy: 0.817064\n",
      "Accuracy on Test data: 0.7625532150268555, 0.5645569562911987\n",
      "Step 4 | Training Loss: 0.621271 | Validation Accuracy: 0.842063\n",
      "Accuracy on Test data: 0.7851756811141968, 0.6045569777488708\n",
      "Step 5 | Training Loss: 0.666263 | Validation Accuracy: 0.850000\n",
      "Accuracy on Test data: 0.7995032072067261, 0.6319831013679504\n",
      "Step 6 | Training Loss: 0.660104 | Validation Accuracy: 0.865476\n",
      "Accuracy on Test data: 0.8093949556350708, 0.649282693862915\n",
      "Step 7 | Training Loss: 0.644506 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.814185619354248, 0.6573839783668518\n",
      "Step 8 | Training Loss: 0.617934 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.8185326457023621, 0.6642194390296936\n",
      "Step 9 | Training Loss: 0.547193 | Validation Accuracy: 0.891667\n",
      "Accuracy on Test data: 0.8223917484283447, 0.6709704399108887\n",
      "Step 10 | Training Loss: 0.623826 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.8244321942329407, 0.6744303703308105\n",
      "Step 1 | Training Loss: 0.599884 | Validation Accuracy: 0.889286\n",
      "Accuracy on Test data: 0.8244321942329407, 0.6743459701538086\n",
      "Step 2 | Training Loss: 0.583727 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.8244765996932983, 0.6744303703308105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 0.628264 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.8242104053497314, 0.6736708879470825\n",
      "Step 4 | Training Loss: 0.653319 | Validation Accuracy: 0.886111\n",
      "Accuracy on Test data: 0.8229240775108337, 0.6712236404418945\n",
      "Step 5 | Training Loss: 0.601120 | Validation Accuracy: 0.894048\n",
      "Accuracy on Test data: 0.8219925761222839, 0.6694514751434326\n",
      "Step 6 | Training Loss: 0.616744 | Validation Accuracy: 0.879365\n",
      "Accuracy on Test data: 0.821415901184082, 0.6682700514793396\n",
      "Step 7 | Training Loss: 0.559821 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.8204843997955322, 0.6664134860038757\n",
      "Step 8 | Training Loss: 0.594883 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.8192867040634155, 0.6641350388526917\n",
      "Step 9 | Training Loss: 0.572105 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8172906041145325, 0.6603375673294067\n",
      "Step 10 | Training Loss: 0.579632 | Validation Accuracy: 0.883730\n",
      "Accuracy on Test data: 0.816847026348114, 0.6594936847686768\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.798465 | Validation Accuracy: 0.523413\n",
      "Accuracy on Test data: 0.44144782423973083, 0.1968776434659958\n",
      "Step 2 | Training Loss: 0.758372 | Validation Accuracy: 0.551984\n",
      "Accuracy on Test data: 0.4560858905315399, 0.2226160317659378\n",
      "Step 3 | Training Loss: 0.725772 | Validation Accuracy: 0.603175\n",
      "Accuracy on Test data: 0.5005322694778442, 0.3037974536418915\n",
      "Step 4 | Training Loss: 0.714224 | Validation Accuracy: 0.737302\n",
      "Accuracy on Test data: 0.6496628522872925, 0.3540928363800049\n",
      "Step 5 | Training Loss: 0.678372 | Validation Accuracy: 0.786508\n",
      "Accuracy on Test data: 0.6966376900672913, 0.43932488560676575\n",
      "Step 6 | Training Loss: 0.617848 | Validation Accuracy: 0.831746\n",
      "Accuracy on Test data: 0.7253371477127075, 0.48902952671051025\n",
      "Step 7 | Training Loss: 0.602573 | Validation Accuracy: 0.876190\n",
      "Accuracy on Test data: 0.7590045928955078, 0.551898717880249\n",
      "Step 8 | Training Loss: 0.624981 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.778477668762207, 0.5876793265342712\n",
      "Step 9 | Training Loss: 0.584152 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.7855748534202576, 0.6008439064025879\n",
      "Step 10 | Training Loss: 0.556384 | Validation Accuracy: 0.916667\n",
      "Accuracy on Test data: 0.7978619337081909, 0.6224472522735596\n",
      "Step 1 | Training Loss: 0.565137 | Validation Accuracy: 0.920635\n",
      "Accuracy on Test data: 0.8005678057670593, 0.6275105476379395\n",
      "Step 2 | Training Loss: 0.574848 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.8020759224891663, 0.6303797364234924\n",
      "Step 3 | Training Loss: 0.601320 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.8034954071044922, 0.6328269839286804\n",
      "Step 4 | Training Loss: 0.544954 | Validation Accuracy: 0.905952\n",
      "Accuracy on Test data: 0.804737389087677, 0.6346835494041443\n",
      "Step 5 | Training Loss: 0.576689 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8060681223869324, 0.6372151970863342\n",
      "Step 6 | Training Loss: 0.559750 | Validation Accuracy: 0.922619\n",
      "Accuracy on Test data: 0.8074876070022583, 0.6399155855178833\n",
      "Step 7 | Training Loss: 0.544933 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6431223750114441\n",
      "Step 8 | Training Loss: 0.585544 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.8103264570236206, 0.6454008221626282\n",
      "Step 9 | Training Loss: 0.541036 | Validation Accuracy: 0.920238\n",
      "Accuracy on Test data: 0.8111692667007446, 0.647004246711731\n",
      "Step 10 | Training Loss: 0.511779 | Validation Accuracy: 0.907540\n",
      "Accuracy on Test data: 0.8118346333503723, 0.6482700705528259\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.673574 | Validation Accuracy: 0.756746\n",
      "Accuracy on Test data: 0.7351845502853394, 0.5636286735534668\n",
      "Step 2 | Training Loss: 0.665653 | Validation Accuracy: 0.812302\n",
      "Accuracy on Test data: 0.7529276013374329, 0.5818565487861633\n",
      "Step 3 | Training Loss: 0.615730 | Validation Accuracy: 0.811508\n",
      "Accuracy on Test data: 0.8061568737030029, 0.6783122420310974\n",
      "Step 4 | Training Loss: 0.636863 | Validation Accuracy: 0.809524\n",
      "Accuracy on Test data: 0.8163591027259827, 0.6949366927146912\n",
      "Step 5 | Training Loss: 0.582661 | Validation Accuracy: 0.863889\n",
      "Accuracy on Test data: 0.8451472520828247, 0.7340084314346313\n",
      "Step 6 | Training Loss: 0.549062 | Validation Accuracy: 0.889286\n",
      "Accuracy on Test data: 0.8517565727233887, 0.7333333492279053\n",
      "Step 7 | Training Loss: 0.558681 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.8291341662406921, 0.6850633025169373\n",
      "Step 8 | Training Loss: 0.512716 | Validation Accuracy: 0.906349\n",
      "Accuracy on Test data: 0.8203956484794617, 0.6675949096679688\n",
      "Step 9 | Training Loss: 0.530932 | Validation Accuracy: 0.918254\n",
      "Accuracy on Test data: 0.8072214126586914, 0.6399155855178833\n",
      "Step 10 | Training Loss: 0.487926 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8000798225402832, 0.6251477003097534\n",
      "Step 1 | Training Loss: 0.498202 | Validation Accuracy: 0.908730\n",
      "Accuracy on Test data: 0.7996806502342224, 0.6242194175720215\n",
      "Step 2 | Training Loss: 0.485956 | Validation Accuracy: 0.921429\n",
      "Accuracy on Test data: 0.7991926670074463, 0.6230379939079285\n",
      "Step 3 | Training Loss: 0.503555 | Validation Accuracy: 0.921825\n",
      "Accuracy on Test data: 0.7986160516738892, 0.6218565106391907\n",
      "Step 4 | Training Loss: 0.465370 | Validation Accuracy: 0.922222\n",
      "Accuracy on Test data: 0.7980837225914001, 0.6208438873291016\n",
      "Step 5 | Training Loss: 0.468696 | Validation Accuracy: 0.928968\n",
      "Accuracy on Test data: 0.7976845502853394, 0.6200844049453735\n",
      "Step 6 | Training Loss: 0.496921 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.7971965670585632, 0.6191561222076416\n",
      "Step 7 | Training Loss: 0.444896 | Validation Accuracy: 0.918651\n",
      "Accuracy on Test data: 0.7967973947525024, 0.6182278394699097\n",
      "Step 8 | Training Loss: 0.471772 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.796353816986084, 0.6173839569091797\n",
      "Step 9 | Training Loss: 0.471214 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.7956884503364563, 0.6159493923187256\n",
      "Step 10 | Training Loss: 0.502042 | Validation Accuracy: 0.917857\n",
      "Accuracy on Test data: 0.7944464087486267, 0.6133333444595337\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.634457 | Validation Accuracy: 0.767857\n",
      "Accuracy on Test data: 0.7056422829627991, 0.4818565547466278\n",
      "Step 2 | Training Loss: 0.593831 | Validation Accuracy: 0.852778\n",
      "Accuracy on Test data: 0.7527058124542236, 0.544641375541687\n",
      "Step 3 | Training Loss: 0.573505 | Validation Accuracy: 0.890476\n",
      "Accuracy on Test data: 0.7696504592895508, 0.5727426409721375\n",
      "Step 4 | Training Loss: 0.538186 | Validation Accuracy: 0.898016\n",
      "Accuracy on Test data: 0.7769694924354553, 0.5861603617668152\n",
      "Step 5 | Training Loss: 0.507895 | Validation Accuracy: 0.903175\n",
      "Accuracy on Test data: 0.783135175704956, 0.5963712930679321\n",
      "Step 6 | Training Loss: 0.462028 | Validation Accuracy: 0.913889\n",
      "Accuracy on Test data: 0.7891234755516052, 0.6073417663574219\n",
      "Step 7 | Training Loss: 0.471538 | Validation Accuracy: 0.923016\n",
      "Accuracy on Test data: 0.7951117753982544, 0.6186497807502747\n",
      "Step 8 | Training Loss: 0.476728 | Validation Accuracy: 0.931349\n",
      "Accuracy on Test data: 0.7985273003578186, 0.6247257590293884\n",
      "Step 9 | Training Loss: 0.438594 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8013662099838257, 0.6300421953201294\n",
      "Step 10 | Training Loss: 0.451600 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8050479292869568, 0.6367088556289673\n",
      "Step 1 | Training Loss: 0.446525 | Validation Accuracy: 0.933333\n",
      "Accuracy on Test data: 0.8053140640258789, 0.6372151970863342\n",
      "Step 2 | Training Loss: 0.415783 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8055358529090881, 0.6376371383666992\n",
      "Step 3 | Training Loss: 0.400128 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.8060237765312195, 0.6384810209274292\n",
      "Step 4 | Training Loss: 0.434832 | Validation Accuracy: 0.944841\n",
      "Accuracy on Test data: 0.8063786625862122, 0.6391561031341553\n",
      "Step 5 | Training Loss: 0.429359 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.8069109320640564, 0.6401687860488892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | Training Loss: 0.425168 | Validation Accuracy: 0.946429\n",
      "Accuracy on Test data: 0.8072658181190491, 0.6408438682556152\n",
      "Step 7 | Training Loss: 0.401036 | Validation Accuracy: 0.938095\n",
      "Accuracy on Test data: 0.8075762987136841, 0.6413502097129822\n",
      "Step 8 | Training Loss: 0.419451 | Validation Accuracy: 0.942857\n",
      "Accuracy on Test data: 0.8079755306243896, 0.6421096920967102\n",
      "Step 9 | Training Loss: 0.400345 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.8080642223358154, 0.6422784924507141\n",
      "Step 10 | Training Loss: 0.440958 | Validation Accuracy: 0.944444\n",
      "Accuracy on Test data: 0.8082416653633118, 0.6426160335540771\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.717074 | Validation Accuracy: 0.629762\n",
      "Accuracy on Test data: 0.6613733172416687, 0.6172995567321777\n",
      "Step 2 | Training Loss: 0.625743 | Validation Accuracy: 0.785714\n",
      "Accuracy on Test data: 0.7724006175994873, 0.5869198441505432\n",
      "Step 3 | Training Loss: 0.604886 | Validation Accuracy: 0.838889\n",
      "Accuracy on Test data: 0.7919623851776123, 0.6196624636650085\n",
      "Step 4 | Training Loss: 0.556251 | Validation Accuracy: 0.862302\n",
      "Accuracy on Test data: 0.801233172416687, 0.6362025141716003\n",
      "Step 5 | Training Loss: 0.559502 | Validation Accuracy: 0.884524\n",
      "Accuracy on Test data: 0.8024751543998718, 0.6379746794700623\n",
      "Step 6 | Training Loss: 0.496559 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.8041163682937622, 0.6394936442375183\n",
      "Step 7 | Training Loss: 0.514024 | Validation Accuracy: 0.880159\n",
      "Accuracy on Test data: 0.8066447973251343, 0.6435443162918091\n",
      "Step 8 | Training Loss: 0.442718 | Validation Accuracy: 0.896032\n",
      "Accuracy on Test data: 0.8131210207939148, 0.6555274128913879\n",
      "Step 9 | Training Loss: 0.462284 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6641350388526917\n",
      "Step 10 | Training Loss: 0.446628 | Validation Accuracy: 0.906746\n",
      "Accuracy on Test data: 0.8195972442626953, 0.6668354272842407\n",
      "Step 1 | Training Loss: 0.437003 | Validation Accuracy: 0.910317\n",
      "Accuracy on Test data: 0.819730281829834, 0.6670042276382446\n",
      "Step 2 | Training Loss: 0.414189 | Validation Accuracy: 0.905556\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6668354272842407\n",
      "Step 3 | Training Loss: 0.443753 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.8194641470909119, 0.6664978861808777\n",
      "Step 4 | Training Loss: 0.445605 | Validation Accuracy: 0.911905\n",
      "Accuracy on Test data: 0.819730281829834, 0.6668354272842407\n",
      "Step 5 | Training Loss: 0.425793 | Validation Accuracy: 0.911508\n",
      "Accuracy on Test data: 0.8198190331459045, 0.6669198274612427\n",
      "Step 6 | Training Loss: 0.414333 | Validation Accuracy: 0.905952\n",
      "Accuracy on Test data: 0.8200408220291138, 0.6671729683876038\n",
      "Step 7 | Training Loss: 0.426744 | Validation Accuracy: 0.901190\n",
      "Accuracy on Test data: 0.8203513026237488, 0.6672573685646057\n",
      "Step 8 | Training Loss: 0.420870 | Validation Accuracy: 0.902778\n",
      "Accuracy on Test data: 0.8204843997955322, 0.6674261689186096\n",
      "Step 9 | Training Loss: 0.449902 | Validation Accuracy: 0.903968\n",
      "Accuracy on Test data: 0.8206618428230286, 0.6677637100219727\n",
      "Step 10 | Training Loss: 0.457960 | Validation Accuracy: 0.915873\n",
      "Accuracy on Test data: 0.8207505345344543, 0.6677637100219727\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.666880 | Validation Accuracy: 0.716667\n",
      "Accuracy on Test data: 0.6829755306243896, 0.4287763833999634\n",
      "Step 2 | Training Loss: 0.627725 | Validation Accuracy: 0.763492\n",
      "Accuracy on Test data: 0.6957505345344543, 0.4430379867553711\n",
      "Step 3 | Training Loss: 0.567120 | Validation Accuracy: 0.796429\n",
      "Accuracy on Test data: 0.7077714800834656, 0.4627847969532013\n",
      "Step 4 | Training Loss: 0.521963 | Validation Accuracy: 0.868651\n",
      "Accuracy on Test data: 0.7175301909446716, 0.477468341588974\n",
      "Step 5 | Training Loss: 0.519057 | Validation Accuracy: 0.902381\n",
      "Accuracy on Test data: 0.740773618221283, 0.5161181688308716\n",
      "Step 6 | Training Loss: 0.539169 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.7694730162620544, 0.5687763690948486\n",
      "Step 7 | Training Loss: 0.457191 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.7613999247550964, 0.552658200263977\n",
      "Step 8 | Training Loss: 0.454548 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.7629967927932739, 0.5551055073738098\n",
      "Step 9 | Training Loss: 0.425283 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.762109637260437, 0.553248941898346\n",
      "Step 10 | Training Loss: 0.424967 | Validation Accuracy: 0.955556\n",
      "Accuracy on Test data: 0.762109637260437, 0.552658200263977\n",
      "Step 1 | Training Loss: 0.439686 | Validation Accuracy: 0.952778\n",
      "Accuracy on Test data: 0.7619765996932983, 0.552405059337616\n",
      "Step 2 | Training Loss: 0.451642 | Validation Accuracy: 0.956746\n",
      "Accuracy on Test data: 0.7620652914047241, 0.5525738596916199\n",
      "Step 3 | Training Loss: 0.440847 | Validation Accuracy: 0.949206\n",
      "Accuracy on Test data: 0.7620209455490112, 0.552320659160614\n",
      "Step 4 | Training Loss: 0.450469 | Validation Accuracy: 0.954365\n",
      "Accuracy on Test data: 0.7625088691711426, 0.5529114007949829\n",
      "Step 5 | Training Loss: 0.445305 | Validation Accuracy: 0.953968\n",
      "Accuracy on Test data: 0.7629081010818481, 0.553248941898346\n",
      "Step 6 | Training Loss: 0.436587 | Validation Accuracy: 0.957143\n",
      "Accuracy on Test data: 0.7632185816764832, 0.553502082824707\n",
      "Step 7 | Training Loss: 0.411490 | Validation Accuracy: 0.957143\n",
      "Accuracy on Test data: 0.7632185816764832, 0.5534177422523499\n",
      "Step 8 | Training Loss: 0.400167 | Validation Accuracy: 0.953175\n",
      "Accuracy on Test data: 0.7634847164154053, 0.553924024105072\n",
      "Step 9 | Training Loss: 0.431218 | Validation Accuracy: 0.956349\n",
      "Accuracy on Test data: 0.7637065052986145, 0.5542616248130798\n",
      "Step 10 | Training Loss: 0.411811 | Validation Accuracy: 0.951587\n",
      "Accuracy on Test data: 0.7634403705596924, 0.5537552833557129\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.695716 | Validation Accuracy: 0.648810\n",
      "Accuracy on Test data: 0.49055179953575134, 0.2415189892053604\n",
      "Step 2 | Training Loss: 0.657457 | Validation Accuracy: 0.746032\n",
      "Accuracy on Test data: 0.5217352509498596, 0.26599156856536865\n",
      "Step 3 | Training Loss: 0.577753 | Validation Accuracy: 0.795238\n",
      "Accuracy on Test data: 0.6045954823493958, 0.30421939492225647\n",
      "Step 4 | Training Loss: 0.620505 | Validation Accuracy: 0.826587\n",
      "Accuracy on Test data: 0.6475780606269836, 0.34329113364219666\n",
      "Step 5 | Training Loss: 0.526357 | Validation Accuracy: 0.848413\n",
      "Accuracy on Test data: 0.6623935699462891, 0.36945146322250366\n",
      "Step 6 | Training Loss: 0.581242 | Validation Accuracy: 0.886508\n",
      "Accuracy on Test data: 0.7108765244483948, 0.46025317907333374\n",
      "Step 7 | Training Loss: 0.491185 | Validation Accuracy: 0.913492\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5204219222068787\n",
      "Step 8 | Training Loss: 0.459930 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.7594481706619263, 0.550717294216156\n",
      "Step 9 | Training Loss: 0.475067 | Validation Accuracy: 0.940079\n",
      "Accuracy on Test data: 0.7687633037567139, 0.5681012868881226\n",
      "Step 10 | Training Loss: 0.466838 | Validation Accuracy: 0.943651\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5837974548339844\n",
      "Step 1 | Training Loss: 0.423484 | Validation Accuracy: 0.948810\n",
      "Accuracy on Test data: 0.7779009938240051, 0.5848101377487183\n",
      "Step 2 | Training Loss: 0.449523 | Validation Accuracy: 0.951587\n",
      "Accuracy on Test data: 0.7783889174461365, 0.5854852199554443\n",
      "Step 3 | Training Loss: 0.437433 | Validation Accuracy: 0.950397\n",
      "Accuracy on Test data: 0.7794978618621826, 0.5874261856079102\n",
      "Step 4 | Training Loss: 0.463657 | Validation Accuracy: 0.950794\n",
      "Accuracy on Test data: 0.7800301909446716, 0.5881856679916382\n",
      "Step 5 | Training Loss: 0.455339 | Validation Accuracy: 0.947222\n",
      "Accuracy on Test data: 0.7808729410171509, 0.5897890329360962\n",
      "Step 6 | Training Loss: 0.478033 | Validation Accuracy: 0.953571\n",
      "Accuracy on Test data: 0.7819375395774841, 0.5916455984115601\n",
      "Step 7 | Training Loss: 0.430545 | Validation Accuracy: 0.942460\n",
      "Accuracy on Test data: 0.7824254631996155, 0.59248948097229\n",
      "Step 8 | Training Loss: 0.458292 | Validation Accuracy: 0.948413\n",
      "Accuracy on Test data: 0.7833569645881653, 0.594092845916748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.465228 | Validation Accuracy: 0.938889\n",
      "Accuracy on Test data: 0.7841997742652893, 0.5955274105072021\n",
      "Step 10 | Training Loss: 0.424569 | Validation Accuracy: 0.939286\n",
      "Accuracy on Test data: 0.7849094867706299, 0.5967932343482971\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.729039 | Validation Accuracy: 0.542460\n",
      "Accuracy on Test data: 0.610007107257843, 0.698818564414978\n",
      "Step 2 | Training Loss: 0.688058 | Validation Accuracy: 0.687698\n",
      "Accuracy on Test data: 0.7564762234687805, 0.7167088389396667\n",
      "Step 3 | Training Loss: 0.628953 | Validation Accuracy: 0.733333\n",
      "Accuracy on Test data: 0.8161373138427734, 0.752489447593689\n",
      "Step 4 | Training Loss: 0.606961 | Validation Accuracy: 0.789286\n",
      "Accuracy on Test data: 0.8443931937217712, 0.7801687717437744\n",
      "Step 5 | Training Loss: 0.559508 | Validation Accuracy: 0.871032\n",
      "Accuracy on Test data: 0.8792583346366882, 0.7829535603523254\n",
      "Step 6 | Training Loss: 0.548479 | Validation Accuracy: 0.915476\n",
      "Accuracy on Test data: 0.8854240775108337, 0.7903797626495361\n",
      "Step 7 | Training Loss: 0.514406 | Validation Accuracy: 0.923413\n",
      "Accuracy on Test data: 0.8904808163642883, 0.7994092702865601\n",
      "Step 8 | Training Loss: 0.489972 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8904364705085754, 0.7992405295372009\n",
      "Step 9 | Training Loss: 0.494152 | Validation Accuracy: 0.935317\n",
      "Accuracy on Test data: 0.8896380662918091, 0.796708881855011\n",
      "Step 10 | Training Loss: 0.453773 | Validation Accuracy: 0.939683\n",
      "Accuracy on Test data: 0.8880411386489868, 0.7934176921844482\n",
      "Step 1 | Training Loss: 0.454881 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8877750039100647, 0.7928270101547241\n",
      "Step 2 | Training Loss: 0.482963 | Validation Accuracy: 0.937698\n",
      "Accuracy on Test data: 0.8875088691711426, 0.7924050688743591\n",
      "Step 3 | Training Loss: 0.443929 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8874645233154297, 0.7922362685203552\n",
      "Step 4 | Training Loss: 0.446339 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8871539831161499, 0.7916455864906311\n",
      "Step 5 | Training Loss: 0.485699 | Validation Accuracy: 0.929762\n",
      "Accuracy on Test data: 0.8871983885765076, 0.7917299866676331\n",
      "Step 6 | Training Loss: 0.427288 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8867548108100891, 0.7908017039299011\n",
      "Step 7 | Training Loss: 0.451785 | Validation Accuracy: 0.940476\n",
      "Accuracy on Test data: 0.8866660594940186, 0.7907173037528992\n",
      "Step 8 | Training Loss: 0.433219 | Validation Accuracy: 0.941667\n",
      "Accuracy on Test data: 0.8864886164665222, 0.7902953624725342\n",
      "Step 9 | Training Loss: 0.426709 | Validation Accuracy: 0.932143\n",
      "Accuracy on Test data: 0.8861337900161743, 0.7894514799118042\n",
      "Step 10 | Training Loss: 0.421854 | Validation Accuracy: 0.938492\n",
      "Accuracy on Test data: 0.8858676552772522, 0.7889451384544373\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.693142 | Validation Accuracy: 0.519841\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693118 | Validation Accuracy: 0.513095\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693104 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692932 | Validation Accuracy: 0.520635\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692336 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692944 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692493 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.693355 | Validation Accuracy: 0.539683\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692225 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692323 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692814 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692811 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692502 | Validation Accuracy: 0.523016\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693627 | Validation Accuracy: 0.525000\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692594 | Validation Accuracy: 0.545238\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692589 | Validation Accuracy: 0.524206\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692584 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692152 | Validation Accuracy: 0.525000\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693542 | Validation Accuracy: 0.551984\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692786 | Validation Accuracy: 0.544841\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.727892 | Validation Accuracy: 0.693651\n",
      "Accuracy on Test data: 0.6739265322685242, 0.43628692626953125\n",
      "Step 2 | Training Loss: 0.651970 | Validation Accuracy: 0.727381\n",
      "Accuracy on Test data: 0.690693736076355, 0.450126588344574\n",
      "Step 3 | Training Loss: 0.677152 | Validation Accuracy: 0.759524\n",
      "Accuracy on Test data: 0.7102998495101929, 0.4703797399997711\n",
      "Step 4 | Training Loss: 0.630961 | Validation Accuracy: 0.795238\n",
      "Accuracy on Test data: 0.7173970937728882, 0.48118144273757935\n",
      "Step 5 | Training Loss: 0.691499 | Validation Accuracy: 0.808730\n",
      "Accuracy on Test data: 0.7257363200187683, 0.4947679340839386\n",
      "Step 6 | Training Loss: 0.645986 | Validation Accuracy: 0.823413\n",
      "Accuracy on Test data: 0.729284942150116, 0.499578058719635\n",
      "Step 7 | Training Loss: 0.628049 | Validation Accuracy: 0.834127\n",
      "Accuracy on Test data: 0.7307487726211548, 0.501603364944458\n",
      "Step 8 | Training Loss: 0.625487 | Validation Accuracy: 0.855952\n",
      "Accuracy on Test data: 0.7330553531646729, 0.504472553730011\n",
      "Step 9 | Training Loss: 0.589104 | Validation Accuracy: 0.861905\n",
      "Accuracy on Test data: 0.7397090196609497, 0.5164557099342346\n",
      "Step 10 | Training Loss: 0.570778 | Validation Accuracy: 0.867460\n",
      "Accuracy on Test data: 0.7505322694778442, 0.5358649492263794\n",
      "Step 1 | Training Loss: 0.550502 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.7510202527046204, 0.5365400910377502\n",
      "Step 2 | Training Loss: 0.588785 | Validation Accuracy: 0.877381\n",
      "Accuracy on Test data: 0.7518630027770996, 0.5381434559822083\n",
      "Step 3 | Training Loss: 0.573079 | Validation Accuracy: 0.885317\n",
      "Accuracy on Test data: 0.7523953318595886, 0.5391561388969421\n",
      "Step 4 | Training Loss: 0.574406 | Validation Accuracy: 0.863095\n",
      "Accuracy on Test data: 0.7530163526535034, 0.5403375625610352\n",
      "Step 5 | Training Loss: 0.580861 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.7536373138427734, 0.5415189862251282\n",
      "Step 6 | Training Loss: 0.615947 | Validation Accuracy: 0.887698\n",
      "Accuracy on Test data: 0.754036545753479, 0.5421940684318542\n",
      "Step 7 | Training Loss: 0.605499 | Validation Accuracy: 0.886905\n",
      "Accuracy on Test data: 0.754347026348114, 0.5427848100662231\n",
      "Step 8 | Training Loss: 0.628538 | Validation Accuracy: 0.886905\n",
      "Accuracy on Test data: 0.754879355430603, 0.543797492980957\n",
      "Step 9 | Training Loss: 0.570749 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.7554559707641602, 0.5448945164680481\n",
      "Step 10 | Training Loss: 0.599167 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.7560326457023621, 0.545907199382782\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.719382 | Validation Accuracy: 0.650794\n",
      "Accuracy on Test data: 0.5951029062271118, 0.3421940803527832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.713435 | Validation Accuracy: 0.766270\n",
      "Accuracy on Test data: 0.6786727905273438, 0.40911391377449036\n",
      "Step 3 | Training Loss: 0.682904 | Validation Accuracy: 0.778175\n",
      "Accuracy on Test data: 0.6959723234176636, 0.44033753871917725\n",
      "Step 4 | Training Loss: 0.731447 | Validation Accuracy: 0.826587\n",
      "Accuracy on Test data: 0.7077714800834656, 0.46084389090538025\n",
      "Step 5 | Training Loss: 0.642505 | Validation Accuracy: 0.840079\n",
      "Accuracy on Test data: 0.7198367714881897, 0.48219409584999084\n",
      "Step 6 | Training Loss: 0.629227 | Validation Accuracy: 0.850794\n",
      "Accuracy on Test data: 0.7305269837379456, 0.50168776512146\n",
      "Step 7 | Training Loss: 0.641998 | Validation Accuracy: 0.848810\n",
      "Accuracy on Test data: 0.7376242280006409, 0.5149366855621338\n",
      "Step 8 | Training Loss: 0.629238 | Validation Accuracy: 0.850794\n",
      "Accuracy on Test data: 0.7460521459579468, 0.5300421714782715\n",
      "Step 9 | Training Loss: 0.630353 | Validation Accuracy: 0.871825\n",
      "Accuracy on Test data: 0.7575408220291138, 0.551476776599884\n",
      "Step 10 | Training Loss: 0.582988 | Validation Accuracy: 0.871825\n",
      "Accuracy on Test data: 0.7660131454467773, 0.5661603212356567\n",
      "Step 1 | Training Loss: 0.641284 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.7672995328903198, 0.5684388279914856\n",
      "Step 2 | Training Loss: 0.571080 | Validation Accuracy: 0.875397\n",
      "Accuracy on Test data: 0.7684971690177917, 0.5704641342163086\n",
      "Step 3 | Training Loss: 0.563159 | Validation Accuracy: 0.883333\n",
      "Accuracy on Test data: 0.7696061134338379, 0.5724050402641296\n",
      "Step 4 | Training Loss: 0.585291 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.7702714800834656, 0.5735865235328674\n",
      "Step 5 | Training Loss: 0.607150 | Validation Accuracy: 0.865079\n",
      "Accuracy on Test data: 0.7705819606781006, 0.5741772055625916\n",
      "Step 6 | Training Loss: 0.632317 | Validation Accuracy: 0.880952\n",
      "Accuracy on Test data: 0.771025538444519, 0.5750210881233215\n",
      "Step 7 | Training Loss: 0.567099 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.7712916731834412, 0.5755274295806885\n",
      "Step 8 | Training Loss: 0.571259 | Validation Accuracy: 0.880159\n",
      "Accuracy on Test data: 0.7717352509498596, 0.5763713121414185\n",
      "Step 9 | Training Loss: 0.620894 | Validation Accuracy: 0.865476\n",
      "Accuracy on Test data: 0.7723562717437744, 0.5775527358055115\n",
      "Step 10 | Training Loss: 0.559228 | Validation Accuracy: 0.879365\n",
      "Accuracy on Test data: 0.7731547355651855, 0.5789873600006104\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.706394 | Validation Accuracy: 0.559524\n",
      "Accuracy on Test data: 0.6743701100349426, 0.8024472594261169\n",
      "Step 2 | Training Loss: 0.647156 | Validation Accuracy: 0.704365\n",
      "Accuracy on Test data: 0.8287792801856995, 0.8059071898460388\n",
      "Step 3 | Training Loss: 0.656966 | Validation Accuracy: 0.749206\n",
      "Accuracy on Test data: 0.8483853936195374, 0.8061603307723999\n",
      "Step 4 | Training Loss: 0.634181 | Validation Accuracy: 0.828175\n",
      "Accuracy on Test data: 0.8822746872901917, 0.8443037867546082\n",
      "Step 5 | Training Loss: 0.647169 | Validation Accuracy: 0.847222\n",
      "Accuracy on Test data: 0.9095103144645691, 0.8741772174835205\n",
      "Step 6 | Training Loss: 0.591617 | Validation Accuracy: 0.869048\n",
      "Accuracy on Test data: 0.9012597799301147, 0.8414345979690552\n",
      "Step 7 | Training Loss: 0.616981 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.8947391510009766, 0.8223628401756287\n",
      "Step 8 | Training Loss: 0.588465 | Validation Accuracy: 0.913095\n",
      "Accuracy on Test data: 0.891146183013916, 0.8046413660049438\n",
      "Step 9 | Training Loss: 0.609957 | Validation Accuracy: 0.924603\n",
      "Accuracy on Test data: 0.8840933442115784, 0.7864134907722473\n",
      "Step 10 | Training Loss: 0.567523 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.869277834892273, 0.7562869191169739\n",
      "Step 1 | Training Loss: 0.587105 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8688786625862122, 0.7554430365562439\n",
      "Step 2 | Training Loss: 0.583821 | Validation Accuracy: 0.917063\n",
      "Accuracy on Test data: 0.8689673542976379, 0.7554430365562439\n",
      "Step 3 | Training Loss: 0.580775 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8684794306755066, 0.75443035364151\n",
      "Step 4 | Training Loss: 0.570522 | Validation Accuracy: 0.928175\n",
      "Accuracy on Test data: 0.8679471015930176, 0.753248929977417\n",
      "Step 5 | Training Loss: 0.528829 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8668825626373291, 0.751223623752594\n",
      "Step 6 | Training Loss: 0.580121 | Validation Accuracy: 0.925397\n",
      "Accuracy on Test data: 0.8650195002555847, 0.7476793527603149\n",
      "Step 7 | Training Loss: 0.557064 | Validation Accuracy: 0.930159\n",
      "Accuracy on Test data: 0.8644428849220276, 0.7465822696685791\n",
      "Step 8 | Training Loss: 0.564991 | Validation Accuracy: 0.931746\n",
      "Accuracy on Test data: 0.8641323447227478, 0.745907187461853\n",
      "Step 9 | Training Loss: 0.583907 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8636887669563293, 0.745063304901123\n",
      "Step 10 | Training Loss: 0.538996 | Validation Accuracy: 0.937302\n",
      "Accuracy on Test data: 0.8636000752449036, 0.7448945045471191\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.722231 | Validation Accuracy: 0.680159\n",
      "Accuracy on Test data: 0.617237389087677, 0.44995781779289246\n",
      "Step 2 | Training Loss: 0.662457 | Validation Accuracy: 0.818254\n",
      "Accuracy on Test data: 0.758915901184082, 0.5577214956283569\n",
      "Step 3 | Training Loss: 0.641045 | Validation Accuracy: 0.871429\n",
      "Accuracy on Test data: 0.8162704110145569, 0.6623628735542297\n",
      "Step 4 | Training Loss: 0.624617 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.8206174373626709, 0.6696202754974365\n",
      "Step 5 | Training Loss: 0.562983 | Validation Accuracy: 0.901984\n",
      "Accuracy on Test data: 0.7996806502342224, 0.6282700300216675\n",
      "Step 6 | Training Loss: 0.583474 | Validation Accuracy: 0.907540\n",
      "Accuracy on Test data: 0.8024308085441589, 0.6324050426483154\n",
      "Step 7 | Training Loss: 0.557322 | Validation Accuracy: 0.909524\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6267510652542114\n",
      "Step 8 | Training Loss: 0.501231 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.7991483211517334, 0.6245569586753845\n",
      "Step 9 | Training Loss: 0.492655 | Validation Accuracy: 0.923810\n",
      "Accuracy on Test data: 0.8006564974784851, 0.6273417472839355\n",
      "Step 10 | Training Loss: 0.480259 | Validation Accuracy: 0.934921\n",
      "Accuracy on Test data: 0.8014993071556091, 0.6288607716560364\n",
      "Step 1 | Training Loss: 0.449233 | Validation Accuracy: 0.926984\n",
      "Accuracy on Test data: 0.8017210960388184, 0.6292827129364014\n",
      "Step 2 | Training Loss: 0.451087 | Validation Accuracy: 0.932540\n",
      "Accuracy on Test data: 0.8017654418945312, 0.6293671131134033\n",
      "Step 3 | Training Loss: 0.459189 | Validation Accuracy: 0.928571\n",
      "Accuracy on Test data: 0.8018097877502441, 0.6294514536857605\n",
      "Step 4 | Training Loss: 0.487975 | Validation Accuracy: 0.930952\n",
      "Accuracy on Test data: 0.8018097877502441, 0.6294514536857605\n",
      "Step 5 | Training Loss: 0.474214 | Validation Accuracy: 0.933730\n",
      "Accuracy on Test data: 0.8018097877502441, 0.6294514536857605\n",
      "Step 6 | Training Loss: 0.425902 | Validation Accuracy: 0.925000\n",
      "Accuracy on Test data: 0.8018985390663147, 0.6296202540397644\n",
      "Step 7 | Training Loss: 0.486625 | Validation Accuracy: 0.927778\n",
      "Accuracy on Test data: 0.8019428849220276, 0.6296202540397644\n",
      "Step 8 | Training Loss: 0.484455 | Validation Accuracy: 0.936111\n",
      "Accuracy on Test data: 0.8021646738052368, 0.6300421953201294\n",
      "Step 9 | Training Loss: 0.465279 | Validation Accuracy: 0.926190\n",
      "Accuracy on Test data: 0.8022090196609497, 0.6301265954971313\n",
      "Step 10 | Training Loss: 0.417278 | Validation Accuracy: 0.934127\n",
      "Accuracy on Test data: 0.8022533655166626, 0.6302109956741333\n",
      "2min 50s ± 9.38 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-17T20:17:51.453918Z",
     "start_time": "2017-06-17T20:17:51.447676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-17T20:17:51.460413Z",
     "start_time": "2017-06-17T20:17:51.455436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:24.803532Z",
     "start_time": "2017-07-17T00:29:24.417366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.909510</td>\n",
       "      <td>0.874177</td>\n",
       "      <td>5.548434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.799409</td>\n",
       "      <td>6.170311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>0.820751</td>\n",
       "      <td>0.667764</td>\n",
       "      <td>15.698435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.808242</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>16.788981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.773155</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>21.901474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "98      6              48              3     0.847222    0.909510   \n",
       "60      8             122              1     0.923413    0.890481   \n",
       "31     22              12              1     0.915873    0.820751   \n",
       "15     22               1              1     0.944444    0.808242   \n",
       "93     22              24              3     0.879365    0.773155   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "98       0.874177    5.548434  \n",
       "60       0.799409    6.170311  \n",
       "31       0.667764   15.698435  \n",
       "15       0.642616   16.788981  \n",
       "93       0.578987   21.901474  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:24.831699Z",
     "start_time": "2017-07-17T00:29:24.806164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.909510</td>\n",
       "      <td>0.874177</td>\n",
       "      <td>5.548434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.799409</td>\n",
       "      <td>6.170311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903968</td>\n",
       "      <td>0.820662</td>\n",
       "      <td>0.667764</td>\n",
       "      <td>14.949828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>0.820751</td>\n",
       "      <td>0.667764</td>\n",
       "      <td>15.698435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.808242</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>16.788981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.773155</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>21.901474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "98      6              48              3     0.847222    0.909510   \n",
       "60      8             122              1     0.923413    0.890481   \n",
       "30     20              12              1     0.903968    0.820662   \n",
       "31     22              12              1     0.915873    0.820751   \n",
       "15     22               1              1     0.944444    0.808242   \n",
       "93     22              24              3     0.879365    0.773155   \n",
       "\n",
       "    test_score_20  time_taken  \n",
       "98       0.874177    5.548434  \n",
       "60       0.799409    6.170311  \n",
       "30       0.667764   14.949828  \n",
       "31       0.667764   15.698435  \n",
       "15       0.642616   16.788981  \n",
       "93       0.578987   21.901474  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:24.895552Z",
     "start_time": "2017-07-17T00:29:24.834088Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.909510</td>\n",
       "      <td>0.874177</td>\n",
       "      <td>5.548434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.799409</td>\n",
       "      <td>6.170311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915476</td>\n",
       "      <td>0.885424</td>\n",
       "      <td>0.790380</td>\n",
       "      <td>5.214121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.828175</td>\n",
       "      <td>0.882275</td>\n",
       "      <td>0.844304</td>\n",
       "      <td>4.351358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871032</td>\n",
       "      <td>0.879258</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>4.389015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.749206</td>\n",
       "      <td>0.848385</td>\n",
       "      <td>0.806160</td>\n",
       "      <td>3.141691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>0.844393</td>\n",
       "      <td>0.780169</td>\n",
       "      <td>3.541118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704365</td>\n",
       "      <td>0.828779</td>\n",
       "      <td>0.805907</td>\n",
       "      <td>2.006240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>0.820751</td>\n",
       "      <td>0.667764</td>\n",
       "      <td>15.698435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903968</td>\n",
       "      <td>0.820662</td>\n",
       "      <td>0.667764</td>\n",
       "      <td>14.949828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.669620</td>\n",
       "      <td>5.514633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.820484</td>\n",
       "      <td>0.667426</td>\n",
       "      <td>14.090263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.820351</td>\n",
       "      <td>0.667257</td>\n",
       "      <td>13.258659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.820041</td>\n",
       "      <td>0.667173</td>\n",
       "      <td>12.505358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>0.819819</td>\n",
       "      <td>0.666920</td>\n",
       "      <td>11.752340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910317</td>\n",
       "      <td>0.819730</td>\n",
       "      <td>0.667004</td>\n",
       "      <td>9.238146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906746</td>\n",
       "      <td>0.819597</td>\n",
       "      <td>0.666835</td>\n",
       "      <td>8.278834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893254</td>\n",
       "      <td>0.817956</td>\n",
       "      <td>0.664135</td>\n",
       "      <td>7.436796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.816270</td>\n",
       "      <td>0.662363</td>\n",
       "      <td>4.178486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.816137</td>\n",
       "      <td>0.752489</td>\n",
       "      <td>2.637325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.813121</td>\n",
       "      <td>0.655527</td>\n",
       "      <td>6.647932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.808242</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>16.788981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.808064</td>\n",
       "      <td>0.642278</td>\n",
       "      <td>15.916996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.807976</td>\n",
       "      <td>0.642110</td>\n",
       "      <td>15.144928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.807576</td>\n",
       "      <td>0.641350</td>\n",
       "      <td>14.309601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.807266</td>\n",
       "      <td>0.640844</td>\n",
       "      <td>13.193370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953175</td>\n",
       "      <td>0.806911</td>\n",
       "      <td>0.640169</td>\n",
       "      <td>12.379952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.806645</td>\n",
       "      <td>0.643544</td>\n",
       "      <td>5.819725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944841</td>\n",
       "      <td>0.806379</td>\n",
       "      <td>0.639156</td>\n",
       "      <td>11.550372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940079</td>\n",
       "      <td>0.806024</td>\n",
       "      <td>0.638481</td>\n",
       "      <td>10.743485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.805536</td>\n",
       "      <td>0.637637</td>\n",
       "      <td>9.945443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.805314</td>\n",
       "      <td>0.637215</td>\n",
       "      <td>9.115473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.805048</td>\n",
       "      <td>0.636709</td>\n",
       "      <td>8.355908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.804116</td>\n",
       "      <td>0.639494</td>\n",
       "      <td>5.067740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.802475</td>\n",
       "      <td>0.637975</td>\n",
       "      <td>4.254191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>0.801233</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>3.466942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931349</td>\n",
       "      <td>0.798527</td>\n",
       "      <td>0.624726</td>\n",
       "      <td>6.704805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.607342</td>\n",
       "      <td>4.863771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.784909</td>\n",
       "      <td>0.596793</td>\n",
       "      <td>16.966625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.784200</td>\n",
       "      <td>0.595527</td>\n",
       "      <td>16.152657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.783357</td>\n",
       "      <td>0.594093</td>\n",
       "      <td>15.321514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>0.782425</td>\n",
       "      <td>0.592489</td>\n",
       "      <td>14.318795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953571</td>\n",
       "      <td>0.781938</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>13.542171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.780873</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>12.755471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.780030</td>\n",
       "      <td>0.588186</td>\n",
       "      <td>11.939057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950397</td>\n",
       "      <td>0.779498</td>\n",
       "      <td>0.587426</td>\n",
       "      <td>11.202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951587</td>\n",
       "      <td>0.778389</td>\n",
       "      <td>0.585485</td>\n",
       "      <td>10.355245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948810</td>\n",
       "      <td>0.777901</td>\n",
       "      <td>0.584810</td>\n",
       "      <td>9.195992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.777324</td>\n",
       "      <td>0.583797</td>\n",
       "      <td>8.347562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898016</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>3.240004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.773155</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>21.901474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.772401</td>\n",
       "      <td>0.586920</td>\n",
       "      <td>1.763685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.865476</td>\n",
       "      <td>0.772356</td>\n",
       "      <td>0.577553</td>\n",
       "      <td>20.821092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.771735</td>\n",
       "      <td>0.576371</td>\n",
       "      <td>19.904007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.771292</td>\n",
       "      <td>0.575527</td>\n",
       "      <td>18.358187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.771026</td>\n",
       "      <td>0.575021</td>\n",
       "      <td>17.319017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.865079</td>\n",
       "      <td>0.770582</td>\n",
       "      <td>0.574177</td>\n",
       "      <td>16.332910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.770271</td>\n",
       "      <td>0.573587</td>\n",
       "      <td>15.240586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.769606</td>\n",
       "      <td>0.572405</td>\n",
       "      <td>14.196526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>0.769473</td>\n",
       "      <td>0.568776</td>\n",
       "      <td>5.267488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.875397</td>\n",
       "      <td>0.768497</td>\n",
       "      <td>0.570464</td>\n",
       "      <td>13.085671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.568439</td>\n",
       "      <td>12.012670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.871825</td>\n",
       "      <td>0.766013</td>\n",
       "      <td>0.566160</td>\n",
       "      <td>11.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925397</td>\n",
       "      <td>0.759448</td>\n",
       "      <td>0.550717</td>\n",
       "      <td>6.679805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.818254</td>\n",
       "      <td>0.758916</td>\n",
       "      <td>0.557721</td>\n",
       "      <td>2.804342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687698</td>\n",
       "      <td>0.756476</td>\n",
       "      <td>0.716709</td>\n",
       "      <td>1.618722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.895635</td>\n",
       "      <td>0.756033</td>\n",
       "      <td>0.545907</td>\n",
       "      <td>20.815290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.755456</td>\n",
       "      <td>0.544895</td>\n",
       "      <td>19.615030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.754879</td>\n",
       "      <td>0.543797</td>\n",
       "      <td>18.620883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.754347</td>\n",
       "      <td>0.542785</td>\n",
       "      <td>17.690050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.754037</td>\n",
       "      <td>0.542194</td>\n",
       "      <td>16.709647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.753637</td>\n",
       "      <td>0.541519</td>\n",
       "      <td>15.785249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863095</td>\n",
       "      <td>0.753016</td>\n",
       "      <td>0.540338</td>\n",
       "      <td>14.873307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.852778</td>\n",
       "      <td>0.752706</td>\n",
       "      <td>0.544641</td>\n",
       "      <td>1.611163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.885317</td>\n",
       "      <td>0.752395</td>\n",
       "      <td>0.539156</td>\n",
       "      <td>13.678719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.751863</td>\n",
       "      <td>0.538143</td>\n",
       "      <td>12.616991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.751020</td>\n",
       "      <td>0.536540</td>\n",
       "      <td>11.616187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.750532</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>10.612506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850794</td>\n",
       "      <td>0.746052</td>\n",
       "      <td>0.530042</td>\n",
       "      <td>8.713374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.902381</td>\n",
       "      <td>0.740774</td>\n",
       "      <td>0.516118</td>\n",
       "      <td>4.414896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.733055</td>\n",
       "      <td>0.504473</td>\n",
       "      <td>8.419276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850794</td>\n",
       "      <td>0.730527</td>\n",
       "      <td>0.501688</td>\n",
       "      <td>6.662893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.729285</td>\n",
       "      <td>0.499578</td>\n",
       "      <td>6.426563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.717530</td>\n",
       "      <td>0.477468</td>\n",
       "      <td>3.516901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.717397</td>\n",
       "      <td>0.481181</td>\n",
       "      <td>4.444997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>0.710877</td>\n",
       "      <td>0.460253</td>\n",
       "      <td>5.038838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.796429</td>\n",
       "      <td>0.707771</td>\n",
       "      <td>0.462785</td>\n",
       "      <td>2.827811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.826587</td>\n",
       "      <td>0.707771</td>\n",
       "      <td>0.460844</td>\n",
       "      <td>4.514104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.705642</td>\n",
       "      <td>0.481857</td>\n",
       "      <td>0.924821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.763492</td>\n",
       "      <td>0.695751</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>1.977054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.727381</td>\n",
       "      <td>0.690694</td>\n",
       "      <td>0.450127</td>\n",
       "      <td>2.462493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.682976</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>1.139127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.766270</td>\n",
       "      <td>0.678673</td>\n",
       "      <td>0.409114</td>\n",
       "      <td>2.178618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.802447</td>\n",
       "      <td>1.035756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.693651</td>\n",
       "      <td>0.673927</td>\n",
       "      <td>0.436287</td>\n",
       "      <td>1.274621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629762</td>\n",
       "      <td>0.661373</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>0.848062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826587</td>\n",
       "      <td>0.647578</td>\n",
       "      <td>0.343291</td>\n",
       "      <td>3.499857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.617237</td>\n",
       "      <td>0.449958</td>\n",
       "      <td>1.446712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542460</td>\n",
       "      <td>0.610007</td>\n",
       "      <td>0.698819</td>\n",
       "      <td>0.926697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.595103</td>\n",
       "      <td>0.342194</td>\n",
       "      <td>1.068785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.521735</td>\n",
       "      <td>0.265992</td>\n",
       "      <td>1.698168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.490552</td>\n",
       "      <td>0.241519</td>\n",
       "      <td>0.857782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.069580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "98       6              48              3     0.847222    0.909510   \n",
       "60       8             122              1     0.923413    0.890481   \n",
       "59       7             122              1     0.915476    0.885424   \n",
       "97       5              48              3     0.828175    0.882275   \n",
       "58       6             122              1     0.871032    0.879258   \n",
       "96       4              48              3     0.749206    0.848385   \n",
       "57       5             122              1     0.789286    0.844393   \n",
       "95       3              48              3     0.704365    0.828779   \n",
       "31      22              12              1     0.915873    0.820751   \n",
       "30      20              12              1     0.903968    0.820662   \n",
       "102      5             122              3     0.891270    0.820617   \n",
       "29      18              12              1     0.902778    0.820484   \n",
       "28      16              12              1     0.901190    0.820351   \n",
       "27      14              12              1     0.905952    0.820041   \n",
       "26      12              12              1     0.911508    0.819819   \n",
       "18       4              12              1     0.910317    0.819730   \n",
       "25      11              12              1     0.906746    0.819597   \n",
       "24      10              12              1     0.893254    0.817956   \n",
       "101      4             122              3     0.871429    0.816270   \n",
       "56       4             122              1     0.733333    0.816137   \n",
       "23       9              12              1     0.896032    0.813121   \n",
       "15      22               1              1     0.944444    0.808242   \n",
       "14      20               1              1     0.948413    0.808064   \n",
       "13      18               1              1     0.942857    0.807976   \n",
       "12      16               1              1     0.938095    0.807576   \n",
       "11      14               1              1     0.946429    0.807266   \n",
       "10      12               1              1     0.953175    0.806911   \n",
       "22       8              12              1     0.880159    0.806645   \n",
       "8       10               1              1     0.944841    0.806379   \n",
       "6        8               1              1     0.940079    0.806024   \n",
       "4        6               1              1     0.934921    0.805536   \n",
       "2        4               1              1     0.933333    0.805314   \n",
       "9       11               1              1     0.937698    0.805048   \n",
       "21       7              12              1     0.881349    0.804116   \n",
       "20       6              12              1     0.884524    0.802475   \n",
       "19       5              12              1     0.862302    0.801233   \n",
       "7        9               1              1     0.931349    0.798527   \n",
       "5        7               1              1     0.913889    0.789123   \n",
       "53      22              48              1     0.939286    0.784909   \n",
       "52      20              48              1     0.938889    0.784200   \n",
       "51      18              48              1     0.948413    0.783357   \n",
       "50      16              48              1     0.942460    0.782425   \n",
       "49      14              48              1     0.953571    0.781938   \n",
       "48      12              48              1     0.947222    0.780873   \n",
       "46      10              48              1     0.950794    0.780030   \n",
       "44       8              48              1     0.950397    0.779498   \n",
       "42       6              48              1     0.951587    0.778389   \n",
       "40       4              48              1     0.948810    0.777901   \n",
       "47      11              48              1     0.943651    0.777324   \n",
       "3        5               1              1     0.898016    0.776969   \n",
       "93      22              24              3     0.879365    0.773155   \n",
       "17       3              12              1     0.785714    0.772401   \n",
       "92      20              24              3     0.865476    0.772356   \n",
       "91      18              24              3     0.880159    0.771735   \n",
       "90      16              24              3     0.881349    0.771292   \n",
       "89      14              24              3     0.880952    0.771026   \n",
       "88      12              24              3     0.865079    0.770582   \n",
       "86      10              24              3     0.881349    0.770271   \n",
       "84       8              24              3     0.883333    0.769606   \n",
       "37       7              24              1     0.926190    0.769473   \n",
       "82       6              24              3     0.875397    0.768497   \n",
       "80       4              24              3     0.880556    0.767300   \n",
       "87      11              24              3     0.871825    0.766013   \n",
       "45       9              48              1     0.925397    0.759448   \n",
       "100      3             122              3     0.818254    0.758916   \n",
       "55       3             122              1     0.687698    0.756476   \n",
       "77      22              12              3     0.895635    0.756033   \n",
       "76      20              12              3     0.891270    0.755456   \n",
       "75      18              12              3     0.886905    0.754879   \n",
       "74      16              12              3     0.886905    0.754347   \n",
       "73      14              12              3     0.887698    0.754037   \n",
       "72      12              12              3     0.880556    0.753637   \n",
       "70      10              12              3     0.863095    0.753016   \n",
       "1        3               1              1     0.852778    0.752706   \n",
       "68       8              12              3     0.885317    0.752395   \n",
       "66       6              12              3     0.877381    0.751863   \n",
       "64       4              12              3     0.871429    0.751020   \n",
       "71      11              12              3     0.867460    0.750532   \n",
       "85       9              24              3     0.850794    0.746052   \n",
       "36       6              24              1     0.902381    0.740774   \n",
       "69       9              12              3     0.855952    0.733055   \n",
       "83       7              24              3     0.850794    0.730527   \n",
       "67       7              12              3     0.823413    0.729285   \n",
       "35       5              24              1     0.868651    0.717530   \n",
       "65       5              12              3     0.795238    0.717397   \n",
       "43       7              48              1     0.886508    0.710877   \n",
       "34       4              24              1     0.796429    0.707771   \n",
       "81       5              24              3     0.826587    0.707771   \n",
       "0        2               1              1     0.767857    0.705642   \n",
       "33       3              24              1     0.763492    0.695751   \n",
       "63       3              12              3     0.727381    0.690694   \n",
       "32       2              24              1     0.716667    0.682976   \n",
       "79       3              24              3     0.766270    0.678673   \n",
       "94       2              48              3     0.559524    0.674370   \n",
       "62       2              12              3     0.693651    0.673927   \n",
       "16       2              12              1     0.629762    0.661373   \n",
       "41       5              48              1     0.826587    0.647578   \n",
       "99       2             122              3     0.680159    0.617237   \n",
       "54       2             122              1     0.542460    0.610007   \n",
       "78       2              24              3     0.650794    0.595103   \n",
       "39       3              48              1     0.746032    0.521735   \n",
       "38       2              48              1     0.648810    0.490552   \n",
       "61       2               1              3     0.519841    0.430758   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "98        0.874177    5.548434  \n",
       "60        0.799409    6.170311  \n",
       "59        0.790380    5.214121  \n",
       "97        0.844304    4.351358  \n",
       "58        0.782954    4.389015  \n",
       "96        0.806160    3.141691  \n",
       "57        0.780169    3.541118  \n",
       "95        0.805907    2.006240  \n",
       "31        0.667764   15.698435  \n",
       "30        0.667764   14.949828  \n",
       "102       0.669620    5.514633  \n",
       "29        0.667426   14.090263  \n",
       "28        0.667257   13.258659  \n",
       "27        0.667173   12.505358  \n",
       "26        0.666920   11.752340  \n",
       "18        0.667004    9.238146  \n",
       "25        0.666835    8.278834  \n",
       "24        0.664135    7.436796  \n",
       "101       0.662363    4.178486  \n",
       "56        0.752489    2.637325  \n",
       "23        0.655527    6.647932  \n",
       "15        0.642616   16.788981  \n",
       "14        0.642278   15.916996  \n",
       "13        0.642110   15.144928  \n",
       "12        0.641350   14.309601  \n",
       "11        0.640844   13.193370  \n",
       "10        0.640169   12.379952  \n",
       "22        0.643544    5.819725  \n",
       "8         0.639156   11.550372  \n",
       "6         0.638481   10.743485  \n",
       "4         0.637637    9.945443  \n",
       "2         0.637215    9.115473  \n",
       "9         0.636709    8.355908  \n",
       "21        0.639494    5.067740  \n",
       "20        0.637975    4.254191  \n",
       "19        0.636203    3.466942  \n",
       "7         0.624726    6.704805  \n",
       "5         0.607342    4.863771  \n",
       "53        0.596793   16.966625  \n",
       "52        0.595527   16.152657  \n",
       "51        0.594093   15.321514  \n",
       "50        0.592489   14.318795  \n",
       "49        0.591646   13.542171  \n",
       "48        0.589789   12.755471  \n",
       "46        0.588186   11.939057  \n",
       "44        0.587426   11.202503  \n",
       "42        0.585485   10.355245  \n",
       "40        0.584810    9.195992  \n",
       "47        0.583797    8.347562  \n",
       "3         0.586160    3.240004  \n",
       "93        0.578987   21.901474  \n",
       "17        0.586920    1.763685  \n",
       "92        0.577553   20.821092  \n",
       "91        0.576371   19.904007  \n",
       "90        0.575527   18.358187  \n",
       "89        0.575021   17.319017  \n",
       "88        0.574177   16.332910  \n",
       "86        0.573587   15.240586  \n",
       "84        0.572405   14.196526  \n",
       "37        0.568776    5.267488  \n",
       "82        0.570464   13.085671  \n",
       "80        0.568439   12.012670  \n",
       "87        0.566160   11.000057  \n",
       "45        0.550717    6.679805  \n",
       "100       0.557721    2.804342  \n",
       "55        0.716709    1.618722  \n",
       "77        0.545907   20.815290  \n",
       "76        0.544895   19.615030  \n",
       "75        0.543797   18.620883  \n",
       "74        0.542785   17.690050  \n",
       "73        0.542194   16.709647  \n",
       "72        0.541519   15.785249  \n",
       "70        0.540338   14.873307  \n",
       "1         0.544641    1.611163  \n",
       "68        0.539156   13.678719  \n",
       "66        0.538143   12.616991  \n",
       "64        0.536540   11.616187  \n",
       "71        0.535865   10.612506  \n",
       "85        0.530042    8.713374  \n",
       "36        0.516118    4.414896  \n",
       "69        0.504473    8.419276  \n",
       "83        0.501688    6.662893  \n",
       "67        0.499578    6.426563  \n",
       "35        0.477468    3.516901  \n",
       "65        0.481181    4.444997  \n",
       "43        0.460253    5.038838  \n",
       "34        0.462785    2.827811  \n",
       "81        0.460844    4.514104  \n",
       "0         0.481857    0.924821  \n",
       "33        0.443038    1.977054  \n",
       "63        0.450127    2.462493  \n",
       "32        0.428776    1.139127  \n",
       "79        0.409114    2.178618  \n",
       "94        0.802447    1.035756  \n",
       "62        0.436287    1.274621  \n",
       "16        0.617300    0.848062  \n",
       "41        0.343291    3.499857  \n",
       "99        0.449958    1.446712  \n",
       "54        0.698819    0.926697  \n",
       "78        0.342194    1.068785  \n",
       "39        0.265992    1.698168  \n",
       "38        0.241519    0.857782  \n",
       "61        0.181603    1.069580  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:25.794712Z",
     "start_time": "2017-07-17T00:29:24.898245Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:25.883110Z",
     "start_time": "2017-07-17T00:29:25.796982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:28.764403Z",
     "start_time": "2017-07-17T00:29:25.887256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8359  0.1641]\n",
      " [ 0.0348  0.9652]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecXGXZxvHfld4bJaQAoUOCECCEiCgRkN5ESpAuEimK\ngkhREPQFRUBBQESQ3psISAlFkJqEBEJvoYQkbCCdQPrmfv84z4bJkuxuNrO7mTPXl898duY57TmT\nYe657/OccxQRmJmZ5Umzpu6AmZlZsTm4mZlZ7ji4mZlZ7ji4mZlZ7ji4mZlZ7ji4mZlZ7ji4mZlZ\n7ji4mZlZ7ji4mZlZ7rRo6g6YmVlxNe+0dsTCOUVbX8yZPCwidi3aChuBg5uZWc7Ewjm03ujAoq1v\n7pi/rVq0lTUSBzczs9wRqLyPOpX33puZWS45czMzyxsBUlP3okk5uJmZ5ZHLkmZmZvnizM3MLI9c\nljQzs3zxaMny3nszM1thkq6V9Jmk1wvaLpT0tqRXJd0rqUvBtDMkjZX0jqRdCtq3kvRamnaplKWf\nklpLuiO1j5DUp7Y+ObiZmeWRVLxH7a4Hql/B5DFg04jYDHgXOCPrlvoCQ4B+aZkrJDVPy/wdOAbY\nID2q1nk0MD0i1gcuBv5UW4cc3MzM8kZkZcliPWoREU8D06q1PRoRC9PL4UDv9Hwf4PaImBcRHwJj\ngYGSegCdImJ4RARwI7BvwTI3pOd3AztWZXXL4uBmZmYN7UfAw+l5L2B8wbQJqa1Xel69fYllUsCc\nCaxS0wY9oMTMLHfqXE6sq1UljSp4fVVEXFWnnki/ARYCtxSzQ7VxcDMzy6PijpacEhEDlrsL0pHA\nnsCOqdQIMBFYs2C23qltIl+VLgvbC5eZIKkF0BmYWtO2XZY0M7Oik7QrcCqwd0TMLph0PzAkjYBc\nh2zgyMiIqAA+lzQoHU87HLivYJkj0vP9gf8WBMulcuZmZpZHjXgSt6TbgMFk5csJwNlkoyNbA4+l\nsR/DI+LYiHhD0p3Am2TlyhMiojKt6niykZdtyY7RVR2nuwa4SdJYsoErQ2rtUy3Bz8zMSkyzDj2j\ndf+ji7a+uc+dO7o+Zcmm5LKkmZnljsuSZmZ541veOLiZmeWSry1pZmaWL87czMxyx3cFcHAzM8uj\nZuV9zK28Q7uZmeWSMzczs7ypuitAGXNwMzPLozI/FaC8Q7uZmeWSMzczs9zxaMny3nszM8slZ25m\nZnlU5sfcHNzMzPLIZUkzM7N8ceZmZpY3ksuSTd0BMzNrAC5LmpmZ5YszNzOzPHJZ0szM8sUncZf3\n3uecpDckDV7GtMGSJtSw7PWSzm2wzpmZNSAHtxIl6SNJO1VrO1LSs1WvI6JfRDzV6J2rQfU+ruwk\nfVfSa5JmSJoq6V5Jveq4bB9JIemLgscrRejTOZJuXtH1FIukDSXdJWmKpJmSXpV0sqTmDbzdOv8A\nk7SBpLnV3zdJB0p6S9IsSW9K2rdhetsEqkZMFuNRghzcrKwoszyf+zeB3YGuQE/gPeDvy7nZLhHR\nIT02X85li05S0Q5HSFoPGAGMB74REZ2BA4CtgI7F2k4R/A14sbAh/Ui5GTgZ6AT8CrhV0uqN370i\nq7rlTbEeJag0e211UpjdSWqbfulOl/QmsHW1ebeQ9FL6BXsH0Kba9D0ljUkZzPOSNqu2nVPSL/aZ\nku6QtMTydezvUQW/oj+Q9JOCaa9L2qvgdcuUKWyRXg9K/Zoh6ZXCcqykpySdJ+k5YDawbsogP0jb\n+lDSIUvrU0R8GhHjIyJSUyWw/vLu2zL290dpf6dLGiZp7YJpf5U0XtLnkkZL+nZq3xX4NXBQYSZY\nPZMvzO4KMsijJX0M/LcO71md3h/gd8DzEXFyRFSk9+ydiDgkImakde2trEQ+I/1bbFKwnZC0fsHr\nxdmYUulc0i8lfSapQtJRadpQ4BDg1PQ+PFDD+zwEmAE8UW1Sb2BGRDwcmQeBL4H1lrUuKx0ObuXj\nbLL/adcDdgGOqJogqRXwb+AmoBtwF/CDgulbANcCPwFWAf4B3C+pdcH6DwR2BdYBNgOOrEcfPwP2\nJPsVfRRwsaQt07QbgUML5t0dqIiIl9Mv8AeBc1P/TwHukbRawfyHAUPJsonJwKXAbhHREdgWGJP2\nda30JbxWwf6vJWkGMCet+4J67NsSJO1DFqT2A1YDngFuK5jlRaB/2p9bgbsktYmIR4A/AHfUIxPc\nHtgE2KWm90xSe5bx/izFTsDdNeznhmm/fpH28yHggfSZq4s1gM5AL+Bo4G+SukbEVcAtwAXpfdgr\nbe8KSVcUbL8T8Huy7Ky6UcBbkvaS1FxZSXIe8God+7YSkzO3pu6ArZB/py/iGenL94oa5j0QOC8i\npkXEeLIvryqDgJbAJRGxICLuZskSzlDgHxExIiIqI+IGsi+BQQXzXBoRn0TENOABsi/m5RIRD0bE\n++lX9P+AR4Fvp8k3A7unLyvIgtVN6fmhwEMR8VBELIqIx8i+uHYvWP31EfFGRCwEFgKLgE0ltY2I\nioh4I/Xh44joEhEfF/Tr44joAqwKnAm8vZy7NqXg3+mU1HYs8MeIeCv16Q9A/6rsLSJujoipEbEw\nIv4MtAY2Ws7tVndORHwZEXOo/T1b6vuzFKsAFTVs8yDgwYh4LCIWABcBbckCZl0sAH6fPpcPAV9Q\nw/sQEcdHxPEFTf8HXBMRXxs8FRGVZD+abiP7PN8K/CQivqxj31ZuPuZmJWzf9EXcJX35Hl/DvD3J\njotUGVdt2sSC0lv16WsDv6wWSNdMy1WZVPB8NtBheXYEQNJukoZLmpa2sTtZQCEiPgGeA34gqQuw\nG9kv96r+HVCtf9sBPQpWv3jf05fXQWQBpkLSg5I2rq1/KXDfANyn5TtutWrBv9NFBX3+a0F/p5Ed\nKemV3otTUslyZpreueq9WAGF//7LfM+W8/2ZypLvc3U9KfgsRcSi1I86DcoBpqbgX6XOny1J/cky\ny4uXMX0nsix8MNCKLLP9Z1rOSpyDW/moIAtIVdaqNq2XtMRPtMLp48myvi4Fj3YRUVhGWyGpxHkP\n2S/77ilYP0T2hV/lBrKM4wDghYiYWNC/m6r1r31EnF+wbGHgJiKGRcT3yL6Y3waurmNXWwCrk5VO\nV8R4siyhsM9tI+L5dHztVLJsu2t6L2by1XsRS1nfl0C7gtdrLGWewuVqfM+W4/15nIIS9lJ8QhZI\ngWxAD9nnsOrfbnYd+r0sS3sfCg0G+gAfS5pEVnr9gaSX0vT+wNMRMSplry+SDY7ZaWkrKzkuS1qZ\nuBM4Q1JXSb2BnxVMe4GsVHeisoEa+wEDC6ZfDRwraRtl2kvaQ1J9R8NJUpvCB9kv59Zkx8MWStoN\n2Lnacv8GtgR+TlZOqnIzsJekXdKxkzZpMELvZWy8u6R90rGleWSlrkXLmHc/SRtJapaO4f0FeDll\ncVUDN56qx3twJdm/R7+0ns6SDkjTOpL9e0wGWkj6LUsG00+BPlpy1OcYYEj69xsA7F/L9pf5ni3P\n+0N2LHdbSRdKWiPty/qSbk4Z9p3AHpJ2lNQS+GVa5/MF/f5h6sOuZNlTXX0KrFvD9KvIjjH3T48r\nyY4z7pKmvwhsV5WppWPL3yYXx9xwWbKpO2CN5ndk5aEPyY5lVR2vIiLmkw1sOJKsPHYQ8K+C6aOA\nY4DLgenAWOo3YKTKtmSDM6o/TiT7MpwO/BC4v3ChdKzoHrJBK4X9Gw9UDdCYTJaV/Iplf76bkQ0w\n+IRsf7cHjoPFg0e+KBhQ0gt4BJgFvEb2Jf/9gnWtSVYuXS4RcS/wJ+B2SZ8Dr5OVWgGGpW2+S/Zv\nNpclS4p3pb9TC7KQs8i+yKeT/VvfWsv2a3rPlvn+LGU97wPfJMuQ3pA0k+zfaBQwKyLeIcu2LwOm\nAHsBe6XPHGQ/VPYiG814CNkPmLq6Buibyqr/BpB0paQrU99mR8SkqgdZkJ4bEZPT9P+RvVd3S5qV\n+v2HiHh0OfpgKykteZjFbOWWspgNI+LQWmduBJLGADtGxNSm7otZlWZd+0TrwWcWbX1z/33M6IgY\nULQVNgJfW9JKhqRuZMPBD2vqvlSJCA8+sJVTiZYTi8VlSSsJko4hK509HBFPN3V/zGzl5szNSkJE\nXE3dRzSalT2Veebm4GZmljPCwc1lSTMzyx1nbvXUsn2XaNNtec43tXK27qrtm7oLViLGfzyOaVOn\nrFjaJZa8/EEZcnCrpzbd1mCLk/7Z1N2wEnHbUVvXPpMZsPsOdb3sZk3ksmRTd8DMzKzYnLmZmeVQ\nuWduDm5mZjlU7sHNZUkzM8sdZ25mZjlU7pmbg5uZWd74VACXJc3MLH+cuZmZ5Yx8npuDm5lZHpV7\ncHNZ0szMcseZm5lZDpV75ubgZmaWQ+Ue3FyWNDOz3HHmZmaWNz7PzcHNzCyPXJY0MzPLGWduZmY5\n45O4HdzMzHKp3IOby5JmZrZCJF0r6TNJrxe0dZP0mKT30t+uBdPOkDRW0juSdilo30rSa2napUoR\nWlJrSXek9hGS+tTWJwc3M7M8UhEftbse2LVa2+nAExGxAfBEeo2kvsAQoF9a5gpJzdMyfweOATZI\nj6p1Hg1Mj4j1gYuBP9XWIQc3M7O8UVaWLNajNhHxNDCtWvM+wA3p+Q3AvgXtt0fEvIj4EBgLDJTU\nA+gUEcMjIoAbqy1Tta67gR1VS8cc3MzMrCF0j4iK9HwS0D097wWML5hvQmrrlZ5Xb19imYhYCMwE\nVqlp4x5QYmaWQ0UeULKqpFEFr6+KiKvqunBEhKQoZodq4+BmZpZDRQ5uUyJiwHIu86mkHhFRkUqO\nn6X2icCaBfP1Tm0T0/Pq7YXLTJDUAugMTK1p4y5LmplZQ7gfOCI9PwK4r6B9SBoBuQ7ZwJGRqYT5\nuaRB6Xja4dWWqVrX/sB/03G5ZXLmZmaWM419Erek24DBZOXLCcDZwPnAnZKOBsYBBwJExBuS7gTe\nBBYCJ0REZVrV8WQjL9sCD6cHwDXATZLGkg1cGVJbnxzczMzyqBHP4Y6Ig5cxacdlzH8ecN5S2kcB\nmy6lfS5wwPL0yWVJMzPLHWduZmZ5I19+y8HNzCyHyj24uSxpZma548zNzCyHyj1zc3AzM8uj8o5t\nLkuamVn+OHMzM8shlyXNzCxX6nqrmjxzWdLMzHLHmZuZWQ6Ve+bm4GZmlkPlHtxcljQzs9xx5mZm\nlkflnbg5uJmZ5ZHLkmZmZjnjzM3MLG98yxsHNzOzvBFQ5rHNZUkzM8sfZ25mZrnjy285uJmZ5VCZ\nxzaXJc3MLH+cuZmZ5ZDLkmZmli9yWdJlSTMzyx1nbmZmOSOgWbPyTt2cuZmZWe44czMzy6FyP+bm\n4GZmlkPlPlrSZUkzM8sdZ25mZnnjUwEc3MzM8ia7K0B5RzeXJc3MLHcc3GyxgX26cPORW3Lrj7bi\nkK17f216+1bN+eM+fbn2sC244fAt2K3f6gC0ai7+8cPNF7cf9c21Fi9z1DfX4p6hW3PNof255tD+\nDFqnKwAtmonTd96A6w/fgmsP24L+vTs3zk5a0Tz5+KN8Z+A3+NZWfbn8kgu/Nn3su++w987bs+4a\nnbjysouXmDZz5gyGHnEw22+zGYO32ZzRI4cvMf0fl19C725tmDZ1CgDTp03lgL13ZsM1V+E3p/6i\n4XYqN7K7AhTrUYpcljQAmglO2mE9Tr7ndSbPms9Vh/Tn2fenMm7anMXzfL9/D8ZNm80Z971J57Yt\nuOWorXjsrcnMrwx+cddrzFmwiObNxN8O2owRH03nzYpZANw1+hNuHz1xie3t9Y01ADjyxpfp0rYl\nF+7Xj6G3jCEab5dtBVRWVnLmqT/n1n89SI+evdljx2+x8657suHGmyyep0vXrvz+/D8z7KH7v7b8\n2Wf8ksE7fo+rbriN+fPnM2fO7MXTPpkwnqeffJxevddc3Na6dRt+9euzeeetN3n7rTcadudyokRj\nUtE4czMANlmjIxNnzKVi5jwWLgqeeHsy2623yhLzREDbls0BaNeyOZ/PXUjloiwczVmwCMgyshbN\nRETNYarPKm15afwMAGbMWcAX8xay8Rodir1b1kDGjH6RPuusx9p91qVVq1bss98BPPrwA0vMs+pq\nq9N/ywG0aNFyifbPP5/JiOef5eDDjgKgVatWdO7cZfH0c35zKr/53R+WyBjatW/PwEHfonXr1g24\nV5YnDm4GwKodWvHZrHmLX0/+Yh6rdWy1xDz/GlPB2qu05d6hA7nu8C259MkPFmdazQTXHNqf+47d\nhlEfz+CtSV8sXm6/LXpw3WFbcNrOG9ChdRYcx07+km+ttwrNBT06tWbD1Tuwekd/cZWKiopP6NHr\nq9L1Gj17UVHxSZ2WHT/uI7qtuhon//QYdtl+G0458Vhmf/klAMMeeoA1evSk76abNUi/y0m5lyUd\n3KzOBvbpwtjPvuT7V43k6Jtf5qQd1qNdqyxYLQo4+uYx7H/1SDZeowPrrNIOgH+/UsGQa0bxo5te\nZuqX8zlh+3UBeOj1T5k8ax5XHdKfnw1elzcqPl+cBVq+LVy4kNdfeZnDjhrKsP+NoF279vztkguZ\nM3s2l/3lAk759W+buoulL50KUKxHKWqw4Cbp+Xos85Gkewpe7y/p+qJ2rPY+nCPplMbc5spgyhfz\nl8icVuvQmsmz5i8xz+79uvP02KkAqYQ5l7W7tV1ini/mVfLy+Jls0ycbODJ99gIWBQTwn9cmsUkq\nPVYGXP6/Dzn65jH8+v636NC6BeOnz8FKQ48ePamYOGHx60mfTKRHj551W7ZnL3r07MWWAwYCsMc+\n3+e1V8fw0UcfMP7jj9j521szaPMNqfhkIrsOHsRnn05qkH2wfGuw4BYR29Zz0a0k9a3PgpI8QKae\n3p40i95d2tKjU2taNBM7brwaz30wbYl5Pp01j63Wyo6NdG3XkjW7teWTGXPp3LbF4nJjqxbNGLBW\nF8ZNywYIrNL+q+Mt315/FT6ckrW3btGMNi2yj9+AtbpQuSiWGLxiK7fNtxzAhx+M5eNxHzJ//nzu\n+9ddfG/XPeu07Ord16Bnr968/967ADz7vyfZYKNN2KTvprzy7niGv/Iuw195lx49e/HIU8NZvfsa\nDbkruVR1nls5lyUbLBhI+iIiOkjqAdwBdErbOy4inqlh0T8DvwEOqba+bsC1wLrAbGBoRLwq6Rxg\nvdT+saRhwL5Ae2AD4CKgFXAYMA/YPSKmSToGGJqmjQUOi4jZlKnKgEuefJ+LfrApzZSVDT+aOpu9\nN8u+WO5/dRI3DB/Pr3fJhu8DXPnMR8ycu5B1V23Hr3fdkOYSEjz57hRe+HA6AMd+ex02WL09ETDp\n87lc9PhYIAuOF+3XjwiY/MV8zn343abZcauXFi1a8H8XXMIh++/FospKDjrkCDbapC83XXc1AIcd\ndQyffTqJ3Xf4Fl/M+pxmzZrxzysv58kXXqZjp078358u5mc/OZL58+ezdp91+PPlV9W6zUGbb8is\nWbNYsGA+wx58gFvv+c8SozNtSSUak4pGtY1qq/eKvwpuvwTaRMR5kpoD7SJi1jKW+QjYBngK2Avo\nD+wZEUdKugyYEhG/k7QD8JeI6J+C217AdhExR9KRwJnAFkAbssB1WkRcKeliYFxEXCJplYiYmrZ7\nLvBpRFyW1vdFRFy0lP4NJQuItO7afauBZ95dlPfK8u+2o7Zu6i5Yidh9h2155eXRKxSa2vfaKDY5\n7spidYnRZ+0wOiIGFG2FjaAxyngvAtdKagn8OyLG1DJ/JXAhcAbwcEH7dsAPACLiv5JWkdQpTbs/\nIgprWk+mADpL0kygaozya0DVMKxNU1DrAnQAhtW2IxFxFXAVQMc1N/boBzNbaZVqObFYGny0ZEQ8\nDXwHmAhcL+nwOix2U1pmzdpmTL6s9npewfNFBa8X8VVAvx74aUR8A/gdWZZnZpYLHi3ZwCStTVby\nuxr4J7BlbctExALgYuCkguZnSMfhJA0mK1F+vgJd6whUpIzykNpmNjOz0tEYZcnBwK8kLQC+AOqS\nuQFcQ3bsrMo5ZOXNV8kGlByxgv06CxgBTE5/O67g+szMVg5yWbLBgltEdEh/bwBuqOMyfQqezwN6\nFryeRjYKsvoy51R7fT1ZyXFp61w8LSL+Dvy9tvWZmZWa7FSApu5F0/IVSszMLHea5KRnSSOA6hcS\nPCwiXmuK/piZ5UvpnnxdLE0S3CJim6bYrplZuSjz2OaypJmZ5Y+vxWhmlkMuS5qZWb6U8MnXxeKy\npJmZ5Y4zNzOznKm65U05c3AzM8uhcg9uLkuamdkKkXSSpDckvS7pNkltJHWT9Jik99LfrgXznyFp\nrKR3JO1S0L6VpNfStEu1AhHawc3MLIca664AknoBJwIDImJToDkwBDgdeCIiNgCeSK+R1DdN7wfs\nClyR7vUJ2SURjyG70fQGaXq9OLiZmeWQpKI96qAF0FZSC6Ad8AmwD19dV/gGvro28D7A7RExLyI+\nJLuh9EBJPYBOETE8srto38hSridcVw5uZmZWm1UljSp4DK2aEBETgYuAj4EKYGZEPAp0j4iKNNsk\noHt63gsYX7DuCamtV3pevb1ePKDEzCxvin+e25SIGLDUTWXH0vYB1gFmAHdJOrRwnogISVHUHtXC\nwc3MLGfUuBdO3gn4MCImA0j6F7At8KmkHhFRkUqOn6X5JwJrFizfO7VNTM+rt9eLy5JmZrYiPgYG\nSWqXRjfuCLwF3M9XN5U+ArgvPb8fGCKptaR1yAaOjEwlzM8lDUrrObxgmeXmzM3MLIcaK3GLiBGS\n7gZeAhYCLwNXAR2AOyUdDYwDDkzzvyHpTuDNNP8JEVGZVnc82Q2l2wIPp0e9OLiZmeVQs0Y8iTsi\nzgbOrtY8jyyLW9r85wHnLaV9FLBpMfrksqSZmeWOMzczsxwq86tvObiZmeVNdmWR8o5uLkuamVnu\nOHMzM8uhZuWduDm4mZnlkcuSZmZmOePMzcwsh8o8cXNwMzPLG5FdX7KcuSxpZma548zNzCyHPFrS\nzMzype530M4tlyXNzCx3nLmZmeVQmSduDm5mZnkjGveWNysjlyXNzCx3nLmZmeVQmSduDm5mZnnk\n0ZJmZmY548zNzCxnspuVNnUvmpaDm5lZDnm0pJmZWc4sM3OT1KmmBSPi8+J3x8zMiqG887aay5Jv\nAMGS71HV6wDWasB+mZnZCij30ZLLDG4RsWZjdsTMzKxY6nTMTdIQSb9Oz3tL2qphu2VmZvWVXX6r\neI9SVGtwk3Q58F3gsNQ0G7iyITtlZmYrIN3ypliPUlSXUwG2jYgtJb0MEBHTJLVq4H6ZmZnVW12C\n2wJJzcgGkSBpFWBRg/bKzMxWSIkmXEVTl+D2N+AeYDVJvwMOBH7XoL0yM7MVUqrlxGKpNbhFxI2S\nRgM7paYDIuL1hu2WmZlZ/dX18lvNgQVkpUlf1cTMbCVWNVqynNVltORvgNuAnkBv4FZJZzR0x8zM\nrP48WrJ2hwNbRMRsAEnnAS8Df2zIjpmZmdVXXYJbRbX5WqQ2MzNbSZVmvlU8NV04+WKyY2zTgDck\nDUuvdwZebJzumZnZ8pJ8y5uaMreqEZFvAA8WtA9vuO6YmZmtuJounHxNY3bEzMyKp8wTt9qPuUla\nDzgP6Au0qWqPiA0bsF9mZmb1Vpdz1q4HriM7PrkbcCdwRwP2yczMVlC5nwpQl+DWLiKGAUTE+xFx\nJlmQMzOzlZRUvEcpqsupAPPShZPfl3QsMBHo2LDdMjMzq7+6BLeTgPbAiWTH3joDP2rITpmZWf0J\n+VSA2maIiBHp6Sy+umGpmZmtrEq4nFgsNZ3EfS/pHm5LExH7NUiPzMzMVlBNmdvljdaLErTh6h14\n9MTtmrobViK6bv3Tpu6ClYh574wvynpKdZRjsdR0EvcTjdkRMzMrnnK/N1m577+ZmeVQXW9WamZm\nJUK4LFnn4CapdUTMa8jOmJlZcfhO3LWQNFDSa8B76fXmki5r8J6ZmZnVU12OuV0K7AlMBYiIV4Dv\nNmSnzMxsxTRT8R6lqC5lyWYRMa5a/baygfpjZmYrKLsmZIlGpSKpS3AbL2kgEJKaAz8D3m3YbpmZ\nmdVfXYLbcWSlybWAT4HHU5uZma2kSrWcWCy1HnOLiM8iYkhErJoeQyJiSmN0zszM6qcxb3kjqYuk\nuyW9LektSd+U1E3SY5LeS3+7Fsx/hqSxkt6RtEtB+1aSXkvTLtUK1Fbrcifuq1nKNSYjYmh9N2pm\nZrnyV+CRiNhfUiugHfBr4ImIOF/S6cDpwGmS+gJDgH5AT+BxSRtGRCXwd+AYYATwELAr8HB9OlSX\nsuTjBc/bAN8HinPxMzMzKzpBo93yRlJn4DvAkQARMR+YL2kfYHCa7QbgKeA0YB/g9nTe9IeSxgID\nJX0EdIqI4Wm9NwL70lDBLSLuqLYjNwHP1mdjZmbWOBrx2orrAJOB6yRtDowGfg50j4iKNM8koHt6\n3gsYXrD8hNS2ID2v3l4v9dn/dfiqk2Zmln+rShpV8Cg8LNUC2BL4e0RsAXxJVoJcLCKCGm6h1hDq\ncsxtOl91qhkwjWodNzOzlUuRq5JTImLAMqZNACYU3Nj6brIY8amkHhFRIakH8FmaPhFYs2D53qlt\nYnpevb1easzc0kiVzYHV0qNrRKwbEXfWd4NmZtawJNGsiI+aRMQksvOhN0pNOwJvAvcDR6S2I4D7\n0vP7gSGSWktaB9gAGJlKmJ9LGpRiz+EFyyy3GjO3iAhJD0XEpvXdgJmZ5d7PgFvSSMkPgKPIkqc7\nJR0NjAMOBIiINyTdSRYAFwInpJGSAMcD1wNtyQaS1GswCdRttOQYSVtExMv13YiZmTWuxrz6VkSM\nAZZWttxxGfOfB5y3lPZRQFGSqWUGN0ktImIhsAXwoqT3yQ4UKutDbFmMDpiZWfGV+xVKasrcRpKN\ngNm7kfpiZmZWFDUFNwFExPuN1BczMyuCxjyJe2VVU3BbTdLJy5oYEX9pgP6YmVkRlHlsqzG4NQc6\nkDI4MzOzUlFTcKuIiN83Wk/MzKw4SvgO2sVS6zE3MzMrPSrzr/CarlCy1PMTzMzMVnbLzNwiYlpj\ndsTMzIqmryihAAAbVUlEQVQjGy3Z1L1oWnW5QomZmZWYcg9ujXjLHzMzs8bhzM3MLIdU5ie6ObiZ\nmeWMj7m5LGlmZjnkzM3MLG/ky285uJmZ5VC5XzjZZUkzM8sdZ25mZjnjASUObmZmuVTmVUmXJc3M\nLH+cuZmZ5Y5oVuZ3BXBwMzPLGeGypMuSZmaWO87czMzyxnfidnAzM8sjn8RtZmaWM87czMxyxgNK\nHNzMzHLJZUkzM7OcceZmZpZDZZ64ObiZmeWNcFmu3PffzMxyyJmbmVneCFTmdUkHNzOzHCrv0Oay\npJmZ5ZAzNzOznMnuxF3euZuDm5lZDpV3aHNZ0szMcsiZm5lZDpV5VdLBzcwsf1T2pwK4LGlmZrnj\nzM3MLGd8+S0HNzOzXHJZ0szMLGcc3GyxR4c9wmb9NqLfxutz4QXnf216RHDyL06k38brs/UWm/Hy\nSy8BMHfuXLb75kAGbrk5W27ej//73dlfW/aSi/9M25ZiypQpACxYsIAfH3UEA/p/g/7f2IQL//TH\nht05K7rvbbsJr9x7Fq/fdzanHPW9r03v0rEtd/z5GEbecQbP3HQKfdfrsXha5w5tufXCoxnzrzN5\n+Z4z2WazdQD4zU925/1h5zL89tMZfvvp7LJdXwB22GZjnrvlVF6889c8d8upbL/1ho2zkyVMRXyU\nIpclDYDKykp+ceIJPPjwY/Tq3ZvtBm3NnnvuzSZ9+y6eZ9gjD/P+2Pd4/a33GDliBCf+9DieeX4E\nrVu35pHH/kuHDh1YsGABO2y/HTvvshvbDBoEwPjx43nisUdZc621Fq/rnrvvYt78eYwa8xqzZ89m\ni836cuBBB7N2nz6NvetWD82aiUtOP5A9jruciZ/O4NlbfsV//vcab38wafE8px69C6+8M4GDfnk1\nG/bpziWnH8jux14GwEWn7s+jz7/JD391DS1bNKddm1aLl7vs5ie55KYnltje1BlfsP8v/kHF5Jn0\nXa8HD1xxAuvtcmbj7Gwp8oWTnblZ5sWRI1lvvfVZZ911adWqFQccNIT/PHDfEvP85/77+OGhhyOJ\nbQYNYubMGVRUVCCJDh06AFlGtnDBgiX+xzr1lJM4748XLNEmidlffsnChQuZM2cOrVq1omOnTo2z\ns7bCtt60D++Pn8JHE6eyYGEldw17iT0Hb7bEPBuvuwb/e/FdAN796FPW7tmN1bt1pFOHNmy35Xpc\nf+8LACxYWMnML+bUuL1X3plAxeSZALz5fgVtWrekVUv/Nrdlc3AzAD75ZCK9e6+5+HWvXr2ZOHFi\nrfN8kuaprKxkm636s1bP1dlhp+8xcJttAHjg/vvo2bMXm22++RLr2u8H+9OufXvWWbMHG667Fr84\n6RS6devWULtnRdZz9c5M+HT64tcTP51Or9U6LzHPa+9OZJ8dsn/3Af3WZq0e3ejVvQt9eq7ClOlf\ncNXvDuWF207jit/+cInM7biDt2fkHWdw5dmH0KVj269t+/s79WfM2+OZv2BhA+1d6asaLVmsRykq\n1X7bSqZ58+aMGD2GsR9NYNSLI3nj9deZPXs2F5z/B357zu+/Nv+LI0fSvFlzPvj4E95670P+esmf\n+fCDD5qg59ZQLrruMTp3bMfw20/nuCHb88o7E6isXESLFs3pv/GaXH3XM3zz4D8xe848TvlRdszu\n6rueYZM9z2abIeczacrnnH/yfkusc5N11+DcE/fhp+fe3hS7VFIkFe1RihotuEl6vp7L9ZcUknYt\naOsi6fiC130k/XAF+vaUpAH1XT4PevbsxYQJ4xe/njhxAr169ap1np7V5unSpQvbD/4ujz76CB+8\n/z7jPvqQgVttzkbr92HihAl8c+CWTJo0iTtvv5Wdd9mVli1bsvrqq/PNb36L0aNHNexOWtF88tlM\nenfvuvh1r+5dmZjKhlVmfTmXn5xzM4OGnM/RZ93Iql078OHEqUz8dDoTP5vBi6+PA+Dex8fQf+Os\nIvDZtFksWhREBNf+6zkGbLr2V9tYvQt3/GUoPz7rJj6cMKUR9tJKWaMFt4jYtp6LHgw8m/5W6QIc\nX/C6D1Dv4GYwYOutGTv2PT768EPmz5/PXXfczh577r3EPHvstTe33nwjEcGI4cPp1KkzPXr0YPLk\nycyYMQOAOXPm8MTjj7HRRhuz6Te+wceffMY7Yz/inbEf0at3b14Y+RJrrLEGvddai6ee/C8AX375\nJSNHDmejjTZu9P22+hn1xjjWX2s11u65Ci1bNOeAXbbkwadeXWKezh3a0rJFcwCO+v62PPvSWGZ9\nOZdPp85iwqTpbLD26gAMHrjR4oEoa6z61XHXfXbYnDffr1i8rn9ddixnXXofL7ziDL8uPFqykUj6\nIiI6SOoB3AF0Sts/LiKeWcYyAg4Avgc8I6lNRMwFzgfWkzQGeAz4NrBJen0DcC9wE9A+reqnEfF8\nWudpwKHAIuDhiDi9YHvNgGuBCRHxtaFYkoYCQ4ElRv7lQYsWLbj4r5ez1x67UFlZyRFH/oi+/fpx\n9T+uBOCYnxzLrrvtzrCHH6LfxuvTrm07/vHP6wCYVFHBMT86gsrKShbFIn6w/4HsvseeNW7v2ONO\nYOiPj2LLzfsRERx2xFF8Y7PNalzGVh6VlYs46U938sAVJ9C8mbjhvuG89cEkfrz/dgD88+5n2Xjd\nNbj694cREbz1fgXH/u6Wxcuf/Ke7uO4PR9KqRXM+mjiFoWffDMB5P9+XzTbqTUQwrmIaPzv3NgCO\nHfId1ltzNc4YuhtnDN0NgL2Ou5zJ079o5D0vHSVaTSwaRUTjbOir4PZLoE1EnCepOdAuImYtY5lv\nAb+PiB0l3QrcExH3SOoD/CciNk3zDQZOiYg90+t2wKKImCtpA+C2iBggaTfgLGCniJgtqVtETJP0\nFHA68HPg9Yg4r7b92WqrAfHcCJfRrG66bv3Tpu6ClYh579zJotmfrVBoWr/f5vHn24cVq0vsu1mP\n0RFRUodummIs7YvAtZJaAv+OiDE1zHswUHXk+HbgcOCeOmyjJXC5pP5AJVB1xudOwHURMRsgIqYV\nLPMP4M66BDYzs5VZNlqyvFO3Rh8tGRFPA98BJgLXSzp8afOlrO4HwG8lfQRcBuwqqWMdNnMS8Cmw\nOTAAaFXz7AA8D3xXUps6zGtmZiuxRg9uktYGPo2Iq4F/AlsuY9YdgVcjYs2I6BMRa5Nlbd8HZgGF\nQa76685ARUQsAg4Dmqf2x4CjUtkSSYUnVl0DPATcKclnh5pZSZOK9yhFTXGe22DgFUkvAwcBf13G\nfAeTDQwpdA9wcERMBZ6T9LqkC4FXgUpJr0g6CbgCOELSK8DGwJcAEfEIcD8wKg0+OaVw5RHxF+Bl\n4KY0uMTMrASpqP/VaYtSc0kvS/pPet1N0mOS3kt/uxbMe4aksZLekbRLQftWkl5L0y7VCpxk12gZ\nSkR0SH9vIBvRWNv8Ry2l7X6y4EREVB/6v0O114VD704rWMf5ZKMtC9c7uOD516/6a2Zmtfk58BbZ\nSHjIBuk9ERHnSzo9vT5NUl9gCNAP6Ak8LmnDiKgE/g4cA4wgq6TtCjxcn844OzEzy6HGLEtK6g3s\nQXaoqco+fJXI3ADsW9B+e0TMi4gPgbHAwHSaWKeIGB7ZMP4bC5ZZbivFsSVJI4DW1ZoPi4jXmqI/\nZmalrAlGS14CnMqSYx+6R0RFej4J6J6e9wKGF8w3IbUtSM+rt9fLShHcImKbpu6DmZkt06qSCk/s\nvSoirgKQtCfwWUSMTuccf01EhKTGOak6WSmCm5mZFVHxRzlOqeEk7m8Be0vaHWgDdJJ0M/CppB4R\nUZFKjp+l+ScCaxYs3zu1TUzPq7fXi4+5mZnlUGMdc4uIMyKid0T0IRso8t+IOJRs8N8RabYjgKob\nRN4PDJHUWtI6wAbAyFTC/FzSoDRK8vCCZZabMzczM2sI55OdN3w0MA44ECAi3pB0J/AmsBA4IY2U\nhOyC+NcDbclGSdZrpCQ4uJmZ5VJdz08rpoh4CngqPZ9KdjGOpc13HvC1Sx1GxChg02L0xcHNzCxn\nBDQr0SuLFIuPuZmZWe44czMzy6GmKEuuTBzczMxyqFQveFwsLkuamVnuOHMzM8shlyXNzCxXPFrS\nZUkzM8shZ25mZrlT95uM5pWDm5lZ3hT/wsklx2VJMzPLHWduZmY5VOaJm4ObmVneZKMlyzu8uSxp\nZma548zNzCyHyjtvc3AzM8unMo9uLkuamVnuOHMzM8shn8RtZma5U+aDJV2WNDOz/HHmZmaWQ2We\nuDm4mZnlUplHN5clzcwsd5y5mZnljPBoSQc3M7O88S1vXJY0M7P8ceZmZpZDZZ64ObiZmeVSmUc3\nlyXNzCx3nLmZmeWOPFqyqTtgZmbF59GSZmZmOePMzcwsZ0TZjydxcDMzy6Uyj24uS5qZWe44czMz\nyyGPljQzs9zxaEkzM7OcceZmZpZDZZ64ObiZmeWOzwVwWdLMzPLHmZuZWQ55tKSZmeWK8GhJlyXN\nzCx3nLmZmeVQmSduDm5mZrlU5tHNZUkzM8sdZ25mZjnk0ZJmZpY7Hi1pZmaWM87czMxyqMwTNwc3\nM7NcKvPo5rKkmZnljjM3M7OcyW4KUN6pm4ObmVneyKMlXZY0M7PcceZWTy+9NHpK25Ya19T9WMms\nCkxp6k5YyfDnZenWLsZKGitxk7QmcCPQHQjgqoj4q6RuwB1AH+Aj4MCImJ6WOQM4GqgEToyIYal9\nK+B6oC3wEPDziIj69MvBrZ4iYrWm7sPKRtKoiBjQ1P2w0uDPSwNrvLLkQuCXEfGSpI7AaEmPAUcC\nT0TE+ZJOB04HTpPUFxgC9AN6Ao9L2jAiKoG/A8cAI8iC267Aw/XplMuSZmZWbxFREREvpeezgLeA\nXsA+wA1pthuAfdPzfYDbI2JeRHwIjAUGSuoBdIqI4Slbu7FgmeXmzM3MLHdU7NGSq0oaVfD6qoi4\n6mtblfoAW5BlXt0joiJNmkRWtoQs8A0vWGxCaluQnldvrxcHNyumr33YzWrgz0sDKvJoySm1lZAl\ndQDuAX4REZ+roAMREZLqdeysvlyWtKJZ2i85s2Xx5yU/JLUkC2y3RMS/UvOnqdRI+vtZap8IrFmw\neO/UNjE9r95eLw5uZmY5oyI/atxWlqJdA7wVEX8pmHQ/cER6fgRwX0H7EEmtJa0DbACMTCXMzyUN\nSus8vGCZ5eaypJlZHjXeaMlvAYcBr0kak9p+DZwP3CnpaGAccCBARLwh6U7gTbKRliekkZIAx/PV\nqQAPU8+RkuDgZmZmKyAinmXZoXTHZSxzHnDeUtpHAZsWo18Obtak0omeq0bEu03dFysdklTfk3vL\nRblfW9LH3KzJSGoDnAj8SNImTd0fW/mlq2HgwFY7qXiPUuTgZk0mIuYCj6eXB6QrF5gtJqmDpFbp\n+SbABekqGGY1cnCzJpFGQ1XV6+8HOgH7O8BZFUntgVuAA1LT7PT4Ig09X/w5sq9rrNGSKysHN2t0\nVcdLJK0jqUVEPA9cB3QmC3AuURoR8SXZhXePknQQ2QV450RmQZrH5UlbKg8osUaXAtsewFnAM5K+\nAC4hu2LF0cChkm6JiDebsp/WdCQ1j4jKiLhV0mTgNGA0sI6kv5Jdmmke0KLauVUGvp8bztysCUga\nBPwBOIjsB9a+wAXAZLILrLYH5jdZB61Jpcy+UtL3JF0QEY8BfyUbVj4f+Dj97UB2DUNbqvIuTDpz\ns0YjqRnZ/Z5WJbv6wMbAd8huhTEUuIjsF/pvUknKylDK7HcErgB+ktoekLQQOBl4NyIeaMo+2srP\nmZs1uIKD/h3S8ZL/RMQrZBnbj9ONCj8j+7HV3YGtfCnTguw+XmdFxH+rRktGxMPAlWT3BKv31eLL\ngfCpAA5u1uAKjrE9IekcSfulSasDQyVtAwwELoqI15uso9bk0o+fhcBcYJCkNhExH0DS1mQ3sNw7\nIup9Qd1yUd5FSQc3awTpiuCHkJUdpwG7pGD3I7Krg/8W+GNEvNp0vbSmUpXZS1pLUtVV4R8GWgLb\np2mbAxcDG0bEtCbpqJUUH3OzBiVpALA5MDEi7pC0GrAL8H2gZUTsKaldRMz2JZXKU0Fm/0fgeUnd\nIuLAdErIYZJOIztN5NxUzrY6KNVyYrE4uFmDkTSYbPTjMLLh/bdFxEuSHgZaAftIGhkRn4DPWSo3\nBec7DiIbLbsnWaZ2raTHI2InSdeT/TiaGRHv+wdQ3ZX7tSUd3KxBpPs0/Ro4LCKeljQWuFnSIRHx\nsqT7gEeqApuVj3RN0QVpuH93YCrZ7VA2IBsd2Rl4StLzEbEt8FLVsg5sVlc+5mZFU3DsZGuyX+Cd\nyUZEEhEXkN3Q8H5JW0XEVAe28pNOB9kW+IWkPcmOt84iu7fXHsC1ETGLLONfK32WrD7KfESJg5sV\nTSoxfYesxPQa2Yna7ST9NE3/M/A3spNvrXy9CuwM3ATcHRGTyL5CK4D1JB1DVqL8XkS82HTdLG1l\nHtsc3Kx4JG0EHAdcHxGjgaeAJ4CNJf0SICLOj4j/+YK35UVSe0m9I2IRsHZqfhLYLQ33X0R2h4jZ\nZIHtyoh4q4m6azngY25WTN8AugM7SXooIiZLeoRsSPdgSWtHxDjwsZMy1Ac4V1LVnZZ/CUwnu77o\nX4DjgQ/IAt4fImKhB4/UXymffF0sztys3gqOsfWW1Dki7ib7svqc7Or+q6TjJw8Av60KbFZ+IuIN\nYCzZIKMR6WT9yWSX2Got6QmyTH9BOonbP4BWkIr4XylycLN6kdQsHWPbjeyE22skPQ28BfwHqDpH\naZWImJWOq1gZkdRFUruCpteBPwOHS9oxIuanE/d/A1wPnBQRw5ugq5ZDLkvacpHUNiLmRMQiSesD\n/wf8JCKel3Qp8G+yk7Rbpr/tyYZ6WxmR1A14F3hc0jMR8beIuCFNGw/8RdIRwAxgv6rb1rgUWUSl\nmXAVjYOb1ZmkzsD5ku6NiEfJvpjeJvsSIyJOlHQbcHpEnC3pxYioaMIuW9OZDjxKNgLyEEkDgWeB\nuyLiaknzgXuAhcAvqhZyYCueMo9tLkvaculEdtzkh+mWJJ8DqwA7FczzEOlebA5s5SsFqZfIBhh9\nh6zs+B3gf5K+SzZwZBvgB+lq/2ZF5czNaiWpYzpuNl7SjcAQsoseTyYbIHC9pI2Bman91Kbrra0s\nIuIiSQ+R/fh5HehPlukPAdYHDvJdIBpOuY+WdHCzGknqA9wtaTRwJ/AecB0wj2w495+AA4DdgJ5k\ngwIe97GT8iapeURUkmVs3ye7ov81KeCtTnbR7ClN2cd8K91RjsXi4Ga1aQP0APYBPiK7wsiVQFfg\nebKh/+dFxF8LF3JgK28psAGMAM4BXoiIi1LbZH8+rKH5mJstUxru/zZZWWkm8DFwEPAJ2bUj90+v\nL0jDvv15ssVS9j4OOBnoUHX3bAe2huc7cTtzsxqk4f7NIuItSYcCt5NdPeIaSXeTXcV9H2BMRMxo\n0s5akyi4bU2zdAmtxQqC2ARg0deXNms4Dm5Wo4IA96KkIcBt6VqAfwPeIbtIss9PKkMFgW1Hssxs\nWETMrT5fRLwu6bSImNgE3bQy5TKS1aowwJGVIc+SdEK1eRzYykgaMBKSdgX+DkxfWmBTpllEjJPU\nTtIqjd/b8lTuZUkHN1us4FqRX/tcFAS40cBewBuN3T9repLWT6eGVErqSjag6Nh0Q9pvSzoinbBd\npVn67HQhO7etW5N0vAyV+7UlXZY0oG4lpmoZnEuR5ak7sLqk4RExXdKTwNHpHmzNgAVkx2JHSmqR\nru7fGbgL+FVEvNd0Xbdy4szN6lxiqpo9LdOW7HQAKyMR8RzZjWg/kNSJ7Dy2kcBlEXEQ2bmQ/SS1\nSoGtK3Av8PuIeLqp+l12iliSdFnSSs7ylpiqTsxNJaanyC69ZWUm3cbo52TnOU6JiL+mC2d/m+xC\n2v+MiPlp9oOBcyPimSbqblkq5l24SzS2uSxZ5lxisnqJiPskLQBGS9oKmEt23uOZEfFgVck6Iq5o\n2p5auXJwK2MR8ZykjmQlps3ISkx7AC+mX+J7A0elEtP8lN3dA5ztX+IWEQ9JWkR2D7+NgNMiYm7B\n8Vsfk21KpZpyFYnLkmXOJSZbERHxCPBjYIuq47RVAc2BrWl5tKSVPZeYbEVExIPg0bO2cnFwM8Al\nJltx/nysXEp1lGOxuCxpi7nEZJYfHi1pVsAlJjPLAwc3WyoHNrMSV6opV5E4uJmZ5VCpjnIsFh9z\nMzOz3HHmZmaWM1V34i5n8qEVyxtJlWQX921BdmrDERExu57rGgycEhF7piu29I2I85cxbxfgh8t7\nPqCkc4AvIuKiurRXm+d64D8RcXcdt9Unzb/p8vTRSoukR4BVi7jKKRGxaxHX1+CcuVkezYmI/gCS\nbgGOBf5SNTHdt04RsWh5VhoR9wP31zBLF+B4wCe7W5MqtUDUEHzMzfLuGWB9SX0kvSPpRuB1YE1J\nO0t6QdJLku6S1AFA0q6S3pb0ErBf1YokHSnp8vS8u6R7Jb2SHtsC5wPrSRoj6cI0368kvSjpVUm/\nK1jXbyS9K+lZspPmayTpmLSeVyTdI6ldweSdJI1K69szzd9c0oUF2/7Jir6RZqXEwc1yS1ILYDey\nEiVkdzi4IiL6AV8CZwI7RcSWwCjgZEltgKvJ7ja+FbDGMlZ/KfC/iNgc2JLszuSnA+9HRP+I+JWk\nndM2BwL9ga0kfSdd4mxIatsd2LoOu/OviNg6be8t4OiCaX3SNvYArkz7cDQwMyK2Tus/RtI6ddiO\nWS64LGl51FbSmPT8GeAaoCcwLiKGp/ZBQF/guaxKSSvgBWBj4MOq2/lIuhkYupRt7AAcDhARlcDM\ndNeEQjunx8vpdQeyYNcRuLfqOKCkmkqdVTaVdC5Z6bMDMKxg2p2pxPqepA/SPuwMbCZp/zRP57Tt\nd+uwLbOS5+BmebT4mFuVFMC+LGwCHouIg6vNt8RyK0jAHyPiH9W28Yt6rOt6YN+IeEXSkcDggmnV\nR4VF2vbPIqIwCFYNKDHLPZclrVwNB74laX0ASe0lbQi8DfSRtF6a7+BlLP8EcFxatnm6iesssqys\nyjDgRwXH8npJWh14GthXUtt0P7296tDfjkCFpJbAIdWmHSCpWerzusA7advHpfmRtKGk9nXYjlku\nOHOzshQRk1MGdJuk1qn5zIh4V9JQ4EFJs8nKmh2XsoqfA1dJOhqoBI6LiBckPSfpdeDhdNxtE+CF\nlDl+ARwaES9JugN4BfgMeLEOXT4LGAFMTn8L+/QxMBLoBByb7ubwT7JjcS+l0aGTgX3r9u6YlT6f\n52ZmZrnjsqSZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeWOg5uZmeXO/wMU\n4mSwcaP0YwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47fce2f278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:29.150543Z",
     "start_time": "2017-07-17T00:29:28.765973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.5144  0.4856]\n",
      " [ 0.046   0.954 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8VVX9//HX+97LPA9KCCqm5IDlgBpfv9XX0hJns1Qq\nldTU0rLJEiu/Wb8oK7+VVmoOBWqmpJmkohmlOYHiLI6YE4giOIAgw7338/tjr4ubyx3h3Ons99PH\nftx91p7WOWzP53zWXnttRQRmZmbloKKjK2BmZlYqDmpmZlY2HNTMzKxsOKiZmVnZcFAzM7Oy4aBm\nZmZlw0HNzMzKhoOamZmVDQc1MzMrG1UdXQEzM2tblf23jKh+p2T7i3deuyUixpdshyXkoGZmVuai\n+h16bHtEyfa38qHfDi3ZzkrMQc3MrOwJVIyrTcV4l2ZmVgjO1MzMyp0AqaNr0S4c1MzMisDNj2Zm\nZl2LMzUzsyIoSPOjMzUzs7KXej+WamruaNJXJT0maa6kr6WywZJulfRM+jsot/4ZkuZJekrSvrny\nsZIeTcvOk5qPzA5qZmZWMpJ2BE4A9gB2Ag6UtA0wCZgZEaOBmek1knYAJgBjgPHA+ZIq0+4uSPsa\nnaZmb/h2UDMzKwKpdFPTtgdmR8SKiKgGbgcOAw4BpqZ1pgKHpvlDgKsiYlVEPAfMA/aQNBzoHxGz\nIiKAy3LbNMpBzcys3IlSNz8OlTQnN52YO9pjwIclDZHUG9gf2BwYFhEL0zqvAMPS/Ajgpdz281PZ\niDRfv7xJ7ihiZmattTgidmtoQUQ8IemnwN+B5cBDQE29dUJStEXFnKmZmZW9EjY9tqAXZURcGhFj\nI+IjwBvA08CrqUmR9HdRWn0BWSZXZ2QqW5Dm65c3yUHNzKwI2rf346bp7xZk19OuBKYDE9MqE4Hr\n0/x0YIKkHpK2IusQcm9qqlwqaVzq9XhMbptGufnRzMxK7VpJQ4A1wCkR8aaks4Fpko4HXgCOAIiI\nuZKmAY8D1Wn9uubKk4EpQC9gRpqapKxTiZmZlauKvsOjx07HlWx/K+/+8f2NXVPraM7UzMzKnh89\nY2Zm1uU4UzMzK3d+9IyZmZUVNz+amZl1Lc7UzMzKXnE6ijiomZkVQUUxrqkVI3SbmVkhOFMzMyt3\ndaP0F0Ax3qWZmRWCMzUzsyLwfWpmZlYeitP7sRjv0szMCsGZmplZEbj50czMyoabH83MzLoWZ2pm\nZuVOcvOjmZmVETc/mpmZdS0OagUgaa6kvRpZtpek+U1sO0XSj9qscmbWPuqaIEsxdWIOal2cpOcl\n7VOv7POS7qx7HRFjIuK2dq9cE+rXsbOT9FFJj0p6U9ISSddJGtHCbUdJCklv56aHS1CnsyRdsbH7\nKRVJ75P0Z0mLJb0l6RFJ35BU2cbHbfEPL0mjJa2s/7lJOkLSE5KWSXpc0qFtU9uOkm6+LtXUiXXu\n2pm1EWVac/4/DuwPDAI2A54BLmjlYQdGRN807dTKbUtOUsmuqUvaGpgNvAS8PyIGAIcDY4F+pTpO\nCfwWuC9fkH6cXAF8A+gPfAu4UtKm7V8921gOagWQz+Yk9Uq/bN+Q9Diwe711d5H0QPrFejXQs97y\nAyU9lDKWuyV9oN5xTku/0N+SdLWkdbZvYX2Pzf1q/o+kk3LLHpN0UO51t5QZ7JJej0v1elPSw/lm\nV0m3SZos6S5gBfDelDH+Jx3rOUmfa6hOEfFqRLwUEZGKaoBtWvveGnm/x6X3+4akWyRtmVt2rqSX\nJC2VdL+kD6fy8cB3gCPzmV/9zD2fzeUyxuMlvQj8swWfWYs+H+AHwN0R8Y2IWJg+s6ci4nMR8Wba\n18HKmsLfTP8W2+eOE5K2yb1em30pNZFL+qakRZIWSjo2LTsR+Bzw7fQ5/K2Jz3kC8CYws96ikcCb\nETEjMjcCy4GtG9tXl+TmRytT3yf7n3VrYF9gYt0CSd2BvwKXA4OBPwOfyi3fBfg9cBIwBPgdMF1S\nj9z+jwDGA1sBHwA+vwF1XAQcSPar+Vjgl5J2TcsuA47Krbs/sDAiHky/uG8EfpTqfxpwraRNcusf\nDZxIlj28BpwH7BcR/YA9gYfSe90ifflukXv/W0h6E3gn7ftnG/De1iHpELLgdBiwCXAH8KfcKvcB\nO6f3cyXwZ0k9I+Jm4MfA1RuQ+f0PsD2wb1OfmaQ+NPL5NGAf4Jom3uf70vv6WnqfNwF/S+dcS7wH\nGACMAI4HfitpUERcBPwR+Fn6HA5Kxztf0vm54/cHfkiWjdU3B3hC0kGSKpU1Pa4CHmlh3Tq/ukfP\nuPnRuoi/pi/gN9OX7vlNrHsEMDkiXo+Il8i+tOqMA7oBv4qINRFxDes21ZwI/C4iZkdETURMJfuf\nf1xunfMi4uWIeB34G9kXcqtExI0R8Wz61Xw78Hfgw2nxFcD+6UsKsiB1eZo/CrgpIm6KiNqIuJXs\nC2v/3O6nRMTciKgGqoFaYEdJvSJiYUTMTXV4MSIGRsSLuXq9GBEDgaHA94AnW/nWFuf+nU5LZV8E\nfhIRT6Q6/RjYuS5bi4grImJJRFRHxP8BPYBtW3nc+s6KiOUR8Q7Nf2YNfj4NGAIsbOKYRwI3RsSt\nEbEGOAfoRRYoW2IN8MN0Xt4EvE0Tn0NEnBwRJ+eK/h9waUSs1ykqImrIfiz9iex8vhI4KSKWt7Bu\n1ok4qJWHQ9MX8MD0pXtyE+tuRnbdo84L9ZYtyDWx1V++JfDNegF087RdnVdy8yuAvq15IwCS9pM0\nS9Lr6Rj7kwUSIuJl4C7gU5IGAvuR/VKvq9/h9er3IWB4bvdr33v60jqSLLAslHSjpO2aq18K2FOB\n69W661JDc/9O5+TqfG6uvq+T/a4ekT6L01LT5Ftp+YC6z2Ij5P/9G/3MWvn5LGHdz7m+zcidSxFR\nm+rRos42wJIU9Ou0+NyStDNZJvnLRpbvQ5Z17wV0J8tkL0nblQl3FLHytZAsENXZot6yEdI6jeb5\n5S+RZXkDc1PviMg3l22U1JR5Ldkv+WEpSN9E9kVfZypZhnE4cE9ELMjV7/J69esTEWfnts0HbCLi\nloj4ONkX8pPAxS2sahWwKVkT6cZ4iSwryNe5V0Tcna6ffZssux6UPou3ePeziAb2txzonXv9ngbW\nyW/X5GfWis/nH+SaqhvwMlkABbKOOmTnYd2/3YoW1LsxDX0OeXsBo4AXJb1C1sT6KUkPpOU7A/+O\niDkpW72PrNPLPg3trMvyNTUrU9OAMyQNkjQS+Epu2T1kTXKnKuuAcRiwR275xcAXJX1QmT6SDpC0\nob3bJKlnfiL7pdyD7HpXtaT9gE/U2+6vwK7AV8majepcARwkad90baRn6mQwspGDD5N0SLp2tIqs\nSau2kXUPk7StpIp0je4XwIMpa6vrkHHbBnwGF5L9e4xJ+xkg6fC0rB/Zv8drQJWk/2XdIPoqMErr\n9uJ8CJiQ/v12Az7dzPEb/cxa8/mQXavdU9LPJb0nvZdtJF2RMuppwAGS9pbUDfhm2ufduXp/NtVh\nPFm21FKvAu9tYvlFZNeQd07ThWTXEfdNy+8DPlSXmaVrxx+mnK6pFYiDWvH8gKwZ6Dmya1V116OI\niNVkHRY+T9YMdiTwl9zyOcAJwG+AN4B5bFhHkDp7knW6qD+dSvYl+AbwWWB6fqN0Lehass4o+fq9\nBNR1vHiNLAv5Fo2f5xVkHQdeJnu//wN8CdZ2Cnk711FkBHAzsAx4lOzL/ZO5fW1O1izaKhFxHfBT\n4CpJS4HHyJpUAW5Jx3ya7N9sJes2Hf45/V2SyzrOJPsCf4Ps3/rKZo7f1GfW6OfTwH6eBf6LLCOa\nK+ktsn+jOcCyiHiKLLv+NbAYOAg4KJ1zkP1AOYisd+LnyH64tNSlwA6p+fSvAJIulHRhqtuKiHil\nbiILzisj4rW0/Hayz+oaSctSvX8cEX9vRR06v4I0P2rdyydmXUPKWt4XEUc1u3I7kPQQsHdELOno\nupjVVzFwy+ix13dLtr+V1590f0TsVrIdlpAHNLYuR9Jgsm7dR3d0XepERBl1KjDrujp3HmlWj6QT\nyJrIZkTEvzu6PmZdgorT+9GZmnUpEXExLe+haGZ1OnmvxVLp3CHXzMysFZypmZkVgAqSqTmobaAh\nQ4bGyC22bH5Fs5yCfK9YCb30wgssWbJ4o84c0b5BTdLXgS+Q3Rj/KNkYrr2Bq8lu+3geOCIi3kjr\nn0HW+asGODUibknlY4EpZEOq3QR8NZrpsu+gtoFGbrElf799VkdXw7oYBzVrrU/8z7jmV+pElA2S\nfSqwQ0S8I2kaMAHYAZgZEWdLmgRMAk6XtENaPoZsOLV/SHpfGpPzArJ7Y2eTBbXxwIymju9ramZm\n5U4lnppXBfRKY6P2JruB/xCyIe5If+sexHoIcFVErIqI58gGddhD0nCgf0TMStnZZbltmjywmZmV\nNZW6+XGopDm51xdF9hggImKBpHOAF8lGCPp7RPxd0rBIz9ojG/h8WJofAeSbveansjVpvn55kxzU\nzMystRY3NqKIpEFk2ddWZMOe/VnSOiP/RERIapPhrBzUzMwKoB07iuwDPFc3tqakv5CN8/qqpOER\nsTA1LS5K6y9g3SeHjExlC9J8/fIm+ZqamVkBSCrZ1IwXgXGSeqdHDO0NPEE2MPnEtM5E4Po0P53s\nyRI9JG0FjAbuTU2VSyWNS/s5JrdNo5ypmZlZyUTEbEnXAA+QPTrpQbLH//QFpkk6nuypE0ek9eem\nHpKPp/VPST0fIXvg8RSyLv0zaKbnIziomZkVQnvepxYR3yd7xl7eKrKsraH1JwOTGyifA+zYmmM7\nqJmZlbuWd8Xv8nxNzczMyoYzNTOzMqfS36fWaTmomZkVQFGCmpsfzcysbDhTMzMrgKJkag5qZmYF\nUJSg5uZHMzMrG87UzMzKXYHuU3NQMzMrADc/mpmZdTHO1MzMypxvvjYzs7JSlKDm5kczMysbztTM\nzIqgGImag5qZWdmTmx/NzMy6HGdqZmYFUJRMzUHNzKwAihLU3PxoZmZlw5mamVmZ883XZmZWXooR\n09z8aGZm5cOZmplZuSvQfWoOamZmBVCUoObmRzMzKxvO1MzMCqAomZqDmplZERQjprn50czMyocz\nNTOzAnDzo5mZlQWpOCOKuPnRzMzKhjM1M7MCKEqm5qBmZlYARQlqbn40M7Oy4aBmZlYEKuHU3KGk\nbSU9lJuWSvqapMGSbpX0TPo7KLfNGZLmSXpK0r658rGSHk3LzlMzKaeDmplZAdT1gCzF1JyIeCoi\ndo6InYGxwArgOmASMDMiRgMz02sk7QBMAMYA44HzJVWm3V0AnACMTtP4po7toGZmZm1pb+DZiHgB\nOASYmsqnAoem+UOAqyJiVUQ8B8wD9pA0HOgfEbMiIoDLcts0yB1FzMzKXekfPTNU0pzc64si4qJG\n1p0A/CnND4uIhWn+FWBYmh8BzMptMz+VrUnz9csb5aBmZlbmBJS48+PiiNit2eNK3YGDgTPqL4uI\nkBQlrRVufjQzs7azH/BARLyaXr+amhRJfxel8gXA5rntRqayBWm+fnmjHNTMzMpe6TqJtLIZ8zO8\n2/QIMB2YmOYnAtfnyidI6iFpK7IOIfempsqlksalXo/H5LZpkJsfzcwKoL3vvZbUB/g4cFKu+Gxg\nmqTjgReAIwAiYq6kacDjQDVwSkTUpG1OBqYAvYAZaWqUg5qZmZVcRCwHhtQrW0LWG7Kh9ScDkxso\nnwPs2NLjOqiZmRWAh8kyMzPrYpypmZmVO7X/NbWO4qBmZlbmBFRUFCOqufnRzMzKhjM1M7MCcPOj\nmZmVDfd+NDMz62KcqZmZlTv3fjQzs3KRjdJfjKjmoGbr6F4p+vWsBME7q2tZsbp2neXdKsXA3pXU\npOJVa2pZntbp37OSHlWiNmDJ8ur19t27ewX9elayaNkaIvfAiQrBkL5VLF+1/vGsa+heKfr2zB5U\nvHJN4/+OVRViUO9Klq6sYVV1dhL06lZBr27ZlZDq2mDpymzIvz7dK+jZrYLadK4sX1XD6prsRWVF\ndr6J7Iv69RXrn29WTA5qto5+vSp5c3k1NQGD+1Sxqrp2bQCrs6Y6ePOdmvW2fWdNLStWBwN6rX9a\nVQi6V4ma2vUfn9SvZyWrq0v+WCVrR/16VvLGimpqAwb1bvi8Aejbo2JtYILsvOjdvWLtj6D+PSvp\nWSVWpvNhxepa3lmz/o4G9Kxi6cpqqmuhGPnHxmr16PpdljuK2FrdKrOgU/eds3JNLT2qWn6KrKkJ\nGohZQPal9/bK9QNhjxToqhvb0Dq9qgpRXfvuv/2q6obPm17dKlhV3fA5Uvd1K9HoOVSne2V2vOoU\n63zmtIxUuqkzc1CztSoEtbkfxbURVDZwhnSrEoP7VDGwd2WDy+vrUSVqc19CdUT2K335Kjc5dmWV\nFfXOm9qg/uAVFcrOg/pZV21k2diQvlUM7VNFBOtkcr27VzC4dxX9elauDXyVaecDelUyqHcVvbv7\na8ze5eZHa5XqmmDxsmqCrDlxYK+qBq+f5fXpUcEby9fP0vr0qGDF6lr/0i6Avj0qebuBHy8iC3ZL\nllcTAQPSddlV1cGK3PXaPt0r6NuzkmUp2+9WKV5fkW0zqHcla2rEmhqfSU1x8+NGknT3BmzzvKRr\nc68/LWlKSSvWfB3OknRaex6zs6gNqMidERXSetdFgnebe1ZXR7PNEVUVUCllv8T7VmWdQvpkf7ul\nTilD+2a/tvv0eLfDgHUdNbX1zpsKrdeE2K1SDOhVyZA+VfSoEv16VNK9Suk6K2s7Dq2qrqVbZXZC\n5TsTvbOmlm4pQ6uNYHVN5LaJtcusESVseuzssbHNMrWI2HMDNx0raYeIeLy1G0qqigh3g9pAa2qC\nygplzZABPbtV8NY7636cdcsgu5YC63751FddC6+9/e4+hvatWvur/I0V72ZvfXpUEEGDnQKsc6uu\nDapy502PqgqWrlz3vMln81nHoFpWV2fbdev+7rdkt6oKqlPGlT/XelRVrL3uuro66J3bpnul3GvW\n1mqzoCbp7YjoK2k4cDXQPx3vSxFxRxOb/h/wXeBz9fY3GPg98F5gBXBiRDwi6Sxg61T+oqRbgEOB\nPsBo4BygO3A0sArYPyJel3QCcGJaNg84OiJWlOTNd2HLVtYwqHcVCFauznqw1WVP76SOI727V2QZ\nWwRv5XpBDuhVSbfK7MttaN8q3l5Vw8o1bhIqgmUraxjYuwqRnSc1tdmPIsg6HDWmujZYVV3L4N5V\na1/X/bDp26Ny7Q+nmoi1TY8BrFhds3abVTW161yHs/X5PrXS+ixwS0RMllQJ9G5m/WnAyZK2qVf+\nA+DBiDhU0seAy4Cd07IdgA9FxDuSPk/26O9dgJ5kAev0iNhF0i+BY4BfAX+JiIsBJP0IOB74dVMV\nk3QiWSBk5OZbNP/Ou6DV1cGS6nV/Zeezp3fWNNzFGlgnwDVm8dsNJ9LuLNK1ra4JXq93bbWxYLas\nXi/Y5avfvXaWt7SB3rJ1VlUHq6rdKNMaBYlp7dL78T7g2JRRvT8iljWzfg3wc+CMeuUfAi4HiIh/\nAkMk9U/LpkfEO7l1/xURyyLiNeAt4G+p/FFgVJrfUdIdkh4lywrHNPdGIuKiiNgtInYbPGRoc6ub\nmVk7a/OgFhH/Bj4CLACmSDqmBZtdnrbZvIWHWV7v9arcfG3udS3vZqdTgC9HxPvJssCeLTyWmVmX\nI6lkU2fW5kFN0pbAq6mp7xJg1+a2iYg1wC+Br+eK7yBdZ5O0F7A4IpZuRNX6AQsldaPe9Tszs3Lj\n3o+lsxfwLUlrgLfJrmm1xKXA93KvzwJ+L+kRso4iEzeyXmcCs4HX0t9+G7k/MzPrYG3Zpb9v+jsV\nmNrCbUbl5lcBm+Vev07Wq7H+NmfVez2FrGmxoX2uXRYRFwAXNLc/M7MuT+79aGZmZSLr0t/RtWgf\nHRLUJM0GetQrPjoiHu2I+piZWXnokKAWER/siOOamRVT5++1WCpufjQzK4CCxDQ/esbMzMqHMzUz\nswJw86OZmZWHLnDTdKm4+dHMzMqGMzUzszLnR8+YmVlZKUpQc/OjmZmVnKSBkq6R9KSkJyT9l6TB\nkm6V9Ez6Oyi3/hmS5kl6StK+ufKxkh5Ny85TM9HZQc3MrAA6YJT+c4GbI2I7YCfgCWASMDMiRgMz\n02sk7QBMIHuu5Xjg/PRQacjG6D0BGJ2m8U0d1EHNzKwA2vN5apIGkD0T81KAiFgdEW8Ch/DuAPdT\neXeQ+kOAqyJiVUQ8B8wD9pA0HOgfEbMiIoDLaGBg+zwHNTMzK7WtyB7r9QdJD0q6RFIfYFhELEzr\nvAIMS/MjgJdy289PZSPSfP3yRjmomZmVuxI2PaZEbaikObnpxHpHrCJ7IPQFEbELsJzU1FgnZV5R\n6rfq3o9mZmVOpR/QeHFE7NbE8vnA/IiYnV5fQxbUXpU0PCIWpqbFRWn5AmDz3PYjU9mCNF+/vFHO\n1MzMrKQi4hXgJUnbpqK9gceB6cDEVDYRuD7NTwcmSOohaSuyDiH3pqbKpZLGpV6Px+S2aZAzNTOz\nAuiA29S+AvxRUnfgP8CxZInUNEnHAy8ARwBExFxJ08gCXzVwSkTUpP2cDEwBegEz0tQoBzUzswKo\naOeoFhEPAQ01Ue7dyPqTgckNlM8Bdmzpcd38aGZmZcOZmplZARRklCwHNTOzcpd1xS9GVHPzo5mZ\nlQ1namZmBVBRjETNQc3MrAjc/GhmZtbFOFMzMyuAgiRqDmpmZuVOZOM/FoGbH83MrGw4UzMzKwD3\nfjQzs/LQwidWlwM3P5qZWdlwpmZmVgAFSdQc1MzMyp1o/0fPdBQ3P5qZWdlwpmZmVgAFSdQc1MzM\nisC9H83MzLoYZ2pmZmUue0hoR9eifTiomZkVgHs/mpmZdTGNZmqS+je1YUQsLX11zMysLRQjT2u6\n+XEuEKz7WdS9DmCLNqyXmZmVUFF6PzYa1CJi8/asiJmZ2cZq0TU1SRMkfSfNj5Q0tm2rZWZmpZIN\nk1W6qTNrNqhJ+g3wUeDoVLQCuLAtK2VmZiWUHj1Tqqkza0mX/j0jYldJDwJExOuSurdxvczMzFqt\nJUFtjaQKss4hSBoC1LZprczMrKQ6eYJVMi0Jar8FrgU2kfQD4AjgB21aKzMzK6nO3mxYKs0GtYi4\nTNL9wD6p6PCIeKxtq2VmZtZ6LR0mqxJYQ9YE6VFIzMy6kLrej0XQkt6P3wX+BGwGjASulHRGW1fM\nzMxKx70f33UMsEtErACQNBl4EPhJW1bMzMystVoS1BbWW68qlZmZWRfRufOr0mlqQONfkl1Dex2Y\nK+mW9PoTwH3tUz0zM9tYUvs/ekbS88AyoAaojojdJA0GrgZGAc8DR0TEG2n9M4Dj0/qnRsQtqXws\nMAXoBdwEfDUiorHjNpWp1fVwnAvcmCuf1bq3ZmZmBfXRiFicez0JmBkRZ0ualF6fLmkHYAIwhqz/\nxj8kvS8iaoALgBOA2WRBbTwwo7EDNjWg8aUb+27MzKxz6CT9Ow4B9krzU4HbgNNT+VURsQp4TtI8\nYI+U7fWPiFkAki4DDqWJoNaS3o9bS7pK0iOSnq6bNvw9mZlZAQRZxnW/pBNT2bCIqOuT8QowLM2P\nAF7KbTs/lY1I8/XLG9WSjiJTgB8B5wD7AcemypqZWRdR4q74QyXNyb2+KCIuqrfOhyJigaRNgVsl\nPZlfGBEhqeSxpCU3Uveuu2AXEc9GxPfIgpuZmXURUukmYHFE7Jab6gc0ImJB+rsIuA7YA3hV0vCs\nPhoOLEqrLwDyz/AcmcoWpPn65Y1qSVBblQY0flbSFyUdBPRrwXZmZlZAkvpI6lc3T9Zr/jFgOjAx\nrTYRuD7NTwcmSOohaStgNHBvaqpcKmmcslTzmNw2DWpJ8+PXgT7AqcBkYABwXCven5mZdSCh9u7S\nPwy4LjV5VgFXRsTNku4Dpkk6HniBbIB8ImKupGnA40A1cErq+QhwMu926Z9BE51E6g7WpIiYnWaX\n8e6DQs3MrKtQ+/Z+jIj/ADs1UL4E2LuRbSaTJU71y+cAO7b02E3dfH0dTXQIiYjDWnoQMzOz9tBU\npvabdqtFF1RVIQb07tbR1bAuZtDuX+7oKlgXs+qpl5pfqQU6+0DEpdLUzdcz27MiZmbWdoryzLCi\nvE8zMyuAlj4k1MzMuijh5sf1SOqRxuUyM7Muxk++TiTtIelR4Jn0eidJv27zmpmZmbVSS66pnQcc\nCCwBiIiHgY+2ZaXMzKy0KlS6qTNrSfNjRUS8UK89tqaxlc3MrHPJxmzs5NGoRFoS1F6StAcQkiqB\nrwB+9IyZmXU6LQlqXyJrgtwCeBX4RyozM7MuorM3G5ZKS8Z+XET2mG0zM+uiCtL62HxQk3QxDYwB\nGREnNrC6mZlZh2lJ8+M/cvM9gU+y7mO3zcysExO096NnOkxLmh+vzr+WdDlwZ5vVyMzMSq4oYyJu\nyPvciuwBcGZmZp1KS66pvcG719QqgNeBSW1ZKTMzK62CtD42HdSU3a23E7AgFdVGRKMPDjUzs85H\nUmGuqTXZ/JgC2E0RUZMmBzQzM+u0WnJN7SFJu7R5TczMrM1kQ2WVZurMGm1+lFQVEdXALsB9kp4F\nlpP1Do2I2LWd6mhmZhvJI4rAvcCuwMHtVBczM7ON0lRQE0BEPNtOdTEzszbgm68zm0j6RmMLI+IX\nbVAfMzNrAwWJaU0GtUqgLyljMzMz6+yaCmoLI+KH7VYTMzNrG13gidWl0uw1NTMz6/pUkK/0pu5T\n27vdamFmZlYCjWZqEfF6e1bEzMzaRtb7saNr0T5a8jw1MzPr4ooS1IryiB0zMysAZ2pmZgWggtyo\n5qBmZlbminRNzc2PZmZWNpypmZmVuy7wyJhScaZmZlYAFenp16WYWkJSpaQHJd2QXg+WdKukZ9Lf\nQbl1z5A0T9JTkvbNlY+V9Ghadp5acGHQQc3MzNrCV4Encq8nATMjYjQwM71G0g7ABGAMMB44X1Jl\n2uYC4ARgdJrGN3dQBzUzszJX11GkVFOzx5NGAgcAl+SKDwGmpvmpwKG58qsiYlVEPAfMA/aQNBzo\nHxGzIiJlqLLxAAAbpUlEQVSAy3LbNMrX1MzMCqCdr6n9Cvg20C9XNiwiFqb5V4BhaX4EMCu33vxU\ntibN1y9vkjM1MzNrraGS5uSmE+sWSDoQWBQR9ze2ccq8oi0q5kzNzKzsiYrSjtK/OCJ2a2TZfwMH\nS9of6An0l3QF8Kqk4RGxMDUtLkrrLwA2z20/MpUtSPP1y5vkTM3MrMyJrPmxVFNTIuKMiBgZEaPI\nOoD8MyKOAqYDE9NqE4Hr0/x0YIKkHpK2IusQcm9qqlwqaVzq9XhMbptGOVMzM7P2cDYwTdLxwAvA\nEQARMVfSNOBxoBo4JSJq0jYnA1OAXsCMNDXJQc3MrNx10JOvI+I24LY0v4RGntMZEZOByQ2UzwF2\nbM0xHdTMzAqgpTdNd3W+pmZmZmXDmZqZWZmr6yhSBA5qZmYF4OZHMzOzLsaZmplZARQkUXNQMzMr\nd6I4zXJFeZ9mZlYAztTMzMqdoAXP1ywLDmpmZgVQjJDm5kczMysjztTMzMpc9uTrYuRqDmpmZgVQ\njJDm5kczMysjztTMzAqgIK2PDmpmZuVPhenS7+ZHMzMrG87UzMzKXJGGyXJQMzMrADc/mpmZdTHO\n1MzMCqAYeZozNWvG32+5mQ+M2ZYx223Dz3929nrLI4JvfO1Uxmy3Dbvv8gEefOCBdZbX1NQwbrdd\nOOyQA9cpP/83v2anHbdj153G8J1J327T92Dt6+N7bs/D153JY9d/n9OO/fh6ywf268XV/3cC9159\nBndcfho7bD187bInb/wB9037DrOumsSdf1z/vPjq0R/jnQd/w5CBfdr0PZSdNKBxqabOzJmaNaqm\npoavnXoKN864lREjR/Khcbtz4IEHs/0OO6xd55abZ/DsvGd47IlnuHf2bE798pe44+7Za5f/5rxz\n2Xb77Vm2dOnasttv+xc3/O167r3/YXr06MGiRYva9X1Z26moEL+adAQHfOk3LHj1Te7847e44fZH\nefI/r6xd59vH78vDT83nyG9ezPtGDeNXk45g/y/+eu3y8Seey5I3l6+375HDBrL3uO15ceHr7fJe\nrGtypmaNuu/ee9l6623Y6r3vpXv37hx+5ARu+Nv166xzw/Tr+exRxyCJD44bx1tvvcnChQsBmD9/\nPjfPuJFjj/vCOttc9LsLOO3bk+jRowcAm266afu8IWtzu+84imdfWszzC5awprqGP9/yAAfu9YF1\n1tnuve/h9vueBuDp519ly80Gs+ngfs3u+2enfYrvnvtXIqJN6l7O6no/lmrqzDp7/awDvfzyAkaO\n3Hzt6xEjRrJgwYJm13k5rfOtb36NyT/5GRUV655m855+mrvuvIMP7/lBPv6x/2HOffe14buw9rTZ\npgOY/+oba18vePUNRmwyYJ11Hn16AYd8bCcAdhuzJVsMH8yIYQOBrDn7xgu/wl1//DbHHfbfa7c5\ncK/38/KiN3n06XXPP2s5Nz+abYSbbryBTTfZlF3HjuXft9+2zrLqmmpef/11/n3XLObcdx9HffYI\nnnj6P53+fxYrjXP+cCvnfOvTzLpqEnOfeZmHn5pPTU0tAHsf+0tefu0tNhnUlxsu/DJPPf8KDzz+\nIt8+bl8OPPk3HVxz6wraLVOTdPcGbrezpJA0Plc2UNLJudejJH12I+p2m6TdNnT7crXZZiOYP/+l\nta8XLJjPiBEjml1nsxEjuOfuu7jhhulsu80ojvncBG771z859pijgCybO/SThyGJ3ffYg4qKChYv\nXtw+b8ra1MuL3mLksEFrX48YNogFr721zjrLlq/kpLOuYNyEszn+zMsYOqgvzy1Ykm2f1n3tjbeZ\n/s9H2H3MKN47chO2HDGEe68+gydv/AEjNh3IPVeezrAhzTdZ2rtUwqkza7egFhF7buCmnwHuTH/r\nDAROzr0eBWxwULOG7bb77syb9wzPP/ccq1ev5s9XX8UBBx68zjoHHHQwV15xGRHB7Fmz6N9/AMOH\nD+f/Tf4Jzz4/n6fmPc9lf7yKvT76Mf5w2RUAHHTwodx+278AeObpp1m9ejVDhw5t9/dnpTdn7gts\ns8UmbLnZELpVVXL4vrty422PrLPOgL696FZVCcCxn9yTOx+Yx7LlK+ndszt9e2fXWXv37M4+/7Ud\nc599mbnzXmbLvc9guwO+z3YHfJ8Fi97kvz77U15dsqzd3591fu3W/Cjp7YjoK2k4cDXQPx3/SxFx\nRyPbCDgc+Dhwh6SeEbESOBvYWtJDwK3Ah4Ht0+upwHXA5UBdv98vR8TdaZ+nA0cBtcCMiJiUO14F\n8HtgfkR8r4H6nAicCLD5Flts1OfRFVRVVfHLc3/DQQfsS01NDRM/fxw7jBnDxb+7EIATTvoi4/fb\nn1tm3MSY7bahd6/e/O6SPzS734nHHsdJXziOsTvvSPdu3bnk91Pd9Fgmampq+fpPp/G380+hskJM\nvX4WT/znFb7w6Q8BcMk1d7Lde9/DxT88mojgiWcX8sUf/BGATYf04+pfnABAVWUlV8+Yw613P9Fh\n76XcFOV/MbVXT6JcUPsm0DMiJkuqBHpHRIM/uST9N/DDiNhb0pXAtRFxraRRwA0RsWNaby/gtIg4\nML3uDdRGxEpJo4E/RcRukvYDzgT2iYgVkgZHxOuSbgMmAV8FHouIyc29n7Fjd4u7Zs/ZqM/EimfQ\n7l/u6CpYF7PqqWnUrli0USFp9Jid4hdX/b1UVeLgD7zn/ojolJdsOqL3433AsZLOAt7fWEBLPgNc\nleavYt0myKZ0Ay6W9CjwZ6Duxqp9gD9ExAqAiMjf8PI7WhjQzMysc2r3oBYR/wY+AiwApkg6pqH1\nUhb3KeB/JT0P/BoYL6klV4e/DrwK7ATsBnRvwTZ3Ax+V1LMF65qZdSlS6abOrN2DmqQtgVcj4mLg\nEmDXRlbdG3gkIjaPiFERsSVwLfBJYBmQD271Xw8AFkZELXA0UJnKbyXLEnunugzObXMpcBMwTZJv\ndTCzMqKS/teZdUTz417Aw5IeBI4Ezm1kvc+QdfjIuxb4TEQsAe6S9JiknwOPADWSHpb0deB8YKKk\nh4HtgOUAEXEzMB2YkzqVnJbfeUT8AngQuDx1GjEzsy6k3TKSiOib/k4l66HY3PrHNlA2nSwoERH1\nu/B/rN7r/Ng8p+f2cTZZ78n8fvfKzX+/ubqZmXU1nb3ZsFScjZiZlbls7EeVbGr2eFJPSfem1rO5\nkn6QygdLulXSM+nvoNw2Z0iaJ+kpSfvmysdKejQtO0/N3P/TKYKapNmSHqo3vb+j62VmZhtkFfCx\niNgJ2Jmsk984slunZkbEaGBmeo2kHYAJwBhgPHB+6iwIcAFwAjA6TeNpQqfoEBERH+zoOpiZla12\n7rUY2Q3Qb6eX3dIUwCFk/Soguwx1G9nloUOAqyJiFfCcpHnAHqnne/+ImAUg6TLgUGBGY8fuFJma\nmZm1rfbu0i+pMnXIWwTcGhGzgWERsTCt8gowLM2PAF7KbT4/lY1I8/XLG+WgZmZmrTVU0pzcdGL9\nFSKiJiJ2BkaSZV071lseZNlbSXWK5kczM2tbJb6/bHFLh8mKiDcl/YvsWtirkoZHxMI0DnDdY+8X\nAJvnNhuZyhak+frljXKmZmZW5gRUqHRTs8eTNpE0MM33IhuU/kmyW7ImptUmAten+enABEk9JG1F\n1iHk3tRUuVTSuNTr8ZjcNg1ypmZmZqU2HJiaejBWANMi4gZJ95CN2nQ88AJwBEBEzJU0DXgcqAZO\niYiatK+TgSlAL7IOIo12EgEHNTOzQmjP4a0i4hFglwbKl5ANgdjQNpOB9QaUj4g5wI7rb9EwBzUz\nswLwiCJmZmZdjDM1M7MC6Oyj65eKg5qZWZmr6/1YBG5+NDOzsuFMzcys7HX+h3uWioOamVm5a+cB\njTuSmx/NzKxsOFMzMyuAgiRqDmpmZuUu6/1YjLDm5kczMysbztTMzAqgGHmag5qZWTEUJKq5+dHM\nzMqGMzUzswLwzddmZlY2CtL50c2PZmZWPpypmZkVQEESNQc1M7NCKEhUc/OjmZmVDWdqZmZlTrj3\no5mZlQs/esbMzKzrcaZmZlYABUnUHNTMzAqhIFHNzY9mZlY2nKmZmZU9ufejmZmVD/d+NDMz62Kc\nqZmZlTlRmH4iDmpmZoVQkKjm5kczMysbztTMzArAvR/NzKxsuPejmZlZF+OgZmZWACrh1OyxpM0l\n/UvS45LmSvpqKh8s6VZJz6S/g3LbnCFpnqSnJO2bKx8r6dG07Dyp6ZzTQc3MrNyVMqK1rBmzGvhm\nROwAjANOkbQDMAmYGRGjgZnpNWnZBGAMMB44X1Jl2tcFwAnA6DSNb+rADmpmZlZSEbEwIh5I88uA\nJ4ARwCHA1LTaVODQNH8IcFVErIqI54B5wB6ShgP9I2JWRARwWW6bBrmjiJlZAXRU70dJo4BdgNnA\nsIhYmBa9AgxL8yOAWbnN5qeyNWm+fnmjHNTMzMqcKHnvx6GS5uReXxQRF613XKkvcC3wtYhYmr8c\nFhEhKUpaKxzUzMys9RZHxG5NrSCpG1lA+2NE/CUVvyppeEQsTE2Li1L5AmDz3OYjU9mCNF+/vFG+\npmZmVgDt3PtRwKXAExHxi9yi6cDEND8RuD5XPkFSD0lbkXUIuTc1VS6VNC7t85jcNg1ypmZmVgTt\ne0ntv4GjgUclPZTKvgOcDUyTdDzwAnAEQETMlTQNeJys5+QpEVGTtjsZmAL0AmakqVEOamZmVlIR\ncSeNh9G9G9lmMjC5gfI5wI4tPbaDmplZAXjsRzMzKxse+9HMzKyLcaZmZlYABUnUHNTMzAqhIFHN\nzY9mZlY2nKmZmZW57KbpYqRqDmpmZuVO7v1oZmbW5ThTMzMrgIIkag5qZmaFUJCo5qC2gR544P7F\nvbrphY6uRyc0FFjc0ZWwLsfnTeO27OgKdCUOahsoIjbp6Dp0RpLmNPecJbP6fN60Nbn3o5mZlQ/3\nfjQzM+tinKlZqV3U0RWwLsnnTRtq6ROry4GDmpVURPjLyVrN5007KEhUc/OjmZmVDWdqZmYFUJTe\nj87UzMysbDhTsw4naTAwNCKe7ui6WNcjSRERHV2Pzs5d+s3agaSewKnAcZK27+j6WNchaXMAB7SW\nUQmnzsxBzTpURKwE/pFeHi5ph46sj3VekvpK6p7mtwd+JqlfB1fLOhkHNeswUtYgEhF3AtOB/sCn\nHdisPkl9gD8Ch6eiFWl6W1K3tE5nTyI6TnqeWqmmzsxBzTpE3XUQSVtJqoqIu4E/AAPIApubIm2t\niFgOXA0cK+lIYBTwTmTWpHXcDNmkYjRAuqOIdYgU0A4AzgTukPQ28CuykSWOB46S9MeIeLwj62kd\nT1JlRNRExJWSXgNOB+4HtpJ0LjAfWAVURcQvOrKu1vGcqVmHkDQO+DFwJNmPq0OBnwGvAVOBPsDq\nDqugdQopo6+R9HFJP4uIW4Fzgb3Jzo8X09++wOwOrGqnJorT/OhMzdqVpAogyJ6fdQywHfARYBJw\nInAO2S/x76YmJyuwlNHvDZwPnJTK/iapGvgG8HRE/K0j69hVdPJYVDLO1Kxd5C7i903XQW6IiIfJ\nMrQvRMQtwCKyH1rDHNBMmSpgPHBmRPyzrvdjRMwALgROlzSiI+tpnYuDmrWL3DW0mZLOknRYWrQp\ncKKkDwJ7AOdExGMdVlHrNNKPn2pgJTBOUs+IWA0gaXfgJuDgiFjQkfXsKorS/OigZu1C0nDgc2TN\ni68D+6YgdxywOfC/wE8i4pGOq6V1tLqMXtIWkkam4hlAN+B/0rKdgF8C74uI1zukol2QSvhfZ+Zr\natbmJO0G7AQsiIirJW0C7At8EugWEQdK6h0RKzzkUbHlMvqfAHdLGhwRR6RbPI6WdDrZbR8/Ss3X\nZutwULM2JWkvst6Mt5B10/9TRDwgaQbQHThE0r0R8TL4XqOiyt23OI6sF+yBZJnZ7yX9IyL2kTSF\n7MfRWxHxrH8AtVLnTrBKxkHN2oykrYDvAEdHxL8lzQOukPS5iHhQ0vXAzXUBzYonjf25JnXbHwYs\nAY4ARpP1dhwA3Cbp7ojYE3igblsHtNYpSEzzNTUrrdw1kd3JfmkPIOvhSET8DLgUmC5pbEQscUAr\nrnR7x57A1yQdSHZddRnwOHAA8PuIWEaW6W+RzimzJjmoWUmlJqSPkDUhPUp2g3VvSV9Oy/8P+C3Z\nzbJmjwCfAC4HromIV8iSioXA1pJOIGuK/HhE3Ndx1ezaStnzsSW9HyX9XtIiSY/lygZLulXSM+nv\noNyyMyTNk/SUpH1z5WMlPZqWndeS8T0d1KykJG0LfAmYEhH3A7cBM4HtJH0TICLOjojbPQBtMUnq\nI2lkRNQCW6bifwH7pW77tWRPblhBFtAujIgnOqi6ZaOdez9OIbu/MG8SMDMiRpN9J0wCSAOYTwDG\npG3Ol1SZtrkAOIGsOXp0A/tcj4Oaldr7gWHAPpI2iYi3gJuBu4FtJdV9ifmaSHGNAn4t6bvAacA3\nga+QPaWhbuzG/5AFuk9FxF/8A6hriYh/k926k3cIWVMy6e+hufKrImJVRDwHzAP2SLcB9Y+IWem7\n4rLcNo1yULONkruGNlLSgIi4hmyQ4qVko+0PSddF/gb8b0S80IHVtU4gIuaSfXF9B5idbrZ/jWwo\nrB6SZpJl+GvSzdf+AVQKpR2kf6ikObnpxBbUYFhELEzzr5D9+AUYAbyUW29+KhuR5uuXN8m9H22D\nSaqIiFpJ+5FdQ3tK0qZkv6ZuAPYju7fo8ohYQtYJwApI0kBgdUSsSEWPAf8HHCPp0YiYCTySsreP\nAy9HxKwOqm5ZKnGquzgidtvQjdO19zb5oeKgZq0mqVdEvJMC2jbA/wNOioi7JZ0H/JXs5upu6W8f\nsq7aVkCSBgNPA/+QdEdE/DYipqZlLwG/kDQReBM4rO7xMb4Prey8Kml4RCxMTYuLUvkCslGF6oxM\nZQvSfP3yJrn50VpF0gCyL6FPpKI3gSfJvrSIiFPJTrxJETGdbOSHFzukstZZvAH8ney8+JykqZJO\nkDQwIi4mu452LdkN+ms7hDiglVYnGPtxOjAxzU8Ers+VT5DUI93bOhq4NzVVLpU0Ll3mOCa3TaMc\n1Ky1+pNdD/msskeCLAWGAPvk1rmJ9Cy0XBu6FVQKTg+QXUP5CFnPuI8At0v6KFmHkA+SdQqZ0VH1\nLG+l7PvYfFST9CfgHrLOYfMlHQ+cDXxc0jNk3xdnw9prrNPI7k+8GTglImrSrk4GLiH7znmWbBzQ\npo/tH0PWEpL6pQ4fpLEbJwDjgJ8ClWRfVNcBb5ENUvxtf0FZnqSbgPPIrqfNIMvwXwe2AY6MiMUd\nWL2ytvOuu8U/7yjdM1SH9K26f2OuqbUlX1OzZkkaBVwj6X6yX1TPAH8AVpF1x/4pcDhZx5DNgK9H\nxD98TcQAJFWmX95TyAax/iVwaUSckzoWdXNAa1t1T74uAgc1a4mewHCy+0meJxsR5EJgENn9Z2cC\nkyPi3PxGDmgGkGtKmg2cBdwTEeekstd8nlgp+ZqaNSl123+SrA38LeBF4EjgZbKxHT+dXv9M0sA0\nnp/ZOlLW/gLwDaCv0tOqHdCs1JypWZNSt/2KiHhC0lHAVcCPI+JSSdeQ9VQ6BHgoIt7s0Mpah8o9\nPqYiDXW1Vi54zQdq19/a2pqbH82SXGC7T9IE4E9pjL7fAk+R3Xjt+4oKLBfQ9ibLxG6JiJX114uI\nxySdHhHN3m9kpdXZn1hdKm4qshbJBzay5sYzJZ1Sbx0HtAJKHUFC0niyAWjfaCigKVMRES9I6i1p\nSPvX1sqdg5qtIzeW43rnRi6w3Q8cBMxt7/pZ5yFpm3SrR016jMiZwBfTA2E/LGmipD1ym9QNqzaQ\n7N60wR1S8SJq50fPdCQ3P9paLWlCqpexucmx2IYBm0qaFRFvSPoXcLyyZ6BVAGtIo0NIqoqI6jQi\nzZ+Bb0XEMx1X9WJ5dxzi8udMzYCWNyHVrZ626UXWrd8KKCLuInsQ7H8k9Se7D+1e4NcRcSTZPY1j\nJHVPAW0Q2Q36P0yPJjErOQe1gmttE1LdjbSpCek2siGyrKDSKDNfJbtfcXFEnJsGtv4w2UDXl0TE\n6rT6Z8jGAr2jg6pbbKV99Eyn5eZHcxOSbZSIuF7SGuB+SWOBlWT3L34vIm6sa6KOiPM7tqbFVpTe\njw5qBRcRd0nqR9aE9AGyJqQDgPvSL+6DgWNTE9LqlM1dC3zfv7itTkTcJKmWbJT9bYHTI2Jl7jqt\nr71au3Dzo7kJyUoiIm4GvgDsUnc9ti6QOaB1PPd+tEJxE5KVQkTcCO4V2xl18lhUMg5qtpabkKxU\nfJ5YR3Hzo63DTUhmZcq9H62o3IRkVn6K0vvRmZo1ygHNzLoaZ2pmZmWuSE++ln+Mm5mVN0k3A0NL\nuMvFETG+hPsrGQc1MzMrG76mZmVLUo2khyQ9JunPknpvxL72knRDmj9Y0qQm1h0o6eQNOMZZkk5r\naXm9daZI+nQrjjVK0mOtraNZZ+egZuXsnYjYOSJ2BFYDX8wvrHtoZWt3GhHTI+LsJlYZCLQ6qJnZ\nxnNQs6K4A9gmZShPSboMeAzYXNInJN0j6YGU0fUFkDRe0pOSHgAOq9uRpM9L+k2aHybpOkkPp2lP\n4Gxg65Ql/jyt9y1J90l6RNIPcvv6rqSnJd1JdsN7kySdkPbzsKRr62Wf+0iak/Z3YFq/UtLPc8c+\naWM/SLPOzEHNyp6kKmA/smd/QfbUgfMjYgywHPgesE9E7ArMAb4hqSdwMdkTvscC72lk9+cBt0fE\nTsCuZE8DnwQ8m7LEb0n6RDrmHsDOwFhJH0nDkU1IZfsDu7fg7fwlInZPx3sCOD63bFQ6xgHAhek9\nHA+8FRG7p/2fIGmrFhzHrEtyl34rZ70kPZTm7wAuBTYDXoiIWal8HLADcJeyPs/dgXuA7YDn6h6t\nI+kK4MQGjvEx4BiAiKgB3kpPMsj7RJoeTK/7kgW5fsB1EbEiHWN6C97TjpJ+RNbE2Re4JbdsWkTU\nAs9I+k96D58APpC73jYgHfvpFhzLrMtxULNy9k5E7JwvSIFreb4IuDUiPlNvvXW220gCfhIRv6t3\njK9twL6mAIdGxMOSPg/slVtWvytzpGN/JSLywQ9Jozbg2GadnpsfrehmAf8taRsASX0kvQ94Ehgl\naeu03mca2X4m8KW0bWV6gOoysiyszi3AcblrdSMkbQr8GzhUUq/0TLuDWlDffsBCSd2Az9Vbdrik\nilTn9wJPpWN/Ka2PpPdJ6tOC45h1Sc7UrNAi4rWU8fxJUo9U/L2IeFrSicCNklaQNV/2a2AXXwUu\nknQ8UAN8KSLukXRX6jI/I11X2x64J2WKbwNHRcQDkq4GHgYWAfe1oMpnArOB19LffJ1eBO4F+gNf\nTE9YuITsWtsDyg7+GnBoyz4ds67HN1+bmVnZcPOjmZmVDQc1MzMrGw5qZmZWNhzUzMysbDiomZlZ\n2XBQMzOzsuGgZmZmZcNBzczMysb/B8Vl7+epclX7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4759bb0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:29.212628Z",
     "start_time": "2017-07-17T00:29:29.152967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_scores_all-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:29:30.005127Z",
     "start_time": "2017-07-17T00:29:29.214568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583259</td>\n",
       "      <td>0.823207</td>\n",
       "      <td>1.737384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.853132</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>2.746407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.817215</td>\n",
       "      <td>3.780096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885247</td>\n",
       "      <td>0.815274</td>\n",
       "      <td>4.823596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.886533</td>\n",
       "      <td>0.812489</td>\n",
       "      <td>5.901322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.552778</td>\n",
       "      <td>0.696460</td>\n",
       "      <td>0.657553</td>\n",
       "      <td>1.041274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.776393</td>\n",
       "      <td>0.669705</td>\n",
       "      <td>2.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.681746</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>3.030958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.834634</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>4.018355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.738228</td>\n",
       "      <td>4.996470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.855305</td>\n",
       "      <td>0.746414</td>\n",
       "      <td>5.974369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.861693</td>\n",
       "      <td>0.757637</td>\n",
       "      <td>7.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>8.021910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.724603</td>\n",
       "      <td>0.734120</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>1.510834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.771825</td>\n",
       "      <td>0.750665</td>\n",
       "      <td>0.542785</td>\n",
       "      <td>2.517899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.651308</td>\n",
       "      <td>11.737579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.830952</td>\n",
       "      <td>0.786152</td>\n",
       "      <td>0.606245</td>\n",
       "      <td>4.757215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.814807</td>\n",
       "      <td>0.652827</td>\n",
       "      <td>12.685921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.623460</td>\n",
       "      <td>6.851718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.931349</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.654430</td>\n",
       "      <td>13.714334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.806379</td>\n",
       "      <td>0.640928</td>\n",
       "      <td>8.793038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.809173</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>9.762101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.648692</td>\n",
       "      <td>10.796256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>0.778819</td>\n",
       "      <td>0.966112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.798810</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>0.768692</td>\n",
       "      <td>2.016091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.879702</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>3.078858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.892965</td>\n",
       "      <td>0.813502</td>\n",
       "      <td>4.012818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.896425</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>5.040853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.538492</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.323962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.486508</td>\n",
       "      <td>0.553584</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>1.417094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.734127</td>\n",
       "      <td>0.799104</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>2.576081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.841820</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>3.832223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.814430</td>\n",
       "      <td>5.245420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.861382</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>6.504427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.843651</td>\n",
       "      <td>0.871540</td>\n",
       "      <td>0.804473</td>\n",
       "      <td>7.869504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.799578</td>\n",
       "      <td>9.173052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>0.796540</td>\n",
       "      <td>10.463114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.749202</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>1.831090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.629705</td>\n",
       "      <td>3.321462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.803983</td>\n",
       "      <td>0.643122</td>\n",
       "      <td>4.869611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.882937</td>\n",
       "      <td>0.805492</td>\n",
       "      <td>0.641857</td>\n",
       "      <td>6.326309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.736072</td>\n",
       "      <td>0.587342</td>\n",
       "      <td>2.207851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.842486</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>4.287650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.868701</td>\n",
       "      <td>0.771308</td>\n",
       "      <td>6.335766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553968</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.559099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.423016</td>\n",
       "      <td>0.321061</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>1.659954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.414257</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>3.252145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.620238</td>\n",
       "      <td>0.485318</td>\n",
       "      <td>0.471561</td>\n",
       "      <td>5.009747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.679762</td>\n",
       "      <td>0.507630</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>7.373743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.724206</td>\n",
       "      <td>0.560903</td>\n",
       "      <td>0.440506</td>\n",
       "      <td>8.960562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.773413</td>\n",
       "      <td>0.584546</td>\n",
       "      <td>0.432068</td>\n",
       "      <td>10.671936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.483898</td>\n",
       "      <td>0.326329</td>\n",
       "      <td>2.087397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.621825</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.344473</td>\n",
       "      <td>4.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.876984</td>\n",
       "      <td>0.761089</td>\n",
       "      <td>0.572489</td>\n",
       "      <td>23.032310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.709127</td>\n",
       "      <td>0.599273</td>\n",
       "      <td>0.387173</td>\n",
       "      <td>8.734228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.858721</td>\n",
       "      <td>0.741772</td>\n",
       "      <td>42.926213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>0.654986</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>12.542290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.807044</td>\n",
       "      <td>0.652996</td>\n",
       "      <td>27.134876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>0.858854</td>\n",
       "      <td>0.742025</td>\n",
       "      <td>44.073521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.665654</td>\n",
       "      <td>29.257227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.838492</td>\n",
       "      <td>0.749068</td>\n",
       "      <td>0.554262</td>\n",
       "      <td>21.210851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.859519</td>\n",
       "      <td>0.743122</td>\n",
       "      <td>45.863152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.831840</td>\n",
       "      <td>0.692405</td>\n",
       "      <td>33.607411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.839070</td>\n",
       "      <td>0.705316</td>\n",
       "      <td>35.433949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.926587</td>\n",
       "      <td>0.861027</td>\n",
       "      <td>0.745907</td>\n",
       "      <td>49.986061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.851801</td>\n",
       "      <td>0.728861</td>\n",
       "      <td>39.395359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.739916</td>\n",
       "      <td>41.226033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.744304</td>\n",
       "      <td>47.749661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.861160</td>\n",
       "      <td>0.746160</td>\n",
       "      <td>51.940331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.861560</td>\n",
       "      <td>0.746751</td>\n",
       "      <td>54.844048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.861648</td>\n",
       "      <td>0.746920</td>\n",
       "      <td>56.632432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.861737</td>\n",
       "      <td>0.747089</td>\n",
       "      <td>58.477479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.748758</td>\n",
       "      <td>0.637637</td>\n",
       "      <td>3.137511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.792460</td>\n",
       "      <td>0.791031</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>6.090391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.848810</td>\n",
       "      <td>0.807177</td>\n",
       "      <td>0.669620</td>\n",
       "      <td>9.000678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.657890</td>\n",
       "      <td>11.817174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583259</td>\n",
       "      <td>0.823207</td>\n",
       "      <td>1.737384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.853132</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>2.746407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.817215</td>\n",
       "      <td>3.780096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885247</td>\n",
       "      <td>0.815274</td>\n",
       "      <td>4.823596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.886533</td>\n",
       "      <td>0.812489</td>\n",
       "      <td>5.901322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.552778</td>\n",
       "      <td>0.696460</td>\n",
       "      <td>0.657553</td>\n",
       "      <td>1.041274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.776393</td>\n",
       "      <td>0.669705</td>\n",
       "      <td>2.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.681746</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>3.030958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.834634</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>4.018355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.738228</td>\n",
       "      <td>4.996470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.855305</td>\n",
       "      <td>0.746414</td>\n",
       "      <td>5.974369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.861693</td>\n",
       "      <td>0.757637</td>\n",
       "      <td>7.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>8.021910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.724603</td>\n",
       "      <td>0.734120</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>1.510834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.771825</td>\n",
       "      <td>0.750665</td>\n",
       "      <td>0.542785</td>\n",
       "      <td>2.517899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.651308</td>\n",
       "      <td>11.737579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.830952</td>\n",
       "      <td>0.786152</td>\n",
       "      <td>0.606245</td>\n",
       "      <td>4.757215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.814807</td>\n",
       "      <td>0.652827</td>\n",
       "      <td>12.685921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.623460</td>\n",
       "      <td>6.851718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.931349</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.654430</td>\n",
       "      <td>13.714334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.806379</td>\n",
       "      <td>0.640928</td>\n",
       "      <td>8.793038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.809173</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>9.762101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.648692</td>\n",
       "      <td>10.796256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>0.778819</td>\n",
       "      <td>0.966112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.798810</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>0.768692</td>\n",
       "      <td>2.016091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.879702</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>3.078858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.892965</td>\n",
       "      <td>0.813502</td>\n",
       "      <td>4.012818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.896425</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>5.040853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.538492</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.323962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.486508</td>\n",
       "      <td>0.553584</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>1.417094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.734127</td>\n",
       "      <td>0.799104</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>2.576081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.841820</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>3.832223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.814430</td>\n",
       "      <td>5.245420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.861382</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>6.504427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.843651</td>\n",
       "      <td>0.871540</td>\n",
       "      <td>0.804473</td>\n",
       "      <td>7.869504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.799578</td>\n",
       "      <td>9.173052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>0.796540</td>\n",
       "      <td>10.463114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.749202</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>1.831090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.629705</td>\n",
       "      <td>3.321462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.803983</td>\n",
       "      <td>0.643122</td>\n",
       "      <td>4.869611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.882937</td>\n",
       "      <td>0.805492</td>\n",
       "      <td>0.641857</td>\n",
       "      <td>6.326309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.736072</td>\n",
       "      <td>0.587342</td>\n",
       "      <td>2.207851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.842486</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>4.287650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.868701</td>\n",
       "      <td>0.771308</td>\n",
       "      <td>6.335766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553968</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.559099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.423016</td>\n",
       "      <td>0.321061</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>1.659954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.414257</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>3.252145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.620238</td>\n",
       "      <td>0.485318</td>\n",
       "      <td>0.471561</td>\n",
       "      <td>5.009747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.679762</td>\n",
       "      <td>0.507630</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>7.373743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.724206</td>\n",
       "      <td>0.560903</td>\n",
       "      <td>0.440506</td>\n",
       "      <td>8.960562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.773413</td>\n",
       "      <td>0.584546</td>\n",
       "      <td>0.432068</td>\n",
       "      <td>10.671936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.483898</td>\n",
       "      <td>0.326329</td>\n",
       "      <td>2.087397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.621825</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.344473</td>\n",
       "      <td>4.483478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.876984</td>\n",
       "      <td>0.761089</td>\n",
       "      <td>0.572489</td>\n",
       "      <td>23.032310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.709127</td>\n",
       "      <td>0.599273</td>\n",
       "      <td>0.387173</td>\n",
       "      <td>8.734228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.858721</td>\n",
       "      <td>0.741772</td>\n",
       "      <td>42.926213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>0.654986</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>12.542290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.807044</td>\n",
       "      <td>0.652996</td>\n",
       "      <td>27.134876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>0.858854</td>\n",
       "      <td>0.742025</td>\n",
       "      <td>44.073521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.665654</td>\n",
       "      <td>29.257227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.838492</td>\n",
       "      <td>0.749068</td>\n",
       "      <td>0.554262</td>\n",
       "      <td>21.210851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.859519</td>\n",
       "      <td>0.743122</td>\n",
       "      <td>45.863152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.831840</td>\n",
       "      <td>0.692405</td>\n",
       "      <td>33.607411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.839070</td>\n",
       "      <td>0.705316</td>\n",
       "      <td>35.433949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.926587</td>\n",
       "      <td>0.861027</td>\n",
       "      <td>0.745907</td>\n",
       "      <td>49.986061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.851801</td>\n",
       "      <td>0.728861</td>\n",
       "      <td>39.395359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.739916</td>\n",
       "      <td>41.226033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.744304</td>\n",
       "      <td>47.749661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.861160</td>\n",
       "      <td>0.746160</td>\n",
       "      <td>51.940331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.861560</td>\n",
       "      <td>0.746751</td>\n",
       "      <td>54.844048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.861648</td>\n",
       "      <td>0.746920</td>\n",
       "      <td>56.632432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.861737</td>\n",
       "      <td>0.747089</td>\n",
       "      <td>58.477479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.748758</td>\n",
       "      <td>0.637637</td>\n",
       "      <td>3.137511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.792460</td>\n",
       "      <td>0.791031</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>6.090391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.848810</td>\n",
       "      <td>0.807177</td>\n",
       "      <td>0.669620</td>\n",
       "      <td>9.000678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.657890</td>\n",
       "      <td>11.817174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.609430</td>\n",
       "      <td>0.435612</td>\n",
       "      <td>1.013389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.800794</td>\n",
       "      <td>0.711631</td>\n",
       "      <td>0.468861</td>\n",
       "      <td>1.974355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>0.785442</td>\n",
       "      <td>0.596371</td>\n",
       "      <td>11.192840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>0.736781</td>\n",
       "      <td>0.511561</td>\n",
       "      <td>4.032335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.598819</td>\n",
       "      <td>12.205366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.909127</td>\n",
       "      <td>0.755279</td>\n",
       "      <td>0.543122</td>\n",
       "      <td>5.938341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951587</td>\n",
       "      <td>0.788502</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>13.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.932540</td>\n",
       "      <td>0.775018</td>\n",
       "      <td>0.579156</td>\n",
       "      <td>7.954791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>0.779143</td>\n",
       "      <td>0.586245</td>\n",
       "      <td>9.007837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.932540</td>\n",
       "      <td>0.783224</td>\n",
       "      <td>0.592405</td>\n",
       "      <td>10.062756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.333659</td>\n",
       "      <td>0.559156</td>\n",
       "      <td>1.034562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.502750</td>\n",
       "      <td>0.648945</td>\n",
       "      <td>2.082721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.947619</td>\n",
       "      <td>0.838183</td>\n",
       "      <td>0.693080</td>\n",
       "      <td>9.227789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.850794</td>\n",
       "      <td>0.781804</td>\n",
       "      <td>0.705992</td>\n",
       "      <td>3.902641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.953175</td>\n",
       "      <td>0.838361</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>10.224311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.570884</td>\n",
       "      <td>0.553080</td>\n",
       "      <td>0.995917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.734127</td>\n",
       "      <td>0.740641</td>\n",
       "      <td>0.601857</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.826587</td>\n",
       "      <td>0.788369</td>\n",
       "      <td>0.651055</td>\n",
       "      <td>2.920137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.806556</td>\n",
       "      <td>0.665232</td>\n",
       "      <td>4.063429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.811214</td>\n",
       "      <td>0.662025</td>\n",
       "      <td>5.163448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.757937</td>\n",
       "      <td>0.771425</td>\n",
       "      <td>0.584051</td>\n",
       "      <td>0.985263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.792857</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0.606920</td>\n",
       "      <td>2.065951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830159</td>\n",
       "      <td>0.791785</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>3.111821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.834921</td>\n",
       "      <td>0.800257</td>\n",
       "      <td>0.632405</td>\n",
       "      <td>4.120334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.803673</td>\n",
       "      <td>0.637890</td>\n",
       "      <td>5.114748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.805758</td>\n",
       "      <td>0.641603</td>\n",
       "      <td>6.066829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.811968</td>\n",
       "      <td>0.652152</td>\n",
       "      <td>7.070870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.531349</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.809197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>0.540454</td>\n",
       "      <td>0.759325</td>\n",
       "      <td>1.415778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.635468</td>\n",
       "      <td>0.745063</td>\n",
       "      <td>2.665348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.846966</td>\n",
       "      <td>0.724135</td>\n",
       "      <td>14.896438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.851180</td>\n",
       "      <td>0.729283</td>\n",
       "      <td>16.263383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.809127</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>0.694093</td>\n",
       "      <td>7.452541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.926984</td>\n",
       "      <td>0.854640</td>\n",
       "      <td>0.735274</td>\n",
       "      <td>17.669458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.701941</td>\n",
       "      <td>10.311922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>0.740169</td>\n",
       "      <td>19.286822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.842708</td>\n",
       "      <td>0.719072</td>\n",
       "      <td>13.495634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.861160</td>\n",
       "      <td>0.740338</td>\n",
       "      <td>27.856055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.743629</td>\n",
       "      <td>21.985776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.861737</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>29.036748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.862314</td>\n",
       "      <td>0.742447</td>\n",
       "      <td>30.103149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.947619</td>\n",
       "      <td>0.862713</td>\n",
       "      <td>0.743207</td>\n",
       "      <td>31.433573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.743544</td>\n",
       "      <td>32.769451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545238</td>\n",
       "      <td>0.433109</td>\n",
       "      <td>0.184979</td>\n",
       "      <td>1.423730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.660317</td>\n",
       "      <td>0.479196</td>\n",
       "      <td>0.196878</td>\n",
       "      <td>3.023336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.651561</td>\n",
       "      <td>16.916582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.538103</td>\n",
       "      <td>0.304641</td>\n",
       "      <td>6.090839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.838094</td>\n",
       "      <td>0.699747</td>\n",
       "      <td>18.565713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.595369</td>\n",
       "      <td>0.404895</td>\n",
       "      <td>8.980543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>0.846123</td>\n",
       "      <td>0.713502</td>\n",
       "      <td>20.113684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.746851</td>\n",
       "      <td>0.529958</td>\n",
       "      <td>12.313494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.783268</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>13.687260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.797330</td>\n",
       "      <td>0.623038</td>\n",
       "      <td>15.197614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.746825</td>\n",
       "      <td>0.613778</td>\n",
       "      <td>0.283376</td>\n",
       "      <td>2.396187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.688165</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>4.404867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.773377</td>\n",
       "      <td>0.574515</td>\n",
       "      <td>23.059733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.888492</td>\n",
       "      <td>0.753194</td>\n",
       "      <td>0.539831</td>\n",
       "      <td>8.638499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.776748</td>\n",
       "      <td>0.580759</td>\n",
       "      <td>25.213438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.898016</td>\n",
       "      <td>0.765747</td>\n",
       "      <td>0.561435</td>\n",
       "      <td>12.886271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.940079</td>\n",
       "      <td>0.778877</td>\n",
       "      <td>0.584810</td>\n",
       "      <td>27.123679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.766812</td>\n",
       "      <td>0.563038</td>\n",
       "      <td>16.882940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.781716</td>\n",
       "      <td>0.590211</td>\n",
       "      <td>29.040926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.925397</td>\n",
       "      <td>0.768408</td>\n",
       "      <td>0.565738</td>\n",
       "      <td>20.753421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.783357</td>\n",
       "      <td>0.593165</td>\n",
       "      <td>31.049070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.555952</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.699689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.489683</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>1.587398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.518254</td>\n",
       "      <td>0.612802</td>\n",
       "      <td>0.778059</td>\n",
       "      <td>3.335802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.836905</td>\n",
       "      <td>0.693444</td>\n",
       "      <td>0.592321</td>\n",
       "      <td>16.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.734127</td>\n",
       "      <td>0.671620</td>\n",
       "      <td>0.639494</td>\n",
       "      <td>8.295849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.754365</td>\n",
       "      <td>0.677564</td>\n",
       "      <td>0.600760</td>\n",
       "      <td>9.963014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.770635</td>\n",
       "      <td>0.680092</td>\n",
       "      <td>0.580675</td>\n",
       "      <td>12.484927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.831746</td>\n",
       "      <td>0.686879</td>\n",
       "      <td>0.581519</td>\n",
       "      <td>14.902415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.550397</td>\n",
       "      <td>0.441004</td>\n",
       "      <td>0.200760</td>\n",
       "      <td>1.896466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.564683</td>\n",
       "      <td>0.460699</td>\n",
       "      <td>0.237806</td>\n",
       "      <td>3.721369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830556</td>\n",
       "      <td>0.666918</td>\n",
       "      <td>0.382110</td>\n",
       "      <td>20.478017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.578175</td>\n",
       "      <td>0.508428</td>\n",
       "      <td>0.257131</td>\n",
       "      <td>7.663091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.898016</td>\n",
       "      <td>0.756343</td>\n",
       "      <td>0.542532</td>\n",
       "      <td>39.714115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.590002</td>\n",
       "      <td>0.282785</td>\n",
       "      <td>11.360219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.851984</td>\n",
       "      <td>0.691625</td>\n",
       "      <td>0.424979</td>\n",
       "      <td>24.273933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.757008</td>\n",
       "      <td>0.543797</td>\n",
       "      <td>41.467146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>0.449958</td>\n",
       "      <td>26.162183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.810317</td>\n",
       "      <td>0.651570</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>18.679377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.757674</td>\n",
       "      <td>0.545063</td>\n",
       "      <td>43.798101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.724361</td>\n",
       "      <td>0.483629</td>\n",
       "      <td>30.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.888492</td>\n",
       "      <td>0.732080</td>\n",
       "      <td>0.498228</td>\n",
       "      <td>31.975581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.758605</td>\n",
       "      <td>0.546835</td>\n",
       "      <td>47.764469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.750177</td>\n",
       "      <td>0.530970</td>\n",
       "      <td>35.611126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.903175</td>\n",
       "      <td>0.755722</td>\n",
       "      <td>0.541350</td>\n",
       "      <td>37.668527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.903175</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.546076</td>\n",
       "      <td>45.657773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.896429</td>\n",
       "      <td>0.758872</td>\n",
       "      <td>0.547342</td>\n",
       "      <td>49.662081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.759315</td>\n",
       "      <td>0.548186</td>\n",
       "      <td>52.129842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.759581</td>\n",
       "      <td>0.548692</td>\n",
       "      <td>53.982351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.887302</td>\n",
       "      <td>0.759892</td>\n",
       "      <td>0.549283</td>\n",
       "      <td>55.768187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.760114</td>\n",
       "      <td>0.549705</td>\n",
       "      <td>57.759473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.394048</td>\n",
       "      <td>0.338449</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>2.851944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.678007</td>\n",
       "      <td>0.499494</td>\n",
       "      <td>5.889095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.862698</td>\n",
       "      <td>0.807754</td>\n",
       "      <td>0.662194</td>\n",
       "      <td>8.706509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.826384</td>\n",
       "      <td>0.692152</td>\n",
       "      <td>11.832607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.837917</td>\n",
       "      <td>0.712996</td>\n",
       "      <td>14.944608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.841022</td>\n",
       "      <td>0.717637</td>\n",
       "      <td>18.392418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>0.843861</td>\n",
       "      <td>0.722447</td>\n",
       "      <td>21.449749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.844881</td>\n",
       "      <td>0.723882</td>\n",
       "      <td>24.750123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.845724</td>\n",
       "      <td>0.723207</td>\n",
       "      <td>27.521660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.532143</td>\n",
       "      <td>0.552919</td>\n",
       "      <td>0.282532</td>\n",
       "      <td>0.894731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.678968</td>\n",
       "      <td>0.635114</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>1.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.925397</td>\n",
       "      <td>0.753682</td>\n",
       "      <td>0.538312</td>\n",
       "      <td>8.936324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.810317</td>\n",
       "      <td>0.694065</td>\n",
       "      <td>0.439156</td>\n",
       "      <td>3.277389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.758162</td>\n",
       "      <td>0.546582</td>\n",
       "      <td>9.707876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.720369</td>\n",
       "      <td>0.484135</td>\n",
       "      <td>4.877893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.759448</td>\n",
       "      <td>0.549030</td>\n",
       "      <td>10.586093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.731725</td>\n",
       "      <td>0.498650</td>\n",
       "      <td>6.523361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.955159</td>\n",
       "      <td>0.760912</td>\n",
       "      <td>0.551308</td>\n",
       "      <td>11.379750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.744899</td>\n",
       "      <td>0.522447</td>\n",
       "      <td>8.160818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.761001</td>\n",
       "      <td>0.548945</td>\n",
       "      <td>13.221132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.959524</td>\n",
       "      <td>0.761844</td>\n",
       "      <td>0.550295</td>\n",
       "      <td>14.015121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.226587</td>\n",
       "      <td>0.262952</td>\n",
       "      <td>0.458650</td>\n",
       "      <td>0.809756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.322619</td>\n",
       "      <td>0.306645</td>\n",
       "      <td>0.518650</td>\n",
       "      <td>1.591012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.430669</td>\n",
       "      <td>0.567173</td>\n",
       "      <td>2.453870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.806349</td>\n",
       "      <td>0.722010</td>\n",
       "      <td>0.655865</td>\n",
       "      <td>3.231255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.856349</td>\n",
       "      <td>0.837961</td>\n",
       "      <td>0.712236</td>\n",
       "      <td>4.068311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.841111</td>\n",
       "      <td>0.709958</td>\n",
       "      <td>4.927074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.846478</td>\n",
       "      <td>0.715612</td>\n",
       "      <td>6.192449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.910317</td>\n",
       "      <td>0.847276</td>\n",
       "      <td>0.716371</td>\n",
       "      <td>7.008124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.707540</td>\n",
       "      <td>0.739798</td>\n",
       "      <td>0.532405</td>\n",
       "      <td>0.815554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.784524</td>\n",
       "      <td>0.773510</td>\n",
       "      <td>0.583038</td>\n",
       "      <td>1.616658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.784022</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>2.473630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.787970</td>\n",
       "      <td>0.601603</td>\n",
       "      <td>3.252410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.791031</td>\n",
       "      <td>0.606160</td>\n",
       "      <td>4.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.906349</td>\n",
       "      <td>0.793914</td>\n",
       "      <td>0.611899</td>\n",
       "      <td>4.837522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.613755</td>\n",
       "      <td>6.127309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.661111</td>\n",
       "      <td>0.641767</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.916941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.764461</td>\n",
       "      <td>0.629789</td>\n",
       "      <td>1.740327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.798572</td>\n",
       "      <td>0.658143</td>\n",
       "      <td>2.566799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.806733</td>\n",
       "      <td>0.659494</td>\n",
       "      <td>3.414125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.917460</td>\n",
       "      <td>0.816137</td>\n",
       "      <td>0.662616</td>\n",
       "      <td>4.222251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.185713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.524603</td>\n",
       "      <td>0.430802</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.844786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.550794</td>\n",
       "      <td>0.490995</td>\n",
       "      <td>0.185654</td>\n",
       "      <td>1.651068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.763095</td>\n",
       "      <td>0.627307</td>\n",
       "      <td>0.308861</td>\n",
       "      <td>11.345461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.570635</td>\n",
       "      <td>0.523643</td>\n",
       "      <td>0.195781</td>\n",
       "      <td>3.504687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>0.686391</td>\n",
       "      <td>0.410717</td>\n",
       "      <td>21.949679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.695635</td>\n",
       "      <td>0.574033</td>\n",
       "      <td>0.216118</td>\n",
       "      <td>5.791350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.641501</td>\n",
       "      <td>0.332743</td>\n",
       "      <td>13.551845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.412827</td>\n",
       "      <td>23.062431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.615951</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>9.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>0.622383</td>\n",
       "      <td>0.300169</td>\n",
       "      <td>10.237164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.688387</td>\n",
       "      <td>0.414515</td>\n",
       "      <td>24.189863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.836905</td>\n",
       "      <td>0.650816</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>16.295669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.821825</td>\n",
       "      <td>0.658934</td>\n",
       "      <td>0.361519</td>\n",
       "      <td>17.445726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.689762</td>\n",
       "      <td>0.417131</td>\n",
       "      <td>26.434530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.857540</td>\n",
       "      <td>0.674148</td>\n",
       "      <td>0.388439</td>\n",
       "      <td>19.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.685593</td>\n",
       "      <td>0.409198</td>\n",
       "      <td>20.869996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.689053</td>\n",
       "      <td>0.415781</td>\n",
       "      <td>25.303043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.690383</td>\n",
       "      <td>0.418312</td>\n",
       "      <td>27.565497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.691315</td>\n",
       "      <td>0.420084</td>\n",
       "      <td>28.660675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.895635</td>\n",
       "      <td>0.692424</td>\n",
       "      <td>0.422194</td>\n",
       "      <td>29.767123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.903968</td>\n",
       "      <td>0.694154</td>\n",
       "      <td>0.425485</td>\n",
       "      <td>30.917812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.695263</td>\n",
       "      <td>0.427595</td>\n",
       "      <td>32.010602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.683730</td>\n",
       "      <td>0.506875</td>\n",
       "      <td>0.259662</td>\n",
       "      <td>1.216905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.740079</td>\n",
       "      <td>0.587873</td>\n",
       "      <td>0.297553</td>\n",
       "      <td>2.472625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.803362</td>\n",
       "      <td>0.632574</td>\n",
       "      <td>13.271930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.832937</td>\n",
       "      <td>0.689097</td>\n",
       "      <td>0.431983</td>\n",
       "      <td>4.909210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>0.813298</td>\n",
       "      <td>0.651814</td>\n",
       "      <td>14.483956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.520675</td>\n",
       "      <td>7.311148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.819508</td>\n",
       "      <td>0.663460</td>\n",
       "      <td>15.621841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.830199</td>\n",
       "      <td>0.683291</td>\n",
       "      <td>25.742428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.940873</td>\n",
       "      <td>0.824033</td>\n",
       "      <td>0.671899</td>\n",
       "      <td>16.823827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.795422</td>\n",
       "      <td>0.617806</td>\n",
       "      <td>12.062848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.830332</td>\n",
       "      <td>0.683460</td>\n",
       "      <td>27.110594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.827537</td>\n",
       "      <td>0.678312</td>\n",
       "      <td>19.163795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.828735</td>\n",
       "      <td>0.680759</td>\n",
       "      <td>20.362294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>0.683713</td>\n",
       "      <td>29.460092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.829933</td>\n",
       "      <td>0.683122</td>\n",
       "      <td>22.672202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.830154</td>\n",
       "      <td>0.683122</td>\n",
       "      <td>23.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.940873</td>\n",
       "      <td>0.830420</td>\n",
       "      <td>0.683629</td>\n",
       "      <td>28.270326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.830509</td>\n",
       "      <td>0.683797</td>\n",
       "      <td>30.650142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.830554</td>\n",
       "      <td>0.683882</td>\n",
       "      <td>31.840021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.940079</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.683966</td>\n",
       "      <td>33.030833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.931349</td>\n",
       "      <td>0.830642</td>\n",
       "      <td>0.684051</td>\n",
       "      <td>34.194960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.813520</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>1.606288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.823016</td>\n",
       "      <td>0.890924</td>\n",
       "      <td>0.859409</td>\n",
       "      <td>3.097433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.866270</td>\n",
       "      <td>0.907337</td>\n",
       "      <td>0.855359</td>\n",
       "      <td>4.684720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.548810</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.428904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.345238</td>\n",
       "      <td>0.349184</td>\n",
       "      <td>0.470042</td>\n",
       "      <td>1.460850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.511533</td>\n",
       "      <td>0.453249</td>\n",
       "      <td>2.885396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.782143</td>\n",
       "      <td>0.665720</td>\n",
       "      <td>0.482194</td>\n",
       "      <td>15.451152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.696032</td>\n",
       "      <td>0.603043</td>\n",
       "      <td>0.452152</td>\n",
       "      <td>5.598635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.704888</td>\n",
       "      <td>0.466329</td>\n",
       "      <td>16.918474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.729762</td>\n",
       "      <td>0.612535</td>\n",
       "      <td>0.457384</td>\n",
       "      <td>8.406757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.705864</td>\n",
       "      <td>0.466160</td>\n",
       "      <td>18.299810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.623093</td>\n",
       "      <td>0.467426</td>\n",
       "      <td>11.201339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.706441</td>\n",
       "      <td>0.466413</td>\n",
       "      <td>19.644307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.782143</td>\n",
       "      <td>0.631476</td>\n",
       "      <td>0.479831</td>\n",
       "      <td>14.077168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.798016</td>\n",
       "      <td>0.706884</td>\n",
       "      <td>0.466076</td>\n",
       "      <td>21.059023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.817064</td>\n",
       "      <td>0.709457</td>\n",
       "      <td>0.470127</td>\n",
       "      <td>22.393581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.816270</td>\n",
       "      <td>0.712074</td>\n",
       "      <td>0.474599</td>\n",
       "      <td>23.821218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.815873</td>\n",
       "      <td>0.712385</td>\n",
       "      <td>0.474008</td>\n",
       "      <td>25.311138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.834127</td>\n",
       "      <td>0.715002</td>\n",
       "      <td>0.476709</td>\n",
       "      <td>26.797488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.844841</td>\n",
       "      <td>0.715268</td>\n",
       "      <td>0.476624</td>\n",
       "      <td>34.629877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.664683</td>\n",
       "      <td>0.483721</td>\n",
       "      <td>0.208101</td>\n",
       "      <td>1.624394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.651984</td>\n",
       "      <td>0.487225</td>\n",
       "      <td>0.213671</td>\n",
       "      <td>3.194861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.784127</td>\n",
       "      <td>0.629347</td>\n",
       "      <td>0.319494</td>\n",
       "      <td>17.155831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.690873</td>\n",
       "      <td>0.492459</td>\n",
       "      <td>0.222532</td>\n",
       "      <td>6.421164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.675257</td>\n",
       "      <td>0.390295</td>\n",
       "      <td>31.138687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.706746</td>\n",
       "      <td>0.568311</td>\n",
       "      <td>0.241350</td>\n",
       "      <td>9.491996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.804365</td>\n",
       "      <td>0.641856</td>\n",
       "      <td>0.337468</td>\n",
       "      <td>20.202149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.898016</td>\n",
       "      <td>0.675790</td>\n",
       "      <td>0.391139</td>\n",
       "      <td>32.651873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.648332</td>\n",
       "      <td>0.347257</td>\n",
       "      <td>21.835074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.745635</td>\n",
       "      <td>0.625399</td>\n",
       "      <td>0.314262</td>\n",
       "      <td>15.704401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.888492</td>\n",
       "      <td>0.676455</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>34.194099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.828175</td>\n",
       "      <td>0.664212</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>24.864797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.864683</td>\n",
       "      <td>0.668870</td>\n",
       "      <td>0.379241</td>\n",
       "      <td>28.021445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.889683</td>\n",
       "      <td>0.674459</td>\n",
       "      <td>0.389030</td>\n",
       "      <td>29.626286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.887302</td>\n",
       "      <td>0.676588</td>\n",
       "      <td>0.392658</td>\n",
       "      <td>35.847988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.677120</td>\n",
       "      <td>0.393502</td>\n",
       "      <td>37.454581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.890079</td>\n",
       "      <td>0.677741</td>\n",
       "      <td>0.394515</td>\n",
       "      <td>39.075829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.678850</td>\n",
       "      <td>0.396456</td>\n",
       "      <td>40.641193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.679915</td>\n",
       "      <td>0.398312</td>\n",
       "      <td>42.206608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.680447</td>\n",
       "      <td>0.399409</td>\n",
       "      <td>43.848318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.681201</td>\n",
       "      <td>0.400759</td>\n",
       "      <td>45.428985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.759127</td>\n",
       "      <td>0.832328</td>\n",
       "      <td>0.750802</td>\n",
       "      <td>2.225065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.784524</td>\n",
       "      <td>0.875754</td>\n",
       "      <td>0.817975</td>\n",
       "      <td>4.425601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.828968</td>\n",
       "      <td>0.886666</td>\n",
       "      <td>0.823797</td>\n",
       "      <td>6.734693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.846825</td>\n",
       "      <td>0.898998</td>\n",
       "      <td>0.836962</td>\n",
       "      <td>8.911242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.570440</td>\n",
       "      <td>0.809620</td>\n",
       "      <td>0.853173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.768364</td>\n",
       "      <td>0.816118</td>\n",
       "      <td>1.723129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>0.840224</td>\n",
       "      <td>0.803122</td>\n",
       "      <td>2.562127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.767063</td>\n",
       "      <td>0.847809</td>\n",
       "      <td>0.794852</td>\n",
       "      <td>3.373452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.796032</td>\n",
       "      <td>0.850293</td>\n",
       "      <td>0.774684</td>\n",
       "      <td>4.169492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.856746</td>\n",
       "      <td>0.876508</td>\n",
       "      <td>0.795105</td>\n",
       "      <td>5.030710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>0.624423</td>\n",
       "      <td>0.391224</td>\n",
       "      <td>0.829649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.610317</td>\n",
       "      <td>0.637819</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>1.595178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.785353</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>9.353316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.665079</td>\n",
       "      <td>0.670866</td>\n",
       "      <td>0.464810</td>\n",
       "      <td>3.260125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.951190</td>\n",
       "      <td>0.787216</td>\n",
       "      <td>0.602447</td>\n",
       "      <td>10.201704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.741395</td>\n",
       "      <td>0.523376</td>\n",
       "      <td>4.934201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.876984</td>\n",
       "      <td>0.751907</td>\n",
       "      <td>0.542278</td>\n",
       "      <td>5.757595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.759936</td>\n",
       "      <td>0.556540</td>\n",
       "      <td>6.686191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.767344</td>\n",
       "      <td>0.569283</td>\n",
       "      <td>7.540108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.775816</td>\n",
       "      <td>0.584557</td>\n",
       "      <td>8.430636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.650397</td>\n",
       "      <td>0.630944</td>\n",
       "      <td>0.432405</td>\n",
       "      <td>0.831454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.781746</td>\n",
       "      <td>0.711276</td>\n",
       "      <td>0.470464</td>\n",
       "      <td>1.628624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.760956</td>\n",
       "      <td>0.550548</td>\n",
       "      <td>8.921503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.734209</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>3.328953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.739753</td>\n",
       "      <td>0.515612</td>\n",
       "      <td>4.109403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>0.531730</td>\n",
       "      <td>4.942526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.749246</td>\n",
       "      <td>0.531899</td>\n",
       "      <td>6.354565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.761489</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>10.726503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.758561</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>8.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.962302</td>\n",
       "      <td>0.763840</td>\n",
       "      <td>0.553249</td>\n",
       "      <td>11.594158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.575794</td>\n",
       "      <td>0.571372</td>\n",
       "      <td>0.478397</td>\n",
       "      <td>0.888204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.730952</td>\n",
       "      <td>0.705154</td>\n",
       "      <td>0.498397</td>\n",
       "      <td>1.795293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.795378</td>\n",
       "      <td>0.614008</td>\n",
       "      <td>9.253452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.864683</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.552236</td>\n",
       "      <td>3.401129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.795999</td>\n",
       "      <td>0.614852</td>\n",
       "      <td>10.054116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.885317</td>\n",
       "      <td>0.774441</td>\n",
       "      <td>0.582869</td>\n",
       "      <td>5.085820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>0.798128</td>\n",
       "      <td>0.618734</td>\n",
       "      <td>10.973660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.926587</td>\n",
       "      <td>0.789789</td>\n",
       "      <td>0.605401</td>\n",
       "      <td>6.692780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.792938</td>\n",
       "      <td>0.610464</td>\n",
       "      <td>7.556019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.794801</td>\n",
       "      <td>0.613418</td>\n",
       "      <td>8.391512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.534524</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.132610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.560714</td>\n",
       "      <td>0.618213</td>\n",
       "      <td>0.383629</td>\n",
       "      <td>0.830756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.637331</td>\n",
       "      <td>0.416287</td>\n",
       "      <td>1.636556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.750976</td>\n",
       "      <td>0.540760</td>\n",
       "      <td>11.132442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.747619</td>\n",
       "      <td>0.688032</td>\n",
       "      <td>0.435021</td>\n",
       "      <td>3.608488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.912698</td>\n",
       "      <td>0.784998</td>\n",
       "      <td>0.600675</td>\n",
       "      <td>21.808366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.700231</td>\n",
       "      <td>0.455612</td>\n",
       "      <td>5.832074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.766324</td>\n",
       "      <td>0.568354</td>\n",
       "      <td>13.330746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.785176</td>\n",
       "      <td>0.600928</td>\n",
       "      <td>22.864484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.886111</td>\n",
       "      <td>0.771691</td>\n",
       "      <td>0.578397</td>\n",
       "      <td>14.415001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>0.740108</td>\n",
       "      <td>0.524219</td>\n",
       "      <td>10.040499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.917460</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.601013</td>\n",
       "      <td>23.909596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.778211</td>\n",
       "      <td>0.590042</td>\n",
       "      <td>16.491284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.780429</td>\n",
       "      <td>0.593502</td>\n",
       "      <td>17.593741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.785353</td>\n",
       "      <td>0.600928</td>\n",
       "      <td>26.096862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.783934</td>\n",
       "      <td>0.599409</td>\n",
       "      <td>19.757640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.784865</td>\n",
       "      <td>0.600422</td>\n",
       "      <td>20.776468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785309</td>\n",
       "      <td>0.601097</td>\n",
       "      <td>25.002603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.785531</td>\n",
       "      <td>0.601181</td>\n",
       "      <td>27.154573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.785575</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>28.818199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0.601772</td>\n",
       "      <td>29.927739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.751984</td>\n",
       "      <td>0.526437</td>\n",
       "      <td>0.309958</td>\n",
       "      <td>1.155823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.803968</td>\n",
       "      <td>0.600115</td>\n",
       "      <td>0.382110</td>\n",
       "      <td>2.306064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.928175</td>\n",
       "      <td>0.775949</td>\n",
       "      <td>0.582616</td>\n",
       "      <td>12.056652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.873810</td>\n",
       "      <td>0.719526</td>\n",
       "      <td>0.492321</td>\n",
       "      <td>4.633337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.928968</td>\n",
       "      <td>0.781849</td>\n",
       "      <td>0.593418</td>\n",
       "      <td>13.251858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.745121</td>\n",
       "      <td>0.534599</td>\n",
       "      <td>6.861597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.940873</td>\n",
       "      <td>0.787482</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>14.374617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.761755</td>\n",
       "      <td>0.558903</td>\n",
       "      <td>8.770915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.789966</td>\n",
       "      <td>0.607764</td>\n",
       "      <td>15.494554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.914683</td>\n",
       "      <td>0.772134</td>\n",
       "      <td>0.576118</td>\n",
       "      <td>10.928107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.943254</td>\n",
       "      <td>0.791430</td>\n",
       "      <td>0.610464</td>\n",
       "      <td>16.673812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.792406</td>\n",
       "      <td>0.611646</td>\n",
       "      <td>17.841789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.795821</td>\n",
       "      <td>0.617806</td>\n",
       "      <td>19.042862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.795955</td>\n",
       "      <td>0.617384</td>\n",
       "      <td>20.175138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.949603</td>\n",
       "      <td>0.796443</td>\n",
       "      <td>0.617975</td>\n",
       "      <td>21.317807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.812698</td>\n",
       "      <td>0.855527</td>\n",
       "      <td>0.788439</td>\n",
       "      <td>1.509054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.857540</td>\n",
       "      <td>0.888795</td>\n",
       "      <td>0.822954</td>\n",
       "      <td>2.939840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.337012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"19\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.528968</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.339978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.724603</td>\n",
       "      <td>0.609120</td>\n",
       "      <td>0.283207</td>\n",
       "      <td>14.178145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.690873</td>\n",
       "      <td>0.572037</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>7.297212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.745635</td>\n",
       "      <td>0.613778</td>\n",
       "      <td>0.291477</td>\n",
       "      <td>15.507364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.697619</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>10.018889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.582417</td>\n",
       "      <td>0.234093</td>\n",
       "      <td>11.364162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.815873</td>\n",
       "      <td>0.643364</td>\n",
       "      <td>0.344388</td>\n",
       "      <td>24.305655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.643542</td>\n",
       "      <td>0.344557</td>\n",
       "      <td>26.380527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.783730</td>\n",
       "      <td>0.626730</td>\n",
       "      <td>0.313840</td>\n",
       "      <td>17.905989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.773016</td>\n",
       "      <td>0.631742</td>\n",
       "      <td>0.323122</td>\n",
       "      <td>18.965414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.644029</td>\n",
       "      <td>0.345232</td>\n",
       "      <td>29.082980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.800397</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>0.340253</td>\n",
       "      <td>21.614187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.811905</td>\n",
       "      <td>0.643142</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>23.008507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.816270</td>\n",
       "      <td>0.643808</td>\n",
       "      <td>0.344895</td>\n",
       "      <td>27.748586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.799603</td>\n",
       "      <td>0.644296</td>\n",
       "      <td>0.345570</td>\n",
       "      <td>30.551478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>0.644606</td>\n",
       "      <td>0.346076</td>\n",
       "      <td>31.993179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.810317</td>\n",
       "      <td>0.644784</td>\n",
       "      <td>0.346413</td>\n",
       "      <td>33.392151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.644872</td>\n",
       "      <td>0.346498</td>\n",
       "      <td>34.839391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.645138</td>\n",
       "      <td>0.347004</td>\n",
       "      <td>36.295973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.552075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>4.564710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.793254</td>\n",
       "      <td>0.640880</td>\n",
       "      <td>0.338650</td>\n",
       "      <td>29.554761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.776587</td>\n",
       "      <td>0.642033</td>\n",
       "      <td>0.340759</td>\n",
       "      <td>31.047097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.654365</td>\n",
       "      <td>0.483854</td>\n",
       "      <td>0.210295</td>\n",
       "      <td>18.755310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.549603</td>\n",
       "      <td>0.435238</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>12.362426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.439186</td>\n",
       "      <td>0.196287</td>\n",
       "      <td>13.932996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.609127</td>\n",
       "      <td>0.466332</td>\n",
       "      <td>0.199241</td>\n",
       "      <td>17.042776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.787302</td>\n",
       "      <td>0.643231</td>\n",
       "      <td>0.342954</td>\n",
       "      <td>32.574180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.690873</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>0.251561</td>\n",
       "      <td>21.784815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.725397</td>\n",
       "      <td>0.600648</td>\n",
       "      <td>0.266498</td>\n",
       "      <td>23.275492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.782540</td>\n",
       "      <td>0.645183</td>\n",
       "      <td>0.346329</td>\n",
       "      <td>35.753286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.626153</td>\n",
       "      <td>0.312236</td>\n",
       "      <td>26.442977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.788095</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.336540</td>\n",
       "      <td>28.004313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.792460</td>\n",
       "      <td>0.644384</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>34.159954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.645981</td>\n",
       "      <td>0.347511</td>\n",
       "      <td>37.346119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.646780</td>\n",
       "      <td>0.348776</td>\n",
       "      <td>38.887693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.647667</td>\n",
       "      <td>0.350295</td>\n",
       "      <td>40.420472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.648687</td>\n",
       "      <td>0.351730</td>\n",
       "      <td>41.793969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.794048</td>\n",
       "      <td>0.649885</td>\n",
       "      <td>0.353586</td>\n",
       "      <td>42.929612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.572791</td>\n",
       "      <td>0.810295</td>\n",
       "      <td>2.196937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.676984</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.799072</td>\n",
       "      <td>4.308048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.751190</td>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.780338</td>\n",
       "      <td>6.438229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.836098</td>\n",
       "      <td>0.757215</td>\n",
       "      <td>8.596534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.836905</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.779072</td>\n",
       "      <td>10.736500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.869322</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>12.943809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.875397</td>\n",
       "      <td>0.870963</td>\n",
       "      <td>0.779662</td>\n",
       "      <td>15.156112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.872028</td>\n",
       "      <td>0.775443</td>\n",
       "      <td>17.370901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.590873</td>\n",
       "      <td>0.628105</td>\n",
       "      <td>0.682278</td>\n",
       "      <td>0.815069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.748016</td>\n",
       "      <td>0.755367</td>\n",
       "      <td>0.700506</td>\n",
       "      <td>1.626414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.851801</td>\n",
       "      <td>0.743629</td>\n",
       "      <td>4.881256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.857168</td>\n",
       "      <td>0.751308</td>\n",
       "      <td>5.683805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.858543</td>\n",
       "      <td>0.750464</td>\n",
       "      <td>6.550267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.723413</td>\n",
       "      <td>0.684129</td>\n",
       "      <td>0.489030</td>\n",
       "      <td>0.884177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.789683</td>\n",
       "      <td>0.709856</td>\n",
       "      <td>0.468945</td>\n",
       "      <td>1.626995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>0.720990</td>\n",
       "      <td>0.486835</td>\n",
       "      <td>2.474918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>0.741838</td>\n",
       "      <td>0.523544</td>\n",
       "      <td>3.262308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.749379</td>\n",
       "      <td>0.536203</td>\n",
       "      <td>4.056794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.759138</td>\n",
       "      <td>0.552405</td>\n",
       "      <td>4.833454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.770981</td>\n",
       "      <td>0.570211</td>\n",
       "      <td>5.670228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.917063</td>\n",
       "      <td>0.780474</td>\n",
       "      <td>0.587764</td>\n",
       "      <td>6.450147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.786329</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>7.243138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>0.789257</td>\n",
       "      <td>0.603713</td>\n",
       "      <td>8.060955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.582143</td>\n",
       "      <td>0.657159</td>\n",
       "      <td>0.669789</td>\n",
       "      <td>0.788849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.812302</td>\n",
       "      <td>0.814363</td>\n",
       "      <td>0.710464</td>\n",
       "      <td>1.604884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>0.825852</td>\n",
       "      <td>0.696709</td>\n",
       "      <td>2.400253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.713492</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.538481</td>\n",
       "      <td>0.863470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.802778</td>\n",
       "      <td>0.776703</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>1.678402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.789611</td>\n",
       "      <td>0.620422</td>\n",
       "      <td>2.471157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.875794</td>\n",
       "      <td>0.803407</td>\n",
       "      <td>0.638734</td>\n",
       "      <td>3.256718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.805004</td>\n",
       "      <td>0.638987</td>\n",
       "      <td>4.119869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.810681</td>\n",
       "      <td>0.647511</td>\n",
       "      <td>4.896393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>0.817912</td>\n",
       "      <td>0.660675</td>\n",
       "      <td>5.754048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.820174</td>\n",
       "      <td>0.663882</td>\n",
       "      <td>6.539913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.948810</td>\n",
       "      <td>0.822214</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.365450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.822569</td>\n",
       "      <td>0.666751</td>\n",
       "      <td>8.129808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.110077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.482937</td>\n",
       "      <td>0.583259</td>\n",
       "      <td>0.817890</td>\n",
       "      <td>1.107688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.636799</td>\n",
       "      <td>0.806835</td>\n",
       "      <td>2.236085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.742016</td>\n",
       "      <td>0.805401</td>\n",
       "      <td>3.375321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.683730</td>\n",
       "      <td>0.800302</td>\n",
       "      <td>0.800422</td>\n",
       "      <td>4.415340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.864132</td>\n",
       "      <td>0.832743</td>\n",
       "      <td>5.555745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.866927</td>\n",
       "      <td>0.817046</td>\n",
       "      <td>6.661717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.868346</td>\n",
       "      <td>0.792321</td>\n",
       "      <td>8.268404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.877928</td>\n",
       "      <td>0.787511</td>\n",
       "      <td>13.263709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.875798</td>\n",
       "      <td>0.786160</td>\n",
       "      <td>12.189661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>0.788354</td>\n",
       "      <td>14.208657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.914683</td>\n",
       "      <td>0.879791</td>\n",
       "      <td>0.788270</td>\n",
       "      <td>15.938927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.661285</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>1.223749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.604365</td>\n",
       "      <td>0.704844</td>\n",
       "      <td>0.805232</td>\n",
       "      <td>2.482930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.869588</td>\n",
       "      <td>0.761857</td>\n",
       "      <td>12.638566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.750794</td>\n",
       "      <td>0.835522</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>4.846637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>0.870476</td>\n",
       "      <td>0.761688</td>\n",
       "      <td>13.807913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.924206</td>\n",
       "      <td>0.871496</td>\n",
       "      <td>0.762363</td>\n",
       "      <td>14.972590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.862935</td>\n",
       "      <td>0.771392</td>\n",
       "      <td>9.065159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.866661</td>\n",
       "      <td>0.763629</td>\n",
       "      <td>10.282121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.868346</td>\n",
       "      <td>0.761857</td>\n",
       "      <td>11.469874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.738889</td>\n",
       "      <td>0.755057</td>\n",
       "      <td>0.678397</td>\n",
       "      <td>1.503476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.773909</td>\n",
       "      <td>0.610802</td>\n",
       "      <td>2.965688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.842063</td>\n",
       "      <td>0.781006</td>\n",
       "      <td>0.600591</td>\n",
       "      <td>4.469086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.904365</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.605907</td>\n",
       "      <td>6.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.532540</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.406102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.606746</td>\n",
       "      <td>0.689363</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>1.546546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.750794</td>\n",
       "      <td>0.821815</td>\n",
       "      <td>0.783713</td>\n",
       "      <td>2.926043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.859697</td>\n",
       "      <td>0.742954</td>\n",
       "      <td>14.970328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.812302</td>\n",
       "      <td>0.830642</td>\n",
       "      <td>0.723544</td>\n",
       "      <td>6.420013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.840873</td>\n",
       "      <td>0.837030</td>\n",
       "      <td>0.726329</td>\n",
       "      <td>7.906267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.851845</td>\n",
       "      <td>0.748861</td>\n",
       "      <td>9.324220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.873810</td>\n",
       "      <td>0.852244</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>10.695553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.856325</td>\n",
       "      <td>0.748523</td>\n",
       "      <td>12.195689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.859475</td>\n",
       "      <td>0.746160</td>\n",
       "      <td>13.659519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"21\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.687698</td>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.276034</td>\n",
       "      <td>1.662758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.723413</td>\n",
       "      <td>0.533756</td>\n",
       "      <td>0.302616</td>\n",
       "      <td>3.245816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.699743</td>\n",
       "      <td>0.503629</td>\n",
       "      <td>17.418769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.748016</td>\n",
       "      <td>0.559794</td>\n",
       "      <td>0.347595</td>\n",
       "      <td>6.329365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857937</td>\n",
       "      <td>0.718107</td>\n",
       "      <td>0.510970</td>\n",
       "      <td>18.917744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.758730</td>\n",
       "      <td>0.584235</td>\n",
       "      <td>0.381350</td>\n",
       "      <td>9.515561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.871032</td>\n",
       "      <td>0.741927</td>\n",
       "      <td>0.533165</td>\n",
       "      <td>20.516767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.780740</td>\n",
       "      <td>0.592911</td>\n",
       "      <td>33.884772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.873413</td>\n",
       "      <td>0.753637</td>\n",
       "      <td>0.546329</td>\n",
       "      <td>22.121617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.694021</td>\n",
       "      <td>0.496034</td>\n",
       "      <td>15.874558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.914683</td>\n",
       "      <td>0.781183</td>\n",
       "      <td>0.593755</td>\n",
       "      <td>35.417294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.875794</td>\n",
       "      <td>0.759404</td>\n",
       "      <td>0.555190</td>\n",
       "      <td>25.144929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.882937</td>\n",
       "      <td>0.763840</td>\n",
       "      <td>0.562363</td>\n",
       "      <td>26.802735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.781405</td>\n",
       "      <td>0.594177</td>\n",
       "      <td>37.753927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.778744</td>\n",
       "      <td>0.589536</td>\n",
       "      <td>29.976984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.906746</td>\n",
       "      <td>0.780651</td>\n",
       "      <td>0.592911</td>\n",
       "      <td>31.512580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.594684</td>\n",
       "      <td>39.392730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.781982</td>\n",
       "      <td>0.595190</td>\n",
       "      <td>40.967759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.782159</td>\n",
       "      <td>0.595190</td>\n",
       "      <td>42.512189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.782603</td>\n",
       "      <td>0.595612</td>\n",
       "      <td>44.022404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.783224</td>\n",
       "      <td>0.596456</td>\n",
       "      <td>45.668234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.718950</td>\n",
       "      <td>0.694008</td>\n",
       "      <td>2.368267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.835610</td>\n",
       "      <td>0.739578</td>\n",
       "      <td>4.307094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.803968</td>\n",
       "      <td>0.869189</td>\n",
       "      <td>0.779494</td>\n",
       "      <td>5.914198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.840079</td>\n",
       "      <td>0.874468</td>\n",
       "      <td>0.782700</td>\n",
       "      <td>7.686258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.854365</td>\n",
       "      <td>0.883650</td>\n",
       "      <td>0.796793</td>\n",
       "      <td>9.796137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>0.889549</td>\n",
       "      <td>0.805654</td>\n",
       "      <td>11.863852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.892610</td>\n",
       "      <td>0.807595</td>\n",
       "      <td>14.130155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.892787</td>\n",
       "      <td>0.806582</td>\n",
       "      <td>16.372354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.685282</td>\n",
       "      <td>0.575527</td>\n",
       "      <td>0.808839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.750397</td>\n",
       "      <td>0.770804</td>\n",
       "      <td>0.596540</td>\n",
       "      <td>1.658094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.810317</td>\n",
       "      <td>0.794624</td>\n",
       "      <td>0.622447</td>\n",
       "      <td>2.427193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.850397</td>\n",
       "      <td>0.811702</td>\n",
       "      <td>0.653080</td>\n",
       "      <td>3.195342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"13\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.560714</td>\n",
       "      <td>0.480749</td>\n",
       "      <td>0.284051</td>\n",
       "      <td>0.869449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.581349</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.351646</td>\n",
       "      <td>1.660651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.789744</td>\n",
       "      <td>0.604810</td>\n",
       "      <td>8.973582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.574610</td>\n",
       "      <td>0.432152</td>\n",
       "      <td>3.296970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.794313</td>\n",
       "      <td>0.613249</td>\n",
       "      <td>9.758208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.739975</td>\n",
       "      <td>0.518143</td>\n",
       "      <td>4.891582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.797019</td>\n",
       "      <td>0.618397</td>\n",
       "      <td>10.618801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.762065</td>\n",
       "      <td>0.553502</td>\n",
       "      <td>6.562347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.803407</td>\n",
       "      <td>0.629114</td>\n",
       "      <td>11.382821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.780341</td>\n",
       "      <td>0.587004</td>\n",
       "      <td>8.183582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.805092</td>\n",
       "      <td>0.632321</td>\n",
       "      <td>12.163382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.955952</td>\n",
       "      <td>0.805935</td>\n",
       "      <td>0.633755</td>\n",
       "      <td>12.949762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.957540</td>\n",
       "      <td>0.806822</td>\n",
       "      <td>0.634684</td>\n",
       "      <td>13.772568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.649206</td>\n",
       "      <td>0.655917</td>\n",
       "      <td>0.439916</td>\n",
       "      <td>0.770497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.673810</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>0.459747</td>\n",
       "      <td>1.556703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.784998</td>\n",
       "      <td>0.599240</td>\n",
       "      <td>8.939052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.836905</td>\n",
       "      <td>0.730350</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>3.214957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.788591</td>\n",
       "      <td>0.605485</td>\n",
       "      <td>9.761599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.842063</td>\n",
       "      <td>0.743391</td>\n",
       "      <td>0.526245</td>\n",
       "      <td>4.847553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.940079</td>\n",
       "      <td>0.790632</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>10.591540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.882937</td>\n",
       "      <td>0.765791</td>\n",
       "      <td>0.565823</td>\n",
       "      <td>6.441645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.901984</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>0.574937</td>\n",
       "      <td>7.283155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.777236</td>\n",
       "      <td>0.584979</td>\n",
       "      <td>8.117945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.790987</td>\n",
       "      <td>0.606498</td>\n",
       "      <td>12.410293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.791253</td>\n",
       "      <td>0.606835</td>\n",
       "      <td>13.192645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.480571</td>\n",
       "      <td>0.303207</td>\n",
       "      <td>0.815238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.739683</td>\n",
       "      <td>0.560903</td>\n",
       "      <td>0.355190</td>\n",
       "      <td>1.603127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.780556</td>\n",
       "      <td>0.582949</td>\n",
       "      <td>0.391814</td>\n",
       "      <td>2.419888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.853571</td>\n",
       "      <td>0.686968</td>\n",
       "      <td>0.432152</td>\n",
       "      <td>3.267168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.756476</td>\n",
       "      <td>0.541688</td>\n",
       "      <td>14.314391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.712207</td>\n",
       "      <td>0.474852</td>\n",
       "      <td>4.960418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.955952</td>\n",
       "      <td>0.746451</td>\n",
       "      <td>0.527679</td>\n",
       "      <td>8.596934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.737092</td>\n",
       "      <td>0.518059</td>\n",
       "      <td>6.169476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.748181</td>\n",
       "      <td>0.529283</td>\n",
       "      <td>9.370682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.756609</td>\n",
       "      <td>0.541857</td>\n",
       "      <td>15.632658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.753770</td>\n",
       "      <td>0.538987</td>\n",
       "      <td>11.019704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.959921</td>\n",
       "      <td>0.756033</td>\n",
       "      <td>0.541941</td>\n",
       "      <td>11.794474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.967460</td>\n",
       "      <td>0.756432</td>\n",
       "      <td>0.541688</td>\n",
       "      <td>13.534852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.959524</td>\n",
       "      <td>0.756698</td>\n",
       "      <td>0.542025</td>\n",
       "      <td>16.480554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.966270</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.542194</td>\n",
       "      <td>17.254168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.136044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.478487</td>\n",
       "      <td>0.331983</td>\n",
       "      <td>1.153312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.536684</td>\n",
       "      <td>0.356371</td>\n",
       "      <td>2.236713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.777014</td>\n",
       "      <td>0.595190</td>\n",
       "      <td>11.856152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.616661</td>\n",
       "      <td>0.471055</td>\n",
       "      <td>4.481458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.810317</td>\n",
       "      <td>0.684129</td>\n",
       "      <td>0.490042</td>\n",
       "      <td>6.198583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.780385</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>14.076809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.932936</td>\n",
       "      <td>0.795067</td>\n",
       "      <td>0.616456</td>\n",
       "      <td>23.555041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.782425</td>\n",
       "      <td>0.598987</td>\n",
       "      <td>15.148457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.768408</td>\n",
       "      <td>0.582363</td>\n",
       "      <td>10.676322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.779675</td>\n",
       "      <td>0.597637</td>\n",
       "      <td>12.946444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.917460</td>\n",
       "      <td>0.784333</td>\n",
       "      <td>0.601603</td>\n",
       "      <td>16.264792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.785974</td>\n",
       "      <td>0.603038</td>\n",
       "      <td>17.363150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.605485</td>\n",
       "      <td>18.517551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.938492</td>\n",
       "      <td>0.789656</td>\n",
       "      <td>0.608439</td>\n",
       "      <td>19.630407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.792583</td>\n",
       "      <td>0.612658</td>\n",
       "      <td>20.741610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.794890</td>\n",
       "      <td>0.616287</td>\n",
       "      <td>21.830405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.616624</td>\n",
       "      <td>25.206463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.795334</td>\n",
       "      <td>0.616878</td>\n",
       "      <td>26.945656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.932936</td>\n",
       "      <td>0.795378</td>\n",
       "      <td>0.616878</td>\n",
       "      <td>28.628550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.795555</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>29.705958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.676984</td>\n",
       "      <td>0.509315</td>\n",
       "      <td>0.247595</td>\n",
       "      <td>1.170462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.512287</td>\n",
       "      <td>0.252068</td>\n",
       "      <td>2.400195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.703380</td>\n",
       "      <td>0.444979</td>\n",
       "      <td>13.039391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.529143</td>\n",
       "      <td>0.279747</td>\n",
       "      <td>4.735582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.771513</td>\n",
       "      <td>0.569789</td>\n",
       "      <td>25.140305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.627528</td>\n",
       "      <td>0.308776</td>\n",
       "      <td>7.078616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.871032</td>\n",
       "      <td>0.725160</td>\n",
       "      <td>0.483629</td>\n",
       "      <td>15.349843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.773421</td>\n",
       "      <td>0.573418</td>\n",
       "      <td>26.354444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.732346</td>\n",
       "      <td>0.497046</td>\n",
       "      <td>16.467137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.852381</td>\n",
       "      <td>0.687810</td>\n",
       "      <td>0.416287</td>\n",
       "      <td>11.858376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>0.774619</td>\n",
       "      <td>0.575696</td>\n",
       "      <td>27.695649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.896429</td>\n",
       "      <td>0.747250</td>\n",
       "      <td>0.524641</td>\n",
       "      <td>18.937314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>0.756077</td>\n",
       "      <td>0.541350</td>\n",
       "      <td>20.204582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.918254</td>\n",
       "      <td>0.775727</td>\n",
       "      <td>0.577806</td>\n",
       "      <td>30.061926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.917857</td>\n",
       "      <td>0.766545</td>\n",
       "      <td>0.560591</td>\n",
       "      <td>22.598675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.918254</td>\n",
       "      <td>0.770538</td>\n",
       "      <td>0.567932</td>\n",
       "      <td>23.816767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.775639</td>\n",
       "      <td>0.577637</td>\n",
       "      <td>28.838756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>0.776171</td>\n",
       "      <td>0.578650</td>\n",
       "      <td>31.220424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.924206</td>\n",
       "      <td>0.776393</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>33.839890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.776792</td>\n",
       "      <td>0.579747</td>\n",
       "      <td>35.120372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.726984</td>\n",
       "      <td>0.578380</td>\n",
       "      <td>0.450211</td>\n",
       "      <td>1.667094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.723918</td>\n",
       "      <td>0.560084</td>\n",
       "      <td>3.179127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.810415</td>\n",
       "      <td>0.665907</td>\n",
       "      <td>4.544612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.827315</td>\n",
       "      <td>0.692658</td>\n",
       "      <td>5.726158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.909127</td>\n",
       "      <td>0.834235</td>\n",
       "      <td>0.701266</td>\n",
       "      <td>6.902444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.837873</td>\n",
       "      <td>0.704219</td>\n",
       "      <td>9.157604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>0.718819</td>\n",
       "      <td>10.720084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.532540</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.318899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"19\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>0.438786</td>\n",
       "      <td>0.734768</td>\n",
       "      <td>1.401034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.417460</td>\n",
       "      <td>0.440295</td>\n",
       "      <td>0.737046</td>\n",
       "      <td>2.789525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>0.444331</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>4.197622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.505412</td>\n",
       "      <td>0.741688</td>\n",
       "      <td>5.546883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.744633</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>22.220947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.693254</td>\n",
       "      <td>0.712252</td>\n",
       "      <td>0.714852</td>\n",
       "      <td>8.283814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.707540</td>\n",
       "      <td>0.716643</td>\n",
       "      <td>0.712152</td>\n",
       "      <td>9.670416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.745963</td>\n",
       "      <td>0.641941</td>\n",
       "      <td>23.707702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.721478</td>\n",
       "      <td>0.699662</td>\n",
       "      <td>12.403055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>0.634852</td>\n",
       "      <td>19.431136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.743080</td>\n",
       "      <td>0.639831</td>\n",
       "      <td>20.878179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.888095</td>\n",
       "      <td>0.747516</td>\n",
       "      <td>0.643291</td>\n",
       "      <td>25.089557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.750133</td>\n",
       "      <td>0.646920</td>\n",
       "      <td>26.446527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.753637</td>\n",
       "      <td>0.651730</td>\n",
       "      <td>27.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>0.755767</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>29.209456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.888095</td>\n",
       "      <td>0.756654</td>\n",
       "      <td>0.655190</td>\n",
       "      <td>30.606091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.757363</td>\n",
       "      <td>0.655190</td>\n",
       "      <td>31.951760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.757541</td>\n",
       "      <td>0.654093</td>\n",
       "      <td>33.399226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.653249</td>\n",
       "      <td>34.715635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.603045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.546825</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>6.192066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.757540</td>\n",
       "      <td>0.604107</td>\n",
       "      <td>0.273587</td>\n",
       "      <td>16.881149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.728132</td>\n",
       "      <td>0.499747</td>\n",
       "      <td>29.490173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.773413</td>\n",
       "      <td>0.629613</td>\n",
       "      <td>0.320591</td>\n",
       "      <td>18.322043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.566847</td>\n",
       "      <td>0.209873</td>\n",
       "      <td>12.285737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.678968</td>\n",
       "      <td>0.572924</td>\n",
       "      <td>0.218312</td>\n",
       "      <td>13.705929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.727599</td>\n",
       "      <td>0.498903</td>\n",
       "      <td>28.020595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.728974</td>\n",
       "      <td>0.500675</td>\n",
       "      <td>31.071805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>0.673394</td>\n",
       "      <td>0.400169</td>\n",
       "      <td>21.395666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.833730</td>\n",
       "      <td>0.689274</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>23.060187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.730926</td>\n",
       "      <td>0.502616</td>\n",
       "      <td>34.265042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>0.481603</td>\n",
       "      <td>25.683904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.862698</td>\n",
       "      <td>0.727067</td>\n",
       "      <td>0.497975</td>\n",
       "      <td>26.825816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.729862</td>\n",
       "      <td>0.501772</td>\n",
       "      <td>32.594195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.864286</td>\n",
       "      <td>0.731813</td>\n",
       "      <td>0.503797</td>\n",
       "      <td>35.816563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.867857</td>\n",
       "      <td>0.732434</td>\n",
       "      <td>0.504726</td>\n",
       "      <td>37.506815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.733188</td>\n",
       "      <td>0.505992</td>\n",
       "      <td>39.048521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.875397</td>\n",
       "      <td>0.733632</td>\n",
       "      <td>0.506835</td>\n",
       "      <td>40.633031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.734031</td>\n",
       "      <td>0.507595</td>\n",
       "      <td>42.204596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.841998</td>\n",
       "      <td>0.770380</td>\n",
       "      <td>2.376861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.828175</td>\n",
       "      <td>0.877262</td>\n",
       "      <td>0.824135</td>\n",
       "      <td>4.570594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.888840</td>\n",
       "      <td>0.834008</td>\n",
       "      <td>6.760869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.842460</td>\n",
       "      <td>0.893808</td>\n",
       "      <td>0.837637</td>\n",
       "      <td>8.998884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.895715</td>\n",
       "      <td>0.835190</td>\n",
       "      <td>11.282426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.503016</td>\n",
       "      <td>0.243376</td>\n",
       "      <td>0.910504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.517255</td>\n",
       "      <td>0.266835</td>\n",
       "      <td>1.687136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>0.751109</td>\n",
       "      <td>0.533502</td>\n",
       "      <td>8.941634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.790873</td>\n",
       "      <td>0.562722</td>\n",
       "      <td>0.349030</td>\n",
       "      <td>3.272226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.549198</td>\n",
       "      <td>9.833709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.864683</td>\n",
       "      <td>0.691182</td>\n",
       "      <td>0.434177</td>\n",
       "      <td>4.904266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.768408</td>\n",
       "      <td>0.565316</td>\n",
       "      <td>10.641123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.723785</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>6.528325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.961508</td>\n",
       "      <td>0.779276</td>\n",
       "      <td>0.586245</td>\n",
       "      <td>11.456399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.928175</td>\n",
       "      <td>0.742903</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>8.138729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.953968</td>\n",
       "      <td>0.785442</td>\n",
       "      <td>0.595105</td>\n",
       "      <td>12.201887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.961508</td>\n",
       "      <td>0.789611</td>\n",
       "      <td>0.602954</td>\n",
       "      <td>13.061194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.963095</td>\n",
       "      <td>0.790809</td>\n",
       "      <td>0.605148</td>\n",
       "      <td>13.888129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.957937</td>\n",
       "      <td>0.790853</td>\n",
       "      <td>0.605148</td>\n",
       "      <td>14.680479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.791741</td>\n",
       "      <td>0.606329</td>\n",
       "      <td>15.529244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.738644</td>\n",
       "      <td>0.529451</td>\n",
       "      <td>0.860585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.768231</td>\n",
       "      <td>0.571224</td>\n",
       "      <td>1.668629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.870635</td>\n",
       "      <td>0.781804</td>\n",
       "      <td>0.595021</td>\n",
       "      <td>2.464633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.792938</td>\n",
       "      <td>0.613080</td>\n",
       "      <td>3.402384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.809173</td>\n",
       "      <td>0.642110</td>\n",
       "      <td>4.239686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.901984</td>\n",
       "      <td>0.819331</td>\n",
       "      <td>0.660928</td>\n",
       "      <td>5.030024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.820928</td>\n",
       "      <td>0.663544</td>\n",
       "      <td>5.840948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.821904</td>\n",
       "      <td>0.664979</td>\n",
       "      <td>6.670229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.728175</td>\n",
       "      <td>0.666519</td>\n",
       "      <td>0.385654</td>\n",
       "      <td>0.899783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.683641</td>\n",
       "      <td>0.414515</td>\n",
       "      <td>1.714528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.802778</td>\n",
       "      <td>0.702537</td>\n",
       "      <td>0.447089</td>\n",
       "      <td>2.484719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.828968</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>0.471899</td>\n",
       "      <td>3.368358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.731148</td>\n",
       "      <td>0.495527</td>\n",
       "      <td>4.190599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.873810</td>\n",
       "      <td>0.741439</td>\n",
       "      <td>0.514599</td>\n",
       "      <td>5.004562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>0.537722</td>\n",
       "      <td>5.826692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.763352</td>\n",
       "      <td>0.555612</td>\n",
       "      <td>6.681455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.923413</td>\n",
       "      <td>0.770937</td>\n",
       "      <td>0.569789</td>\n",
       "      <td>7.490652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.782825</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>8.270839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.653571</td>\n",
       "      <td>0.677475</td>\n",
       "      <td>0.486498</td>\n",
       "      <td>0.641767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.762302</td>\n",
       "      <td>0.718595</td>\n",
       "      <td>0.503038</td>\n",
       "      <td>1.235688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.607679</td>\n",
       "      <td>7.860441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.836508</td>\n",
       "      <td>0.741794</td>\n",
       "      <td>0.530127</td>\n",
       "      <td>2.433098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.754170</td>\n",
       "      <td>0.551308</td>\n",
       "      <td>3.157930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.763928</td>\n",
       "      <td>0.567342</td>\n",
       "      <td>3.955057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.773288</td>\n",
       "      <td>0.584135</td>\n",
       "      <td>4.727972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.777324</td>\n",
       "      <td>0.590127</td>\n",
       "      <td>5.474877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.781139</td>\n",
       "      <td>0.593080</td>\n",
       "      <td>6.217043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.787349</td>\n",
       "      <td>0.604641</td>\n",
       "      <td>7.046358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545238</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.038483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.424414</td>\n",
       "      <td>0.179747</td>\n",
       "      <td>1.097890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.536905</td>\n",
       "      <td>0.425790</td>\n",
       "      <td>0.180759</td>\n",
       "      <td>2.143623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.530208</td>\n",
       "      <td>0.289620</td>\n",
       "      <td>11.259707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.561508</td>\n",
       "      <td>0.438831</td>\n",
       "      <td>0.181181</td>\n",
       "      <td>4.255471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.898413</td>\n",
       "      <td>0.691093</td>\n",
       "      <td>0.429367</td>\n",
       "      <td>22.300597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.642460</td>\n",
       "      <td>0.472410</td>\n",
       "      <td>0.189958</td>\n",
       "      <td>6.194484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.798810</td>\n",
       "      <td>0.586010</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>13.533677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.431055</td>\n",
       "      <td>23.404547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.832540</td>\n",
       "      <td>0.633561</td>\n",
       "      <td>0.328776</td>\n",
       "      <td>14.577337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.731746</td>\n",
       "      <td>0.505323</td>\n",
       "      <td>0.244473</td>\n",
       "      <td>10.171338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.898413</td>\n",
       "      <td>0.692867</td>\n",
       "      <td>0.432321</td>\n",
       "      <td>24.543459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.853968</td>\n",
       "      <td>0.649530</td>\n",
       "      <td>0.355274</td>\n",
       "      <td>16.771005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.864286</td>\n",
       "      <td>0.656405</td>\n",
       "      <td>0.368017</td>\n",
       "      <td>17.882679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.902381</td>\n",
       "      <td>0.694464</td>\n",
       "      <td>0.435021</td>\n",
       "      <td>26.844587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.886111</td>\n",
       "      <td>0.680713</td>\n",
       "      <td>0.411899</td>\n",
       "      <td>20.058650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.690161</td>\n",
       "      <td>0.427848</td>\n",
       "      <td>21.185926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.693976</td>\n",
       "      <td>0.434093</td>\n",
       "      <td>25.716757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.694863</td>\n",
       "      <td>0.435781</td>\n",
       "      <td>27.960191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.695529</td>\n",
       "      <td>0.436962</td>\n",
       "      <td>29.027340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.696150</td>\n",
       "      <td>0.438143</td>\n",
       "      <td>30.049937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.696593</td>\n",
       "      <td>0.438903</td>\n",
       "      <td>31.125288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.696726</td>\n",
       "      <td>0.439156</td>\n",
       "      <td>32.222431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.616971</td>\n",
       "      <td>0.426414</td>\n",
       "      <td>1.195384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826587</td>\n",
       "      <td>0.726712</td>\n",
       "      <td>0.527595</td>\n",
       "      <td>2.318904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.803983</td>\n",
       "      <td>0.632321</td>\n",
       "      <td>12.117837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.753637</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>4.590592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.815250</td>\n",
       "      <td>0.653840</td>\n",
       "      <td>13.141996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.915476</td>\n",
       "      <td>0.776171</td>\n",
       "      <td>0.586751</td>\n",
       "      <td>6.918323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.584979</td>\n",
       "      <td>8.676533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.826961</td>\n",
       "      <td>0.675359</td>\n",
       "      <td>15.049278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.618734</td>\n",
       "      <td>10.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.822347</td>\n",
       "      <td>0.667004</td>\n",
       "      <td>14.017167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.829179</td>\n",
       "      <td>0.679578</td>\n",
       "      <td>16.208178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.829933</td>\n",
       "      <td>0.681013</td>\n",
       "      <td>17.381170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.830199</td>\n",
       "      <td>0.681266</td>\n",
       "      <td>19.179069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.830554</td>\n",
       "      <td>0.681772</td>\n",
       "      <td>20.313943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.630412</td>\n",
       "      <td>0.659747</td>\n",
       "      <td>1.532302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.846825</td>\n",
       "      <td>0.834812</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>3.066649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.839026</td>\n",
       "      <td>0.709030</td>\n",
       "      <td>5.453015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.844571</td>\n",
       "      <td>0.716878</td>\n",
       "      <td>6.928237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.879762</td>\n",
       "      <td>0.846833</td>\n",
       "      <td>0.718481</td>\n",
       "      <td>8.421714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.852067</td>\n",
       "      <td>0.724641</td>\n",
       "      <td>16.195730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>0.853886</td>\n",
       "      <td>0.728017</td>\n",
       "      <td>17.664799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.331508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.405436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.450397</td>\n",
       "      <td>0.569508</td>\n",
       "      <td>0.818734</td>\n",
       "      <td>2.777832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.863068</td>\n",
       "      <td>0.795612</td>\n",
       "      <td>14.802773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>0.571505</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>5.437043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.866306</td>\n",
       "      <td>0.795781</td>\n",
       "      <td>16.250514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.676984</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>7.986766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.866971</td>\n",
       "      <td>0.795274</td>\n",
       "      <td>17.737847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.838893</td>\n",
       "      <td>0.794346</td>\n",
       "      <td>10.757020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.845413</td>\n",
       "      <td>0.792152</td>\n",
       "      <td>12.104453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>0.851712</td>\n",
       "      <td>0.793502</td>\n",
       "      <td>13.462587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.874423</td>\n",
       "      <td>0.776962</td>\n",
       "      <td>22.464884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>0.569331</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.640981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.818312</td>\n",
       "      <td>3.389922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.574255</td>\n",
       "      <td>0.820084</td>\n",
       "      <td>4.984748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.507540</td>\n",
       "      <td>0.586276</td>\n",
       "      <td>0.820844</td>\n",
       "      <td>6.491150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.707540</td>\n",
       "      <td>0.777945</td>\n",
       "      <td>0.801013</td>\n",
       "      <td>8.023460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.788492</td>\n",
       "      <td>0.853974</td>\n",
       "      <td>0.812743</td>\n",
       "      <td>9.607423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.881521</td>\n",
       "      <td>0.822363</td>\n",
       "      <td>11.145439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.889727</td>\n",
       "      <td>0.817131</td>\n",
       "      <td>12.703154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.875794</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.819494</td>\n",
       "      <td>14.221701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.693254</td>\n",
       "      <td>0.619012</td>\n",
       "      <td>0.620591</td>\n",
       "      <td>2.205522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.798016</td>\n",
       "      <td>0.789567</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>4.312284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.863778</td>\n",
       "      <td>0.809030</td>\n",
       "      <td>6.395371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.874823</td>\n",
       "      <td>0.809536</td>\n",
       "      <td>8.521467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.876553</td>\n",
       "      <td>0.806835</td>\n",
       "      <td>10.668059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.877839</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>12.819965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.903968</td>\n",
       "      <td>0.884271</td>\n",
       "      <td>0.796793</td>\n",
       "      <td>14.985993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.493651</td>\n",
       "      <td>0.423838</td>\n",
       "      <td>0.195696</td>\n",
       "      <td>0.807331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.522619</td>\n",
       "      <td>0.430979</td>\n",
       "      <td>0.192827</td>\n",
       "      <td>1.596510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.721744</td>\n",
       "      <td>0.483291</td>\n",
       "      <td>8.853109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.582937</td>\n",
       "      <td>0.484164</td>\n",
       "      <td>0.276962</td>\n",
       "      <td>3.191427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.778832</td>\n",
       "      <td>0.584641</td>\n",
       "      <td>16.026343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.563742</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>4.781629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.547004</td>\n",
       "      <td>10.412220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.955952</td>\n",
       "      <td>0.779143</td>\n",
       "      <td>0.585232</td>\n",
       "      <td>16.817441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.946825</td>\n",
       "      <td>0.761356</td>\n",
       "      <td>0.555106</td>\n",
       "      <td>11.200721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.854365</td>\n",
       "      <td>0.706707</td>\n",
       "      <td>0.456118</td>\n",
       "      <td>8.028947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.964683</td>\n",
       "      <td>0.779276</td>\n",
       "      <td>0.585485</td>\n",
       "      <td>17.569122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>0.768941</td>\n",
       "      <td>0.567679</td>\n",
       "      <td>12.652271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.952778</td>\n",
       "      <td>0.770715</td>\n",
       "      <td>0.570717</td>\n",
       "      <td>13.253711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.962698</td>\n",
       "      <td>0.779542</td>\n",
       "      <td>0.585992</td>\n",
       "      <td>19.157119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.955159</td>\n",
       "      <td>0.777280</td>\n",
       "      <td>0.582363</td>\n",
       "      <td>14.447026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.957540</td>\n",
       "      <td>0.778611</td>\n",
       "      <td>0.584388</td>\n",
       "      <td>15.205530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.779454</td>\n",
       "      <td>0.585823</td>\n",
       "      <td>18.398555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0.967857</td>\n",
       "      <td>0.779853</td>\n",
       "      <td>0.586582</td>\n",
       "      <td>19.983089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0.969841</td>\n",
       "      <td>0.780119</td>\n",
       "      <td>0.587089</td>\n",
       "      <td>20.762034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>0.962302</td>\n",
       "      <td>0.780385</td>\n",
       "      <td>0.587511</td>\n",
       "      <td>21.580799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.961508</td>\n",
       "      <td>0.780429</td>\n",
       "      <td>0.587595</td>\n",
       "      <td>22.352542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.780784</td>\n",
       "      <td>0.588270</td>\n",
       "      <td>23.211684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.429826</td>\n",
       "      <td>0.610886</td>\n",
       "      <td>0.769770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.737302</td>\n",
       "      <td>0.680669</td>\n",
       "      <td>0.610211</td>\n",
       "      <td>1.608776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.758339</td>\n",
       "      <td>0.587173</td>\n",
       "      <td>2.349010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.776393</td>\n",
       "      <td>0.597890</td>\n",
       "      <td>3.219053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.873413</td>\n",
       "      <td>0.791785</td>\n",
       "      <td>0.621688</td>\n",
       "      <td>3.978208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.805846</td>\n",
       "      <td>0.640506</td>\n",
       "      <td>4.819912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.809351</td>\n",
       "      <td>0.644557</td>\n",
       "      <td>5.625120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.810814</td>\n",
       "      <td>0.645907</td>\n",
       "      <td>6.522778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.917063</td>\n",
       "      <td>0.811790</td>\n",
       "      <td>0.646920</td>\n",
       "      <td>7.331714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.713405</td>\n",
       "      <td>0.473840</td>\n",
       "      <td>0.840305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.816270</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>0.525823</td>\n",
       "      <td>1.625310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>8.782956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.774619</td>\n",
       "      <td>0.582532</td>\n",
       "      <td>3.201481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.937302</td>\n",
       "      <td>0.812677</td>\n",
       "      <td>0.650464</td>\n",
       "      <td>9.596403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>0.784954</td>\n",
       "      <td>0.600928</td>\n",
       "      <td>4.833746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.792273</td>\n",
       "      <td>0.613671</td>\n",
       "      <td>5.602313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.926587</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.629367</td>\n",
       "      <td>6.412395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>0.805358</td>\n",
       "      <td>0.638228</td>\n",
       "      <td>7.183477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.645992</td>\n",
       "      <td>8.012530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.729365</td>\n",
       "      <td>0.610406</td>\n",
       "      <td>0.304726</td>\n",
       "      <td>0.841745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.811508</td>\n",
       "      <td>0.673705</td>\n",
       "      <td>0.404557</td>\n",
       "      <td>1.695190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.751641</td>\n",
       "      <td>0.536962</td>\n",
       "      <td>8.785692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.867857</td>\n",
       "      <td>0.691137</td>\n",
       "      <td>0.428608</td>\n",
       "      <td>3.253160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.754524</td>\n",
       "      <td>0.541603</td>\n",
       "      <td>9.645123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.707505</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>4.797040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>0.757142</td>\n",
       "      <td>0.544979</td>\n",
       "      <td>10.489040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.728176</td>\n",
       "      <td>0.494093</td>\n",
       "      <td>6.365857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.961508</td>\n",
       "      <td>0.761489</td>\n",
       "      <td>0.551561</td>\n",
       "      <td>11.270747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.946825</td>\n",
       "      <td>0.745875</td>\n",
       "      <td>0.526414</td>\n",
       "      <td>7.940228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.764372</td>\n",
       "      <td>0.556371</td>\n",
       "      <td>12.076853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.765791</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>12.866027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.519444</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.093403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.757674</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>6.181096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.782071</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>17.835649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.764949</td>\n",
       "      <td>0.561013</td>\n",
       "      <td>8.806329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.768763</td>\n",
       "      <td>0.567004</td>\n",
       "      <td>9.904373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.782204</td>\n",
       "      <td>0.590970</td>\n",
       "      <td>20.038939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.900397</td>\n",
       "      <td>0.773909</td>\n",
       "      <td>0.576624</td>\n",
       "      <td>12.157693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.775683</td>\n",
       "      <td>0.579831</td>\n",
       "      <td>13.306773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>0.776437</td>\n",
       "      <td>0.581266</td>\n",
       "      <td>14.452175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.777768</td>\n",
       "      <td>0.583797</td>\n",
       "      <td>15.543692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.782026</td>\n",
       "      <td>0.590633</td>\n",
       "      <td>16.732997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.925397</td>\n",
       "      <td>0.782115</td>\n",
       "      <td>0.590802</td>\n",
       "      <td>18.944045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.782381</td>\n",
       "      <td>0.591308</td>\n",
       "      <td>21.114165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.924206</td>\n",
       "      <td>0.782559</td>\n",
       "      <td>0.591646</td>\n",
       "      <td>23.359160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.782736</td>\n",
       "      <td>0.591983</td>\n",
       "      <td>24.386522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.625621</td>\n",
       "      <td>0.693671</td>\n",
       "      <td>1.187153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.737269</td>\n",
       "      <td>0.672911</td>\n",
       "      <td>2.314086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.821150</td>\n",
       "      <td>0.672827</td>\n",
       "      <td>10.685101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.812145</td>\n",
       "      <td>0.684557</td>\n",
       "      <td>4.496789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.813964</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>6.777124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.827005</td>\n",
       "      <td>0.677384</td>\n",
       "      <td>20.235193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.820263</td>\n",
       "      <td>0.675696</td>\n",
       "      <td>8.968101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.823501</td>\n",
       "      <td>0.671224</td>\n",
       "      <td>14.632145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.824521</td>\n",
       "      <td>0.672911</td>\n",
       "      <td>15.710437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.825763</td>\n",
       "      <td>0.675190</td>\n",
       "      <td>16.846556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.826694</td>\n",
       "      <td>0.676878</td>\n",
       "      <td>17.945738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.909127</td>\n",
       "      <td>0.826783</td>\n",
       "      <td>0.676962</td>\n",
       "      <td>19.093679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.921032</td>\n",
       "      <td>0.827182</td>\n",
       "      <td>0.677722</td>\n",
       "      <td>21.320162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.827271</td>\n",
       "      <td>0.677806</td>\n",
       "      <td>24.279881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.827315</td>\n",
       "      <td>0.677806</td>\n",
       "      <td>25.972584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.677890</td>\n",
       "      <td>27.045139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.666270</td>\n",
       "      <td>0.696238</td>\n",
       "      <td>0.557975</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.803175</td>\n",
       "      <td>0.763840</td>\n",
       "      <td>0.581181</td>\n",
       "      <td>2.442370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.798039</td>\n",
       "      <td>0.624388</td>\n",
       "      <td>12.652347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.776570</td>\n",
       "      <td>0.594599</td>\n",
       "      <td>4.858522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.802520</td>\n",
       "      <td>0.632574</td>\n",
       "      <td>13.861901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.783668</td>\n",
       "      <td>0.602025</td>\n",
       "      <td>7.257311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.806334</td>\n",
       "      <td>0.638481</td>\n",
       "      <td>15.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.807754</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>16.278363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>0.792140</td>\n",
       "      <td>0.613586</td>\n",
       "      <td>11.410274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.644641</td>\n",
       "      <td>17.451793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.651392</td>\n",
       "      <td>18.608358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.938492</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.659072</td>\n",
       "      <td>19.794503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.728175</td>\n",
       "      <td>0.699787</td>\n",
       "      <td>0.486835</td>\n",
       "      <td>1.123082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.803968</td>\n",
       "      <td>0.797463</td>\n",
       "      <td>0.659578</td>\n",
       "      <td>2.612590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.875133</td>\n",
       "      <td>0.792827</td>\n",
       "      <td>4.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.890969</td>\n",
       "      <td>0.804641</td>\n",
       "      <td>5.754416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.893630</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>7.219536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500397</td>\n",
       "      <td>0.471123</td>\n",
       "      <td>0.271392</td>\n",
       "      <td>1.376509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.473075</td>\n",
       "      <td>0.269789</td>\n",
       "      <td>2.793978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.750794</td>\n",
       "      <td>0.556112</td>\n",
       "      <td>0.338481</td>\n",
       "      <td>14.227843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.631349</td>\n",
       "      <td>0.513751</td>\n",
       "      <td>0.276034</td>\n",
       "      <td>5.504664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.820635</td>\n",
       "      <td>0.624956</td>\n",
       "      <td>0.425401</td>\n",
       "      <td>28.022108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.708730</td>\n",
       "      <td>0.533357</td>\n",
       "      <td>0.301013</td>\n",
       "      <td>8.269593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.572525</td>\n",
       "      <td>0.368523</td>\n",
       "      <td>16.913385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.586409</td>\n",
       "      <td>0.394093</td>\n",
       "      <td>18.364748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.550612</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>12.886392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.634626</td>\n",
       "      <td>0.428101</td>\n",
       "      <td>30.803165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.805159</td>\n",
       "      <td>0.594837</td>\n",
       "      <td>0.408776</td>\n",
       "      <td>21.142184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.598563</td>\n",
       "      <td>0.414852</td>\n",
       "      <td>22.553295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.642699</td>\n",
       "      <td>0.429114</td>\n",
       "      <td>33.540665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.602067</td>\n",
       "      <td>0.419494</td>\n",
       "      <td>25.264237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.620697</td>\n",
       "      <td>0.424557</td>\n",
       "      <td>26.673722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.426076</td>\n",
       "      <td>29.328593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.640836</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>32.154849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.818254</td>\n",
       "      <td>0.646159</td>\n",
       "      <td>0.429536</td>\n",
       "      <td>34.924899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.828175</td>\n",
       "      <td>0.650018</td>\n",
       "      <td>0.430464</td>\n",
       "      <td>36.292585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.654276</td>\n",
       "      <td>0.431308</td>\n",
       "      <td>37.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.830952</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.431983</td>\n",
       "      <td>38.982371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.824206</td>\n",
       "      <td>0.658534</td>\n",
       "      <td>0.432658</td>\n",
       "      <td>40.437156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.468651</td>\n",
       "      <td>0.505101</td>\n",
       "      <td>0.616878</td>\n",
       "      <td>1.471498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.692460</td>\n",
       "      <td>0.681201</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>2.879770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.728442</td>\n",
       "      <td>0.620759</td>\n",
       "      <td>4.227787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.749601</td>\n",
       "      <td>0.532405</td>\n",
       "      <td>23.238795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.739044</td>\n",
       "      <td>0.512574</td>\n",
       "      <td>15.625380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.867857</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.522785</td>\n",
       "      <td>16.633543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.745076</td>\n",
       "      <td>0.523882</td>\n",
       "      <td>17.731955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.746141</td>\n",
       "      <td>0.525823</td>\n",
       "      <td>19.028329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.874603</td>\n",
       "      <td>0.747649</td>\n",
       "      <td>0.528692</td>\n",
       "      <td>20.448555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.530380</td>\n",
       "      <td>21.791425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>0.876587</td>\n",
       "      <td>0.750798</td>\n",
       "      <td>0.534684</td>\n",
       "      <td>24.596785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>0.752928</td>\n",
       "      <td>0.538397</td>\n",
       "      <td>25.994330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.756210</td>\n",
       "      <td>0.544473</td>\n",
       "      <td>27.368817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.884127</td>\n",
       "      <td>0.757186</td>\n",
       "      <td>0.546160</td>\n",
       "      <td>28.746180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.757452</td>\n",
       "      <td>0.546498</td>\n",
       "      <td>30.116699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.532958</td>\n",
       "      <td>0.749030</td>\n",
       "      <td>1.629867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.496825</td>\n",
       "      <td>0.551366</td>\n",
       "      <td>0.767764</td>\n",
       "      <td>3.185793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.873891</td>\n",
       "      <td>0.775021</td>\n",
       "      <td>16.964964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.668254</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.762110</td>\n",
       "      <td>6.242048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.779831</td>\n",
       "      <td>30.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.812698</td>\n",
       "      <td>0.807488</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>9.253996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.876198</td>\n",
       "      <td>0.776878</td>\n",
       "      <td>20.063834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.851190</td>\n",
       "      <td>0.866528</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>12.327079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>0.877040</td>\n",
       "      <td>0.777806</td>\n",
       "      <td>21.599068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.865873</td>\n",
       "      <td>0.873270</td>\n",
       "      <td>0.775527</td>\n",
       "      <td>15.472204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.877129</td>\n",
       "      <td>0.777975</td>\n",
       "      <td>23.123441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.878194</td>\n",
       "      <td>0.779831</td>\n",
       "      <td>26.295696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>0.904365</td>\n",
       "      <td>0.878238</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>28.607611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.780084</td>\n",
       "      <td>33.232840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.678007</td>\n",
       "      <td>0.731392</td>\n",
       "      <td>2.281325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.797995</td>\n",
       "      <td>0.731308</td>\n",
       "      <td>4.541728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.778968</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>6.730656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.800397</td>\n",
       "      <td>0.830642</td>\n",
       "      <td>0.729789</td>\n",
       "      <td>8.857712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.833481</td>\n",
       "      <td>0.715359</td>\n",
       "      <td>11.024981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.871825</td>\n",
       "      <td>0.837074</td>\n",
       "      <td>0.704979</td>\n",
       "      <td>16.240678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1335 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                  2     0.500000    0.583259       0.823207   \n",
       "               1                  3     0.773810    0.853132       0.821350   \n",
       "               1                  4     0.819444    0.873181       0.817215   \n",
       "               1                  5     0.857143    0.885247       0.815274   \n",
       "               1                  6     0.858730    0.886533       0.812489   \n",
       "8              1                  2     0.552778    0.696460       0.657553   \n",
       "               1                  3     0.641667    0.776393       0.669705   \n",
       "               1                  4     0.681746    0.800435       0.688439   \n",
       "               1                  5     0.791270    0.834634       0.718987   \n",
       "               1                  6     0.851587    0.848829       0.738228   \n",
       "               1                  7     0.872222    0.855305       0.746414   \n",
       "               1                  8     0.892063    0.861693       0.757637   \n",
       "               1                  9     0.909921    0.862890       0.759409   \n",
       "32             1                  2     0.724603    0.734120       0.525485   \n",
       "               1                  3     0.771825    0.750665       0.542785   \n",
       "               1                  4     0.911905    0.814319       0.651308   \n",
       "               1                  5     0.830952    0.786152       0.606245   \n",
       "               1                  6     0.922619    0.814807       0.652827   \n",
       "               1                  7     0.868651    0.796576       0.623460   \n",
       "               1                  8     0.931349    0.815871       0.654430   \n",
       "               1                  9     0.894048    0.806379       0.640928   \n",
       "               1                 10     0.905952    0.809173       0.644810   \n",
       "               1                 11     0.922222    0.812544       0.648692   \n",
       "122            1                  2     0.679365    0.775328       0.778819   \n",
       "               1                  3     0.798810    0.852466       0.768692   \n",
       "               1                  4     0.860317    0.879702       0.804810   \n",
       "               1                  5     0.885714    0.892965       0.813502   \n",
       "               1                  6     0.894048    0.896425       0.816203   \n",
       "1              3                  2     0.538492    0.430758       0.181603   \n",
       "8              3                  2     0.486508    0.553584       0.777300   \n",
       "               3                  3     0.734127    0.799104       0.784473   \n",
       "               3                  4     0.774603    0.841820       0.804979   \n",
       "               3                  5     0.785714    0.857745       0.814430   \n",
       "               3                  6     0.811111    0.861382       0.805823   \n",
       "               3                  7     0.843651    0.871540       0.804473   \n",
       "               3                  8     0.869841    0.880988       0.799578   \n",
       "               3                  9     0.894048    0.884847       0.796540   \n",
       "32             3                  2     0.776190    0.749202       0.626414   \n",
       "               3                  3     0.842857    0.786773       0.629705   \n",
       "               3                  4     0.875000    0.803983       0.643122   \n",
       "               3                  5     0.882937    0.805492       0.641857   \n",
       "122            3                  2     0.722222    0.736072       0.587342   \n",
       "               3                  3     0.858730    0.842486       0.737890   \n",
       "               3                  4     0.903571    0.868701       0.771308   \n",
       "1              5                  2     0.553968    0.430758       0.181603   \n",
       "8              5                  2     0.423016    0.321061       0.499325   \n",
       "               5                  3     0.540079    0.414257       0.487511   \n",
       "               5                  4     0.620238    0.485318       0.471561   \n",
       "               5                  6     0.679762    0.507630       0.443966   \n",
       "               5                  7     0.724206    0.560903       0.440506   \n",
       "               5                  8     0.773413    0.584546       0.432068   \n",
       "32             5                  2     0.553571    0.483898       0.326329   \n",
       "               5                  3     0.621825    0.524973       0.344473   \n",
       "               5                  4     0.876984    0.761089       0.572489   \n",
       "               5                  5     0.709127    0.599273       0.387173   \n",
       "               5                  6     0.924603    0.858721       0.741772   \n",
       "               5                  7     0.748810    0.654986       0.459325   \n",
       "               5                  8     0.905556    0.807044       0.652996   \n",
       "               5                  9     0.925794    0.858854       0.742025   \n",
       "               5                 10     0.906349    0.815871       0.665654   \n",
       "               5                 11     0.838492    0.749068       0.554262   \n",
       "               5                 12     0.937698    0.859519       0.743122   \n",
       "               5                 14     0.913889    0.831840       0.692405   \n",
       "               5                 16     0.923810    0.839070       0.705316   \n",
       "               5                 18     0.926587    0.861027       0.745907   \n",
       "               5                 20     0.936905    0.851801       0.728861   \n",
       "               5                 22     0.924603    0.857745       0.739916   \n",
       "               5                 15     0.927381    0.860140       0.744304   \n",
       "               5                 21     0.926984    0.861160       0.746160   \n",
       "               5                 27     0.929762    0.861560       0.746751   \n",
       "               5                 30     0.924603    0.861648       0.746920   \n",
       "               5                 33     0.937698    0.861737       0.747089   \n",
       "122            5                  2     0.742460    0.748758       0.637637   \n",
       "               5                  3     0.792460    0.791031       0.662700   \n",
       "               5                  4     0.848810    0.807177       0.669620   \n",
       "               5                  5     0.898810    0.810016       0.657890   \n",
       "1              1                  2     0.500000    0.583259       0.823207   \n",
       "               1                  3     0.773810    0.853132       0.821350   \n",
       "               1                  4     0.819444    0.873181       0.817215   \n",
       "               1                  5     0.857143    0.885247       0.815274   \n",
       "               1                  6     0.858730    0.886533       0.812489   \n",
       "8              1                  2     0.552778    0.696460       0.657553   \n",
       "               1                  3     0.641667    0.776393       0.669705   \n",
       "               1                  4     0.681746    0.800435       0.688439   \n",
       "               1                  5     0.791270    0.834634       0.718987   \n",
       "               1                  6     0.851587    0.848829       0.738228   \n",
       "               1                  7     0.872222    0.855305       0.746414   \n",
       "               1                  8     0.892063    0.861693       0.757637   \n",
       "               1                  9     0.909921    0.862890       0.759409   \n",
       "32             1                  2     0.724603    0.734120       0.525485   \n",
       "               1                  3     0.771825    0.750665       0.542785   \n",
       "               1                  4     0.911905    0.814319       0.651308   \n",
       "               1                  5     0.830952    0.786152       0.606245   \n",
       "               1                  6     0.922619    0.814807       0.652827   \n",
       "               1                  7     0.868651    0.796576       0.623460   \n",
       "               1                  8     0.931349    0.815871       0.654430   \n",
       "               1                  9     0.894048    0.806379       0.640928   \n",
       "               1                 10     0.905952    0.809173       0.644810   \n",
       "               1                 11     0.922222    0.812544       0.648692   \n",
       "122            1                  2     0.679365    0.775328       0.778819   \n",
       "               1                  3     0.798810    0.852466       0.768692   \n",
       "               1                  4     0.860317    0.879702       0.804810   \n",
       "               1                  5     0.885714    0.892965       0.813502   \n",
       "               1                  6     0.894048    0.896425       0.816203   \n",
       "1              3                  2     0.538492    0.430758       0.181603   \n",
       "8              3                  2     0.486508    0.553584       0.777300   \n",
       "               3                  3     0.734127    0.799104       0.784473   \n",
       "               3                  4     0.774603    0.841820       0.804979   \n",
       "               3                  5     0.785714    0.857745       0.814430   \n",
       "               3                  6     0.811111    0.861382       0.805823   \n",
       "               3                  7     0.843651    0.871540       0.804473   \n",
       "               3                  8     0.869841    0.880988       0.799578   \n",
       "               3                  9     0.894048    0.884847       0.796540   \n",
       "32             3                  2     0.776190    0.749202       0.626414   \n",
       "               3                  3     0.842857    0.786773       0.629705   \n",
       "               3                  4     0.875000    0.803983       0.643122   \n",
       "               3                  5     0.882937    0.805492       0.641857   \n",
       "122            3                  2     0.722222    0.736072       0.587342   \n",
       "               3                  3     0.858730    0.842486       0.737890   \n",
       "               3                  4     0.903571    0.868701       0.771308   \n",
       "1              5                  2     0.553968    0.430758       0.181603   \n",
       "8              5                  2     0.423016    0.321061       0.499325   \n",
       "               5                  3     0.540079    0.414257       0.487511   \n",
       "               5                  4     0.620238    0.485318       0.471561   \n",
       "               5                  6     0.679762    0.507630       0.443966   \n",
       "               5                  7     0.724206    0.560903       0.440506   \n",
       "               5                  8     0.773413    0.584546       0.432068   \n",
       "32             5                  2     0.553571    0.483898       0.326329   \n",
       "               5                  3     0.621825    0.524973       0.344473   \n",
       "               5                  4     0.876984    0.761089       0.572489   \n",
       "               5                  5     0.709127    0.599273       0.387173   \n",
       "               5                  6     0.924603    0.858721       0.741772   \n",
       "               5                  7     0.748810    0.654986       0.459325   \n",
       "               5                  8     0.905556    0.807044       0.652996   \n",
       "               5                  9     0.925794    0.858854       0.742025   \n",
       "               5                 10     0.906349    0.815871       0.665654   \n",
       "               5                 11     0.838492    0.749068       0.554262   \n",
       "               5                 12     0.937698    0.859519       0.743122   \n",
       "               5                 14     0.913889    0.831840       0.692405   \n",
       "               5                 16     0.923810    0.839070       0.705316   \n",
       "               5                 18     0.926587    0.861027       0.745907   \n",
       "               5                 20     0.936905    0.851801       0.728861   \n",
       "               5                 22     0.924603    0.857745       0.739916   \n",
       "               5                 15     0.927381    0.860140       0.744304   \n",
       "               5                 21     0.926984    0.861160       0.746160   \n",
       "               5                 27     0.929762    0.861560       0.746751   \n",
       "               5                 30     0.924603    0.861648       0.746920   \n",
       "               5                 33     0.937698    0.861737       0.747089   \n",
       "122            5                  2     0.742460    0.748758       0.637637   \n",
       "               5                  3     0.792460    0.791031       0.662700   \n",
       "               5                  4     0.848810    0.807177       0.669620   \n",
       "               5                  5     0.898810    0.810016       0.657890   \n",
       "1              1                  2     0.752381    0.609430       0.435612   \n",
       "               1                  3     0.800794    0.711631       0.468861   \n",
       "               1                  4     0.942460    0.785442       0.596371   \n",
       "               1                  5     0.862302    0.736781       0.511561   \n",
       "               1                  6     0.950794    0.787571       0.598819   \n",
       "               1                  7     0.909127    0.755279       0.543122   \n",
       "               1                  8     0.951587    0.788502       0.599747   \n",
       "               1                  9     0.932540    0.775018       0.579156   \n",
       "               1                 10     0.930159    0.779143       0.586245   \n",
       "               1                 11     0.932540    0.783224       0.592405   \n",
       "8              1                  2     0.305556    0.333659       0.559156   \n",
       "               1                  3     0.513492    0.502750       0.648945   \n",
       "               1                  4     0.947619    0.838183       0.693080   \n",
       "               1                  5     0.850794    0.781804       0.705992   \n",
       "               1                  6     0.953175    0.838361       0.693333   \n",
       "32             1                  2     0.553571    0.570884       0.553080   \n",
       "               1                  3     0.734127    0.740641       0.601857   \n",
       "               1                  4     0.826587    0.788369       0.651055   \n",
       "               1                  5     0.863492    0.806556       0.665232   \n",
       "               1                  6     0.897222    0.811214       0.662025   \n",
       "122            1                  2     0.757937    0.771425       0.584051   \n",
       "               1                  3     0.792857    0.785885       0.606920   \n",
       "               1                  4     0.830159    0.791785       0.617131   \n",
       "               1                  5     0.834921    0.800257       0.632405   \n",
       "               1                  6     0.841270    0.803673       0.637890   \n",
       "               1                  7     0.863492    0.805758       0.641603   \n",
       "               1                  8     0.901587    0.811968       0.652152   \n",
       "1              3                  2     0.531349    0.430758       0.181603   \n",
       "8              3                  2     0.451190    0.540454       0.759325   \n",
       "               3                  3     0.575000    0.635468       0.745063   \n",
       "               3                  4     0.908333    0.846966       0.724135   \n",
       "               3                  6     0.912302    0.851180       0.729283   \n",
       "               3                  7     0.809127    0.818178       0.694093   \n",
       "               3                  8     0.926984    0.854640       0.735274   \n",
       "               3                  9     0.861111    0.829400       0.701941   \n",
       "               3                 10     0.930159    0.858055       0.740169   \n",
       "               3                 11     0.890873    0.842708       0.719072   \n",
       "               3                 12     0.947222    0.861160       0.740338   \n",
       "               3                 14     0.934127    0.860939       0.743629   \n",
       "               3                 15     0.939683    0.861737       0.741435   \n",
       "               3                 18     0.944444    0.862314       0.742447   \n",
       "               3                 21     0.947619    0.862713       0.743207   \n",
       "               3                 24     0.948413    0.862890       0.743544   \n",
       "32             3                  2     0.545238    0.433109       0.184979   \n",
       "               3                  3     0.660317    0.479196       0.196878   \n",
       "               3                  4     0.928571    0.812500       0.651561   \n",
       "               3                  5     0.774603    0.538103       0.304641   \n",
       "               3                  6     0.945238    0.838094       0.699747   \n",
       "               3                  7     0.841667    0.595369       0.404895   \n",
       "               3                  8     0.946032    0.846123       0.713502   \n",
       "               3                  9     0.897619    0.746851       0.529958   \n",
       "               3                 10     0.911905    0.783268       0.596878   \n",
       "               3                 11     0.928571    0.797330       0.623038   \n",
       "122            3                  2     0.746825    0.613778       0.283376   \n",
       "               3                  3     0.821032    0.688165       0.417384   \n",
       "               3                  4     0.937698    0.773377       0.574515   \n",
       "               3                  5     0.888492    0.753194       0.539831   \n",
       "               3                  6     0.935714    0.776748       0.580759   \n",
       "               3                  7     0.898016    0.765747       0.561435   \n",
       "               3                  8     0.940079    0.778877       0.584810   \n",
       "               3                  9     0.911905    0.766812       0.563038   \n",
       "               3                 10     0.950000    0.781716       0.590211   \n",
       "               3                 11     0.925397    0.768408       0.565738   \n",
       "               3                 12     0.950000    0.783357       0.593165   \n",
       "1              5                  2     0.555952    0.430758       0.181603   \n",
       "8              5                  2     0.489683    0.592308       0.793333   \n",
       "               5                  3     0.518254    0.612802       0.778059   \n",
       "               5                  6     0.836905    0.693444       0.592321   \n",
       "               5                  7     0.734127    0.671620       0.639494   \n",
       "               5                  8     0.754365    0.677564       0.600760   \n",
       "               5                 10     0.770635    0.680092       0.580675   \n",
       "               5                  4     0.831746    0.686879       0.581519   \n",
       "32             5                  2     0.550397    0.441004       0.200760   \n",
       "               5                  3     0.564683    0.460699       0.237806   \n",
       "               5                  4     0.830556    0.666918       0.382110   \n",
       "               5                  5     0.578175    0.508428       0.257131   \n",
       "               5                  6     0.898016    0.756343       0.542532   \n",
       "               5                  7     0.658333    0.590002       0.282785   \n",
       "               5                  8     0.851984    0.691625       0.424979   \n",
       "               5                  9     0.907937    0.757008       0.543797   \n",
       "               5                 10     0.872222    0.705908       0.449958   \n",
       "               5                 11     0.810317    0.651570       0.354093   \n",
       "               5                 12     0.891667    0.757674       0.545063   \n",
       "               5                 14     0.894841    0.724361       0.483629   \n",
       "               5                 16     0.888492    0.732080       0.498228   \n",
       "               5                 18     0.894444    0.758605       0.546835   \n",
       "               5                 20     0.896032    0.750177       0.530970   \n",
       "               5                 22     0.903175    0.755722       0.541350   \n",
       "               5                 15     0.903175    0.758206       0.546076   \n",
       "               5                 21     0.896429    0.758872       0.547342   \n",
       "               5                 24     0.897222    0.759315       0.548186   \n",
       "               5                 27     0.894444    0.759581       0.548692   \n",
       "               5                 30     0.887302    0.759892       0.549283   \n",
       "               5                 33     0.895238    0.760114       0.549705   \n",
       "122            5                  2     0.394048    0.338449       0.311224   \n",
       "               5                  3     0.688889    0.678007       0.499494   \n",
       "               5                  4     0.862698    0.807754       0.662194   \n",
       "               5                  5     0.881746    0.826384       0.692152   \n",
       "               5                  6     0.890873    0.837917       0.712996   \n",
       "               5                  7     0.905952    0.841022       0.717637   \n",
       "               5                  8     0.906349    0.843861       0.722447   \n",
       "               5                  9     0.900794    0.844881       0.723882   \n",
       "               5                 10     0.919841    0.845724       0.723207   \n",
       "1              1                  2     0.532143    0.552919       0.282532   \n",
       "               1                  3     0.678968    0.635114       0.367089   \n",
       "               1                  4     0.925397    0.753682       0.538312   \n",
       "               1                  5     0.810317    0.694065       0.439156   \n",
       "               1                  6     0.936111    0.758162       0.546582   \n",
       "               1                  7     0.849206    0.720369       0.484135   \n",
       "               1                  8     0.944048    0.759448       0.549030   \n",
       "               1                  9     0.888889    0.731725       0.498650   \n",
       "               1                 10     0.955159    0.760912       0.551308   \n",
       "               1                 11     0.919048    0.744899       0.522447   \n",
       "               1                 16     0.961111    0.761001       0.548945   \n",
       "               1                 18     0.959524    0.761844       0.550295   \n",
       "8              1                  2     0.226587    0.262952       0.458650   \n",
       "               1                  3     0.322619    0.306645       0.518650   \n",
       "               1                  4     0.523810    0.430669       0.567173   \n",
       "               1                  5     0.806349    0.722010       0.655865   \n",
       "               1                  6     0.856349    0.837961       0.712236   \n",
       "               1                  7     0.880556    0.841111       0.709958   \n",
       "               1                  9     0.892063    0.846478       0.715612   \n",
       "               1                 10     0.910317    0.847276       0.716371   \n",
       "32             1                  2     0.707540    0.739798       0.532405   \n",
       "               1                  3     0.784524    0.773510       0.583038   \n",
       "               1                  4     0.855952    0.784022       0.597300   \n",
       "               1                  5     0.871429    0.787970       0.601603   \n",
       "               1                  6     0.907937    0.791031       0.606160   \n",
       "               1                  7     0.906349    0.793914       0.611899   \n",
       "               1                  9     0.934921    0.795200       0.613755   \n",
       "122            1                  2     0.661111    0.641767       0.640000   \n",
       "               1                  3     0.815476    0.764461       0.629789   \n",
       "               1                  4     0.863889    0.798572       0.658143   \n",
       "               1                  5     0.888889    0.806733       0.659494   \n",
       "               1                  6     0.917460    0.816137       0.662616   \n",
       "1              3                  2     0.544444    0.430758       0.181603   \n",
       "8              3                  2     0.524603    0.430802       0.181688   \n",
       "               3                  3     0.550794    0.490995       0.185654   \n",
       "               3                  4     0.763095    0.627307       0.308861   \n",
       "               3                  5     0.570635    0.523643       0.195781   \n",
       "               3                  6     0.884127    0.686391       0.410717   \n",
       "               3                  7     0.695635    0.574033       0.216118   \n",
       "               3                  8     0.783333    0.641501       0.332743   \n",
       "               3                  9     0.879365    0.687500       0.412827   \n",
       "               3                 10     0.719444    0.615951       0.289114   \n",
       "               3                 11     0.748810    0.622383       0.300169   \n",
       "               3                 12     0.882540    0.688387       0.414515   \n",
       "               3                 14     0.836905    0.650816       0.346667   \n",
       "               3                 16     0.821825    0.658934       0.361519   \n",
       "               3                 18     0.878571    0.689762       0.417131   \n",
       "               3                 20     0.857540    0.674148       0.388439   \n",
       "               3                 22     0.872222    0.685593       0.409198   \n",
       "               3                 15     0.873016    0.689053       0.415781   \n",
       "               3                 21     0.890476    0.690383       0.418312   \n",
       "               3                 24     0.894841    0.691315       0.420084   \n",
       "               3                 27     0.895635    0.692424       0.422194   \n",
       "               3                 30     0.903968    0.694154       0.425485   \n",
       "               3                 33     0.908333    0.695263       0.427595   \n",
       "32             3                  2     0.683730    0.506875       0.259662   \n",
       "               3                  3     0.740079    0.587873       0.297553   \n",
       "               3                  4     0.909921    0.803362       0.632574   \n",
       "               3                  5     0.832937    0.689097       0.431983   \n",
       "               3                  6     0.909524    0.813298       0.651814   \n",
       "               3                  7     0.877381    0.740286       0.520675   \n",
       "               3                  8     0.930556    0.819508       0.663460   \n",
       "               3                  9     0.944048    0.830199       0.683291   \n",
       "               3                 10     0.940873    0.824033       0.671899   \n",
       "               3                 11     0.910714    0.795422       0.617806   \n",
       "               3                 12     0.938095    0.830332       0.683460   \n",
       "               3                 14     0.938889    0.827537       0.678312   \n",
       "               3                 16     0.935714    0.828735       0.680759   \n",
       "               3                 18     0.933730    0.830465       0.683713   \n",
       "               3                 20     0.936508    0.829933       0.683122   \n",
       "               3                 22     0.935714    0.830154       0.683122   \n",
       "               3                 15     0.940873    0.830420       0.683629   \n",
       "               3                 21     0.945238    0.830509       0.683797   \n",
       "               3                 24     0.939683    0.830554       0.683882   \n",
       "               3                 27     0.940079    0.830598       0.683966   \n",
       "               3                 30     0.931349    0.830642       0.684051   \n",
       "122            3                  2     0.707143    0.813520       0.824473   \n",
       "               3                  3     0.823016    0.890924       0.859409   \n",
       "               3                  4     0.866270    0.907337       0.855359   \n",
       "1              5                  2     0.548810    0.430758       0.181603   \n",
       "8              5                  2     0.345238    0.349184       0.470042   \n",
       "               5                  3     0.549206    0.511533       0.453249   \n",
       "               5                  4     0.782143    0.665720       0.482194   \n",
       "               5                  5     0.696032    0.603043       0.452152   \n",
       "               5                  6     0.790476    0.704888       0.466329   \n",
       "               5                  7     0.729762    0.612535       0.457384   \n",
       "               5                  8     0.790476    0.705864       0.466160   \n",
       "               5                  9     0.780952    0.623093       0.467426   \n",
       "               5                 10     0.807540    0.706441       0.466413   \n",
       "               5                 11     0.782143    0.631476       0.479831   \n",
       "               5                 12     0.798016    0.706884       0.466076   \n",
       "               5                 14     0.817064    0.709457       0.470127   \n",
       "               5                 16     0.816270    0.712074       0.474599   \n",
       "               5                 18     0.815873    0.712385       0.474008   \n",
       "               5                 20     0.834127    0.715002       0.476709   \n",
       "               5                 33     0.844841    0.715268       0.476624   \n",
       "32             5                  2     0.664683    0.483721       0.208101   \n",
       "               5                  3     0.651984    0.487225       0.213671   \n",
       "               5                  4     0.784127    0.629347       0.319494   \n",
       "               5                  5     0.690873    0.492459       0.222532   \n",
       "               5                  6     0.897222    0.675257       0.390295   \n",
       "               5                  7     0.706746    0.568311       0.241350   \n",
       "               5                  8     0.804365    0.641856       0.337468   \n",
       "               5                  9     0.898016    0.675790       0.391139   \n",
       "               5                 10     0.800000    0.648332       0.347257   \n",
       "               5                 11     0.745635    0.625399       0.314262   \n",
       "               5                 12     0.888492    0.676455       0.392405   \n",
       "               5                 14     0.828175    0.664212       0.373333   \n",
       "               5                 20     0.864683    0.668870       0.379241   \n",
       "               5                 22     0.889683    0.674459       0.389030   \n",
       "               5                 15     0.887302    0.676588       0.392658   \n",
       "               5                 18     0.890873    0.677120       0.393502   \n",
       "               5                 21     0.890079    0.677741       0.394515   \n",
       "               5                 24     0.885714    0.678850       0.396456   \n",
       "               5                 27     0.909921    0.679915       0.398312   \n",
       "               5                 30     0.896825    0.680447       0.399409   \n",
       "               5                 33     0.902778    0.681201       0.400759   \n",
       "122            5                  2     0.759127    0.832328       0.750802   \n",
       "               5                  3     0.784524    0.875754       0.817975   \n",
       "               5                  4     0.828968    0.886666       0.823797   \n",
       "               5                  5     0.846825    0.898998       0.836962   \n",
       "1              1                  2     0.391667    0.570440       0.809620   \n",
       "               1                  3     0.647619    0.768364       0.816118   \n",
       "               1                  4     0.748810    0.840224       0.803122   \n",
       "               1                  5     0.767063    0.847809       0.794852   \n",
       "               1                  6     0.796032    0.850293       0.774684   \n",
       "               1                  7     0.856746    0.876508       0.795105   \n",
       "8              1                  2     0.598016    0.624423       0.391224   \n",
       "               1                  3     0.610317    0.637819       0.411392   \n",
       "               1                  4     0.931746    0.785353       0.600253   \n",
       "               1                  5     0.665079    0.670866       0.464810   \n",
       "               1                  6     0.951190    0.787216       0.602447   \n",
       "               1                  7     0.855159    0.741395       0.523376   \n",
       "               1                  8     0.876984    0.751907       0.542278   \n",
       "               1                  9     0.889286    0.759936       0.556540   \n",
       "               1                 10     0.909921    0.767344       0.569283   \n",
       "               1                 11     0.923810    0.775816       0.584557   \n",
       "32             1                  2     0.650397    0.630944       0.432405   \n",
       "               1                  3     0.781746    0.711276       0.470464   \n",
       "               1                  4     0.942857    0.760956       0.550548   \n",
       "               1                  5     0.851587    0.734209       0.506329   \n",
       "               1                  6     0.863889    0.739753       0.515612   \n",
       "               1                  7     0.896032    0.748802       0.531730   \n",
       "               1                  9     0.916667    0.749246       0.531899   \n",
       "               1                 10     0.960714    0.761489       0.549198   \n",
       "               1                 11     0.936905    0.758561       0.546667   \n",
       "               1                 12     0.962302    0.763840       0.553249   \n",
       "122            1                  2     0.575794    0.571372       0.478397   \n",
       "               1                  3     0.730952    0.705154       0.498397   \n",
       "               1                  4     0.934524    0.795378       0.614008   \n",
       "               1                  5     0.864683    0.748447       0.552236   \n",
       "               1                  6     0.938095    0.795999       0.614852   \n",
       "               1                  7     0.885317    0.774441       0.582869   \n",
       "               1                  8     0.948016    0.798128       0.618734   \n",
       "               1                  9     0.926587    0.789789       0.605401   \n",
       "               1                 10     0.928571    0.792938       0.610464   \n",
       "               1                 11     0.936111    0.794801       0.613418   \n",
       "1              3                  2     0.534524    0.430758       0.181603   \n",
       "8              3                  2     0.560714    0.618213       0.383629   \n",
       "               3                  3     0.615079    0.637331       0.416287   \n",
       "               3                  4     0.863492    0.750976       0.540760   \n",
       "               3                  5     0.747619    0.688032       0.435021   \n",
       "               3                  6     0.912698    0.784998       0.600675   \n",
       "               3                  7     0.769444    0.700231       0.455612   \n",
       "               3                  8     0.882143    0.766324       0.568354   \n",
       "               3                  9     0.918651    0.785176       0.600928   \n",
       "               3                 10     0.886111    0.771691       0.578397   \n",
       "               3                 11     0.859524    0.740108       0.524219   \n",
       "               3                 12     0.917460    0.785264       0.601013   \n",
       "               3                 14     0.894444    0.778211       0.590042   \n",
       "               3                 16     0.903571    0.780429       0.593502   \n",
       "               3                 18     0.918651    0.785353       0.600928   \n",
       "               3                 20     0.902778    0.783934       0.599409   \n",
       "               3                 22     0.910714    0.784865       0.600422   \n",
       "               3                 15     0.916667    0.785309       0.601097   \n",
       "               3                 21     0.916270    0.785531       0.601181   \n",
       "               3                 27     0.912302    0.785575       0.601266   \n",
       "               3                 30     0.916667    0.785885       0.601772   \n",
       "32             3                  2     0.751984    0.526437       0.309958   \n",
       "               3                  3     0.803968    0.600115       0.382110   \n",
       "               3                  4     0.928175    0.775949       0.582616   \n",
       "               3                  5     0.873810    0.719526       0.492321   \n",
       "               3                  6     0.928968    0.781849       0.593418   \n",
       "               3                  7     0.902778    0.745121       0.534599   \n",
       "               3                  8     0.940873    0.787482       0.603460   \n",
       "               3                  9     0.925000    0.761755       0.558903   \n",
       "               3                 10     0.931746    0.789966       0.607764   \n",
       "               3                 11     0.914683    0.772134       0.576118   \n",
       "               3                 12     0.943254    0.791430       0.610464   \n",
       "               3                 14     0.935714    0.792406       0.611646   \n",
       "               3                 16     0.935714    0.795821       0.617806   \n",
       "               3                 18     0.939683    0.795955       0.617384   \n",
       "               3                 20     0.949603    0.796443       0.617975   \n",
       "122            3                  2     0.812698    0.855527       0.788439   \n",
       "               3                  3     0.857540    0.888795       0.822954   \n",
       "1              5                  2     0.544444    0.430758       0.181603   \n",
       "8              5                  2     0.528968    0.430758       0.181603   \n",
       "               5                  8     0.724603    0.609120       0.283207   \n",
       "               5                  9     0.690873    0.572037       0.215021   \n",
       "               5                 10     0.745635    0.613778       0.291477   \n",
       "               5                 11     0.697619    0.579400       0.228608   \n",
       "               5                  4     0.682540    0.582417       0.234093   \n",
       "               5                  6     0.815873    0.643364       0.344388   \n",
       "               5                 12     0.816667    0.643542       0.344557   \n",
       "               5                 14     0.783730    0.626730       0.313840   \n",
       "               5                 16     0.773016    0.631742       0.323122   \n",
       "               5                 18     0.805556    0.644029       0.345232   \n",
       "               5                 20     0.800397    0.641013       0.340253   \n",
       "               5                 22     0.811905    0.643142       0.344051   \n",
       "               5                 15     0.816270    0.643808       0.344895   \n",
       "               5                 21     0.799603    0.644296       0.345570   \n",
       "               5                 24     0.813889    0.644606       0.346076   \n",
       "               5                 27     0.810317    0.644784       0.346413   \n",
       "               5                 30     0.791270    0.644872       0.346498   \n",
       "               5                 33     0.808333    0.645138       0.347004   \n",
       "32             5                  2     0.540079    0.430758       0.181603   \n",
       "               5                  5     0.547619    0.430846       0.181772   \n",
       "               5                  6     0.793254    0.640880       0.338650   \n",
       "               5                  9     0.776587    0.642033       0.340759   \n",
       "               5                 10     0.654365    0.483854       0.210295   \n",
       "               5                 11     0.549603    0.435238       0.189873   \n",
       "               5                  4     0.550000    0.439186       0.196287   \n",
       "               5                  8     0.609127    0.466332       0.199241   \n",
       "               5                 12     0.787302    0.643231       0.342954   \n",
       "               5                 14     0.690873    0.556689       0.251561   \n",
       "               5                 16     0.725397    0.600648       0.266498   \n",
       "               5                 18     0.782540    0.645183       0.346329   \n",
       "               5                 20     0.767857    0.626153       0.312236   \n",
       "               5                 22     0.788095    0.639594       0.336540   \n",
       "               5                 15     0.792460    0.644384       0.344979   \n",
       "               5                 21     0.788889    0.645981       0.347511   \n",
       "               5                 24     0.771429    0.646780       0.348776   \n",
       "               5                 27     0.805556    0.647667       0.350295   \n",
       "               5                 30     0.788889    0.648687       0.351730   \n",
       "               5                 33     0.794048    0.649885       0.353586   \n",
       "122            5                  2     0.460317    0.572791       0.810295   \n",
       "               5                  3     0.676984    0.771868       0.799072   \n",
       "               5                  4     0.751190    0.827981       0.780338   \n",
       "               5                  5     0.785714    0.836098       0.757215   \n",
       "               5                  6     0.836905    0.860140       0.779072   \n",
       "               5                  7     0.857143    0.869322       0.783797   \n",
       "               5                  8     0.875397    0.870963       0.779662   \n",
       "               5                  9     0.884524    0.872028       0.775443   \n",
       "1              1                  2     0.590873    0.628105       0.682278   \n",
       "               1                  3     0.748016    0.755367       0.700506   \n",
       "...                             ...          ...         ...            ...   \n",
       "               1                  7     0.863889    0.851801       0.743629   \n",
       "               1                  8     0.880556    0.857168       0.751308   \n",
       "               1                  9     0.903571    0.858543       0.750464   \n",
       "8              1                  2     0.723413    0.684129       0.489030   \n",
       "               1                  3     0.789683    0.709856       0.468945   \n",
       "               1                  4     0.846032    0.720990       0.486835   \n",
       "               1                  5     0.859524    0.741838       0.523544   \n",
       "               1                  6     0.877778    0.749379       0.536203   \n",
       "               1                  7     0.889286    0.759138       0.552405   \n",
       "               1                  8     0.905556    0.770981       0.570211   \n",
       "               1                  9     0.917063    0.780474       0.587764   \n",
       "               1                 10     0.939683    0.786329       0.598734   \n",
       "               1                 11     0.948016    0.789257       0.603713   \n",
       "32             1                  2     0.582143    0.657159       0.669789   \n",
       "               1                  3     0.812302    0.814363       0.710464   \n",
       "               1                  4     0.846429    0.825852       0.696709   \n",
       "122            1                  2     0.713492    0.663636       0.538481   \n",
       "               1                  3     0.802778    0.776703       0.611814   \n",
       "               1                  4     0.855952    0.789611       0.620422   \n",
       "               1                  5     0.875794    0.803407       0.638734   \n",
       "               1                  6     0.894444    0.805004       0.638987   \n",
       "               1                  7     0.922619    0.810681       0.647511   \n",
       "               1                  8     0.930952    0.817912       0.660675   \n",
       "               1                  9     0.939286    0.820174       0.663882   \n",
       "               1                 10     0.948810    0.822214       0.666667   \n",
       "               1                 11     0.941667    0.822569       0.666751   \n",
       "1              3                  2     0.545635    0.430758       0.181603   \n",
       "8              3                  2     0.482937    0.583259       0.817890   \n",
       "               3                  3     0.531746    0.636799       0.806835   \n",
       "               3                  4     0.630556    0.742016       0.805401   \n",
       "               3                  5     0.683730    0.800302       0.800422   \n",
       "               3                  6     0.801587    0.864132       0.832743   \n",
       "               3                  7     0.808333    0.866927       0.817046   \n",
       "               3                  9     0.858333    0.868346       0.792321   \n",
       "               3                 10     0.907540    0.877928       0.787511   \n",
       "               3                  8     0.896825    0.875798       0.786160   \n",
       "               3                 12     0.901190    0.878903       0.788354   \n",
       "               3                 18     0.914683    0.879791       0.788270   \n",
       "32             3                  2     0.546032    0.661285       0.810464   \n",
       "               3                  3     0.604365    0.704844       0.805232   \n",
       "               3                  4     0.911905    0.869588       0.761857   \n",
       "               3                  5     0.750794    0.835522       0.838565   \n",
       "               3                  6     0.909524    0.870476       0.761688   \n",
       "               3                  8     0.924206    0.871496       0.762363   \n",
       "               3                  9     0.880159    0.862935       0.771392   \n",
       "               3                 10     0.892460    0.866661       0.763629   \n",
       "               3                 11     0.912302    0.868346       0.761857   \n",
       "122            3                  2     0.738889    0.755057       0.678397   \n",
       "               3                  3     0.831349    0.773909       0.610802   \n",
       "               3                  4     0.842063    0.781006       0.600591   \n",
       "               3                  5     0.904365    0.788281       0.605907   \n",
       "1              5                  2     0.532540    0.430758       0.181603   \n",
       "8              5                  2     0.606746    0.689363       0.798650   \n",
       "               5                  3     0.750794    0.821815       0.783713   \n",
       "               5                  4     0.925000    0.859697       0.742954   \n",
       "               5                  6     0.812302    0.830642       0.723544   \n",
       "               5                  7     0.840873    0.837030       0.726329   \n",
       "               5                  8     0.863889    0.851845       0.748861   \n",
       "               5                  9     0.873810    0.852244       0.746835   \n",
       "               5                 10     0.900000    0.856325       0.748523   \n",
       "               5                 11     0.905952    0.859475       0.746160   \n",
       "32             5                  2     0.687698    0.519074       0.276034   \n",
       "               5                  3     0.723413    0.533756       0.302616   \n",
       "               5                  4     0.828571    0.699743       0.503629   \n",
       "               5                  5     0.748016    0.559794       0.347595   \n",
       "               5                  6     0.857937    0.718107       0.510970   \n",
       "               5                  7     0.758730    0.584235       0.381350   \n",
       "               5                  8     0.871032    0.741927       0.533165   \n",
       "               5                  9     0.907937    0.780740       0.592911   \n",
       "               5                 10     0.873413    0.753637       0.546329   \n",
       "               5                 11     0.816667    0.694021       0.496034   \n",
       "               5                 12     0.914683    0.781183       0.593755   \n",
       "               5                 14     0.875794    0.759404       0.555190   \n",
       "               5                 16     0.882937    0.763840       0.562363   \n",
       "               5                 18     0.904762    0.781405       0.594177   \n",
       "               5                 20     0.892063    0.778744       0.589536   \n",
       "               5                 22     0.906746    0.780651       0.592911   \n",
       "               5                 21     0.902778    0.781671       0.594684   \n",
       "               5                 24     0.905556    0.781982       0.595190   \n",
       "               5                 27     0.911111    0.782159       0.595190   \n",
       "               5                 30     0.909921    0.782603       0.595612   \n",
       "               5                 33     0.907540    0.783224       0.596456   \n",
       "122            5                  2     0.664286    0.718950       0.694008   \n",
       "               5                  3     0.758333    0.835610       0.739578   \n",
       "               5                  4     0.803968    0.869189       0.779494   \n",
       "               5                  5     0.840079    0.874468       0.782700   \n",
       "               5                  6     0.854365    0.883650       0.796793   \n",
       "               5                  7     0.859524    0.889549       0.805654   \n",
       "               5                  8     0.890476    0.892610       0.807595   \n",
       "               5                  9     0.894841    0.892787       0.806582   \n",
       "1              1                  2     0.652381    0.685282       0.575527   \n",
       "               1                  3     0.750397    0.770804       0.596540   \n",
       "               1                  4     0.810317    0.794624       0.622447   \n",
       "               1                  5     0.850397    0.811702       0.653080   \n",
       "8              1                  2     0.560714    0.480749       0.284051   \n",
       "               1                  3     0.581349    0.522800       0.351646   \n",
       "               1                  4     0.916270    0.789744       0.604810   \n",
       "               1                  5     0.668651    0.574610       0.432152   \n",
       "               1                  6     0.944048    0.794313       0.613249   \n",
       "               1                  7     0.868651    0.739975       0.518143   \n",
       "               1                  8     0.950794    0.797019       0.618397   \n",
       "               1                  9     0.901190    0.762065       0.553502   \n",
       "               1                 10     0.961111    0.803407       0.629114   \n",
       "               1                 11     0.923016    0.780341       0.587004   \n",
       "               1                 12     0.950794    0.805092       0.632321   \n",
       "               1                 14     0.955952    0.805935       0.633755   \n",
       "               1                 16     0.957540    0.806822       0.634684   \n",
       "32             1                  2     0.649206    0.655917       0.439916   \n",
       "               1                  3     0.673810    0.668692       0.459747   \n",
       "               1                  4     0.927778    0.784998       0.599240   \n",
       "               1                  5     0.836905    0.730350       0.503207   \n",
       "               1                  6     0.934127    0.788591       0.605485   \n",
       "               1                  7     0.842063    0.743391       0.526245   \n",
       "               1                  8     0.940079    0.790632       0.607595   \n",
       "               1                  9     0.882937    0.765791       0.565823   \n",
       "               1                 10     0.901984    0.771203       0.574937   \n",
       "               1                 11     0.916270    0.777236       0.584979   \n",
       "               1                 14     0.955556    0.790987       0.606498   \n",
       "               1                 16     0.964286    0.791253       0.606835   \n",
       "122            1                  2     0.592857    0.480571       0.303207   \n",
       "               1                  3     0.739683    0.560903       0.355190   \n",
       "               1                  4     0.780556    0.582949       0.391814   \n",
       "               1                  5     0.853571    0.686968       0.432152   \n",
       "               1                  6     0.964286    0.756476       0.541688   \n",
       "               1                  7     0.895238    0.712207       0.474852   \n",
       "               1                  8     0.955952    0.746451       0.527679   \n",
       "               1                  9     0.920635    0.737092       0.518059   \n",
       "               1                 10     0.956746    0.748181       0.529283   \n",
       "               1                 12     0.964286    0.756609       0.541857   \n",
       "               1                 14     0.961111    0.753770       0.538987   \n",
       "               1                 16     0.959921    0.756033       0.541941   \n",
       "               1                 22     0.967460    0.756432       0.541688   \n",
       "               1                 15     0.959524    0.756698       0.542025   \n",
       "               1                 18     0.966270    0.756787       0.542194   \n",
       "1              3                  2     0.525000    0.430758       0.181603   \n",
       "8              3                  2     0.516667    0.478487       0.331983   \n",
       "               3                  3     0.633333    0.536684       0.356371   \n",
       "               3                  4     0.890873    0.777014       0.595190   \n",
       "               3                  5     0.746032    0.616661       0.471055   \n",
       "               3                  7     0.810317    0.684129       0.490042   \n",
       "               3                  8     0.923413    0.780385       0.597046   \n",
       "               3                  9     0.932936    0.795067       0.616456   \n",
       "               3                 10     0.936905    0.782425       0.598987   \n",
       "               3                 11     0.880159    0.768408       0.582363   \n",
       "               3                  6     0.907937    0.779675       0.597637   \n",
       "               3                 12     0.917460    0.784333       0.601603   \n",
       "               3                 14     0.943651    0.785974       0.603038   \n",
       "               3                 16     0.931746    0.787793       0.605485   \n",
       "               3                 18     0.938492    0.789656       0.608439   \n",
       "               3                 20     0.934921    0.792583       0.612658   \n",
       "               3                 22     0.927381    0.794890       0.616287   \n",
       "               3                 15     0.936905    0.795200       0.616624   \n",
       "               3                 21     0.938095    0.795334       0.616878   \n",
       "               3                 27     0.932936    0.795378       0.616878   \n",
       "               3                 30     0.939286    0.795555       0.617131   \n",
       "32             3                  2     0.676984    0.509315       0.247595   \n",
       "               3                  3     0.702381    0.512287       0.252068   \n",
       "               3                  4     0.874206    0.703380       0.444979   \n",
       "               3                  5     0.760714    0.529143       0.279747   \n",
       "               3                  6     0.920635    0.771513       0.569789   \n",
       "               3                  7     0.803571    0.627528       0.308776   \n",
       "               3                  8     0.871032    0.725160       0.483629   \n",
       "               3                  9     0.916270    0.773421       0.573418   \n",
       "               3                 10     0.894841    0.732346       0.497046   \n",
       "               3                 11     0.852381    0.687810       0.416287   \n",
       "               3                 12     0.926190    0.774619       0.575696   \n",
       "               3                 14     0.896429    0.747250       0.524641   \n",
       "               3                 16     0.907143    0.756077       0.541350   \n",
       "               3                 18     0.918254    0.775727       0.577806   \n",
       "               3                 20     0.917857    0.766545       0.560591   \n",
       "               3                 22     0.918254    0.770538       0.567932   \n",
       "               3                 15     0.927778    0.775639       0.577637   \n",
       "               3                 21     0.925794    0.776171       0.578650   \n",
       "               3                 30     0.924206    0.776393       0.578987   \n",
       "               3                 33     0.922619    0.776792       0.579747   \n",
       "122            3                  2     0.726984    0.578380       0.450211   \n",
       "               3                  3     0.841667    0.723918       0.560084   \n",
       "               3                  4     0.890476    0.810415       0.665907   \n",
       "               3                  5     0.891270    0.827315       0.692658   \n",
       "               3                  6     0.909127    0.834235       0.701266   \n",
       "               3                  8     0.927778    0.837873       0.704219   \n",
       "               3                  9     0.928571    0.846700       0.718819   \n",
       "1              5                  2     0.532540    0.430758       0.181603   \n",
       "8              5                  2     0.398810    0.438786       0.734768   \n",
       "               5                  3     0.417460    0.440295       0.737046   \n",
       "               5                  4     0.421429    0.444331       0.740506   \n",
       "               5                  5     0.484127    0.505412       0.741688   \n",
       "               5                  6     0.882143    0.744633       0.641013   \n",
       "               5                  7     0.693254    0.712252       0.714852   \n",
       "               5                  8     0.707540    0.716643       0.712152   \n",
       "               5                  9     0.881746    0.745963       0.641941   \n",
       "               5                 10     0.730159    0.721478       0.699662   \n",
       "               5                 20     0.846429    0.721700       0.634852   \n",
       "               5                 22     0.880556    0.743080       0.639831   \n",
       "               5                 12     0.888095    0.747516       0.643291   \n",
       "               5                 15     0.874206    0.750133       0.646920   \n",
       "               5                 18     0.876190    0.753637       0.651730   \n",
       "               5                 21     0.884127    0.755767       0.654768   \n",
       "               5                 24     0.888095    0.756654       0.655190   \n",
       "               5                 27     0.894444    0.757363       0.655190   \n",
       "               5                 30     0.897222    0.757541       0.654093   \n",
       "               5                 33     0.894444    0.758206       0.653249   \n",
       "32             5                  2     0.509921    0.430758       0.181603   \n",
       "               5                  7     0.546825    0.431024       0.181772   \n",
       "               5                  8     0.757540    0.604107       0.273587   \n",
       "               5                  9     0.863889    0.728132       0.499747   \n",
       "               5                 10     0.773413    0.629613       0.320591   \n",
       "               5                 11     0.675000    0.566847       0.209873   \n",
       "               5                  4     0.678968    0.572924       0.218312   \n",
       "               5                  6     0.874206    0.727599       0.498903   \n",
       "               5                 12     0.869048    0.728974       0.500675   \n",
       "               5                 14     0.802381    0.673394       0.400169   \n",
       "               5                 16     0.833730    0.689274       0.428776   \n",
       "               5                 18     0.867460    0.730926       0.502616   \n",
       "               5                 20     0.863889    0.717663       0.481603   \n",
       "               5                 22     0.862698    0.727067       0.497975   \n",
       "               5                 15     0.876190    0.729862       0.501772   \n",
       "               5                 21     0.864286    0.731813       0.503797   \n",
       "               5                 24     0.867857    0.732434       0.504726   \n",
       "               5                 27     0.886905    0.733188       0.505992   \n",
       "               5                 30     0.875397    0.733632       0.506835   \n",
       "               5                 33     0.872222    0.734031       0.507595   \n",
       "122            5                  2     0.827381    0.841998       0.770380   \n",
       "               5                  3     0.828175    0.877262       0.824135   \n",
       "               5                  4     0.844444    0.888840       0.834008   \n",
       "               5                  5     0.842460    0.893808       0.837637   \n",
       "               5                  6     0.863492    0.895715       0.835190   \n",
       "1              1                  2     0.694444    0.503016       0.243376   \n",
       "               1                  3     0.736111    0.517255       0.266835   \n",
       "               1                  4     0.946032    0.751109       0.533502   \n",
       "               1                  5     0.790873    0.562722       0.349030   \n",
       "               1                  6     0.944048    0.759670       0.549198   \n",
       "               1                  7     0.864683    0.691182       0.434177   \n",
       "               1                  8     0.944048    0.768408       0.565316   \n",
       "               1                  9     0.911111    0.723785       0.486667   \n",
       "               1                 10     0.961508    0.779276       0.586245   \n",
       "               1                 11     0.928175    0.742903       0.519325   \n",
       "               1                 12     0.953968    0.785442       0.595105   \n",
       "               1                 14     0.961508    0.789611       0.602954   \n",
       "               1                 16     0.963095    0.790809       0.605148   \n",
       "               1                 18     0.957937    0.790853       0.605148   \n",
       "               1                 20     0.969048    0.791741       0.606329   \n",
       "8              1                  2     0.791270    0.738644       0.529451   \n",
       "               1                  3     0.851587    0.768231       0.571224   \n",
       "               1                  4     0.870635    0.781804       0.595021   \n",
       "               1                  5     0.891270    0.792938       0.613080   \n",
       "               1                  6     0.908333    0.809173       0.642110   \n",
       "               1                  7     0.901984    0.819331       0.660928   \n",
       "               1                  8     0.924603    0.820928       0.663544   \n",
       "               1                  9     0.921429    0.821904       0.664979   \n",
       "32             1                  2     0.728175    0.666519       0.385654   \n",
       "               1                  3     0.773810    0.683641       0.414515   \n",
       "               1                  4     0.802778    0.702537       0.447089   \n",
       "               1                  5     0.828968    0.717663       0.471899   \n",
       "               1                  6     0.858730    0.731148       0.495527   \n",
       "               1                  7     0.873810    0.741439       0.514599   \n",
       "               1                  8     0.897619    0.753815       0.537722   \n",
       "               1                  9     0.908333    0.763352       0.555612   \n",
       "               1                 10     0.923413    0.770937       0.569789   \n",
       "               1                 11     0.944048    0.782825       0.589789   \n",
       "122            1                  2     0.653571    0.677475       0.486498   \n",
       "               1                  3     0.762302    0.718595       0.503038   \n",
       "               1                  4     0.919048    0.789123       0.607679   \n",
       "               1                  5     0.836508    0.741794       0.530127   \n",
       "               1                  6     0.860317    0.754170       0.551308   \n",
       "               1                  7     0.871429    0.763928       0.567342   \n",
       "               1                  8     0.886905    0.773288       0.584135   \n",
       "               1                  9     0.880556    0.777324       0.590127   \n",
       "               1                 10     0.881349    0.781139       0.593080   \n",
       "               1                 11     0.919444    0.787349       0.604641   \n",
       "1              3                  2     0.545238    0.430758       0.181603   \n",
       "8              3                  2     0.515873    0.424414       0.179747   \n",
       "               3                  3     0.536905    0.425790       0.180759   \n",
       "               3                  4     0.744444    0.530208       0.289620   \n",
       "               3                  5     0.561508    0.438831       0.181181   \n",
       "               3                  6     0.898413    0.691093       0.429367   \n",
       "               3                  7     0.642460    0.472410       0.189958   \n",
       "               3                  8     0.798810    0.586010       0.316456   \n",
       "               3                  9     0.897222    0.692113       0.431055   \n",
       "               3                 10     0.832540    0.633561       0.328776   \n",
       "               3                 11     0.731746    0.505323       0.244473   \n",
       "               3                 12     0.898413    0.692867       0.432321   \n",
       "               3                 14     0.853968    0.649530       0.355274   \n",
       "               3                 16     0.864286    0.656405       0.368017   \n",
       "               3                 18     0.902381    0.694464       0.435021   \n",
       "               3                 20     0.886111    0.680713       0.411899   \n",
       "               3                 22     0.896032    0.690161       0.427848   \n",
       "               3                 15     0.896032    0.693976       0.434093   \n",
       "               3                 21     0.895238    0.694863       0.435781   \n",
       "               3                 24     0.900000    0.695529       0.436962   \n",
       "               3                 27     0.903571    0.696150       0.438143   \n",
       "               3                 30     0.905556    0.696593       0.438903   \n",
       "               3                 33     0.905556    0.696726       0.439156   \n",
       "32             3                  2     0.691667    0.616971       0.426414   \n",
       "               3                  3     0.826587    0.726712       0.527595   \n",
       "               3                  4     0.930556    0.803983       0.632321   \n",
       "               3                  5     0.883333    0.753637       0.553333   \n",
       "               3                  6     0.939286    0.815250       0.653840   \n",
       "               3                  7     0.915476    0.776171       0.586751   \n",
       "               3                  9     0.909524    0.778256       0.584979   \n",
       "               3                 10     0.931746    0.826961       0.675359   \n",
       "               3                 11     0.924603    0.796709       0.618734   \n",
       "               3                  8     0.941667    0.822347       0.667004   \n",
       "               3                 12     0.934127    0.829179       0.679578   \n",
       "               3                 14     0.935714    0.829933       0.681013   \n",
       "               3                 18     0.944048    0.830199       0.681266   \n",
       "               3                 20     0.939286    0.830554       0.681772   \n",
       "122            3                  2     0.633333    0.630412       0.659747   \n",
       "               3                  3     0.846825    0.834812       0.706667   \n",
       "               3                  5     0.869048    0.839026       0.709030   \n",
       "               3                  6     0.863889    0.844571       0.716878   \n",
       "               3                  7     0.879762    0.846833       0.718481   \n",
       "               3                  8     0.934921    0.852067       0.724641   \n",
       "               3                 10     0.948016    0.853886       0.728017   \n",
       "1              5                  2     0.521032    0.430758       0.181603   \n",
       "8              5                  2     0.455556    0.569242       0.818397   \n",
       "               5                  3     0.450397    0.569508       0.818734   \n",
       "               5                  4     0.793651    0.863068       0.795612   \n",
       "               5                  5     0.459524    0.571505       0.821350   \n",
       "               5                  6     0.810714    0.866306       0.795781   \n",
       "               5                  7     0.676984    0.793426       0.813165   \n",
       "               5                  8     0.829365    0.866971       0.795274   \n",
       "               5                  9     0.755556    0.838893       0.794346   \n",
       "               5                 10     0.776190    0.845413       0.792152   \n",
       "               5                 11     0.789286    0.851712       0.793502   \n",
       "               5                 20     0.877381    0.874423       0.776962   \n",
       "32             5                  2     0.467857    0.569331       0.818397   \n",
       "               5                  3     0.469048    0.570263       0.818312   \n",
       "               5                  4     0.478571    0.574255       0.820084   \n",
       "               5                  5     0.507540    0.586276       0.820844   \n",
       "               5                  6     0.707540    0.777945       0.801013   \n",
       "               5                  7     0.788492    0.853974       0.812743   \n",
       "               5                  8     0.833333    0.881521       0.822363   \n",
       "               5                  9     0.858333    0.889727       0.817131   \n",
       "               5                 10     0.875794    0.895050       0.819494   \n",
       "122            5                  2     0.693254    0.619012       0.620591   \n",
       "               5                  3     0.798016    0.789567       0.769114   \n",
       "               5                  4     0.859127    0.863778       0.809030   \n",
       "               5                  5     0.880952    0.874823       0.809536   \n",
       "               5                  6     0.880952    0.876553       0.806835   \n",
       "               5                  7     0.891270    0.877839       0.798650   \n",
       "               5                  8     0.903968    0.884271       0.796793   \n",
       "1              1                  2     0.493651    0.423838       0.195696   \n",
       "               1                  3     0.522619    0.430979       0.192827   \n",
       "               1                  4     0.881746    0.721744       0.483291   \n",
       "               1                  5     0.582937    0.484164       0.276962   \n",
       "               1                  6     0.960317    0.778832       0.584641   \n",
       "               1                  7     0.744048    0.563742       0.334177   \n",
       "               1                  8     0.927778    0.756787       0.547004   \n",
       "               1                  9     0.955952    0.779143       0.585232   \n",
       "               1                 10     0.946825    0.761356       0.555106   \n",
       "               1                 11     0.854365    0.706707       0.456118   \n",
       "               1                 12     0.964683    0.779276       0.585485   \n",
       "               1                 14     0.954365    0.768941       0.567679   \n",
       "               1                 16     0.952778    0.770715       0.570717   \n",
       "               1                 18     0.962698    0.779542       0.585992   \n",
       "               1                 20     0.955159    0.777280       0.582363   \n",
       "               1                 22     0.957540    0.778611       0.584388   \n",
       "               1                 15     0.958333    0.779454       0.585823   \n",
       "               1                 21     0.967857    0.779853       0.586582   \n",
       "               1                 24     0.969841    0.780119       0.587089   \n",
       "               1                 27     0.962302    0.780385       0.587511   \n",
       "               1                 30     0.961508    0.780429       0.587595   \n",
       "               1                 33     0.963889    0.780784       0.588270   \n",
       "8              1                  2     0.420635    0.429826       0.610886   \n",
       "               1                  3     0.737302    0.680669       0.610211   \n",
       "               1                  4     0.813492    0.758339       0.587173   \n",
       "               1                  5     0.854762    0.776393       0.597890   \n",
       "               1                  6     0.873413    0.791785       0.621688   \n",
       "               1                  7     0.892460    0.805846       0.640506   \n",
       "               1                  8     0.895238    0.809351       0.644557   \n",
       "               1                  9     0.909921    0.810814       0.645907   \n",
       "               1                 10     0.917063    0.811790       0.646920   \n",
       "32             1                  2     0.753571    0.713405       0.473840   \n",
       "               1                  3     0.816270    0.742947       0.525823   \n",
       "               1                  4     0.941667    0.812544       0.650549   \n",
       "               1                  5     0.882540    0.774619       0.582532   \n",
       "               1                  6     0.937302    0.812677       0.650464   \n",
       "               1                  7     0.909524    0.784954       0.600928   \n",
       "               1                  8     0.916270    0.792273       0.613671   \n",
       "               1                  9     0.926587    0.800435       0.629367   \n",
       "               1                 10     0.941270    0.805358       0.638228   \n",
       "               1                 11     0.933730    0.809661       0.645992   \n",
       "122            1                  2     0.729365    0.610406       0.304726   \n",
       "               1                  3     0.811508    0.673705       0.404557   \n",
       "               1                  4     0.938889    0.751641       0.536962   \n",
       "               1                  5     0.867857    0.691137       0.428608   \n",
       "               1                  6     0.945238    0.754524       0.541603   \n",
       "               1                  7     0.882143    0.707505       0.458143   \n",
       "               1                  8     0.946032    0.757142       0.544979   \n",
       "               1                  9     0.900000    0.728176       0.494093   \n",
       "               1                 10     0.961508    0.761489       0.551561   \n",
       "               1                 11     0.946825    0.745875       0.526414   \n",
       "               1                 12     0.956746    0.764372       0.556371   \n",
       "               1                 14     0.964286    0.765791       0.558734   \n",
       "1              3                  2     0.519444    0.430758       0.181603   \n",
       "               3                 11     0.847222    0.757674       0.550633   \n",
       "               3                  6     0.919444    0.782071       0.590717   \n",
       "               3                  8     0.882540    0.764949       0.561013   \n",
       "               3                 10     0.905952    0.768763       0.567004   \n",
       "               3                 12     0.918651    0.782204       0.590970   \n",
       "               3                 14     0.900397    0.773909       0.576624   \n",
       "               3                 16     0.907540    0.775683       0.579831   \n",
       "               3                 18     0.930952    0.776437       0.581266   \n",
       "               3                 20     0.919444    0.777768       0.583797   \n",
       "               3                 22     0.914286    0.782026       0.590633   \n",
       "               3                  9     0.925397    0.782115       0.590802   \n",
       "               3                 15     0.919444    0.782381       0.591308   \n",
       "               3                 24     0.924206    0.782559       0.591646   \n",
       "               3                 27     0.916270    0.782736       0.591983   \n",
       "8              3                  2     0.563889    0.625621       0.693671   \n",
       "               3                  3     0.678571    0.737269       0.672911   \n",
       "               3                  4     0.880159    0.821150       0.672827   \n",
       "               3                  5     0.823413    0.812145       0.684557   \n",
       "               3                  8     0.849206    0.813964       0.668692   \n",
       "               3                  9     0.923016    0.827005       0.677384   \n",
       "               3                 10     0.859127    0.820263       0.675696   \n",
       "               3                 16     0.921429    0.823501       0.671224   \n",
       "               3                 18     0.912302    0.824521       0.672911   \n",
       "               3                 20     0.907540    0.825763       0.675190   \n",
       "               3                 22     0.923016    0.826694       0.676878   \n",
       "               3                  6     0.909127    0.826783       0.676962   \n",
       "               3                 12     0.921032    0.827182       0.677722   \n",
       "               3                 24     0.922619    0.827271       0.677806   \n",
       "               3                 30     0.921429    0.827315       0.677806   \n",
       "               3                 33     0.918651    0.827404       0.677890   \n",
       "32             3                  2     0.666270    0.696238       0.557975   \n",
       "               3                  3     0.803175    0.763840       0.581181   \n",
       "               3                  4     0.908730    0.798039       0.624388   \n",
       "               3                  5     0.868651    0.776570       0.594599   \n",
       "               3                  6     0.920635    0.802520       0.632574   \n",
       "               3                  7     0.882540    0.783668       0.602025   \n",
       "               3                  8     0.923016    0.806334       0.638481   \n",
       "               3                 10     0.937698    0.807754       0.641013   \n",
       "               3                 11     0.911508    0.792140       0.613586   \n",
       "               3                 12     0.936111    0.809661       0.644641   \n",
       "               3                 14     0.937698    0.814319       0.651392   \n",
       "               3                 16     0.938492    0.818400       0.659072   \n",
       "122            3                  2     0.728175    0.699787       0.486835   \n",
       "               3                  3     0.803968    0.797463       0.659578   \n",
       "               3                  4     0.884524    0.875133       0.792827   \n",
       "               3                  5     0.908730    0.890969       0.804641   \n",
       "               3                  6     0.910714    0.893630       0.808945   \n",
       "1              5                  2     0.500397    0.471123       0.271392   \n",
       "               5                  3     0.507937    0.473075       0.269789   \n",
       "               5                  4     0.750794    0.556112       0.338481   \n",
       "               5                  5     0.631349    0.513751       0.276034   \n",
       "               5                  6     0.820635    0.624956       0.425401   \n",
       "               5                  7     0.708730    0.533357       0.301013   \n",
       "               5                  8     0.783333    0.572525       0.368523   \n",
       "               5                 10     0.790476    0.586409       0.394093   \n",
       "               5                 11     0.760714    0.550612       0.329114   \n",
       "               5                 12     0.823810    0.634626       0.428101   \n",
       "               5                 14     0.805159    0.594837       0.408776   \n",
       "               5                 16     0.821032    0.598563       0.414852   \n",
       "               5                 18     0.829365    0.642699       0.429114   \n",
       "               5                 20     0.821032    0.602067       0.419494   \n",
       "               5                 22     0.817857    0.620697       0.424557   \n",
       "               5                  9     0.823413    0.629524       0.426076   \n",
       "               5                 15     0.831349    0.640836       0.428776   \n",
       "               5                 21     0.818254    0.646159       0.429536   \n",
       "               5                 24     0.828175    0.650018       0.430464   \n",
       "               5                 27     0.814286    0.654276       0.431308   \n",
       "               5                 30     0.830952    0.656893       0.431983   \n",
       "               5                 33     0.824206    0.658534       0.432658   \n",
       "8              5                  2     0.468651    0.505101       0.616878   \n",
       "               5                  3     0.692460    0.681201       0.622700   \n",
       "               5                  4     0.783333    0.728442       0.620759   \n",
       "               5                 18     0.869841    0.749601       0.532405   \n",
       "               5                 20     0.868651    0.739044       0.512574   \n",
       "               5                 22     0.867857    0.744500       0.522785   \n",
       "               5                  6     0.863492    0.745076       0.523882   \n",
       "               5                  9     0.868651    0.746141       0.525823   \n",
       "               5                 12     0.874603    0.747649       0.528692   \n",
       "               5                 15     0.869841    0.748536       0.530380   \n",
       "               5                 21     0.876587    0.750798       0.534684   \n",
       "               5                 24     0.884127    0.752928       0.538397   \n",
       "               5                 27     0.879365    0.756210       0.544473   \n",
       "               5                 30     0.884127    0.757186       0.546160   \n",
       "               5                 33     0.868651    0.757452       0.546498   \n",
       "32             5                  2     0.477778    0.532958       0.749030   \n",
       "               5                  3     0.496825    0.551366       0.767764   \n",
       "               5                  4     0.881349    0.873891       0.775021   \n",
       "               5                  5     0.668254    0.685016       0.762110   \n",
       "               5                  6     0.892460    0.878282       0.779831   \n",
       "               5                  7     0.812698    0.807488       0.746835   \n",
       "               5                  8     0.879365    0.876198       0.776878   \n",
       "               5                  9     0.851190    0.866528       0.779747   \n",
       "               5                 10     0.886508    0.877040       0.777806   \n",
       "               5                 11     0.865873    0.873270       0.775527   \n",
       "               5                 12     0.887698    0.877129       0.777975   \n",
       "               5                 18     0.894444    0.878194       0.779831   \n",
       "               5                 22     0.904365    0.878238       0.779747   \n",
       "               5                 15     0.903571    0.878460       0.780084   \n",
       "122            5                  2     0.597222    0.678007       0.731392   \n",
       "               5                  3     0.730159    0.797995       0.731308   \n",
       "               5                  4     0.778968    0.820617       0.727848   \n",
       "               5                  5     0.800397    0.830642       0.729789   \n",
       "               5                  6     0.827381    0.833481       0.715359   \n",
       "               5                  9     0.871825    0.837074       0.704979   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                1.737384  \n",
       "               1                2.746407  \n",
       "               1                3.780096  \n",
       "               1                4.823596  \n",
       "               1                5.901322  \n",
       "8              1                1.041274  \n",
       "               1                2.013830  \n",
       "               1                3.030958  \n",
       "               1                4.018355  \n",
       "               1                4.996470  \n",
       "               1                5.974369  \n",
       "               1                7.018800  \n",
       "               1                8.021910  \n",
       "32             1                1.510834  \n",
       "               1                2.517899  \n",
       "               1               11.737579  \n",
       "               1                4.757215  \n",
       "               1               12.685921  \n",
       "               1                6.851718  \n",
       "               1               13.714334  \n",
       "               1                8.793038  \n",
       "               1                9.762101  \n",
       "               1               10.796256  \n",
       "122            1                0.966112  \n",
       "               1                2.016091  \n",
       "               1                3.078858  \n",
       "               1                4.012818  \n",
       "               1                5.040853  \n",
       "1              3                1.323962  \n",
       "8              3                1.417094  \n",
       "               3                2.576081  \n",
       "               3                3.832223  \n",
       "               3                5.245420  \n",
       "               3                6.504427  \n",
       "               3                7.869504  \n",
       "               3                9.173052  \n",
       "               3               10.463114  \n",
       "32             3                1.831090  \n",
       "               3                3.321462  \n",
       "               3                4.869611  \n",
       "               3                6.326309  \n",
       "122            3                2.207851  \n",
       "               3                4.287650  \n",
       "               3                6.335766  \n",
       "1              5                1.559099  \n",
       "8              5                1.659954  \n",
       "               5                3.252145  \n",
       "               5                5.009747  \n",
       "               5                7.373743  \n",
       "               5                8.960562  \n",
       "               5               10.671936  \n",
       "32             5                2.087397  \n",
       "               5                4.483478  \n",
       "               5               23.032310  \n",
       "               5                8.734228  \n",
       "               5               42.926213  \n",
       "               5               12.542290  \n",
       "               5               27.134876  \n",
       "               5               44.073521  \n",
       "               5               29.257227  \n",
       "               5               21.210851  \n",
       "               5               45.863152  \n",
       "               5               33.607411  \n",
       "               5               35.433949  \n",
       "               5               49.986061  \n",
       "               5               39.395359  \n",
       "               5               41.226033  \n",
       "               5               47.749661  \n",
       "               5               51.940331  \n",
       "               5               54.844048  \n",
       "               5               56.632432  \n",
       "               5               58.477479  \n",
       "122            5                3.137511  \n",
       "               5                6.090391  \n",
       "               5                9.000678  \n",
       "               5               11.817174  \n",
       "1              1                1.737384  \n",
       "               1                2.746407  \n",
       "               1                3.780096  \n",
       "               1                4.823596  \n",
       "               1                5.901322  \n",
       "8              1                1.041274  \n",
       "               1                2.013830  \n",
       "               1                3.030958  \n",
       "               1                4.018355  \n",
       "               1                4.996470  \n",
       "               1                5.974369  \n",
       "               1                7.018800  \n",
       "               1                8.021910  \n",
       "32             1                1.510834  \n",
       "               1                2.517899  \n",
       "               1               11.737579  \n",
       "               1                4.757215  \n",
       "               1               12.685921  \n",
       "               1                6.851718  \n",
       "               1               13.714334  \n",
       "               1                8.793038  \n",
       "               1                9.762101  \n",
       "               1               10.796256  \n",
       "122            1                0.966112  \n",
       "               1                2.016091  \n",
       "               1                3.078858  \n",
       "               1                4.012818  \n",
       "               1                5.040853  \n",
       "1              3                1.323962  \n",
       "8              3                1.417094  \n",
       "               3                2.576081  \n",
       "               3                3.832223  \n",
       "               3                5.245420  \n",
       "               3                6.504427  \n",
       "               3                7.869504  \n",
       "               3                9.173052  \n",
       "               3               10.463114  \n",
       "32             3                1.831090  \n",
       "               3                3.321462  \n",
       "               3                4.869611  \n",
       "               3                6.326309  \n",
       "122            3                2.207851  \n",
       "               3                4.287650  \n",
       "               3                6.335766  \n",
       "1              5                1.559099  \n",
       "8              5                1.659954  \n",
       "               5                3.252145  \n",
       "               5                5.009747  \n",
       "               5                7.373743  \n",
       "               5                8.960562  \n",
       "               5               10.671936  \n",
       "32             5                2.087397  \n",
       "               5                4.483478  \n",
       "               5               23.032310  \n",
       "               5                8.734228  \n",
       "               5               42.926213  \n",
       "               5               12.542290  \n",
       "               5               27.134876  \n",
       "               5               44.073521  \n",
       "               5               29.257227  \n",
       "               5               21.210851  \n",
       "               5               45.863152  \n",
       "               5               33.607411  \n",
       "               5               35.433949  \n",
       "               5               49.986061  \n",
       "               5               39.395359  \n",
       "               5               41.226033  \n",
       "               5               47.749661  \n",
       "               5               51.940331  \n",
       "               5               54.844048  \n",
       "               5               56.632432  \n",
       "               5               58.477479  \n",
       "122            5                3.137511  \n",
       "               5                6.090391  \n",
       "               5                9.000678  \n",
       "               5               11.817174  \n",
       "1              1                1.013389  \n",
       "               1                1.974355  \n",
       "               1               11.192840  \n",
       "               1                4.032335  \n",
       "               1               12.205366  \n",
       "               1                5.938341  \n",
       "               1               13.187299  \n",
       "               1                7.954791  \n",
       "               1                9.007837  \n",
       "               1               10.062756  \n",
       "8              1                1.034562  \n",
       "               1                2.082721  \n",
       "               1                9.227789  \n",
       "               1                3.902641  \n",
       "               1               10.224311  \n",
       "32             1                0.995917  \n",
       "               1                1.934400  \n",
       "               1                2.920137  \n",
       "               1                4.063429  \n",
       "               1                5.163448  \n",
       "122            1                0.985263  \n",
       "               1                2.065951  \n",
       "               1                3.111821  \n",
       "               1                4.120334  \n",
       "               1                5.114748  \n",
       "               1                6.066829  \n",
       "               1                7.070870  \n",
       "1              3                1.809197  \n",
       "8              3                1.415778  \n",
       "               3                2.665348  \n",
       "               3               14.896438  \n",
       "               3               16.263383  \n",
       "               3                7.452541  \n",
       "               3               17.669458  \n",
       "               3               10.311922  \n",
       "               3               19.286822  \n",
       "               3               13.495634  \n",
       "               3               27.856055  \n",
       "               3               21.985776  \n",
       "               3               29.036748  \n",
       "               3               30.103149  \n",
       "               3               31.433573  \n",
       "               3               32.769451  \n",
       "32             3                1.423730  \n",
       "               3                3.023336  \n",
       "               3               16.916582  \n",
       "               3                6.090839  \n",
       "               3               18.565713  \n",
       "               3                8.980543  \n",
       "               3               20.113684  \n",
       "               3               12.313494  \n",
       "               3               13.687260  \n",
       "               3               15.197614  \n",
       "122            3                2.396187  \n",
       "               3                4.404867  \n",
       "               3               23.059733  \n",
       "               3                8.638499  \n",
       "               3               25.213438  \n",
       "               3               12.886271  \n",
       "               3               27.123679  \n",
       "               3               16.882940  \n",
       "               3               29.040926  \n",
       "               3               20.753421  \n",
       "               3               31.049070  \n",
       "1              5                1.699689  \n",
       "8              5                1.587398  \n",
       "               5                3.335802  \n",
       "               5               16.502883  \n",
       "               5                8.295849  \n",
       "               5                9.963014  \n",
       "               5               12.484927  \n",
       "               5               14.902415  \n",
       "32             5                1.896466  \n",
       "               5                3.721369  \n",
       "               5               20.478017  \n",
       "               5                7.663091  \n",
       "               5               39.714115  \n",
       "               5               11.360219  \n",
       "               5               24.273933  \n",
       "               5               41.467146  \n",
       "               5               26.162183  \n",
       "               5               18.679377  \n",
       "               5               43.798101  \n",
       "               5               30.126800  \n",
       "               5               31.975581  \n",
       "               5               47.764469  \n",
       "               5               35.611126  \n",
       "               5               37.668527  \n",
       "               5               45.657773  \n",
       "               5               49.662081  \n",
       "               5               52.129842  \n",
       "               5               53.982351  \n",
       "               5               55.768187  \n",
       "               5               57.759473  \n",
       "122            5                2.851944  \n",
       "               5                5.889095  \n",
       "               5                8.706509  \n",
       "               5               11.832607  \n",
       "               5               14.944608  \n",
       "               5               18.392418  \n",
       "               5               21.449749  \n",
       "               5               24.750123  \n",
       "               5               27.521660  \n",
       "1              1                0.894731  \n",
       "               1                1.677100  \n",
       "               1                8.936324  \n",
       "               1                3.277389  \n",
       "               1                9.707876  \n",
       "               1                4.877893  \n",
       "               1               10.586093  \n",
       "               1                6.523361  \n",
       "               1               11.379750  \n",
       "               1                8.160818  \n",
       "               1               13.221132  \n",
       "               1               14.015121  \n",
       "8              1                0.809756  \n",
       "               1                1.591012  \n",
       "               1                2.453870  \n",
       "               1                3.231255  \n",
       "               1                4.068311  \n",
       "               1                4.927074  \n",
       "               1                6.192449  \n",
       "               1                7.008124  \n",
       "32             1                0.815554  \n",
       "               1                1.616658  \n",
       "               1                2.473630  \n",
       "               1                3.252410  \n",
       "               1                4.003867  \n",
       "               1                4.837522  \n",
       "               1                6.127309  \n",
       "122            1                0.916941  \n",
       "               1                1.740327  \n",
       "               1                2.566799  \n",
       "               1                3.414125  \n",
       "               1                4.222251  \n",
       "1              3                1.185713  \n",
       "8              3                0.844786  \n",
       "               3                1.651068  \n",
       "               3               11.345461  \n",
       "               3                3.504687  \n",
       "               3               21.949679  \n",
       "               3                5.791350  \n",
       "               3               13.551845  \n",
       "               3               23.062431  \n",
       "               3                9.119050  \n",
       "               3               10.237164  \n",
       "               3               24.189863  \n",
       "               3               16.295669  \n",
       "               3               17.445726  \n",
       "               3               26.434530  \n",
       "               3               19.644563  \n",
       "               3               20.869996  \n",
       "               3               25.303043  \n",
       "               3               27.565497  \n",
       "               3               28.660675  \n",
       "               3               29.767123  \n",
       "               3               30.917812  \n",
       "               3               32.010602  \n",
       "32             3                1.216905  \n",
       "               3                2.472625  \n",
       "               3               13.271930  \n",
       "               3                4.909210  \n",
       "               3               14.483956  \n",
       "               3                7.311148  \n",
       "               3               15.621841  \n",
       "               3               25.742428  \n",
       "               3               16.823827  \n",
       "               3               12.062848  \n",
       "               3               27.110594  \n",
       "               3               19.163795  \n",
       "               3               20.362294  \n",
       "               3               29.460092  \n",
       "               3               22.672202  \n",
       "               3               23.830000  \n",
       "               3               28.270326  \n",
       "               3               30.650142  \n",
       "               3               31.840021  \n",
       "               3               33.030833  \n",
       "               3               34.194960  \n",
       "122            3                1.606288  \n",
       "               3                3.097433  \n",
       "               3                4.684720  \n",
       "1              5                1.428904  \n",
       "8              5                1.460850  \n",
       "               5                2.885396  \n",
       "               5               15.451152  \n",
       "               5                5.598635  \n",
       "               5               16.918474  \n",
       "               5                8.406757  \n",
       "               5               18.299810  \n",
       "               5               11.201339  \n",
       "               5               19.644307  \n",
       "               5               14.077168  \n",
       "               5               21.059023  \n",
       "               5               22.393581  \n",
       "               5               23.821218  \n",
       "               5               25.311138  \n",
       "               5               26.797488  \n",
       "               5               34.629877  \n",
       "32             5                1.624394  \n",
       "               5                3.194861  \n",
       "               5               17.155831  \n",
       "               5                6.421164  \n",
       "               5               31.138687  \n",
       "               5                9.491996  \n",
       "               5               20.202149  \n",
       "               5               32.651873  \n",
       "               5               21.835074  \n",
       "               5               15.704401  \n",
       "               5               34.194099  \n",
       "               5               24.864797  \n",
       "               5               28.021445  \n",
       "               5               29.626286  \n",
       "               5               35.847988  \n",
       "               5               37.454581  \n",
       "               5               39.075829  \n",
       "               5               40.641193  \n",
       "               5               42.206608  \n",
       "               5               43.848318  \n",
       "               5               45.428985  \n",
       "122            5                2.225065  \n",
       "               5                4.425601  \n",
       "               5                6.734693  \n",
       "               5                8.911242  \n",
       "1              1                0.853173  \n",
       "               1                1.723129  \n",
       "               1                2.562127  \n",
       "               1                3.373452  \n",
       "               1                4.169492  \n",
       "               1                5.030710  \n",
       "8              1                0.829649  \n",
       "               1                1.595178  \n",
       "               1                9.353316  \n",
       "               1                3.260125  \n",
       "               1               10.201704  \n",
       "               1                4.934201  \n",
       "               1                5.757595  \n",
       "               1                6.686191  \n",
       "               1                7.540108  \n",
       "               1                8.430636  \n",
       "32             1                0.831454  \n",
       "               1                1.628624  \n",
       "               1                8.921503  \n",
       "               1                3.328953  \n",
       "               1                4.109403  \n",
       "               1                4.942526  \n",
       "               1                6.354565  \n",
       "               1               10.726503  \n",
       "               1                8.009633  \n",
       "               1               11.594158  \n",
       "122            1                0.888204  \n",
       "               1                1.795293  \n",
       "               1                9.253452  \n",
       "               1                3.401129  \n",
       "               1               10.054116  \n",
       "               1                5.085820  \n",
       "               1               10.973660  \n",
       "               1                6.692780  \n",
       "               1                7.556019  \n",
       "               1                8.391512  \n",
       "1              3                1.132610  \n",
       "8              3                0.830756  \n",
       "               3                1.636556  \n",
       "               3               11.132442  \n",
       "               3                3.608488  \n",
       "               3               21.808366  \n",
       "               3                5.832074  \n",
       "               3               13.330746  \n",
       "               3               22.864484  \n",
       "               3               14.415001  \n",
       "               3               10.040499  \n",
       "               3               23.909596  \n",
       "               3               16.491284  \n",
       "               3               17.593741  \n",
       "               3               26.096862  \n",
       "               3               19.757640  \n",
       "               3               20.776468  \n",
       "               3               25.002603  \n",
       "               3               27.154573  \n",
       "               3               28.818199  \n",
       "               3               29.927739  \n",
       "32             3                1.155823  \n",
       "               3                2.306064  \n",
       "               3               12.056652  \n",
       "               3                4.633337  \n",
       "               3               13.251858  \n",
       "               3                6.861597  \n",
       "               3               14.374617  \n",
       "               3                8.770915  \n",
       "               3               15.494554  \n",
       "               3               10.928107  \n",
       "               3               16.673812  \n",
       "               3               17.841789  \n",
       "               3               19.042862  \n",
       "               3               20.175138  \n",
       "               3               21.317807  \n",
       "122            3                1.509054  \n",
       "               3                2.939840  \n",
       "1              5                1.337012  \n",
       "8              5                1.339978  \n",
       "               5               14.178145  \n",
       "               5                7.297212  \n",
       "               5               15.507364  \n",
       "               5               10.018889  \n",
       "               5               11.364162  \n",
       "               5               24.305655  \n",
       "               5               26.380527  \n",
       "               5               17.905989  \n",
       "               5               18.965414  \n",
       "               5               29.082980  \n",
       "               5               21.614187  \n",
       "               5               23.008507  \n",
       "               5               27.748586  \n",
       "               5               30.551478  \n",
       "               5               31.993179  \n",
       "               5               33.392151  \n",
       "               5               34.839391  \n",
       "               5               36.295973  \n",
       "32             5                1.552075  \n",
       "               5                4.564710  \n",
       "               5               29.554761  \n",
       "               5               31.047097  \n",
       "               5               18.755310  \n",
       "               5               12.362426  \n",
       "               5               13.932996  \n",
       "               5               17.042776  \n",
       "               5               32.574180  \n",
       "               5               21.784815  \n",
       "               5               23.275492  \n",
       "               5               35.753286  \n",
       "               5               26.442977  \n",
       "               5               28.004313  \n",
       "               5               34.159954  \n",
       "               5               37.346119  \n",
       "               5               38.887693  \n",
       "               5               40.420472  \n",
       "               5               41.793969  \n",
       "               5               42.929612  \n",
       "122            5                2.196937  \n",
       "               5                4.308048  \n",
       "               5                6.438229  \n",
       "               5                8.596534  \n",
       "               5               10.736500  \n",
       "               5               12.943809  \n",
       "               5               15.156112  \n",
       "               5               17.370901  \n",
       "1              1                0.815069  \n",
       "               1                1.626414  \n",
       "...                                  ...  \n",
       "               1                4.881256  \n",
       "               1                5.683805  \n",
       "               1                6.550267  \n",
       "8              1                0.884177  \n",
       "               1                1.626995  \n",
       "               1                2.474918  \n",
       "               1                3.262308  \n",
       "               1                4.056794  \n",
       "               1                4.833454  \n",
       "               1                5.670228  \n",
       "               1                6.450147  \n",
       "               1                7.243138  \n",
       "               1                8.060955  \n",
       "32             1                0.788849  \n",
       "               1                1.604884  \n",
       "               1                2.400253  \n",
       "122            1                0.863470  \n",
       "               1                1.678402  \n",
       "               1                2.471157  \n",
       "               1                3.256718  \n",
       "               1                4.119869  \n",
       "               1                4.896393  \n",
       "               1                5.754048  \n",
       "               1                6.539913  \n",
       "               1                7.365450  \n",
       "               1                8.129808  \n",
       "1              3                1.110077  \n",
       "8              3                1.107688  \n",
       "               3                2.236085  \n",
       "               3                3.375321  \n",
       "               3                4.415340  \n",
       "               3                5.555745  \n",
       "               3                6.661717  \n",
       "               3                8.268404  \n",
       "               3               13.263709  \n",
       "               3               12.189661  \n",
       "               3               14.208657  \n",
       "               3               15.938927  \n",
       "32             3                1.223749  \n",
       "               3                2.482930  \n",
       "               3               12.638566  \n",
       "               3                4.846637  \n",
       "               3               13.807913  \n",
       "               3               14.972590  \n",
       "               3                9.065159  \n",
       "               3               10.282121  \n",
       "               3               11.469874  \n",
       "122            3                1.503476  \n",
       "               3                2.965688  \n",
       "               3                4.469086  \n",
       "               3                6.037410  \n",
       "1              5                1.406102  \n",
       "8              5                1.546546  \n",
       "               5                2.926043  \n",
       "               5               14.970328  \n",
       "               5                6.420013  \n",
       "               5                7.906267  \n",
       "               5                9.324220  \n",
       "               5               10.695553  \n",
       "               5               12.195689  \n",
       "               5               13.659519  \n",
       "32             5                1.662758  \n",
       "               5                3.245816  \n",
       "               5               17.418769  \n",
       "               5                6.329365  \n",
       "               5               18.917744  \n",
       "               5                9.515561  \n",
       "               5               20.516767  \n",
       "               5               33.884772  \n",
       "               5               22.121617  \n",
       "               5               15.874558  \n",
       "               5               35.417294  \n",
       "               5               25.144929  \n",
       "               5               26.802735  \n",
       "               5               37.753927  \n",
       "               5               29.976984  \n",
       "               5               31.512580  \n",
       "               5               39.392730  \n",
       "               5               40.967759  \n",
       "               5               42.512189  \n",
       "               5               44.022404  \n",
       "               5               45.668234  \n",
       "122            5                2.368267  \n",
       "               5                4.307094  \n",
       "               5                5.914198  \n",
       "               5                7.686258  \n",
       "               5                9.796137  \n",
       "               5               11.863852  \n",
       "               5               14.130155  \n",
       "               5               16.372354  \n",
       "1              1                0.808839  \n",
       "               1                1.658094  \n",
       "               1                2.427193  \n",
       "               1                3.195342  \n",
       "8              1                0.869449  \n",
       "               1                1.660651  \n",
       "               1                8.973582  \n",
       "               1                3.296970  \n",
       "               1                9.758208  \n",
       "               1                4.891582  \n",
       "               1               10.618801  \n",
       "               1                6.562347  \n",
       "               1               11.382821  \n",
       "               1                8.183582  \n",
       "               1               12.163382  \n",
       "               1               12.949762  \n",
       "               1               13.772568  \n",
       "32             1                0.770497  \n",
       "               1                1.556703  \n",
       "               1                8.939052  \n",
       "               1                3.214957  \n",
       "               1                9.761599  \n",
       "               1                4.847553  \n",
       "               1               10.591540  \n",
       "               1                6.441645  \n",
       "               1                7.283155  \n",
       "               1                8.117945  \n",
       "               1               12.410293  \n",
       "               1               13.192645  \n",
       "122            1                0.815238  \n",
       "               1                1.603127  \n",
       "               1                2.419888  \n",
       "               1                3.267168  \n",
       "               1               14.314391  \n",
       "               1                4.960418  \n",
       "               1                8.596934  \n",
       "               1                6.169476  \n",
       "               1                9.370682  \n",
       "               1               15.632658  \n",
       "               1               11.019704  \n",
       "               1               11.794474  \n",
       "               1               13.534852  \n",
       "               1               16.480554  \n",
       "               1               17.254168  \n",
       "1              3                1.136044  \n",
       "8              3                1.153312  \n",
       "               3                2.236713  \n",
       "               3               11.856152  \n",
       "               3                4.481458  \n",
       "               3                6.198583  \n",
       "               3               14.076809  \n",
       "               3               23.555041  \n",
       "               3               15.148457  \n",
       "               3               10.676322  \n",
       "               3               12.946444  \n",
       "               3               16.264792  \n",
       "               3               17.363150  \n",
       "               3               18.517551  \n",
       "               3               19.630407  \n",
       "               3               20.741610  \n",
       "               3               21.830405  \n",
       "               3               25.206463  \n",
       "               3               26.945656  \n",
       "               3               28.628550  \n",
       "               3               29.705958  \n",
       "32             3                1.170462  \n",
       "               3                2.400195  \n",
       "               3               13.039391  \n",
       "               3                4.735582  \n",
       "               3               25.140305  \n",
       "               3                7.078616  \n",
       "               3               15.349843  \n",
       "               3               26.354444  \n",
       "               3               16.467137  \n",
       "               3               11.858376  \n",
       "               3               27.695649  \n",
       "               3               18.937314  \n",
       "               3               20.204582  \n",
       "               3               30.061926  \n",
       "               3               22.598675  \n",
       "               3               23.816767  \n",
       "               3               28.838756  \n",
       "               3               31.220424  \n",
       "               3               33.839890  \n",
       "               3               35.120372  \n",
       "122            3                1.667094  \n",
       "               3                3.179127  \n",
       "               3                4.544612  \n",
       "               3                5.726158  \n",
       "               3                6.902444  \n",
       "               3                9.157604  \n",
       "               3               10.720084  \n",
       "1              5                1.318899  \n",
       "8              5                1.401034  \n",
       "               5                2.789525  \n",
       "               5                4.197622  \n",
       "               5                5.546883  \n",
       "               5               22.220947  \n",
       "               5                8.283814  \n",
       "               5                9.670416  \n",
       "               5               23.707702  \n",
       "               5               12.403055  \n",
       "               5               19.431136  \n",
       "               5               20.878179  \n",
       "               5               25.089557  \n",
       "               5               26.446527  \n",
       "               5               27.857000  \n",
       "               5               29.209456  \n",
       "               5               30.606091  \n",
       "               5               31.951760  \n",
       "               5               33.399226  \n",
       "               5               34.715635  \n",
       "32             5                1.603045  \n",
       "               5                6.192066  \n",
       "               5               16.881149  \n",
       "               5               29.490173  \n",
       "               5               18.322043  \n",
       "               5               12.285737  \n",
       "               5               13.705929  \n",
       "               5               28.020595  \n",
       "               5               31.071805  \n",
       "               5               21.395666  \n",
       "               5               23.060187  \n",
       "               5               34.265042  \n",
       "               5               25.683904  \n",
       "               5               26.825816  \n",
       "               5               32.594195  \n",
       "               5               35.816563  \n",
       "               5               37.506815  \n",
       "               5               39.048521  \n",
       "               5               40.633031  \n",
       "               5               42.204596  \n",
       "122            5                2.376861  \n",
       "               5                4.570594  \n",
       "               5                6.760869  \n",
       "               5                8.998884  \n",
       "               5               11.282426  \n",
       "1              1                0.910504  \n",
       "               1                1.687136  \n",
       "               1                8.941634  \n",
       "               1                3.272226  \n",
       "               1                9.833709  \n",
       "               1                4.904266  \n",
       "               1               10.641123  \n",
       "               1                6.528325  \n",
       "               1               11.456399  \n",
       "               1                8.138729  \n",
       "               1               12.201887  \n",
       "               1               13.061194  \n",
       "               1               13.888129  \n",
       "               1               14.680479  \n",
       "               1               15.529244  \n",
       "8              1                0.860585  \n",
       "               1                1.668629  \n",
       "               1                2.464633  \n",
       "               1                3.402384  \n",
       "               1                4.239686  \n",
       "               1                5.030024  \n",
       "               1                5.840948  \n",
       "               1                6.670229  \n",
       "32             1                0.899783  \n",
       "               1                1.714528  \n",
       "               1                2.484719  \n",
       "               1                3.368358  \n",
       "               1                4.190599  \n",
       "               1                5.004562  \n",
       "               1                5.826692  \n",
       "               1                6.681455  \n",
       "               1                7.490652  \n",
       "               1                8.270839  \n",
       "122            1                0.641767  \n",
       "               1                1.235688  \n",
       "               1                7.860441  \n",
       "               1                2.433098  \n",
       "               1                3.157930  \n",
       "               1                3.955057  \n",
       "               1                4.727972  \n",
       "               1                5.474877  \n",
       "               1                6.217043  \n",
       "               1                7.046358  \n",
       "1              3                1.038483  \n",
       "8              3                1.097890  \n",
       "               3                2.143623  \n",
       "               3               11.259707  \n",
       "               3                4.255471  \n",
       "               3               22.300597  \n",
       "               3                6.194484  \n",
       "               3               13.533677  \n",
       "               3               23.404547  \n",
       "               3               14.577337  \n",
       "               3               10.171338  \n",
       "               3               24.543459  \n",
       "               3               16.771005  \n",
       "               3               17.882679  \n",
       "               3               26.844587  \n",
       "               3               20.058650  \n",
       "               3               21.185926  \n",
       "               3               25.716757  \n",
       "               3               27.960191  \n",
       "               3               29.027340  \n",
       "               3               30.049937  \n",
       "               3               31.125288  \n",
       "               3               32.222431  \n",
       "32             3                1.195384  \n",
       "               3                2.318904  \n",
       "               3               12.117837  \n",
       "               3                4.590592  \n",
       "               3               13.141996  \n",
       "               3                6.918323  \n",
       "               3                8.676533  \n",
       "               3               15.049278  \n",
       "               3               10.999747  \n",
       "               3               14.017167  \n",
       "               3               16.208178  \n",
       "               3               17.381170  \n",
       "               3               19.179069  \n",
       "               3               20.313943  \n",
       "122            3                1.532302  \n",
       "               3                3.066649  \n",
       "               3                5.453015  \n",
       "               3                6.928237  \n",
       "               3                8.421714  \n",
       "               3               16.195730  \n",
       "               3               17.664799  \n",
       "1              5                1.331508  \n",
       "8              5                1.405436  \n",
       "               5                2.777832  \n",
       "               5               14.802773  \n",
       "               5                5.437043  \n",
       "               5               16.250514  \n",
       "               5                7.986766  \n",
       "               5               17.737847  \n",
       "               5               10.757020  \n",
       "               5               12.104453  \n",
       "               5               13.462587  \n",
       "               5               22.464884  \n",
       "32             5                1.640981  \n",
       "               5                3.389922  \n",
       "               5                4.984748  \n",
       "               5                6.491150  \n",
       "               5                8.023460  \n",
       "               5                9.607423  \n",
       "               5               11.145439  \n",
       "               5               12.703154  \n",
       "               5               14.221701  \n",
       "122            5                2.205522  \n",
       "               5                4.312284  \n",
       "               5                6.395371  \n",
       "               5                8.521467  \n",
       "               5               10.668059  \n",
       "               5               12.819965  \n",
       "               5               14.985993  \n",
       "1              1                0.807331  \n",
       "               1                1.596510  \n",
       "               1                8.853109  \n",
       "               1                3.191427  \n",
       "               1               16.026343  \n",
       "               1                4.781629  \n",
       "               1               10.412220  \n",
       "               1               16.817441  \n",
       "               1               11.200721  \n",
       "               1                8.028947  \n",
       "               1               17.569122  \n",
       "               1               12.652271  \n",
       "               1               13.253711  \n",
       "               1               19.157119  \n",
       "               1               14.447026  \n",
       "               1               15.205530  \n",
       "               1               18.398555  \n",
       "               1               19.983089  \n",
       "               1               20.762034  \n",
       "               1               21.580799  \n",
       "               1               22.352542  \n",
       "               1               23.211684  \n",
       "8              1                0.769770  \n",
       "               1                1.608776  \n",
       "               1                2.349010  \n",
       "               1                3.219053  \n",
       "               1                3.978208  \n",
       "               1                4.819912  \n",
       "               1                5.625120  \n",
       "               1                6.522778  \n",
       "               1                7.331714  \n",
       "32             1                0.840305  \n",
       "               1                1.625310  \n",
       "               1                8.782956  \n",
       "               1                3.201481  \n",
       "               1                9.596403  \n",
       "               1                4.833746  \n",
       "               1                5.602313  \n",
       "               1                6.412395  \n",
       "               1                7.183477  \n",
       "               1                8.012530  \n",
       "122            1                0.841745  \n",
       "               1                1.695190  \n",
       "               1                8.785692  \n",
       "               1                3.253160  \n",
       "               1                9.645123  \n",
       "               1                4.797040  \n",
       "               1               10.489040  \n",
       "               1                6.365857  \n",
       "               1               11.270747  \n",
       "               1                7.940228  \n",
       "               1               12.076853  \n",
       "               1               12.866027  \n",
       "1              3                1.093403  \n",
       "               3                6.181096  \n",
       "               3               17.835649  \n",
       "               3                8.806329  \n",
       "               3                9.904373  \n",
       "               3               20.038939  \n",
       "               3               12.157693  \n",
       "               3               13.306773  \n",
       "               3               14.452175  \n",
       "               3               15.543692  \n",
       "               3               16.732997  \n",
       "               3               18.944045  \n",
       "               3               21.114165  \n",
       "               3               23.359160  \n",
       "               3               24.386522  \n",
       "8              3                1.187153  \n",
       "               3                2.314086  \n",
       "               3               10.685101  \n",
       "               3                4.496789  \n",
       "               3                6.777124  \n",
       "               3               20.235193  \n",
       "               3                8.968101  \n",
       "               3               14.632145  \n",
       "               3               15.710437  \n",
       "               3               16.846556  \n",
       "               3               17.945738  \n",
       "               3               19.093679  \n",
       "               3               21.320162  \n",
       "               3               24.279881  \n",
       "               3               25.972584  \n",
       "               3               27.045139  \n",
       "32             3                1.247300  \n",
       "               3                2.442370  \n",
       "               3               12.652347  \n",
       "               3                4.858522  \n",
       "               3               13.861901  \n",
       "               3                7.257311  \n",
       "               3               15.031495  \n",
       "               3               16.278363  \n",
       "               3               11.410274  \n",
       "               3               17.451793  \n",
       "               3               18.608358  \n",
       "               3               19.794503  \n",
       "122            3                1.123082  \n",
       "               3                2.612590  \n",
       "               3                4.247366  \n",
       "               3                5.754416  \n",
       "               3                7.219536  \n",
       "1              5                1.376509  \n",
       "               5                2.793978  \n",
       "               5               14.227843  \n",
       "               5                5.504664  \n",
       "               5               28.022108  \n",
       "               5                8.269593  \n",
       "               5               16.913385  \n",
       "               5               18.364748  \n",
       "               5               12.886392  \n",
       "               5               30.803165  \n",
       "               5               21.142184  \n",
       "               5               22.553295  \n",
       "               5               33.540665  \n",
       "               5               25.264237  \n",
       "               5               26.673722  \n",
       "               5               29.328593  \n",
       "               5               32.154849  \n",
       "               5               34.924899  \n",
       "               5               36.292585  \n",
       "               5               37.616300  \n",
       "               5               38.982371  \n",
       "               5               40.437156  \n",
       "8              5                1.471498  \n",
       "               5                2.879770  \n",
       "               5                4.227787  \n",
       "               5               23.238795  \n",
       "               5               15.625380  \n",
       "               5               16.633543  \n",
       "               5               17.731955  \n",
       "               5               19.028329  \n",
       "               5               20.448555  \n",
       "               5               21.791425  \n",
       "               5               24.596785  \n",
       "               5               25.994330  \n",
       "               5               27.368817  \n",
       "               5               28.746180  \n",
       "               5               30.116699  \n",
       "32             5                1.629867  \n",
       "               5                3.185793  \n",
       "               5               16.964964  \n",
       "               5                6.242048  \n",
       "               5               30.114800  \n",
       "               5                9.253996  \n",
       "               5               20.063834  \n",
       "               5               12.327079  \n",
       "               5               21.599068  \n",
       "               5               15.472204  \n",
       "               5               23.123441  \n",
       "               5               26.295696  \n",
       "               5               28.607611  \n",
       "               5               33.232840  \n",
       "122            5                2.281325  \n",
       "               5                4.541728  \n",
       "               5                6.730656  \n",
       "               5                8.857712  \n",
       "               5               11.024981  \n",
       "               5               16.240678  \n",
       "\n",
       "[1335 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:57:29.804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.907337</td>\n",
       "      <td>0.859409</td>\n",
       "      <td>31.049070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.949603</td>\n",
       "      <td>0.900639</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>35.120372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.919841</td>\n",
       "      <td>0.898998</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>27.521660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.967460</td>\n",
       "      <td>0.896425</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>17.254168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.822363</td>\n",
       "      <td>58.477479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.889993</td>\n",
       "      <td>0.822447</td>\n",
       "      <td>36.295973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.969841</td>\n",
       "      <td>0.886533</td>\n",
       "      <td>0.823207</td>\n",
       "      <td>23.211684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>0.832743</td>\n",
       "      <td>32.769451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.975794</td>\n",
       "      <td>0.865774</td>\n",
       "      <td>0.773502</td>\n",
       "      <td>21.436236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0.967857</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>23.059453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>0.782736</td>\n",
       "      <td>0.591983</td>\n",
       "      <td>24.386522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.658534</td>\n",
       "      <td>0.432658</td>\n",
       "      <td>40.437156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "122            3                 12     0.950000    0.907337       0.859409   \n",
       "32             3                 33     0.949603    0.900639       0.838565   \n",
       "122            5                 10     0.919841    0.898998       0.867089   \n",
       "               1                 22     0.967460    0.896425       0.816203   \n",
       "32             5                 33     0.937698    0.895050       0.822363   \n",
       "8              5                 33     0.925000    0.889993       0.822447   \n",
       "1              1                 33     0.969841    0.886533       0.823207   \n",
       "8              3                 33     0.948413    0.884847       0.832743   \n",
       "32             1                 33     0.975794    0.865774       0.773502   \n",
       "8              1                 33     0.967857    0.862890       0.759409   \n",
       "1              3                 27     0.930952    0.782736       0.591983   \n",
       "               5                 33     0.831349    0.658534       0.432658   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "122            3               31.049070  \n",
       "32             3               35.120372  \n",
       "122            5               27.521660  \n",
       "               1               17.254168  \n",
       "32             5               58.477479  \n",
       "8              5               36.295973  \n",
       "1              1               23.211684  \n",
       "8              3               32.769451  \n",
       "32             1               21.436236  \n",
       "8              1               23.059453  \n",
       "1              3               24.386522  \n",
       "               5               40.437156  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.max().sort_values(by=\"test_score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:57:29.807Z"
    }
   },
   "outputs": [],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.min().sort_values(by=\"test_score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:57:29.812Z"
    }
   },
   "outputs": [],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.mean().sort_values(by=\"test_score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:57:29.818Z"
    }
   },
   "outputs": [],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.std().sort_values(by=\"test_score\", ascending = False)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
