{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.306925Z",
     "start_time": "2017-07-23T22:08:57.782502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.322118Z",
     "start_time": "2017-07-23T22:08:58.309367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/tf_vae_only_nsl_kdd_all-.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_only_nsl_kdd_all-.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.391242Z",
     "start_time": "2017-07-23T22:08:58.324128Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train__2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.402421Z",
     "start_time": "2017-07-23T22:08:58.393271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.410246Z",
     "start_time": "2017-07-23T22:08:58.404810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:08:58.959231Z",
     "start_time": "2017-07-23T22:08:58.412831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97133071128034443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    x_test_ = np.hstack((x_test_, np.random.normal(size = (x_test_.shape[0], y_train.shape[1]))))\n",
    "\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:09:00.281191Z",
     "start_time": "2017-07-23T22:08:58.961686Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:09:00.845677Z",
     "start_time": "2017-07-23T22:09:00.283026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            \n",
    "            loss = tf.clip_by_value(loss, -1e-4, 1e-4)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-4, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-4), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-4), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate= self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:09:01.337604Z",
     "start_time": "2017-07-23T22:09:00.847856Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h, f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_nsl_kdd-/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                      net.regularized_loss, \n",
    "                                                      ], #net.summary_op\n",
    "                                                      feed_dict={net.x: x_train[i,:], \n",
    "                                                                 net.y_: y_train[i,:], \n",
    "                                                                 net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "\n",
    "                        count = 10\n",
    "                        if((train_loss > 1e9 or np.isnan(train_loss) ) and epoch > 1 and count > 1):\n",
    "                            print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_vae_only_nsl_kdd-/hidden layers_{}_features count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "                            count -= 1\n",
    "\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        #if(train_loss > 1e9):\n",
    "\n",
    "                        #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "\n",
    "                    #print(\"\")\n",
    "                    valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "\n",
    "                    accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, test_loss_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                                           net.pred, \n",
    "                                                                                           net.actual, net.y], \n",
    "                                                                                          feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                                                     net.y_: preprocess.y_test_, \n",
    "                                                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    #print(\"*************** \\n\")\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "                    #print(\"*************** \\n\")\n",
    "                    #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "\n",
    "                        #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                        #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                        #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "                        Train.best_acc = accuracy\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_vae_only_nsl_kdd-/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(epochs,h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:09:01.427402Z",
     "start_time": "2017-07-23T22:09:01.339167Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "\n",
    "\n",
    "    def start_training():\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [15]\n",
    "        lrs = [1e-2, 1e-2, 1e-3]\n",
    "\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_only_nsl_kdd_all-.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all-.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "            \n",
    "        past_scores.to_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all-.pkl\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:40.734088Z",
     "start_time": "2017-07-23T22:09:01.428928Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.323836 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.691829 | Validation Accuracy: 0.548016\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: -0.754257 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.547523 | Validation Accuracy: 0.550794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: -0.000786 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.035695 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.318277 | Validation Accuracy: 0.544048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -0.512744 | Validation Accuracy: 0.521429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 1.305139 | Validation Accuracy: 0.522619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.481198 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: -0.231506 | Validation Accuracy: 0.526587\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: -0.704337 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.241507 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: -0.778782 | Validation Accuracy: 0.536905\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 1.412460 | Validation Accuracy: 0.552778\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.306618 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: -0.742113 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 1.579602 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.687746 | Validation Accuracy: 0.536111\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.245421 | Validation Accuracy: 0.546429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 1.246377 | Validation Accuracy: 0.546429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.256240 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -0.629047 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.323759 | Validation Accuracy: 0.538889\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.254614 | Validation Accuracy: 0.536111\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.711429 | Validation Accuracy: 0.550794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: -1.406902 | Validation Accuracy: 0.542063\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 1.117682 | Validation Accuracy: 0.542857\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: -1.544911 | Validation Accuracy: 0.523413\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.411465 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: -0.313428 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 1.936397 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.406893 | Validation Accuracy: 0.541270\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 1.811579 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: -0.096351 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 1.226899 | Validation Accuracy: 0.511508\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: -1.649435 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: -0.887878 | Validation Accuracy: 0.522619\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.136309 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 1.117020 | Validation Accuracy: 0.554762\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: -0.070521 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: -0.595402 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 1.016807 | Validation Accuracy: 0.545635\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.528737 | Validation Accuracy: 0.528175\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: -1.676362 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: -0.402321 | Validation Accuracy: 0.518651\n",
      "Accuracy on Test data: 0.43927431106567383, 0.324894517660141\n",
      "Step 2 | Training Loss: 1.031781 | Validation Accuracy: 0.495238\n",
      "Accuracy on Test data: 0.43980661034584045, 0.32455697655677795\n",
      "Step 3 | Training Loss: -0.662190 | Validation Accuracy: 0.508730\n",
      "Accuracy on Test data: 0.43829843401908875, 0.3199999928474426\n",
      "Step 4 | Training Loss: 0.153042 | Validation Accuracy: 0.508333\n",
      "Accuracy on Test data: 0.4409599006175995, 0.32295358180999756\n",
      "Step 5 | Training Loss: 2.254114 | Validation Accuracy: 0.511508\n",
      "Accuracy on Test data: 0.43577003479003906, 0.32194092869758606\n",
      "Step 6 | Training Loss: -0.134715 | Validation Accuracy: 0.513889\n",
      "Accuracy on Test data: 0.4418914020061493, 0.322025328874588\n",
      "Step 7 | Training Loss: 0.152103 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.4402945339679718, 0.3297046422958374\n",
      "Step 8 | Training Loss: 1.013059 | Validation Accuracy: 0.525397\n",
      "Accuracy on Test data: 0.43545955419540405, 0.3145991563796997\n",
      "Step 9 | Training Loss: -2.508973 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.438342809677124, 0.323713093996048\n",
      "Step 10 | Training Loss: 1.131800 | Validation Accuracy: 0.507937\n",
      "Accuracy on Test data: 0.44224628806114197, 0.32675105333328247\n",
      "Step 11 | Training Loss: 0.233715 | Validation Accuracy: 0.517460\n",
      "Accuracy on Test data: 0.43989530205726624, 0.31654009222984314\n",
      "Step 12 | Training Loss: -0.499647 | Validation Accuracy: 0.494048\n",
      "Accuracy on Test data: 0.43545955419540405, 0.324050635099411\n",
      "Step 13 | Training Loss: -1.129073 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.43763306736946106, 0.3263291120529175\n",
      "Step 14 | Training Loss: -1.605130 | Validation Accuracy: 0.525000\n",
      "Accuracy on Test data: 0.442379355430603, 0.33198311924934387\n",
      "Step 15 | Training Loss: -2.571621 | Validation Accuracy: 0.521825\n",
      "Accuracy on Test data: 0.4369233548641205, 0.323713093996048\n",
      "Step 1 | Training Loss: -0.765491 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.4384315013885498, 0.32337552309036255\n",
      "Step 2 | Training Loss: 0.588719 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.4384315013885498, 0.3202531635761261\n",
      "Step 3 | Training Loss: 0.858557 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.4356813430786133, 0.3231223523616791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: -1.163308 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.436479777097702, 0.3259071707725525\n",
      "Step 5 | Training Loss: -0.501946 | Validation Accuracy: 0.503571\n",
      "Accuracy on Test data: 0.4394517242908478, 0.3272573947906494\n",
      "Step 6 | Training Loss: -0.466201 | Validation Accuracy: 0.516270\n",
      "Accuracy on Test data: 0.4415809214115143, 0.323713093996048\n",
      "Step 7 | Training Loss: 0.495949 | Validation Accuracy: 0.503571\n",
      "Accuracy on Test data: 0.4413147568702698, 0.3299578130245209\n",
      "Step 8 | Training Loss: 1.234320 | Validation Accuracy: 0.520238\n",
      "Accuracy on Test data: 0.43741127848625183, 0.3302953541278839\n",
      "Step 9 | Training Loss: 0.613170 | Validation Accuracy: 0.496429\n",
      "Accuracy on Test data: 0.441536545753479, 0.324894517660141\n",
      "Step 10 | Training Loss: -1.264403 | Validation Accuracy: 0.514286\n",
      "Accuracy on Test data: 0.44140347838401794, 0.3218565285205841\n",
      "Step 11 | Training Loss: 0.384534 | Validation Accuracy: 0.508333\n",
      "Accuracy on Test data: 0.436479777097702, 0.3333333432674408\n",
      "Step 12 | Training Loss: 0.723307 | Validation Accuracy: 0.497619\n",
      "Accuracy on Test data: 0.44512951374053955, 0.32092827558517456\n",
      "Step 13 | Training Loss: -2.145337 | Validation Accuracy: 0.498810\n",
      "Accuracy on Test data: 0.432620644569397, 0.31409281492233276\n",
      "Step 14 | Training Loss: 0.302916 | Validation Accuracy: 0.508730\n",
      "Accuracy on Test data: 0.44047197699546814, 0.32295358180999756\n",
      "Step 15 | Training Loss: 0.192366 | Validation Accuracy: 0.505556\n",
      "Accuracy on Test data: 0.44233497977256775, 0.323713093996048\n",
      "Step 1 | Training Loss: -0.610899 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.4387863874435425, 0.31949368119239807\n",
      "Step 2 | Training Loss: 1.165902 | Validation Accuracy: 0.526587\n",
      "Accuracy on Test data: 0.4390081763267517, 0.3278481066226959\n",
      "Step 3 | Training Loss: 0.234147 | Validation Accuracy: 0.515873\n",
      "Accuracy on Test data: 0.4399396777153015, 0.32194092869758606\n",
      "Step 4 | Training Loss: -0.587893 | Validation Accuracy: 0.503571\n",
      "Accuracy on Test data: 0.43581441044807434, 0.32151898741722107\n",
      "Step 5 | Training Loss: 1.150877 | Validation Accuracy: 0.520238\n",
      "Accuracy on Test data: 0.4411816895008087, 0.32573840022087097\n",
      "Step 6 | Training Loss: -2.003144 | Validation Accuracy: 0.520238\n",
      "Accuracy on Test data: 0.44109296798706055, 0.32776370644569397\n",
      "Step 7 | Training Loss: 0.802351 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.4422019124031067, 0.32801687717437744\n",
      "Step 8 | Training Loss: 2.079521 | Validation Accuracy: 0.505952\n",
      "Accuracy on Test data: 0.43891945481300354, 0.32396623492240906\n",
      "Step 9 | Training Loss: 0.675069 | Validation Accuracy: 0.514286\n",
      "Accuracy on Test data: 0.4365241229534149, 0.3231223523616791\n",
      "Step 10 | Training Loss: -2.072013 | Validation Accuracy: 0.499206\n",
      "Accuracy on Test data: 0.4390081763267517, 0.32075950503349304\n",
      "Step 11 | Training Loss: -0.160000 | Validation Accuracy: 0.509524\n",
      "Accuracy on Test data: 0.4406493902206421, 0.322447270154953\n",
      "Step 12 | Training Loss: -0.559168 | Validation Accuracy: 0.505159\n",
      "Accuracy on Test data: 0.4355926215648651, 0.32320675253868103\n",
      "Step 13 | Training Loss: 0.609659 | Validation Accuracy: 0.529762\n",
      "Accuracy on Test data: 0.43674591183662415, 0.33400842547416687\n",
      "Step 14 | Training Loss: -0.921139 | Validation Accuracy: 0.509127\n",
      "Accuracy on Test data: 0.4353708326816559, 0.3199999928474426\n",
      "Step 15 | Training Loss: 1.051159 | Validation Accuracy: 0.509127\n",
      "Accuracy on Test data: 0.4371451437473297, 0.32118144631385803\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.916064 | Validation Accuracy: 0.422222\n",
      "Accuracy on Test data: 0.4925035536289215, 0.6606751084327698\n",
      "Step 2 | Training Loss: -0.472402 | Validation Accuracy: 0.404762\n",
      "Accuracy on Test data: 0.49121716618537903, 0.654177188873291\n",
      "Step 3 | Training Loss: -1.453709 | Validation Accuracy: 0.405159\n",
      "Accuracy on Test data: 0.4938342869281769, 0.648101270198822\n",
      "Step 4 | Training Loss: -1.390249 | Validation Accuracy: 0.417460\n",
      "Accuracy on Test data: 0.49112847447395325, 0.6563712954521179\n",
      "Step 5 | Training Loss: -1.187219 | Validation Accuracy: 0.415476\n",
      "Accuracy on Test data: 0.49050745368003845, 0.6574683785438538\n",
      "Step 6 | Training Loss: -1.796881 | Validation Accuracy: 0.412302\n",
      "Accuracy on Test data: 0.4887331426143646, 0.6597468256950378\n",
      "Step 7 | Training Loss: 0.205875 | Validation Accuracy: 0.424603\n",
      "Accuracy on Test data: 0.4977821111679077, 0.6610126495361328\n",
      "Step 8 | Training Loss: -0.277252 | Validation Accuracy: 0.419841\n",
      "Accuracy on Test data: 0.4916607439517975, 0.6540084481239319\n",
      "Step 9 | Training Loss: 1.011940 | Validation Accuracy: 0.423810\n",
      "Accuracy on Test data: 0.48895493149757385, 0.652911365032196\n",
      "Step 10 | Training Loss: -0.639872 | Validation Accuracy: 0.421825\n",
      "Accuracy on Test data: 0.49698367714881897, 0.6563712954521179\n",
      "Step 11 | Training Loss: -0.723823 | Validation Accuracy: 0.416667\n",
      "Accuracy on Test data: 0.49174946546554565, 0.6573839783668518\n",
      "Step 12 | Training Loss: -0.071671 | Validation Accuracy: 0.401190\n",
      "Accuracy on Test data: 0.4978708326816559, 0.6535865068435669\n",
      "Step 13 | Training Loss: -0.486101 | Validation Accuracy: 0.408730\n",
      "Accuracy on Test data: 0.4922817647457123, 0.6557806134223938\n",
      "Step 14 | Training Loss: -0.112014 | Validation Accuracy: 0.414683\n",
      "Accuracy on Test data: 0.4929027557373047, 0.6560337543487549\n",
      "Step 15 | Training Loss: -0.291639 | Validation Accuracy: 0.408730\n",
      "Accuracy on Test data: 0.4950319230556488, 0.6540084481239319\n",
      "Step 1 | Training Loss: -2.221437 | Validation Accuracy: 0.406349\n",
      "Accuracy on Test data: 0.49024131894111633, 0.656286895275116\n",
      "Step 2 | Training Loss: 0.859794 | Validation Accuracy: 0.401587\n",
      "Accuracy on Test data: 0.495120644569397, 0.6565400958061218\n",
      "Step 3 | Training Loss: 0.571015 | Validation Accuracy: 0.425000\n",
      "Accuracy on Test data: 0.4901082217693329, 0.6616877913475037\n",
      "Step 4 | Training Loss: -0.291962 | Validation Accuracy: 0.412302\n",
      "Accuracy on Test data: 0.4935237765312195, 0.654683530330658\n",
      "Step 5 | Training Loss: -0.322103 | Validation Accuracy: 0.402778\n",
      "Accuracy on Test data: 0.4941447973251343, 0.6582278609275818\n",
      "Step 6 | Training Loss: -1.052508 | Validation Accuracy: 0.405159\n",
      "Accuracy on Test data: 0.49170511960983276, 0.6600843667984009\n",
      "Step 7 | Training Loss: 1.132748 | Validation Accuracy: 0.413492\n",
      "Accuracy on Test data: 0.49086230993270874, 0.6655696034431458\n",
      "Step 8 | Training Loss: 1.793616 | Validation Accuracy: 0.408730\n",
      "Accuracy on Test data: 0.49454399943351746, 0.6609282493591309\n",
      "Step 9 | Training Loss: 0.142892 | Validation Accuracy: 0.416270\n",
      "Accuracy on Test data: 0.4937899112701416, 0.6592404842376709\n",
      "Step 10 | Training Loss: 0.620488 | Validation Accuracy: 0.402778\n",
      "Accuracy on Test data: 0.49055179953575134, 0.6615189909934998\n",
      "Step 11 | Training Loss: -1.805167 | Validation Accuracy: 0.426587\n",
      "Accuracy on Test data: 0.49614086747169495, 0.6622784733772278\n",
      "Step 12 | Training Loss: -1.487145 | Validation Accuracy: 0.395635\n",
      "Accuracy on Test data: 0.4935237765312195, 0.6615189909934998\n",
      "Step 13 | Training Loss: -0.806061 | Validation Accuracy: 0.394048\n",
      "Accuracy on Test data: 0.49179381132125854, 0.657130777835846\n",
      "Step 14 | Training Loss: -0.242587 | Validation Accuracy: 0.410317\n",
      "Accuracy on Test data: 0.4901082217693329, 0.6590717434883118\n",
      "Step 15 | Training Loss: -1.465446 | Validation Accuracy: 0.413889\n",
      "Accuracy on Test data: 0.4916607439517975, 0.6555274128913879\n",
      "Step 1 | Training Loss: -0.710419 | Validation Accuracy: 0.417460\n",
      "Accuracy on Test data: 0.4963183104991913, 0.6552742719650269\n",
      "Step 2 | Training Loss: -0.667464 | Validation Accuracy: 0.408333\n",
      "Accuracy on Test data: 0.49241483211517334, 0.6559493541717529\n",
      "Step 3 | Training Loss: -1.838987 | Validation Accuracy: 0.401190\n",
      "Accuracy on Test data: 0.4975603222846985, 0.6592404842376709\n",
      "Step 4 | Training Loss: -1.259902 | Validation Accuracy: 0.411508\n",
      "Accuracy on Test data: 0.49263662099838257, 0.6589873433113098\n",
      "Step 5 | Training Loss: -0.936995 | Validation Accuracy: 0.405556\n",
      "Accuracy on Test data: 0.4921930432319641, 0.6566244959831238\n",
      "Step 6 | Training Loss: -0.295595 | Validation Accuracy: 0.412302\n",
      "Accuracy on Test data: 0.49210432171821594, 0.6544303894042969\n",
      "Step 7 | Training Loss: -0.077332 | Validation Accuracy: 0.420238\n",
      "Accuracy on Test data: 0.4950762987136841, 0.6605063080787659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: -1.783312 | Validation Accuracy: 0.417857\n",
      "Accuracy on Test data: 0.49711674451828003, 0.6614345908164978\n",
      "Step 9 | Training Loss: -0.512126 | Validation Accuracy: 0.415873\n",
      "Accuracy on Test data: 0.4933907091617584, 0.6576371192932129\n",
      "Step 10 | Training Loss: -0.626216 | Validation Accuracy: 0.426587\n",
      "Accuracy on Test data: 0.49334633350372314, 0.6606751084327698\n",
      "Step 11 | Training Loss: 0.497934 | Validation Accuracy: 0.416667\n",
      "Accuracy on Test data: 0.49365684390068054, 0.6572151780128479\n",
      "Step 12 | Training Loss: 1.344081 | Validation Accuracy: 0.408333\n",
      "Accuracy on Test data: 0.49520936608314514, 0.654177188873291\n",
      "Step 13 | Training Loss: 0.827288 | Validation Accuracy: 0.415873\n",
      "Accuracy on Test data: 0.4938342869281769, 0.6647257208824158\n",
      "Step 14 | Training Loss: -1.125139 | Validation Accuracy: 0.403968\n",
      "Accuracy on Test data: 0.4894428551197052, 0.6574683785438538\n",
      "Step 15 | Training Loss: 0.586435 | Validation Accuracy: 0.403175\n",
      "Accuracy on Test data: 0.49835875630378723, 0.6572995781898499\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 1.177376 | Validation Accuracy: 0.449603\n",
      "Accuracy on Test data: 0.45511001348495483, 0.48683544993400574\n",
      "Step 2 | Training Loss: 0.658469 | Validation Accuracy: 0.448810\n",
      "Accuracy on Test data: 0.4623403251171112, 0.49324893951416016\n",
      "Step 3 | Training Loss: 0.060655 | Validation Accuracy: 0.447619\n",
      "Accuracy on Test data: 0.46100959181785583, 0.4857383966445923\n",
      "Step 4 | Training Loss: 0.076248 | Validation Accuracy: 0.443254\n",
      "Accuracy on Test data: 0.4548882246017456, 0.49805906414985657\n",
      "Step 5 | Training Loss: -0.284888 | Validation Accuracy: 0.434127\n",
      "Accuracy on Test data: 0.458392471075058, 0.48945146799087524\n",
      "Step 6 | Training Loss: -1.472106 | Validation Accuracy: 0.438889\n",
      "Accuracy on Test data: 0.46163058280944824, 0.4850632846355438\n",
      "Step 7 | Training Loss: 0.090970 | Validation Accuracy: 0.436905\n",
      "Accuracy on Test data: 0.45595279335975647, 0.4899578094482422\n",
      "Step 8 | Training Loss: 0.977902 | Validation Accuracy: 0.458730\n",
      "Accuracy on Test data: 0.4588804244995117, 0.49088606238365173\n",
      "Step 9 | Training Loss: -0.526073 | Validation Accuracy: 0.443651\n",
      "Accuracy on Test data: 0.46469128131866455, 0.48523205518722534\n",
      "Step 10 | Training Loss: -0.915914 | Validation Accuracy: 0.450000\n",
      "Accuracy on Test data: 0.45945706963539124, 0.4864978790283203\n",
      "Step 11 | Training Loss: -0.551519 | Validation Accuracy: 0.457540\n",
      "Accuracy on Test data: 0.4646025598049164, 0.48599156737327576\n",
      "Step 12 | Training Loss: 1.209077 | Validation Accuracy: 0.446825\n",
      "Accuracy on Test data: 0.45821505784988403, 0.49147680401802063\n",
      "Step 13 | Training Loss: -0.684570 | Validation Accuracy: 0.459921\n",
      "Accuracy on Test data: 0.4648243486881256, 0.4850632846355438\n",
      "Step 14 | Training Loss: 0.219022 | Validation Accuracy: 0.449206\n",
      "Accuracy on Test data: 0.4617636501789093, 0.4815189838409424\n",
      "Step 15 | Training Loss: -1.234077 | Validation Accuracy: 0.447619\n",
      "Accuracy on Test data: 0.4610539376735687, 0.4867510497570038\n",
      "Step 1 | Training Loss: -0.335578 | Validation Accuracy: 0.440873\n",
      "Accuracy on Test data: 0.46123138070106506, 0.49122363328933716\n",
      "Step 2 | Training Loss: 0.789972 | Validation Accuracy: 0.433730\n",
      "Accuracy on Test data: 0.45502129197120667, 0.48784810304641724\n",
      "Step 3 | Training Loss: -1.926679 | Validation Accuracy: 0.460317\n",
      "Accuracy on Test data: 0.45728352665901184, 0.48523205518722534\n",
      "Step 4 | Training Loss: 0.705110 | Validation Accuracy: 0.437698\n",
      "Accuracy on Test data: 0.46318310499191284, 0.4876793324947357\n",
      "Step 5 | Training Loss: -0.087888 | Validation Accuracy: 0.448810\n",
      "Accuracy on Test data: 0.4602111279964447, 0.4800843894481659\n",
      "Step 6 | Training Loss: -0.682227 | Validation Accuracy: 0.451587\n",
      "Accuracy on Test data: 0.46779629588127136, 0.4883544445037842\n",
      "Step 7 | Training Loss: -0.308697 | Validation Accuracy: 0.430556\n",
      "Accuracy on Test data: 0.4598562717437744, 0.4841350317001343\n",
      "Step 8 | Training Loss: -0.383137 | Validation Accuracy: 0.448810\n",
      "Accuracy on Test data: 0.4604772925376892, 0.49569621682167053\n",
      "Step 9 | Training Loss: -1.463610 | Validation Accuracy: 0.436508\n",
      "Accuracy on Test data: 0.4571061134338379, 0.48801687359809875\n",
      "Step 10 | Training Loss: -0.117304 | Validation Accuracy: 0.436111\n",
      "Accuracy on Test data: 0.46003371477127075, 0.4870886206626892\n",
      "Step 11 | Training Loss: -1.354244 | Validation Accuracy: 0.434524\n",
      "Accuracy on Test data: 0.4571061134338379, 0.48599156737327576\n",
      "Step 12 | Training Loss: -1.656140 | Validation Accuracy: 0.445635\n",
      "Accuracy on Test data: 0.46029984951019287, 0.501940906047821\n",
      "Step 13 | Training Loss: -0.225853 | Validation Accuracy: 0.439286\n",
      "Accuracy on Test data: 0.4614975154399872, 0.4885232150554657\n",
      "Step 14 | Training Loss: -2.319096 | Validation Accuracy: 0.437302\n",
      "Accuracy on Test data: 0.45697301626205444, 0.4850632846355438\n",
      "Step 15 | Training Loss: -1.044550 | Validation Accuracy: 0.456746\n",
      "Accuracy on Test data: 0.46100959181785583, 0.4881012737751007\n",
      "Step 1 | Training Loss: 0.310323 | Validation Accuracy: 0.445635\n",
      "Accuracy on Test data: 0.45475515723228455, 0.4897046387195587\n",
      "Step 2 | Training Loss: 0.954790 | Validation Accuracy: 0.451190\n",
      "Accuracy on Test data: 0.4596344828605652, 0.48582279682159424\n",
      "Step 3 | Training Loss: -0.226127 | Validation Accuracy: 0.462698\n",
      "Accuracy on Test data: 0.4641589820384979, 0.4903797507286072\n",
      "Step 4 | Training Loss: 0.116851 | Validation Accuracy: 0.440476\n",
      "Accuracy on Test data: 0.4626508057117462, 0.49400845170021057\n",
      "Step 5 | Training Loss: 0.464153 | Validation Accuracy: 0.452778\n",
      "Accuracy on Test data: 0.4607878029346466, 0.48556962609291077\n",
      "Step 6 | Training Loss: -1.462924 | Validation Accuracy: 0.435317\n",
      "Accuracy on Test data: 0.462118536233902, 0.4863291084766388\n",
      "Step 7 | Training Loss: -1.806085 | Validation Accuracy: 0.442460\n",
      "Accuracy on Test data: 0.4555535912513733, 0.48379746079444885\n",
      "Step 8 | Training Loss: -0.306295 | Validation Accuracy: 0.439286\n",
      "Accuracy on Test data: 0.4527590572834015, 0.4881012737751007\n",
      "Step 9 | Training Loss: -0.996028 | Validation Accuracy: 0.458333\n",
      "Accuracy on Test data: 0.45737224817276, 0.4832911491394043\n",
      "Step 10 | Training Loss: -0.279012 | Validation Accuracy: 0.458730\n",
      "Accuracy on Test data: 0.46123138070106506, 0.4919831156730652\n",
      "Step 11 | Training Loss: -0.505544 | Validation Accuracy: 0.438095\n",
      "Accuracy on Test data: 0.4586586356163025, 0.49080169200897217\n",
      "Step 12 | Training Loss: 0.920861 | Validation Accuracy: 0.429365\n",
      "Accuracy on Test data: 0.45786017179489136, 0.4832911491394043\n",
      "Step 13 | Training Loss: -0.885194 | Validation Accuracy: 0.444048\n",
      "Accuracy on Test data: 0.4588804244995117, 0.4947679340839386\n",
      "Step 14 | Training Loss: 0.304951 | Validation Accuracy: 0.455952\n",
      "Accuracy on Test data: 0.46100959181785583, 0.49383965134620667\n",
      "Step 15 | Training Loss: 1.561402 | Validation Accuracy: 0.451190\n",
      "Accuracy on Test data: 0.45945706963539124, 0.4792405068874359\n",
      "Current Layer Attributes - epochs:15 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: -0.987809 | Validation Accuracy: 0.379365\n",
      "Accuracy on Test data: 0.45413413643836975, 0.5205063223838806\n",
      "Step 2 | Training Loss: -0.235014 | Validation Accuracy: 0.403175\n",
      "Accuracy on Test data: 0.4555535912513733, 0.5240506529808044\n",
      "Step 3 | Training Loss: -1.570721 | Validation Accuracy: 0.398413\n",
      "Accuracy on Test data: 0.4571504592895508, 0.5099577903747559\n",
      "Step 4 | Training Loss: -0.606335 | Validation Accuracy: 0.396032\n",
      "Accuracy on Test data: 0.4473917782306671, 0.5118143558502197\n",
      "Step 5 | Training Loss: 3.262986 | Validation Accuracy: 0.378968\n",
      "Accuracy on Test data: 0.4496096670627594, 0.5207595229148865\n",
      "Step 6 | Training Loss: -0.459731 | Validation Accuracy: 0.383333\n",
      "Accuracy on Test data: 0.4502306580543518, 0.5175527334213257\n",
      "Step 7 | Training Loss: 0.359904 | Validation Accuracy: 0.411111\n",
      "Accuracy on Test data: 0.4449964463710785, 0.5145991444587708\n",
      "Step 8 | Training Loss: 0.880148 | Validation Accuracy: 0.401587\n",
      "Accuracy on Test data: 0.45227110385894775, 0.5119830965995789\n",
      "Step 9 | Training Loss: 0.258869 | Validation Accuracy: 0.405159\n",
      "Accuracy on Test data: 0.4499645233154297, 0.5089451670646667\n",
      "Step 10 | Training Loss: -0.196523 | Validation Accuracy: 0.394444\n",
      "Accuracy on Test data: 0.44974273443222046, 0.5265823006629944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 | Training Loss: 0.016282 | Validation Accuracy: 0.385714\n",
      "Accuracy on Test data: 0.4526703357696533, 0.5137552618980408\n",
      "Step 12 | Training Loss: 1.317881 | Validation Accuracy: 0.375794\n",
      "Accuracy on Test data: 0.4495652914047241, 0.5187341570854187\n",
      "Step 13 | Training Loss: -0.856439 | Validation Accuracy: 0.397222\n",
      "Accuracy on Test data: 0.4487668573856354, 0.5291982889175415\n",
      "Step 14 | Training Loss: -1.661043 | Validation Accuracy: 0.380556\n",
      "Accuracy on Test data: 0.4480571448802948, 0.5234599113464355\n",
      "Step 15 | Training Loss: -0.483050 | Validation Accuracy: 0.382937\n",
      "Accuracy on Test data: 0.45262598991394043, 0.5260759592056274\n",
      "Step 1 | Training Loss: -0.475000 | Validation Accuracy: 0.385714\n",
      "Accuracy on Test data: 0.45351311564445496, 0.5229535698890686\n",
      "Step 2 | Training Loss: -1.322364 | Validation Accuracy: 0.404762\n",
      "Accuracy on Test data: 0.444419801235199, 0.5140084624290466\n",
      "Step 3 | Training Loss: -1.893875 | Validation Accuracy: 0.411905\n",
      "Accuracy on Test data: 0.4468151032924652, 0.5175527334213257\n",
      "Step 4 | Training Loss: -2.124953 | Validation Accuracy: 0.378175\n",
      "Accuracy on Test data: 0.44233497977256775, 0.5196624398231506\n",
      "Step 5 | Training Loss: -0.609365 | Validation Accuracy: 0.392857\n",
      "Accuracy on Test data: 0.4470812678337097, 0.5170463919639587\n",
      "Step 6 | Training Loss: 0.039768 | Validation Accuracy: 0.401190\n",
      "Accuracy on Test data: 0.45253726840019226, 0.5154430270195007\n",
      "Step 7 | Training Loss: 1.828707 | Validation Accuracy: 0.394841\n",
      "Accuracy on Test data: 0.4504967927932739, 0.5124050378799438\n",
      "Step 8 | Training Loss: -0.513766 | Validation Accuracy: 0.385317\n",
      "Accuracy on Test data: 0.448456346988678, 0.5199156403541565\n",
      "Step 9 | Training Loss: 0.232687 | Validation Accuracy: 0.392460\n",
      "Accuracy on Test data: 0.45688432455062866, 0.5167932510375977\n",
      "Step 10 | Training Loss: -0.781606 | Validation Accuracy: 0.404762\n",
      "Accuracy on Test data: 0.445573091506958, 0.5151054859161377\n",
      "Step 11 | Training Loss: -0.934012 | Validation Accuracy: 0.376984\n",
      "Accuracy on Test data: 0.443399578332901, 0.5113924145698547\n",
      "Step 12 | Training Loss: -0.662679 | Validation Accuracy: 0.398413\n",
      "Accuracy on Test data: 0.4523598253726959, 0.5168776512145996\n",
      "Step 13 | Training Loss: -1.128522 | Validation Accuracy: 0.407540\n",
      "Accuracy on Test data: 0.454355925321579, 0.5149366855621338\n",
      "Step 14 | Training Loss: -0.718220 | Validation Accuracy: 0.373016\n",
      "Accuracy on Test data: 0.45005321502685547, 0.5227848291397095\n",
      "Step 15 | Training Loss: -0.132595 | Validation Accuracy: 0.384524\n",
      "Accuracy on Test data: 0.45440027117729187, 0.5137552618980408\n",
      "Step 1 | Training Loss: -0.100541 | Validation Accuracy: 0.382540\n",
      "Accuracy on Test data: 0.45169445872306824, 0.5196624398231506\n",
      "Step 2 | Training Loss: 0.681874 | Validation Accuracy: 0.389683\n",
      "Accuracy on Test data: 0.45692867040634155, 0.5110548734664917\n",
      "Step 3 | Training Loss: 0.608463 | Validation Accuracy: 0.392063\n",
      "Accuracy on Test data: 0.4511621594429016, 0.5261603593826294\n",
      "Step 4 | Training Loss: 1.788103 | Validation Accuracy: 0.396429\n",
      "Accuracy on Test data: 0.44916608929634094, 0.5104641318321228\n",
      "Step 5 | Training Loss: -0.222813 | Validation Accuracy: 0.396429\n",
      "Accuracy on Test data: 0.4434439241886139, 0.5218565464019775\n",
      "Step 6 | Training Loss: -1.176794 | Validation Accuracy: 0.368254\n",
      "Accuracy on Test data: 0.4524485468864441, 0.5184810161590576\n",
      "Step 7 | Training Loss: -0.946261 | Validation Accuracy: 0.383730\n",
      "Accuracy on Test data: 0.4456618130207062, 0.5190717577934265\n",
      "Step 8 | Training Loss: 0.620899 | Validation Accuracy: 0.402778\n",
      "Accuracy on Test data: 0.44819021224975586, 0.5213502049446106\n",
      "Step 9 | Training Loss: -0.556859 | Validation Accuracy: 0.372619\n",
      "Accuracy on Test data: 0.45599716901779175, 0.5185654163360596\n",
      "Step 10 | Training Loss: -1.636942 | Validation Accuracy: 0.411111\n",
      "Accuracy on Test data: 0.45222675800323486, 0.5206751227378845\n",
      "Step 11 | Training Loss: -2.528335 | Validation Accuracy: 0.381746\n",
      "Accuracy on Test data: 0.4511178135871887, 0.5210126638412476\n",
      "Step 12 | Training Loss: -1.038503 | Validation Accuracy: 0.391667\n",
      "Accuracy on Test data: 0.45231547951698303, 0.5276793241500854\n",
      "Step 13 | Training Loss: 0.894104 | Validation Accuracy: 0.397619\n",
      "Accuracy on Test data: 0.4536905586719513, 0.5167088508605957\n",
      "Step 14 | Training Loss: 0.086623 | Validation Accuracy: 0.370635\n",
      "Accuracy on Test data: 0.44938787817955017, 0.5212658047676086\n",
      "Step 15 | Training Loss: -1.140465 | Validation Accuracy: 0.379762\n",
      "Accuracy on Test data: 0.45133960247039795, 0.5164557099342346\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: -1.385110 | Validation Accuracy: 0.462302\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: 1.716821 | Validation Accuracy: 0.453175\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: -0.387485 | Validation Accuracy: 0.470238\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: 0.598210 | Validation Accuracy: 0.460714\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: -1.002855 | Validation Accuracy: 0.474206\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: 1.471208 | Validation Accuracy: 0.455159\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: -0.220069 | Validation Accuracy: 0.468651\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: -0.476793 | Validation Accuracy: 0.456746\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: -0.068501 | Validation Accuracy: 0.442857\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: 0.702546 | Validation Accuracy: 0.462302\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -0.157852 | Validation Accuracy: 0.455159\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: -0.505582 | Validation Accuracy: 0.461508\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: -1.517800 | Validation Accuracy: 0.469444\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: -0.323850 | Validation Accuracy: 0.459127\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 15 | Training Loss: 0.691066 | Validation Accuracy: 0.453175\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 1 | Training Loss: -0.795270 | Validation Accuracy: 0.476984\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: 1.606780 | Validation Accuracy: 0.459921\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: -0.580686 | Validation Accuracy: 0.471032\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: -1.127040 | Validation Accuracy: 0.462302\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: 0.758011 | Validation Accuracy: 0.467857\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: 1.858188 | Validation Accuracy: 0.459921\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: 1.195904 | Validation Accuracy: 0.452778\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: 1.081043 | Validation Accuracy: 0.460317\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: -0.750089 | Validation Accuracy: 0.461905\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: 0.776126 | Validation Accuracy: 0.482540\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -1.512459 | Validation Accuracy: 0.470238\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: 0.581993 | Validation Accuracy: 0.459921\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: -1.048251 | Validation Accuracy: 0.475794\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: 1.074134 | Validation Accuracy: 0.453968\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 | Training Loss: 0.498253 | Validation Accuracy: 0.456349\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 1 | Training Loss: 0.107780 | Validation Accuracy: 0.442063\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 2 | Training Loss: -0.220905 | Validation Accuracy: 0.453571\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 3 | Training Loss: -0.272965 | Validation Accuracy: 0.455952\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 4 | Training Loss: -0.565880 | Validation Accuracy: 0.470635\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 5 | Training Loss: -0.271646 | Validation Accuracy: 0.464683\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 6 | Training Loss: -0.130638 | Validation Accuracy: 0.461508\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 7 | Training Loss: 0.114356 | Validation Accuracy: 0.471032\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 8 | Training Loss: -0.656342 | Validation Accuracy: 0.471825\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 9 | Training Loss: 0.733499 | Validation Accuracy: 0.459524\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 10 | Training Loss: -0.853060 | Validation Accuracy: 0.478571\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 11 | Training Loss: -0.856876 | Validation Accuracy: 0.490476\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 12 | Training Loss: 2.104938 | Validation Accuracy: 0.460714\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 13 | Training Loss: 0.071889 | Validation Accuracy: 0.462698\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 14 | Training Loss: -0.481935 | Validation Accuracy: 0.467460\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Step 15 | Training Loss: 0.131929 | Validation Accuracy: 0.459127\n",
      "Accuracy on Test data: 0.5692423582077026, 0.8183966279029846\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: -0.249296 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.4188697636127472, 0.20607595145702362\n",
      "Step 2 | Training Loss: -0.061784 | Validation Accuracy: 0.506349\n",
      "Accuracy on Test data: 0.415764719247818, 0.20742616057395935\n",
      "Step 3 | Training Loss: 1.470679 | Validation Accuracy: 0.505952\n",
      "Accuracy on Test data: 0.42020049691200256, 0.20582278072834015\n",
      "Step 4 | Training Loss: 0.707889 | Validation Accuracy: 0.519841\n",
      "Accuracy on Test data: 0.41931334137916565, 0.20548522472381592\n",
      "Step 5 | Training Loss: -1.183862 | Validation Accuracy: 0.531746\n",
      "Accuracy on Test data: 0.41931334137916565, 0.20455695688724518\n",
      "Step 6 | Training Loss: 0.784995 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.41736161708831787, 0.20464135706424713\n",
      "Step 7 | Training Loss: -0.596322 | Validation Accuracy: 0.524206\n",
      "Accuracy on Test data: 0.4191358983516693, 0.20405063033103943\n",
      "Step 8 | Training Loss: 0.523990 | Validation Accuracy: 0.510317\n",
      "Accuracy on Test data: 0.4181600511074066, 0.20759493112564087\n",
      "Step 9 | Training Loss: 0.598612 | Validation Accuracy: 0.524206\n",
      "Accuracy on Test data: 0.4175390303134918, 0.2059071660041809\n",
      "Step 10 | Training Loss: -0.260474 | Validation Accuracy: 0.496032\n",
      "Accuracy on Test data: 0.41687366366386414, 0.20776371657848358\n",
      "Step 11 | Training Loss: 2.120972 | Validation Accuracy: 0.503571\n",
      "Accuracy on Test data: 0.4178938865661621, 0.20464135706424713\n",
      "Step 12 | Training Loss: 0.359490 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.41833746433258057, 0.20827004313468933\n",
      "Step 13 | Training Loss: 2.635729 | Validation Accuracy: 0.515079\n",
      "Accuracy on Test data: 0.4163413643836975, 0.20624472200870514\n",
      "Step 14 | Training Loss: 0.608849 | Validation Accuracy: 0.512698\n",
      "Accuracy on Test data: 0.42059972882270813, 0.20548522472381592\n",
      "Step 15 | Training Loss: -1.079058 | Validation Accuracy: 0.500794\n",
      "Accuracy on Test data: 0.41842618584632874, 0.2022784799337387\n",
      "Step 1 | Training Loss: -0.272809 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.4190915524959564, 0.20708860456943512\n",
      "Step 2 | Training Loss: 0.152876 | Validation Accuracy: 0.513492\n",
      "Accuracy on Test data: 0.4185149073600769, 0.20185653865337372\n",
      "Step 3 | Training Loss: -0.443560 | Validation Accuracy: 0.505159\n",
      "Accuracy on Test data: 0.418781042098999, 0.20497889816761017\n",
      "Step 4 | Training Loss: -0.136173 | Validation Accuracy: 0.509921\n",
      "Accuracy on Test data: 0.41762775182724, 0.2053164541721344\n",
      "Step 5 | Training Loss: -1.181677 | Validation Accuracy: 0.517857\n",
      "Accuracy on Test data: 0.41811567544937134, 0.20902954041957855\n",
      "Step 6 | Training Loss: -0.521678 | Validation Accuracy: 0.513492\n",
      "Accuracy on Test data: 0.41931334137916565, 0.20565401017665863\n",
      "Step 7 | Training Loss: -2.866873 | Validation Accuracy: 0.512302\n",
      "Accuracy on Test data: 0.4191358983516693, 0.20481012761592865\n",
      "Step 8 | Training Loss: 1.580534 | Validation Accuracy: 0.525397\n",
      "Accuracy on Test data: 0.41842618584632874, 0.20565401017665863\n",
      "Step 9 | Training Loss: -0.880889 | Validation Accuracy: 0.541667\n",
      "Accuracy on Test data: 0.41895848512649536, 0.2038818597793579\n",
      "Step 10 | Training Loss: -0.979880 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.4186036288738251, 0.20548522472381592\n",
      "Step 11 | Training Loss: 1.201837 | Validation Accuracy: 0.509524\n",
      "Accuracy on Test data: 0.41864797472953796, 0.20582278072834015\n",
      "Step 12 | Training Loss: -1.275627 | Validation Accuracy: 0.480952\n",
      "Accuracy on Test data: 0.4170067310333252, 0.20506329834461212\n",
      "Step 13 | Training Loss: -0.664886 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.4176720976829529, 0.2063291072845459\n",
      "Step 14 | Training Loss: 0.582387 | Validation Accuracy: 0.517063\n",
      "Accuracy on Test data: 0.4194020628929138, 0.20599156618118286\n",
      "Step 15 | Training Loss: -0.155715 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.42020049691200256, 0.20624472200870514\n",
      "Step 1 | Training Loss: -1.803632 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.4197125732898712, 0.2063291072845459\n",
      "Step 2 | Training Loss: 0.760177 | Validation Accuracy: 0.504365\n",
      "Accuracy on Test data: 0.4188254177570343, 0.20700421929359436\n",
      "Step 3 | Training Loss: 0.706345 | Validation Accuracy: 0.505952\n",
      "Accuracy on Test data: 0.41833746433258057, 0.20759493112564087\n",
      "Step 4 | Training Loss: 0.164025 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.41771647334098816, 0.2073417752981186\n",
      "Step 5 | Training Loss: 1.059018 | Validation Accuracy: 0.520238\n",
      "Accuracy on Test data: 0.4160308837890625, 0.20902954041957855\n",
      "Step 6 | Training Loss: -0.863067 | Validation Accuracy: 0.508333\n",
      "Accuracy on Test data: 0.4182487726211548, 0.20278480648994446\n",
      "Step 7 | Training Loss: 2.357160 | Validation Accuracy: 0.505952\n",
      "Accuracy on Test data: 0.41713982820510864, 0.20548522472381592\n",
      "Step 8 | Training Loss: -1.342099 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.41869232058525085, 0.20725739002227783\n",
      "Step 9 | Training Loss: -0.208693 | Validation Accuracy: 0.509127\n",
      "Accuracy on Test data: 0.4172285199165344, 0.2038818597793579\n",
      "Step 10 | Training Loss: -1.362562 | Validation Accuracy: 0.515476\n",
      "Accuracy on Test data: 0.4182043969631195, 0.2059071660041809\n",
      "Step 11 | Training Loss: 0.528323 | Validation Accuracy: 0.512698\n",
      "Accuracy on Test data: 0.41687366366386414, 0.2037130743265152\n",
      "Step 12 | Training Loss: 0.342865 | Validation Accuracy: 0.503571\n",
      "Accuracy on Test data: 0.41713982820510864, 0.2059071660041809\n",
      "Step 13 | Training Loss: 0.680924 | Validation Accuracy: 0.539286\n",
      "Accuracy on Test data: 0.41718417406082153, 0.20447257161140442\n",
      "Step 14 | Training Loss: -0.506816 | Validation Accuracy: 0.495635\n",
      "Accuracy on Test data: 0.4182487726211548, 0.20084388554096222\n",
      "Step 15 | Training Loss: 0.083248 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.41776081919670105, 0.20827004313468933\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 2.206794 | Validation Accuracy: 0.536905\n",
      "Accuracy on Test data: 0.4322657883167267, 0.18776370584964752\n",
      "Step 2 | Training Loss: -0.215389 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43177786469459534, 0.188016876578331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: -0.176164 | Validation Accuracy: 0.528968\n",
      "Accuracy on Test data: 0.4315560758113861, 0.18691983819007874\n",
      "Step 4 | Training Loss: -0.317056 | Validation Accuracy: 0.535317\n",
      "Accuracy on Test data: 0.43053585290908813, 0.18734177947044373\n",
      "Step 5 | Training Loss: -0.962547 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.4322214424610138, 0.18742616474628448\n",
      "Step 6 | Training Loss: -0.614799 | Validation Accuracy: 0.529365\n",
      "Accuracy on Test data: 0.43293115496635437, 0.1886075884103775\n",
      "Step 7 | Training Loss: -0.631634 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.4328868091106415, 0.18632911145687103\n",
      "Step 8 | Training Loss: -2.509959 | Validation Accuracy: 0.561111\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18793249130249023\n",
      "Step 9 | Training Loss: -1.107783 | Validation Accuracy: 0.545238\n",
      "Accuracy on Test data: 0.4322657883167267, 0.18742616474628448\n",
      "Step 10 | Training Loss: 0.470990 | Validation Accuracy: 0.519444\n",
      "Accuracy on Test data: 0.4331529438495636, 0.18886075913906097\n",
      "Step 11 | Training Loss: 0.512852 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.4310237765312195, 0.18666666746139526\n",
      "Step 12 | Training Loss: 0.076366 | Validation Accuracy: 0.536905\n",
      "Accuracy on Test data: 0.4318665862083435, 0.18742616474628448\n",
      "Step 13 | Training Loss: 1.361508 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.4327980875968933, 0.18886075913906097\n",
      "Step 14 | Training Loss: -0.836809 | Validation Accuracy: 0.555556\n",
      "Accuracy on Test data: 0.432443231344223, 0.1864134967327118\n",
      "Step 15 | Training Loss: -0.996434 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.43306422233581543, 0.1890295296907425\n",
      "Step 1 | Training Loss: 0.080132 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.43199965357780457, 0.18683543801307678\n",
      "Step 2 | Training Loss: 0.908423 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.4318222105503082, 0.1870042234659195\n",
      "Step 3 | Training Loss: 1.067681 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.4324875771999359, 0.18827004730701447\n",
      "Step 4 | Training Loss: 0.210720 | Validation Accuracy: 0.558730\n",
      "Accuracy on Test data: 0.4322214424610138, 0.18742616474628448\n",
      "Step 5 | Training Loss: -0.653318 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.4334190785884857, 0.18742616474628448\n",
      "Step 6 | Training Loss: 0.091769 | Validation Accuracy: 0.538492\n",
      "Accuracy on Test data: 0.43297550082206726, 0.18894514441490173\n",
      "Step 7 | Training Loss: -1.584743 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.4318222105503082, 0.19012658298015594\n",
      "Step 8 | Training Loss: 0.392808 | Validation Accuracy: 0.543254\n",
      "Accuracy on Test data: 0.4318665862083435, 0.18945147097110748\n",
      "Step 9 | Training Loss: 1.956690 | Validation Accuracy: 0.538095\n",
      "Accuracy on Test data: 0.4328868091106415, 0.1870042234659195\n",
      "Step 10 | Training Loss: -0.303784 | Validation Accuracy: 0.534921\n",
      "Accuracy on Test data: 0.4318665862083435, 0.187594935297966\n",
      "Step 11 | Training Loss: -0.513655 | Validation Accuracy: 0.542063\n",
      "Accuracy on Test data: 0.4333747327327728, 0.18953587114810944\n",
      "Step 12 | Training Loss: 0.623752 | Validation Accuracy: 0.527381\n",
      "Accuracy on Test data: 0.432443231344223, 0.1859915554523468\n",
      "Step 13 | Training Loss: 0.255285 | Validation Accuracy: 0.534524\n",
      "Accuracy on Test data: 0.4318222105503082, 0.18708860874176025\n",
      "Step 14 | Training Loss: 0.178638 | Validation Accuracy: 0.532143\n",
      "Accuracy on Test data: 0.43301987648010254, 0.18734177947044373\n",
      "Step 15 | Training Loss: -0.755432 | Validation Accuracy: 0.529762\n",
      "Accuracy on Test data: 0.4331972897052765, 0.1897890269756317\n",
      "Step 1 | Training Loss: 0.497114 | Validation Accuracy: 0.546429\n",
      "Accuracy on Test data: 0.432620644569397, 0.188016876578331\n",
      "Step 2 | Training Loss: 0.047705 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.4313342869281769, 0.18675105273723602\n",
      "Step 3 | Training Loss: 0.384734 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.4328868091106415, 0.18573839962482452\n",
      "Step 4 | Training Loss: 0.137342 | Validation Accuracy: 0.548016\n",
      "Accuracy on Test data: 0.43301987648010254, 0.18691983819007874\n",
      "Step 5 | Training Loss: 0.156170 | Validation Accuracy: 0.541667\n",
      "Accuracy on Test data: 0.43266502022743225, 0.1870042234659195\n",
      "Step 6 | Training Loss: -0.227096 | Validation Accuracy: 0.511905\n",
      "Accuracy on Test data: 0.4324875771999359, 0.18632911145687103\n",
      "Step 7 | Training Loss: 0.409700 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.4319552779197693, 0.1886075884103775\n",
      "Step 8 | Training Loss: -0.387109 | Validation Accuracy: 0.540079\n",
      "Accuracy on Test data: 0.4321770668029785, 0.18624472618103027\n",
      "Step 9 | Training Loss: 0.921574 | Validation Accuracy: 0.539286\n",
      "Accuracy on Test data: 0.4318222105503082, 0.187594935297966\n",
      "Step 10 | Training Loss: 0.186226 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43204399943351746, 0.18767932057380676\n",
      "Step 11 | Training Loss: -1.091575 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.43239885568618774, 0.18675105273723602\n",
      "Step 12 | Training Loss: 0.426858 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.4313342869281769, 0.1890295296907425\n",
      "Step 13 | Training Loss: -0.373422 | Validation Accuracy: 0.516270\n",
      "Accuracy on Test data: 0.4318665862083435, 0.1891983151435852\n",
      "Step 14 | Training Loss: -0.149036 | Validation Accuracy: 0.548016\n",
      "Accuracy on Test data: 0.4322214424610138, 0.18776370584964752\n",
      "Step 15 | Training Loss: 1.501287 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.4328868091106415, 0.188016876578331\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: -0.685657 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.5347764492034912, 0.4982278347015381\n",
      "Step 2 | Training Loss: 0.379653 | Validation Accuracy: 0.528571\n",
      "Accuracy on Test data: 0.5354861617088318, 0.502362847328186\n",
      "Step 3 | Training Loss: 0.338650 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.5362846255302429, 0.4982278347015381\n",
      "Step 4 | Training Loss: 0.866715 | Validation Accuracy: 0.532937\n",
      "Accuracy on Test data: 0.531981885433197, 0.4929113984107971\n",
      "Step 5 | Training Loss: 0.875474 | Validation Accuracy: 0.519048\n",
      "Accuracy on Test data: 0.5348207950592041, 0.501603364944458\n",
      "Step 6 | Training Loss: 0.505996 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.5339336395263672, 0.5090295076370239\n",
      "Step 7 | Training Loss: -1.121361 | Validation Accuracy: 0.521429\n",
      "Accuracy on Test data: 0.5327803492546082, 0.502362847328186\n",
      "Step 8 | Training Loss: -1.492867 | Validation Accuracy: 0.519841\n",
      "Accuracy on Test data: 0.5381032824516296, 0.49873417615890503\n",
      "Step 9 | Training Loss: -0.237200 | Validation Accuracy: 0.521825\n",
      "Accuracy on Test data: 0.5352200269699097, 0.49544304609298706\n",
      "Step 10 | Training Loss: 1.055535 | Validation Accuracy: 0.508730\n",
      "Accuracy on Test data: 0.5396558046340942, 0.5063291192054749\n",
      "Step 11 | Training Loss: 0.472918 | Validation Accuracy: 0.523413\n",
      "Accuracy on Test data: 0.5310503840446472, 0.499240517616272\n",
      "Step 12 | Training Loss: 1.481300 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.5293204188346863, 0.5017721652984619\n",
      "Step 13 | Training Loss: -1.695759 | Validation Accuracy: 0.527778\n",
      "Accuracy on Test data: 0.5335787534713745, 0.4897046387195587\n",
      "Step 14 | Training Loss: 0.195622 | Validation Accuracy: 0.508730\n",
      "Accuracy on Test data: 0.5329133868217468, 0.49848100543022156\n",
      "Step 15 | Training Loss: 1.472393 | Validation Accuracy: 0.528968\n",
      "Accuracy on Test data: 0.5386798977851868, 0.4978058934211731\n",
      "Step 1 | Training Loss: 1.002597 | Validation Accuracy: 0.520635\n",
      "Accuracy on Test data: 0.5374822616577148, 0.499578058719635\n",
      "Step 2 | Training Loss: -0.055763 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.5381476283073425, 0.4981434643268585\n",
      "Step 3 | Training Loss: 0.704571 | Validation Accuracy: 0.513095\n",
      "Accuracy on Test data: 0.5368168950080872, 0.5047257542610168\n",
      "Step 4 | Training Loss: -0.807165 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.5323367714881897, 0.4870886206626892\n",
      "Step 5 | Training Loss: -1.561144 | Validation Accuracy: 0.513889\n",
      "Accuracy on Test data: 0.5370386838912964, 0.4965400695800781\n",
      "Step 6 | Training Loss: 0.150347 | Validation Accuracy: 0.521429\n",
      "Accuracy on Test data: 0.5376597046852112, 0.49831223487854004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 | Training Loss: 0.810321 | Validation Accuracy: 0.532540\n",
      "Accuracy on Test data: 0.5323811173439026, 0.507341742515564\n",
      "Step 8 | Training Loss: 1.985820 | Validation Accuracy: 0.517063\n",
      "Accuracy on Test data: 0.5338449478149414, 0.4967932403087616\n",
      "Step 9 | Training Loss: 0.018185 | Validation Accuracy: 0.524603\n",
      "Accuracy on Test data: 0.536018431186676, 0.5005063414573669\n",
      "Step 10 | Training Loss: 1.164853 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.5358410477638245, 0.49729958176612854\n",
      "Step 11 | Training Loss: -1.032303 | Validation Accuracy: 0.523016\n",
      "Accuracy on Test data: 0.5387242436408997, 0.500421941280365\n",
      "Step 12 | Training Loss: 0.018382 | Validation Accuracy: 0.509524\n",
      "Accuracy on Test data: 0.5339779853820801, 0.49603375792503357\n",
      "Step 13 | Training Loss: 0.242169 | Validation Accuracy: 0.507937\n",
      "Accuracy on Test data: 0.5343328714370728, 0.4933333396911621\n",
      "Step 14 | Training Loss: -0.700934 | Validation Accuracy: 0.542460\n",
      "Accuracy on Test data: 0.5393452644348145, 0.499578058719635\n",
      "Step 15 | Training Loss: -0.659305 | Validation Accuracy: 0.526984\n",
      "Accuracy on Test data: 0.5336675047874451, 0.49586498737335205\n",
      "Step 1 | Training Loss: 0.195334 | Validation Accuracy: 0.513492\n",
      "Accuracy on Test data: 0.5302075743675232, 0.5110548734664917\n",
      "Step 2 | Training Loss: 0.105271 | Validation Accuracy: 0.510714\n",
      "Accuracy on Test data: 0.5369499921798706, 0.49223628640174866\n",
      "Step 3 | Training Loss: -0.590467 | Validation Accuracy: 0.511111\n",
      "Accuracy on Test data: 0.5371717810630798, 0.5021941065788269\n",
      "Step 4 | Training Loss: 0.384258 | Validation Accuracy: 0.515873\n",
      "Accuracy on Test data: 0.5338892936706543, 0.503713071346283\n",
      "Step 5 | Training Loss: 0.560851 | Validation Accuracy: 0.523016\n",
      "Accuracy on Test data: 0.536550760269165, 0.49890294671058655\n",
      "Step 6 | Training Loss: 2.442774 | Validation Accuracy: 0.507540\n",
      "Accuracy on Test data: 0.5380589365959167, 0.505316436290741\n",
      "Step 7 | Training Loss: -1.840437 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.5304737687110901, 0.4953586459159851\n",
      "Step 8 | Training Loss: 0.268576 | Validation Accuracy: 0.508730\n",
      "Accuracy on Test data: 0.5357522964477539, 0.4947679340839386\n",
      "Step 9 | Training Loss: 1.493465 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.5387686491012573, 0.500337541103363\n",
      "Step 10 | Training Loss: 0.068924 | Validation Accuracy: 0.509524\n",
      "Accuracy on Test data: 0.5315826535224915, 0.502869188785553\n",
      "Step 11 | Training Loss: -0.114047 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.5292317271232605, 0.5102953314781189\n",
      "Step 12 | Training Loss: 0.475160 | Validation Accuracy: 0.515079\n",
      "Accuracy on Test data: 0.5361071825027466, 0.5033755302429199\n",
      "Step 13 | Training Loss: -0.023679 | Validation Accuracy: 0.526587\n",
      "Accuracy on Test data: 0.5359740853309631, 0.4929957687854767\n",
      "Step 14 | Training Loss: 0.413639 | Validation Accuracy: 0.537698\n",
      "Accuracy on Test data: 0.5384581089019775, 0.4981434643268585\n",
      "Step 15 | Training Loss: 2.403820 | Validation Accuracy: 0.519841\n",
      "Accuracy on Test data: 0.5320706367492676, 0.5086075663566589\n",
      "Current Layer Attributes - epochs:15 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: -1.482739 | Validation Accuracy: 0.533333\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18168775737285614\n",
      "Step 2 | Training Loss: 0.389804 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18143460154533386\n",
      "Step 3 | Training Loss: 1.876210 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.4308907091617584, 0.1817721575498581\n",
      "Step 4 | Training Loss: 0.207261 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.4306689202785492, 0.1817721575498581\n",
      "Step 5 | Training Loss: -0.610006 | Validation Accuracy: 0.560317\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 6 | Training Loss: 0.182840 | Validation Accuracy: 0.518651\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 7 | Training Loss: 1.097769 | Validation Accuracy: 0.547619\n",
      "Accuracy on Test data: 0.4306245446205139, 0.18168775737285614\n",
      "Step 8 | Training Loss: -0.528752 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.4307132661342621, 0.1819409281015396\n",
      "Step 9 | Training Loss: -0.485554 | Validation Accuracy: 0.549206\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18185654282569885\n",
      "Step 10 | Training Loss: 0.611451 | Validation Accuracy: 0.521032\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 11 | Training Loss: 0.172890 | Validation Accuracy: 0.525794\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.221721 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18168775737285614\n",
      "Step 13 | Training Loss: 1.078540 | Validation Accuracy: 0.540476\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 14 | Training Loss: -0.321203 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 15 | Training Loss: -0.896776 | Validation Accuracy: 0.524603\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18185654282569885\n",
      "Step 1 | Training Loss: 2.262970 | Validation Accuracy: 0.544841\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18143460154533386\n",
      "Step 2 | Training Loss: 0.891318 | Validation Accuracy: 0.552381\n",
      "Accuracy on Test data: 0.4306689202785492, 0.1817721575498581\n",
      "Step 3 | Training Loss: -0.259903 | Validation Accuracy: 0.538889\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 4 | Training Loss: 0.354406 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.4309350550174713, 0.1817721575498581\n",
      "Step 5 | Training Loss: 0.492913 | Validation Accuracy: 0.550397\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18168775737285614\n",
      "Step 6 | Training Loss: -0.709737 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18160337209701538\n",
      "Step 7 | Training Loss: -0.551431 | Validation Accuracy: 0.550794\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 8 | Training Loss: 1.203854 | Validation Accuracy: 0.547222\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18160337209701538\n",
      "Step 9 | Training Loss: -0.394882 | Validation Accuracy: 0.530952\n",
      "Accuracy on Test data: 0.4307132661342621, 0.1817721575498581\n",
      "Step 10 | Training Loss: -0.701990 | Validation Accuracy: 0.532143\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18151898682117462\n",
      "Step 11 | Training Loss: 1.116959 | Validation Accuracy: 0.529762\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 12 | Training Loss: 0.369438 | Validation Accuracy: 0.546825\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18168775737285614\n",
      "Step 13 | Training Loss: -0.129340 | Validation Accuracy: 0.519841\n",
      "Accuracy on Test data: 0.43080198764801025, 0.18160337209701538\n",
      "Step 14 | Training Loss: 1.431088 | Validation Accuracy: 0.549206\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.842032 | Validation Accuracy: 0.522222\n",
      "Accuracy on Test data: 0.43075764179229736, 0.1817721575498581\n",
      "Step 1 | Training Loss: -0.679774 | Validation Accuracy: 0.537302\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1817721575498581\n",
      "Step 2 | Training Loss: -0.522323 | Validation Accuracy: 0.539683\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1817721575498581\n",
      "Step 3 | Training Loss: -0.378693 | Validation Accuracy: 0.530159\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18185654282569885\n",
      "Step 4 | Training Loss: -0.175573 | Validation Accuracy: 0.531349\n",
      "Accuracy on Test data: 0.4308907091617584, 0.18185654282569885\n",
      "Step 5 | Training Loss: 1.554441 | Validation Accuracy: 0.523413\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18160337209701538\n",
      "Step 6 | Training Loss: 1.894517 | Validation Accuracy: 0.513095\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18168775737285614\n",
      "Step 7 | Training Loss: 0.060500 | Validation Accuracy: 0.507143\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18185654282569885\n",
      "Step 8 | Training Loss: 2.453114 | Validation Accuracy: 0.526190\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1817721575498581\n",
      "Step 9 | Training Loss: 0.496585 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18185654282569885\n",
      "Step 10 | Training Loss: 0.571701 | Validation Accuracy: 0.514286\n",
      "Accuracy on Test data: 0.4306689202785492, 0.18168775737285614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 | Training Loss: 0.240536 | Validation Accuracy: 0.528968\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18151898682117462\n",
      "Step 12 | Training Loss: -0.065930 | Validation Accuracy: 0.542063\n",
      "Accuracy on Test data: 0.4307132661342621, 0.18185654282569885\n",
      "Step 13 | Training Loss: 0.823768 | Validation Accuracy: 0.544048\n",
      "Accuracy on Test data: 0.43084633350372314, 0.1817721575498581\n",
      "Step 14 | Training Loss: 0.668088 | Validation Accuracy: 0.545238\n",
      "Accuracy on Test data: 0.43080198764801025, 0.1819409281015396\n",
      "Step 15 | Training Loss: 0.552141 | Validation Accuracy: 0.523810\n",
      "Accuracy on Test data: 0.43084633350372314, 0.18168775737285614\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 10\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:40.866279Z",
     "start_time": "2017-07-23T22:22:40.736190Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions-.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions-__.pkl\")\n",
    "\n",
    "#df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:40.954007Z",
     "start_time": "2017-07-23T22:22:40.868285Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:40.962412Z",
     "start_time": "2017-07-23T22:22:40.955751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_nsl_kdd_all-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:40.993544Z",
     "start_time": "2017-07-23T22:22:40.964628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.462302</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>2.100314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401190</td>\n",
       "      <td>0.497871</td>\n",
       "      <td>0.637365</td>\n",
       "      <td>0.653587</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>14.581527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.498359</td>\n",
       "      <td>0.637265</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.784597</td>\n",
       "      <td>54.410407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.424603</td>\n",
       "      <td>0.497782</td>\n",
       "      <td>0.636952</td>\n",
       "      <td>0.661013</td>\n",
       "      <td>0.786682</td>\n",
       "      <td>8.533468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.492504</td>\n",
       "      <td>0.633970</td>\n",
       "      <td>0.660675</td>\n",
       "      <td>0.786764</td>\n",
       "      <td>1.280938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.405159</td>\n",
       "      <td>0.493834</td>\n",
       "      <td>0.632957</td>\n",
       "      <td>0.648101</td>\n",
       "      <td>0.777553</td>\n",
       "      <td>3.710555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.455554</td>\n",
       "      <td>0.541330</td>\n",
       "      <td>0.524051</td>\n",
       "      <td>0.654073</td>\n",
       "      <td>3.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398413</td>\n",
       "      <td>0.457150</td>\n",
       "      <td>0.540443</td>\n",
       "      <td>0.509958</td>\n",
       "      <td>0.641654</td>\n",
       "      <td>4.821059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.508730</td>\n",
       "      <td>0.539656</td>\n",
       "      <td>0.540227</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.620327</td>\n",
       "      <td>24.777366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.379365</td>\n",
       "      <td>0.454134</td>\n",
       "      <td>0.537785</td>\n",
       "      <td>0.520506</td>\n",
       "      <td>0.651753</td>\n",
       "      <td>1.648145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.534776</td>\n",
       "      <td>0.535765</td>\n",
       "      <td>0.498228</td>\n",
       "      <td>0.613293</td>\n",
       "      <td>2.517389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.538103</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.498734</td>\n",
       "      <td>0.612221</td>\n",
       "      <td>19.813704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.536285</td>\n",
       "      <td>0.534799</td>\n",
       "      <td>0.498228</td>\n",
       "      <td>0.612285</td>\n",
       "      <td>7.478467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.535486</td>\n",
       "      <td>0.533084</td>\n",
       "      <td>0.502363</td>\n",
       "      <td>0.615806</td>\n",
       "      <td>4.963780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.467796</td>\n",
       "      <td>0.529453</td>\n",
       "      <td>0.488354</td>\n",
       "      <td>0.610472</td>\n",
       "      <td>27.995775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>0.464691</td>\n",
       "      <td>0.525853</td>\n",
       "      <td>0.485232</td>\n",
       "      <td>0.605178</td>\n",
       "      <td>11.977637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.448810</td>\n",
       "      <td>0.462340</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.493249</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>2.737370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449603</td>\n",
       "      <td>0.455110</td>\n",
       "      <td>0.515309</td>\n",
       "      <td>0.486835</td>\n",
       "      <td>0.608006</td>\n",
       "      <td>1.441661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.440960</td>\n",
       "      <td>0.339846</td>\n",
       "      <td>0.322954</td>\n",
       "      <td>0.361379</td>\n",
       "      <td>4.640715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.441891</td>\n",
       "      <td>0.339319</td>\n",
       "      <td>0.322025</td>\n",
       "      <td>0.356765</td>\n",
       "      <td>7.055703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.497619</td>\n",
       "      <td>0.445130</td>\n",
       "      <td>0.339162</td>\n",
       "      <td>0.320928</td>\n",
       "      <td>0.361603</td>\n",
       "      <td>30.998132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.442246</td>\n",
       "      <td>0.337862</td>\n",
       "      <td>0.326751</td>\n",
       "      <td>0.362474</td>\n",
       "      <td>11.762343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.439807</td>\n",
       "      <td>0.336538</td>\n",
       "      <td>0.324557</td>\n",
       "      <td>0.360396</td>\n",
       "      <td>2.358098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.442379</td>\n",
       "      <td>0.334833</td>\n",
       "      <td>0.331983</td>\n",
       "      <td>0.366720</td>\n",
       "      <td>16.290812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518651</td>\n",
       "      <td>0.439274</td>\n",
       "      <td>0.333456</td>\n",
       "      <td>0.324895</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>1.248816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.418870</td>\n",
       "      <td>0.076158</td>\n",
       "      <td>0.206076</td>\n",
       "      <td>0.076379</td>\n",
       "      <td>2.193563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.505952</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.075797</td>\n",
       "      <td>0.205823</td>\n",
       "      <td>0.078347</td>\n",
       "      <td>6.469548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.512698</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.072960</td>\n",
       "      <td>0.205485</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>29.935190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.432931</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.188608</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>14.112990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.519444</td>\n",
       "      <td>0.433153</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.188861</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>23.373540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.537698</td>\n",
       "      <td>0.433419</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.187426</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>46.690186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.536905</td>\n",
       "      <td>0.432266</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>0.187764</td>\n",
       "      <td>0.020556</td>\n",
       "      <td>2.533037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.430935</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>47.273161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.540873</td>\n",
       "      <td>0.430891</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>7.601697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>2.623904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.133690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "20      2               1              3     0.462302    0.569242  0.725500   \n",
       "11     13              24              1     0.401190    0.497871  0.637365   \n",
       "12     48              24              1     0.403175    0.498359  0.637265   \n",
       "10      8              24              1     0.424603    0.497782  0.636952   \n",
       "8       2              24              1     0.422222    0.492504  0.633970   \n",
       "9       4              24              1     0.405159    0.493834  0.632957   \n",
       "18      3             122              1     0.403175    0.455554  0.541330   \n",
       "19      4             122              1     0.398413    0.457150  0.540443   \n",
       "32     11              48              3     0.508730    0.539656  0.540227   \n",
       "17      2             122              1     0.379365    0.454134  0.537785   \n",
       "28      2              48              3     0.521032    0.534776  0.535765   \n",
       "31      9              48              3     0.519841    0.538103  0.535652   \n",
       "30      4              48              3     0.519048    0.536285  0.534799   \n",
       "29      3              48              3     0.528571    0.535486  0.533084   \n",
       "16     14              48              1     0.451587    0.467796  0.529453   \n",
       "15     10              48              1     0.443651    0.464691  0.525853   \n",
       "14      3              48              1     0.448810    0.462340  0.524648   \n",
       "13      2              48              1     0.449603    0.455110  0.515309   \n",
       "3       5              12              1     0.508333    0.440960  0.339846   \n",
       "4       7              12              1     0.513889    0.441891  0.339319   \n",
       "7      26              12              1     0.497619    0.445130  0.339162   \n",
       "5      11              12              1     0.507937    0.442246  0.337862   \n",
       "2       3              12              1     0.495238    0.439807  0.336538   \n",
       "6      15              12              1     0.525000    0.442379  0.334833   \n",
       "1       2              12              1     0.518651    0.439274  0.333456   \n",
       "21      2              12              3     0.522222    0.418870  0.076158   \n",
       "22      4              12              3     0.505952    0.420200  0.075797   \n",
       "23     15              12              3     0.512698    0.420600  0.072960   \n",
       "25      7              24              3     0.529365    0.432931  0.028424   \n",
       "26     11              24              3     0.519444    0.433153  0.026214   \n",
       "27     12              24              3     0.537698    0.433419  0.025483   \n",
       "24      2              24              3     0.536905    0.432266  0.023946   \n",
       "35     10             122              3     0.535714    0.430935  0.000623   \n",
       "34      4             122              3     0.540873    0.430891  0.000623   \n",
       "33      2             122              3     0.533333    0.430846  0.000467   \n",
       "0       2               1              1     0.521032    0.430758  0.000000   \n",
       "\n",
       "    test_score_20  f1_score_20  time_taken  \n",
       "20       0.818397     0.900130    2.100314  \n",
       "11       0.653587     0.781614   14.581527  \n",
       "12       0.657300     0.784597   54.410407  \n",
       "10       0.661013     0.786682    8.533468  \n",
       "8        0.660675     0.786764    1.280938  \n",
       "9        0.648101     0.777553    3.710555  \n",
       "18       0.524051     0.654073    3.247700  \n",
       "19       0.509958     0.641654    4.821059  \n",
       "32       0.506329     0.620327   24.777366  \n",
       "17       0.520506     0.651753    1.648145  \n",
       "28       0.498228     0.613293    2.517389  \n",
       "31       0.498734     0.612221   19.813704  \n",
       "30       0.498228     0.612285    7.478467  \n",
       "29       0.502363     0.615806    4.963780  \n",
       "16       0.488354     0.610472   27.995775  \n",
       "15       0.485232     0.605178   11.977637  \n",
       "14       0.493249     0.614000    2.737370  \n",
       "13       0.486835     0.608006    1.441661  \n",
       "3        0.322954     0.361379    4.640715  \n",
       "4        0.322025     0.356765    7.055703  \n",
       "7        0.320928     0.361603   30.998132  \n",
       "5        0.326751     0.362474   11.762343  \n",
       "2        0.324557     0.360396    2.358098  \n",
       "6        0.331983     0.366720   16.290812  \n",
       "1        0.324895     0.363361    1.248816  \n",
       "21       0.206076     0.076379    2.193563  \n",
       "22       0.205823     0.078347    6.469548  \n",
       "23       0.205485     0.075601   29.935190  \n",
       "25       0.188608     0.021971   14.112990  \n",
       "26       0.188861     0.023766   23.373540  \n",
       "27       0.187426     0.021940   46.690186  \n",
       "24       0.187764     0.020556    2.533037  \n",
       "35       0.181772     0.000412   47.273161  \n",
       "34       0.181772     0.000412    7.601697  \n",
       "33       0.181688     0.000206    2.623904  \n",
       "0        0.181603     0.000000    1.133690  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:41.019665Z",
     "start_time": "2017-07-23T22:22:40.995923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.462302</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>2.100314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.508730</td>\n",
       "      <td>0.539656</td>\n",
       "      <td>0.540227</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.620327</td>\n",
       "      <td>24.777366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.498359</td>\n",
       "      <td>0.637265</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.784597</td>\n",
       "      <td>54.410407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.467796</td>\n",
       "      <td>0.529453</td>\n",
       "      <td>0.488354</td>\n",
       "      <td>0.610472</td>\n",
       "      <td>27.995775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.398413</td>\n",
       "      <td>0.457150</td>\n",
       "      <td>0.540443</td>\n",
       "      <td>0.509958</td>\n",
       "      <td>0.641654</td>\n",
       "      <td>4.821059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0.497619</td>\n",
       "      <td>0.445130</td>\n",
       "      <td>0.339162</td>\n",
       "      <td>0.320928</td>\n",
       "      <td>0.361603</td>\n",
       "      <td>30.998132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.537698</td>\n",
       "      <td>0.433419</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.187426</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>46.690186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.430935</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>47.273161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.133690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.512698</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.072960</td>\n",
       "      <td>0.205485</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>29.935190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "1              3                  2     0.462302    0.569242  0.725500   \n",
       "48             3                 11     0.508730    0.539656  0.540227   \n",
       "24             1                 48     0.403175    0.498359  0.637265   \n",
       "48             1                 14     0.451587    0.467796  0.529453   \n",
       "122            1                  4     0.398413    0.457150  0.540443   \n",
       "12             1                 26     0.497619    0.445130  0.339162   \n",
       "24             3                 12     0.537698    0.433419  0.025483   \n",
       "122            3                 10     0.535714    0.430935  0.000623   \n",
       "1              1                  2     0.521032    0.430758  0.000000   \n",
       "12             3                 15     0.512698    0.420600  0.072960   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "1              3                   0.818397     0.900130    2.100314  \n",
       "48             3                   0.506329     0.620327   24.777366  \n",
       "24             1                   0.657300     0.784597   54.410407  \n",
       "48             1                   0.488354     0.610472   27.995775  \n",
       "122            1                   0.509958     0.641654    4.821059  \n",
       "12             1                   0.320928     0.361603   30.998132  \n",
       "24             3                   0.187426     0.021940   46.690186  \n",
       "122            3                   0.181772     0.000412   47.273161  \n",
       "1              1                   0.181603     0.000000    1.133690  \n",
       "12             3                   0.205485     0.075601   29.935190  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:41.042377Z",
     "start_time": "2017-07-23T22:22:41.022225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.462302</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>2.100314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.519444</td>\n",
       "      <td>0.536861</td>\n",
       "      <td>0.535905</td>\n",
       "      <td>0.500776</td>\n",
       "      <td>0.614786</td>\n",
       "      <td>11.910141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.411270</td>\n",
       "      <td>0.496070</td>\n",
       "      <td>0.635702</td>\n",
       "      <td>0.656135</td>\n",
       "      <td>0.783442</td>\n",
       "      <td>16.503379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.448413</td>\n",
       "      <td>0.462484</td>\n",
       "      <td>0.523816</td>\n",
       "      <td>0.488418</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>11.038111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.393651</td>\n",
       "      <td>0.455613</td>\n",
       "      <td>0.539853</td>\n",
       "      <td>0.518172</td>\n",
       "      <td>0.649160</td>\n",
       "      <td>3.238968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>9.857143</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.441670</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>0.324870</td>\n",
       "      <td>0.361814</td>\n",
       "      <td>10.622089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.530853</td>\n",
       "      <td>0.432942</td>\n",
       "      <td>0.026016</td>\n",
       "      <td>0.188165</td>\n",
       "      <td>0.022058</td>\n",
       "      <td>21.677438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.536640</td>\n",
       "      <td>0.430891</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.181744</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>19.166254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.133690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.513624</td>\n",
       "      <td>0.419890</td>\n",
       "      <td>0.074972</td>\n",
       "      <td>0.205795</td>\n",
       "      <td>0.076776</td>\n",
       "      <td>12.866100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                 \n",
       "1              3               2.000000     0.462302    0.569242  0.725500   \n",
       "48             3               5.800000     0.519444    0.536861  0.535905   \n",
       "24             1              15.000000     0.411270    0.496070  0.635702   \n",
       "48             1               7.250000     0.448413    0.462484  0.523816   \n",
       "122            1               3.000000     0.393651    0.455613  0.539853   \n",
       "12             1               9.857143     0.509524    0.441670  0.337288   \n",
       "24             3               8.000000     0.530853    0.432942  0.026016   \n",
       "122            3               5.333333     0.536640    0.430891  0.000571   \n",
       "1              1               2.000000     0.521032    0.430758  0.000000   \n",
       "12             3               7.000000     0.513624    0.419890  0.074972   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "1              3                   0.818397     0.900130    2.100314  \n",
       "48             3                   0.500776     0.614786   11.910141  \n",
       "24             1                   0.656135     0.783442   16.503379  \n",
       "48             1                   0.488418     0.609414   11.038111  \n",
       "122            1                   0.518172     0.649160    3.238968  \n",
       "12             1                   0.324870     0.361814   10.622089  \n",
       "24             3                   0.188165     0.022058   21.677438  \n",
       "122            3                   0.181744     0.000344   19.166254  \n",
       "1              1                   0.181603     0.000000    1.133690  \n",
       "12             3                   0.205795     0.076776   12.866100  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:22:41.077659Z",
     "start_time": "2017-07-23T22:22:41.044848Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions-.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions-__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:28:12.888081Z",
     "start_time": "2017-07-23T22:28:12.875103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19593</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.030478</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "19593     1.0     0.008749     0.030478         1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoch_nof_hidden\n",
    "Train.predictions[\"2_1_3\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:15.350265Z",
     "start_time": "2017-07-23T22:32:15.334428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Attack_prob  Normal_prob  Prediction\n",
       "1621     1.0     0.000597     0.003845         1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"2_1_3\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:30.734791Z",
     "start_time": "2017-07-23T22:32:30.720154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions[\"2_1_3\"].dropna()\n",
    "df_ = Train.predictions_[\"2_1_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:32.460910Z",
     "start_time": "2017-07-23T22:32:32.449724Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:33.354721Z",
     "start_time": "2017-07-23T22:32:33.299098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.72550</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.818397</td>\n",
       "      <td>0.90013</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision  Recall      Scenario\n",
       "0  0.569242   0.72550   0.569242     1.0  Train+/Test+\n",
       "1  0.818397   0.90013   0.818397     1.0  Train+/Test-"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:34.690238Z",
     "start_time": "2017-07-23T22:32:34.681932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:36.970716Z",
     "start_time": "2017-07-23T22:32:36.601030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAGkCAYAAABdFwDgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naJQEtoiIaJT/A+ZydZ8V7PtXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe29a28fb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:37.798117Z",
     "start_time": "2017-07-23T22:32:37.788619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:38.978435Z",
     "start_time": "2017-07-23T22:32:38.632116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGkCAYAAABZ4tDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe29ee197b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T22:32:39.687179Z",
     "start_time": "2017-07-23T22:32:39.662903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                                            (nan, nan)\n",
       "                3                                            (nan, nan)\n",
       "12              1                      (0.332501487048, 0.342074653086)\n",
       "                3                    (0.0715380594206, 0.0784052738095)\n",
       "24              1                      (0.631624957331, 0.639778869737)\n",
       "                3                    (0.0223658966078, 0.0296668834749)\n",
       "48              1                        (0.51200267417, 0.53562894811)\n",
       "                3                      (0.530724761885, 0.541085791068)\n",
       "122             1                      (0.536237511327, 0.543468428541)\n",
       "                3                (0.000394975978807, 0.000747522364523)\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
