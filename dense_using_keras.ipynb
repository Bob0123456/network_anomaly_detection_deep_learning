{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",40)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_train_2labels_y = pd.read_pickle(\"dataset/kdd_train_2labels_y.pkl\")\n",
    "    \n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test_2labels_y = pd.read_pickle(\"dataset/kdd_test_2labels_y.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_2labels = LabelEncoder()\n",
    "dataset.y_train_2labels = le_2labels.fit_transform(dataset.kdd_train_2labels_y)\n",
    "dataset.y_test_2labels = le_2labels.transform(dataset.kdd_test_2labels_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class preprocessing:\n",
    "    x_train = dataset.kdd_train_2labels.iloc[:,:-2].values\n",
    "    y_train = np.array(dataset.y_train_2labels)\n",
    "\n",
    "    x_test, y_test = (dataset.kdd_test_2labels.iloc[:,:-2].values, \n",
    "                      np.array(dataset.y_test_2labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "class Train:\n",
    "    score = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "    #model_detail = namedtuple(\"model_detail\", ['epoch', 'no_of_features','hidden_layers', 'model'])\n",
    "    scores = []\n",
    "    predictions = pd.DataFrame()\n",
    "    #models = []\n",
    "    def execute(x_train, x_test, \n",
    "                y_train, y_test, \n",
    "                input_dim, no_of_features, hidden_layers,\n",
    "                epochs = 10, keep_prob = 0.4):\n",
    "        \n",
    "        print(\"Training for no_of_features: {}, hidden_layer: {}\".format(no_of_features, hidden_layers\n",
    "                                                                        ))\n",
    "        model = Sequential()\n",
    "        model.add(Dense(no_of_features, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dropout(keep_prob))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        for i in range(hidden_layers - 1):\n",
    "            model.add(Dense(no_of_features, activation='relu'))\n",
    "            model.add(Dropout(keep_prob))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        \n",
    "        model.add(Dense(1, activation=None))\n",
    "\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=\"Adam\",\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=.6)\n",
    "        \n",
    "        model.fit(x_train, y_train,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  epochs=epochs,\n",
    "                  batch_size=128,\n",
    "                  verbose = 0)\n",
    "        \n",
    "        curr_score_valid = model.evaluate(x_valid, y_valid) #, batch_size=128)\n",
    "        curr_score_test = model.evaluate(x_test, y_test) #, batch_size=128)\n",
    "        pred_value = model.predict(x_test)\n",
    "        \n",
    "        print(\"\\n Train Accuracy: {}, Test Accuracy: {}\".format(curr_score_valid[1], curr_score_test[1])  )\n",
    "        Train.scores.append(Train.score(epochs,no_of_features,hidden_layers,curr_score_valid[1], curr_score_test[1]))\n",
    "        #Train.models.append(Train.model_detail(epochs,no_of_features,hidden_layers,model))\n",
    "        y_pred = pred_value[:,-1]\n",
    "        y_pred[y_pred >= pred_value[:,-1].mean()] = 1\n",
    "        y_pred[y_pred < pred_value[:,-1].mean()] = 0\n",
    "        curr_pred = pd.DataFrame({\"{}_{}_{}\".format(epochs,f,h):y_pred},)\n",
    "        Train.predictions = pd.concat([Train.predictions, curr_pred], axis = 1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for no_of_features: 4, hidden_layer: 2\n",
      "21472/22544 [===========================>..] - ETA: 0s\n",
      " Train Accuracy: 0.8895665749364945, Test Accuracy: 0.7222764371894961\n",
      "Training for no_of_features: 4, hidden_layer: 4\n",
      "22272/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.8694035774767146, Test Accuracy: 0.6386178140525195\n",
      "Training for no_of_features: 4, hidden_layer: 6\n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5343855842506351, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 4, hidden_layer: 50\n",
      "75584/75584 [==============================] - 18s    \n",
      "22368/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5331816257408976, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 4, hidden_layer: 100\n",
      "75584/75584 [==============================] - 34s    \n",
      "22544/22544 [==============================] - 9s     \n",
      "\n",
      " Train Accuracy: 0.5348618755292125, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 8, hidden_layer: 2\n",
      "22240/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9181043607112617, Test Accuracy: 0.738555713271824\n",
      "Training for no_of_features: 8, hidden_layer: 4\n",
      "21920/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9460335520745131, Test Accuracy: 0.7541696238466998\n",
      "Training for no_of_features: 8, hidden_layer: 6\n",
      "22272/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9169268628281118, Test Accuracy: 0.6952625975869411\n",
      "Training for no_of_features: 8, hidden_layer: 50\n",
      "22400/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5355498518204911, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 8, hidden_layer: 100\n",
      "22544/22544 [==============================] - 11s    \n",
      "\n",
      " Train Accuracy: 0.5348221845893311, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 16, hidden_layer: 2\n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5367008890770534, Test Accuracy: 0.43142299503193754\n",
      "Training for no_of_features: 16, hidden_layer: 4\n",
      "22368/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9665008467400508, Test Accuracy: 0.7479151880766501\n",
      "Training for no_of_features: 16, hidden_layer: 6\n",
      "22048/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.94304350127011, Test Accuracy: 0.6937100780695529\n",
      "Training for no_of_features: 16, hidden_layer: 50\n",
      "22400/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5347295723962744, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 16, hidden_layer: 100\n",
      "22432/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.534901566469094, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 32, hidden_layer: 2\n",
      "22400/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9590521803556308, Test Accuracy: 0.7312810503903477\n",
      "Training for no_of_features: 32, hidden_layer: 4\n",
      "22368/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9594623200677392, Test Accuracy: 0.7116749467707594\n",
      "Training for no_of_features: 32, hidden_layer: 6\n",
      "22336/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9629418924640135, Test Accuracy: 0.7279098651525905\n",
      "Training for no_of_features: 32, hidden_layer: 50\n",
      "22432/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5365553556308214, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 32, hidden_layer: 100\n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5329037891617273, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 64, hidden_layer: 2\n",
      "22544/22544 [==============================] - 3s     \n",
      "\n",
      " Train Accuracy: 0.9636298687552921, Test Accuracy: 0.7275106458481192\n",
      "Training for no_of_features: 64, hidden_layer: 4\n",
      "22368/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9663156223539373, Test Accuracy: 0.7276437189496097\n",
      "Training for no_of_features: 64, hidden_layer: 6\n",
      "22464/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9457292548687553, Test Accuracy: 0.6949077359829666\n",
      "Training for no_of_features: 64, hidden_layer: 50\n",
      "22464/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5337769898391194, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 64, hidden_layer: 100\n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5370184165961049, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 128, hidden_layer: 2\n",
      "22400/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9552550804403048, Test Accuracy: 0.7519073811213627\n",
      "Training for no_of_features: 128, hidden_layer: 4\n",
      "22528/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.953058848433531, Test Accuracy: 0.700762952448545\n",
      "Training for no_of_features: 128, hidden_layer: 6\n",
      "22544/22544 [==============================] - 6s     \n",
      "\n",
      " Train Accuracy: 0.9657334885690093, Test Accuracy: 0.7119410929737402\n",
      "Training for no_of_features: 128, hidden_layer: 50\n",
      "22528/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.5347692633361558, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 128, hidden_layer: 100\n",
      "22544/22544 [==============================] - 22s    \n",
      "\n",
      " Train Accuracy: 0.5367008890770534, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 256, hidden_layer: 2\n",
      "22464/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.969332133784928, Test Accuracy: 0.7243168914123492\n",
      "Training for no_of_features: 256, hidden_layer: 4\n",
      "22528/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9580334462320068, Test Accuracy: 0.707416607523066\n",
      "Training for no_of_features: 256, hidden_layer: 6\n",
      "75584/75584 [==============================] - 24s    \n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9601370660457239, Test Accuracy: 0.7075053229240597\n",
      "Training for no_of_features: 256, hidden_layer: 50\n",
      "22544/22544 [==============================] - 21s    \n",
      "\n",
      " Train Accuracy: 0.5340415961049958, Test Accuracy: 0.43075762952448543\n",
      "Training for no_of_features: 256, hidden_layer: 100\n",
      "75584/75584 [==============================] - 117s   \n",
      "22544/22544 [==============================] - 34s    \n",
      "\n",
      " Train Accuracy: 0.46491320914479256, Test Accuracy: 0.5692423704755145\n",
      "Training for no_of_features: 1024, hidden_layer: 2\n",
      "22432/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.7038394369178662, Test Accuracy: 0.5009315117104329\n",
      "Training for no_of_features: 1024, hidden_layer: 4\n",
      "22528/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9127593141405589, Test Accuracy: 0.6736603974449965\n",
      "Training for no_of_features: 1024, hidden_layer: 6\n",
      "75584/75584 [==============================] - 45s    \n",
      "22496/22544 [============================>.] - ETA: 0s\n",
      " Train Accuracy: 0.9580731371718882, Test Accuracy: 0.6955287437899219\n",
      "Training for no_of_features: 1024, hidden_layer: 50\n",
      "75584/75584 [==============================] - 152s   \n",
      "22544/22544 [==============================] - 45s    \n",
      "\n",
      " Train Accuracy: 0.5334065410668924, Test Accuracy: 0.4304471256210078\n",
      "Training for no_of_features: 1024, hidden_layer: 100\n",
      "75584/75584 [==============================] - 279s   \n",
      "22544/22544 [==============================] - 84s    \n",
      "\n",
      " Train Accuracy: 0.46611716765453004, Test Accuracy: 0.569153655074521\n"
     ]
    }
   ],
   "source": [
    "features_arr = [4, 8, 16, 32, 64, 128, 256, 1024]\n",
    "hidden_layers_arr = [2, 4, 6, 50, 100]\n",
    "\n",
    "\n",
    "for f, h in product(features_arr, hidden_layers_arr):\n",
    "    Train.execute(preprocessing.x_train, preprocessing.x_test, \n",
    "                  preprocessing.y_train, preprocessing.y_test, \n",
    "                 122, f, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889567</td>\n",
       "      <td>0.722276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.869404</td>\n",
       "      <td>0.638618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534386</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.533182</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.534862</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.918104</td>\n",
       "      <td>0.738556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.754170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916927</td>\n",
       "      <td>0.695263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.535550</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.534822</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.536701</td>\n",
       "      <td>0.431423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966501</td>\n",
       "      <td>0.747915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.943044</td>\n",
       "      <td>0.693710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>0.534730</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.534902</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.959052</td>\n",
       "      <td>0.731281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.959462</td>\n",
       "      <td>0.711675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>0.727910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>0.536555</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.532904</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.963630</td>\n",
       "      <td>0.727511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966316</td>\n",
       "      <td>0.727644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.694908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.533777</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>0.537018</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955255</td>\n",
       "      <td>0.751907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.953059</td>\n",
       "      <td>0.700763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.965733</td>\n",
       "      <td>0.711941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>0.534769</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.536701</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.969332</td>\n",
       "      <td>0.724317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.958033</td>\n",
       "      <td>0.707417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>0.960137</td>\n",
       "      <td>0.707505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.534042</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.464913</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>0.703839</td>\n",
       "      <td>0.500932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.912759</td>\n",
       "      <td>0.673660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>6</td>\n",
       "      <td>0.958073</td>\n",
       "      <td>0.695529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>50</td>\n",
       "      <td>0.533407</td>\n",
       "      <td>0.430447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>100</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.569154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0      10               4              2     0.889567    0.722276\n",
       "1      10               4              4     0.869404    0.638618\n",
       "2      10               4              6     0.534386    0.430758\n",
       "3      10               4             50     0.533182    0.430758\n",
       "4      10               4            100     0.534862    0.430758\n",
       "5      10               8              2     0.918104    0.738556\n",
       "6      10               8              4     0.946034    0.754170\n",
       "7      10               8              6     0.916927    0.695263\n",
       "8      10               8             50     0.535550    0.430758\n",
       "9      10               8            100     0.534822    0.430758\n",
       "10     10              16              2     0.536701    0.431423\n",
       "11     10              16              4     0.966501    0.747915\n",
       "12     10              16              6     0.943044    0.693710\n",
       "13     10              16             50     0.534730    0.430758\n",
       "14     10              16            100     0.534902    0.430758\n",
       "15     10              32              2     0.959052    0.731281\n",
       "16     10              32              4     0.959462    0.711675\n",
       "17     10              32              6     0.962942    0.727910\n",
       "18     10              32             50     0.536555    0.430758\n",
       "19     10              32            100     0.532904    0.430758\n",
       "20     10              64              2     0.963630    0.727511\n",
       "21     10              64              4     0.966316    0.727644\n",
       "22     10              64              6     0.945729    0.694908\n",
       "23     10              64             50     0.533777    0.430758\n",
       "24     10              64            100     0.537018    0.430758\n",
       "25     10             128              2     0.955255    0.751907\n",
       "26     10             128              4     0.953059    0.700763\n",
       "27     10             128              6     0.965733    0.711941\n",
       "28     10             128             50     0.534769    0.430758\n",
       "29     10             128            100     0.536701    0.430758\n",
       "30     10             256              2     0.969332    0.724317\n",
       "31     10             256              4     0.958033    0.707417\n",
       "32     10             256              6     0.960137    0.707505\n",
       "33     10             256             50     0.534042    0.430758\n",
       "34     10             256            100     0.464913    0.569242\n",
       "35     10            1024              2     0.703839    0.500932\n",
       "36     10            1024              4     0.912759    0.673660\n",
       "37     10            1024              6     0.958073    0.695529\n",
       "38     10            1024             50     0.533407    0.430447\n",
       "39     10            1024            100     0.466117    0.569154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Train.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for m in Train.models:\n",
    "#    m.model.save(\"dataset/keras_model_epoch_{}_no_of_features_{}_hidden_layers_{}\".format(m.epoch,\n",
    "#                                                                                         m.no_of_features,\n",
    "#                                                                                         m.hidden_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions.to_pickle(\"dataset/dense_only_predictions.pkl\")\n",
    "pd.DataFrame(Train.scores).to_pickle(\"dataset/dense_only_scores.pkl\")"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0f6c9677d5f316c57d9d8bd6e0fe8850"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Final Hyper parameter tuning",
    "public": false
   },
   "id": "0f6c9677d5f316c57d9d8bd6e0fe8850"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
