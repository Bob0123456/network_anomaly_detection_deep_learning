{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:30.167158Z",
     "start_time": "2017-06-19T14:38:29.591286Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:30.701854Z",
     "start_time": "2017-06-19T14:38:30.168993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train__2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:30.710824Z",
     "start_time": "2017-06-19T14:38:30.703866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:30.717982Z",
     "start_time": "2017-06-19T14:38:30.712936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:31.271485Z",
     "start_time": "2017-06-19T14:38:30.720462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:40.368568Z",
     "start_time": "2017-06-19T14:38:31.273494Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:40.695751Z",
     "start_time": "2017-06-19T14:38:40.370708Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-19T14:38:41.144741Z",
     "start_time": "2017-06-19T14:38:40.699840Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd-/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.770Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        features_arr = [1, 8, 32, 122]\n",
    "        hidden_layers_arr = [1, 3, 5]\n",
    "\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-5, 1e-5, 1e-6]\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/tf_dense_only_nsl_kdd_scores_all-.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_scores_all-.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores_all-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min 46s ± 30.4 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10\n",
    "%%capture\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-17T20:17:51.453918Z",
     "start_time": "2017-06-17T20:17:51.447676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-17T20:17:51.460413Z",
     "start_time": "2017-06-17T20:17:51.455436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.893630</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>7.219536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.780084</td>\n",
       "      <td>33.232840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.677890</td>\n",
       "      <td>27.045139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.782736</td>\n",
       "      <td>0.591983</td>\n",
       "      <td>24.386522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "100      6             122              3     0.910714    0.893630   \n",
       "151     15              32              5     0.903571    0.878460   \n",
       "83      33               8              3     0.918651    0.827404   \n",
       "67      27               1              3     0.916270    0.782736   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "100       0.808945    7.219536  \n",
       "151       0.780084   33.232840  \n",
       "83        0.677890   27.045139  \n",
       "67        0.591983   24.386522  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.893630</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>7.219536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.780084</td>\n",
       "      <td>33.232840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.625621</td>\n",
       "      <td>0.693671</td>\n",
       "      <td>1.187153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.782736</td>\n",
       "      <td>0.591983</td>\n",
       "      <td>24.386522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "100      6             122              3     0.910714    0.893630   \n",
       "151     15              32              5     0.903571    0.878460   \n",
       "68       2               8              3     0.563889    0.625621   \n",
       "67      27               1              3     0.916270    0.782736   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "100       0.808945    7.219536  \n",
       "151       0.780084   33.232840  \n",
       "68        0.693671    1.187153  \n",
       "67        0.591983   24.386522  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.893630</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>7.219536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.890969</td>\n",
       "      <td>0.804641</td>\n",
       "      <td>5.754416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.878460</td>\n",
       "      <td>0.780084</td>\n",
       "      <td>33.232840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.779831</td>\n",
       "      <td>30.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.904365</td>\n",
       "      <td>0.878238</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>28.607611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.878194</td>\n",
       "      <td>0.779831</td>\n",
       "      <td>26.295696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.877129</td>\n",
       "      <td>0.777975</td>\n",
       "      <td>23.123441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>0.877040</td>\n",
       "      <td>0.777806</td>\n",
       "      <td>21.599068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.876198</td>\n",
       "      <td>0.776878</td>\n",
       "      <td>20.063834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.875133</td>\n",
       "      <td>0.792827</td>\n",
       "      <td>4.247366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.881349</td>\n",
       "      <td>0.873891</td>\n",
       "      <td>0.775021</td>\n",
       "      <td>16.964964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.865873</td>\n",
       "      <td>0.873270</td>\n",
       "      <td>0.775527</td>\n",
       "      <td>15.472204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.851190</td>\n",
       "      <td>0.866528</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>12.327079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.871825</td>\n",
       "      <td>0.837074</td>\n",
       "      <td>0.704979</td>\n",
       "      <td>16.240678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.833481</td>\n",
       "      <td>0.715359</td>\n",
       "      <td>11.024981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800397</td>\n",
       "      <td>0.830642</td>\n",
       "      <td>0.729789</td>\n",
       "      <td>8.857712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.677890</td>\n",
       "      <td>27.045139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.827315</td>\n",
       "      <td>0.677806</td>\n",
       "      <td>25.972584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.827271</td>\n",
       "      <td>0.677806</td>\n",
       "      <td>24.279881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.921032</td>\n",
       "      <td>0.827182</td>\n",
       "      <td>0.677722</td>\n",
       "      <td>21.320162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.827005</td>\n",
       "      <td>0.677384</td>\n",
       "      <td>20.235193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909127</td>\n",
       "      <td>0.826783</td>\n",
       "      <td>0.676962</td>\n",
       "      <td>19.093679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.826694</td>\n",
       "      <td>0.676878</td>\n",
       "      <td>17.945738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.825763</td>\n",
       "      <td>0.675190</td>\n",
       "      <td>16.846556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.824521</td>\n",
       "      <td>0.672911</td>\n",
       "      <td>15.710437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.823501</td>\n",
       "      <td>0.671224</td>\n",
       "      <td>14.632145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.821150</td>\n",
       "      <td>0.672827</td>\n",
       "      <td>10.685101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.778968</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>6.730656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.820263</td>\n",
       "      <td>0.675696</td>\n",
       "      <td>8.968101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.938492</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.659072</td>\n",
       "      <td>19.794503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.651392</td>\n",
       "      <td>18.608358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.813964</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>6.777124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937302</td>\n",
       "      <td>0.812677</td>\n",
       "      <td>0.650464</td>\n",
       "      <td>9.596403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>8.782956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.812145</td>\n",
       "      <td>0.684557</td>\n",
       "      <td>4.496789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917063</td>\n",
       "      <td>0.811790</td>\n",
       "      <td>0.646920</td>\n",
       "      <td>7.331714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.810814</td>\n",
       "      <td>0.645907</td>\n",
       "      <td>6.522778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.644641</td>\n",
       "      <td>17.451793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.645992</td>\n",
       "      <td>8.012530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.809351</td>\n",
       "      <td>0.644557</td>\n",
       "      <td>5.625120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937698</td>\n",
       "      <td>0.807754</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>16.278363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812698</td>\n",
       "      <td>0.807488</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>9.253996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.806334</td>\n",
       "      <td>0.638481</td>\n",
       "      <td>15.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.805846</td>\n",
       "      <td>0.640506</td>\n",
       "      <td>4.819912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>0.805358</td>\n",
       "      <td>0.638228</td>\n",
       "      <td>7.183477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.802520</td>\n",
       "      <td>0.632574</td>\n",
       "      <td>13.861901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926587</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.629367</td>\n",
       "      <td>6.412395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.798039</td>\n",
       "      <td>0.624388</td>\n",
       "      <td>12.652347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.797995</td>\n",
       "      <td>0.731308</td>\n",
       "      <td>4.541728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.803968</td>\n",
       "      <td>0.797463</td>\n",
       "      <td>0.659578</td>\n",
       "      <td>2.612590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816270</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>0.525823</td>\n",
       "      <td>1.625310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.739044</td>\n",
       "      <td>0.512574</td>\n",
       "      <td>15.625380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.737269</td>\n",
       "      <td>0.672911</td>\n",
       "      <td>2.314086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.728442</td>\n",
       "      <td>0.620759</td>\n",
       "      <td>4.227787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.728176</td>\n",
       "      <td>0.494093</td>\n",
       "      <td>6.365857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.721744</td>\n",
       "      <td>0.483291</td>\n",
       "      <td>8.853109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.713405</td>\n",
       "      <td>0.473840</td>\n",
       "      <td>0.840305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.707505</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>4.797040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854365</td>\n",
       "      <td>0.706707</td>\n",
       "      <td>0.456118</td>\n",
       "      <td>8.028947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.728175</td>\n",
       "      <td>0.699787</td>\n",
       "      <td>0.486835</td>\n",
       "      <td>1.123082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666270</td>\n",
       "      <td>0.696238</td>\n",
       "      <td>0.557975</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867857</td>\n",
       "      <td>0.691137</td>\n",
       "      <td>0.428608</td>\n",
       "      <td>3.253160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.668254</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.762110</td>\n",
       "      <td>6.242048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.692460</td>\n",
       "      <td>0.681201</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>2.879770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.737302</td>\n",
       "      <td>0.680669</td>\n",
       "      <td>0.610211</td>\n",
       "      <td>1.608776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.678007</td>\n",
       "      <td>0.731392</td>\n",
       "      <td>2.281325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.811508</td>\n",
       "      <td>0.673705</td>\n",
       "      <td>0.404557</td>\n",
       "      <td>1.695190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.824206</td>\n",
       "      <td>0.658534</td>\n",
       "      <td>0.432658</td>\n",
       "      <td>40.437156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.830952</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.431983</td>\n",
       "      <td>38.982371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.654276</td>\n",
       "      <td>0.431308</td>\n",
       "      <td>37.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.828175</td>\n",
       "      <td>0.650018</td>\n",
       "      <td>0.430464</td>\n",
       "      <td>36.292585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.818254</td>\n",
       "      <td>0.646159</td>\n",
       "      <td>0.429536</td>\n",
       "      <td>34.924899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.642699</td>\n",
       "      <td>0.429114</td>\n",
       "      <td>33.540665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.640836</td>\n",
       "      <td>0.428776</td>\n",
       "      <td>32.154849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.634626</td>\n",
       "      <td>0.428101</td>\n",
       "      <td>30.803165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.426076</td>\n",
       "      <td>29.328593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.625621</td>\n",
       "      <td>0.693671</td>\n",
       "      <td>1.187153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820635</td>\n",
       "      <td>0.624956</td>\n",
       "      <td>0.425401</td>\n",
       "      <td>28.022108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.620697</td>\n",
       "      <td>0.424557</td>\n",
       "      <td>26.673722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729365</td>\n",
       "      <td>0.610406</td>\n",
       "      <td>0.304726</td>\n",
       "      <td>0.841745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.602067</td>\n",
       "      <td>0.419494</td>\n",
       "      <td>25.264237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.598563</td>\n",
       "      <td>0.414852</td>\n",
       "      <td>22.553295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.805159</td>\n",
       "      <td>0.594837</td>\n",
       "      <td>0.408776</td>\n",
       "      <td>21.142184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.586409</td>\n",
       "      <td>0.394093</td>\n",
       "      <td>18.364748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.572525</td>\n",
       "      <td>0.368523</td>\n",
       "      <td>16.913385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.563742</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>4.781629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750794</td>\n",
       "      <td>0.556112</td>\n",
       "      <td>0.338481</td>\n",
       "      <td>14.227843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.496825</td>\n",
       "      <td>0.551366</td>\n",
       "      <td>0.767764</td>\n",
       "      <td>3.185793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.550612</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>12.886392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.708730</td>\n",
       "      <td>0.533357</td>\n",
       "      <td>0.301013</td>\n",
       "      <td>8.269593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.532958</td>\n",
       "      <td>0.749030</td>\n",
       "      <td>1.629867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.631349</td>\n",
       "      <td>0.513751</td>\n",
       "      <td>0.276034</td>\n",
       "      <td>5.504664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.468651</td>\n",
       "      <td>0.505101</td>\n",
       "      <td>0.616878</td>\n",
       "      <td>1.471498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582937</td>\n",
       "      <td>0.484164</td>\n",
       "      <td>0.276962</td>\n",
       "      <td>3.191427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.473075</td>\n",
       "      <td>0.269789</td>\n",
       "      <td>2.793978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500397</td>\n",
       "      <td>0.471123</td>\n",
       "      <td>0.271392</td>\n",
       "      <td>1.376509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522619</td>\n",
       "      <td>0.430979</td>\n",
       "      <td>0.192827</td>\n",
       "      <td>1.596510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.519444</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.093403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.429826</td>\n",
       "      <td>0.610886</td>\n",
       "      <td>0.769770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493651</td>\n",
       "      <td>0.423838</td>\n",
       "      <td>0.195696</td>\n",
       "      <td>0.807331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "100      6             122              3     0.910714    0.893630   \n",
       "99       5             122              3     0.908730    0.890969   \n",
       "151     15              32              5     0.903571    0.878460   \n",
       "142      6              32              5     0.892460    0.878282   \n",
       "150     22              32              5     0.904365    0.878238   \n",
       "149     18              32              5     0.894444    0.878194   \n",
       "148     12              32              5     0.887698    0.877129   \n",
       "146     10              32              5     0.886508    0.877040   \n",
       "144      8              32              5     0.879365    0.876198   \n",
       "98       4             122              3     0.884524    0.875133   \n",
       "140      4              32              5     0.881349    0.873891   \n",
       "147     11              32              5     0.865873    0.873270   \n",
       "145      9              32              5     0.851190    0.866528   \n",
       "157      9             122              5     0.871825    0.837074   \n",
       "156      6             122              5     0.827381    0.833481   \n",
       "155      5             122              5     0.800397    0.830642   \n",
       "83      33               8              3     0.918651    0.827404   \n",
       "82      30               8              3     0.921429    0.827315   \n",
       "81      24               8              3     0.922619    0.827271   \n",
       "80      12               8              3     0.921032    0.827182   \n",
       "73       9               8              3     0.923016    0.827005   \n",
       "79       6               8              3     0.909127    0.826783   \n",
       "78      22               8              3     0.923016    0.826694   \n",
       "77      20               8              3     0.907540    0.825763   \n",
       "76      18               8              3     0.912302    0.824521   \n",
       "75      16               8              3     0.921429    0.823501   \n",
       "70       4               8              3     0.880159    0.821150   \n",
       "154      4             122              5     0.778968    0.820617   \n",
       "74      10               8              3     0.859127    0.820263   \n",
       "95      16              32              3     0.938492    0.818400   \n",
       "94      14              32              3     0.937698    0.814319   \n",
       "72       8               8              3     0.849206    0.813964   \n",
       "35       6              32              1     0.937302    0.812677   \n",
       "33       4              32              1     0.941667    0.812544   \n",
       "71       5               8              3     0.823413    0.812145   \n",
       "30      10               8              1     0.917063    0.811790   \n",
       "29       9               8              1     0.909921    0.810814   \n",
       "93      12              32              3     0.936111    0.809661   \n",
       "40      11              32              1     0.933730    0.809661   \n",
       "28       8               8              1     0.895238    0.809351   \n",
       "91      10              32              3     0.937698    0.807754   \n",
       "143      7              32              5     0.812698    0.807488   \n",
       "90       8              32              3     0.923016    0.806334   \n",
       "27       7               8              1     0.892460    0.805846   \n",
       "39      10              32              1     0.941270    0.805358   \n",
       "88       6              32              3     0.920635    0.802520   \n",
       "38       9              32              1     0.926587    0.800435   \n",
       "86       4              32              3     0.908730    0.798039   \n",
       "153      3             122              5     0.730159    0.797995   \n",
       "97       3             122              3     0.803968    0.797463   \n",
       "..     ...             ...            ...          ...         ...   \n",
       "32       3              32              1     0.816270    0.742947   \n",
       "127     20               8              5     0.868651    0.739044   \n",
       "69       3               8              3     0.678571    0.737269   \n",
       "125      4               8              5     0.783333    0.728442   \n",
       "48       9             122              1     0.900000    0.728176   \n",
       "2        4               1              1     0.881746    0.721744   \n",
       "31       2              32              1     0.753571    0.713405   \n",
       "46       7             122              1     0.882143    0.707505   \n",
       "9       11               1              1     0.854365    0.706707   \n",
       "96       2             122              3     0.728175    0.699787   \n",
       "84       2              32              3     0.666270    0.696238   \n",
       "44       5             122              1     0.867857    0.691137   \n",
       "141      5              32              5     0.668254    0.685016   \n",
       "124      3               8              5     0.692460    0.681201   \n",
       "23       3               8              1     0.737302    0.680669   \n",
       "152      2             122              5     0.597222    0.678007   \n",
       "42       3             122              1     0.811508    0.673705   \n",
       "122     33               1              5     0.824206    0.658534   \n",
       "121     30               1              5     0.830952    0.656893   \n",
       "120     27               1              5     0.814286    0.654276   \n",
       "119     24               1              5     0.828175    0.650018   \n",
       "118     21               1              5     0.818254    0.646159   \n",
       "113     18               1              5     0.829365    0.642699   \n",
       "117     15               1              5     0.831349    0.640836   \n",
       "110     12               1              5     0.823810    0.634626   \n",
       "116      9               1              5     0.823413    0.629524   \n",
       "68       2               8              3     0.563889    0.625621   \n",
       "105      6               1              5     0.820635    0.624956   \n",
       "115     22               1              5     0.817857    0.620697   \n",
       "41       2             122              1     0.729365    0.610406   \n",
       "114     20               1              5     0.821032    0.602067   \n",
       "112     16               1              5     0.821032    0.598563   \n",
       "111     14               1              5     0.805159    0.594837   \n",
       "108     10               1              5     0.790476    0.586409   \n",
       "107      8               1              5     0.783333    0.572525   \n",
       "5        7               1              1     0.744048    0.563742   \n",
       "103      4               1              5     0.750794    0.556112   \n",
       "139      3              32              5     0.496825    0.551366   \n",
       "109     11               1              5     0.760714    0.550612   \n",
       "106      7               1              5     0.708730    0.533357   \n",
       "138      2              32              5     0.477778    0.532958   \n",
       "104      5               1              5     0.631349    0.513751   \n",
       "123      2               8              5     0.468651    0.505101   \n",
       "3        5               1              1     0.582937    0.484164   \n",
       "102      3               1              5     0.507937    0.473075   \n",
       "101      2               1              5     0.500397    0.471123   \n",
       "1        3               1              1     0.522619    0.430979   \n",
       "53       2               1              3     0.519444    0.430758   \n",
       "22       2               8              1     0.420635    0.429826   \n",
       "0        2               1              1     0.493651    0.423838   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "100       0.808945    7.219536  \n",
       "99        0.804641    5.754416  \n",
       "151       0.780084   33.232840  \n",
       "142       0.779831   30.114800  \n",
       "150       0.779747   28.607611  \n",
       "149       0.779831   26.295696  \n",
       "148       0.777975   23.123441  \n",
       "146       0.777806   21.599068  \n",
       "144       0.776878   20.063834  \n",
       "98        0.792827    4.247366  \n",
       "140       0.775021   16.964964  \n",
       "147       0.775527   15.472204  \n",
       "145       0.779747   12.327079  \n",
       "157       0.704979   16.240678  \n",
       "156       0.715359   11.024981  \n",
       "155       0.729789    8.857712  \n",
       "83        0.677890   27.045139  \n",
       "82        0.677806   25.972584  \n",
       "81        0.677806   24.279881  \n",
       "80        0.677722   21.320162  \n",
       "73        0.677384   20.235193  \n",
       "79        0.676962   19.093679  \n",
       "78        0.676878   17.945738  \n",
       "77        0.675190   16.846556  \n",
       "76        0.672911   15.710437  \n",
       "75        0.671224   14.632145  \n",
       "70        0.672827   10.685101  \n",
       "154       0.727848    6.730656  \n",
       "74        0.675696    8.968101  \n",
       "95        0.659072   19.794503  \n",
       "94        0.651392   18.608358  \n",
       "72        0.668692    6.777124  \n",
       "35        0.650464    9.596403  \n",
       "33        0.650549    8.782956  \n",
       "71        0.684557    4.496789  \n",
       "30        0.646920    7.331714  \n",
       "29        0.645907    6.522778  \n",
       "93        0.644641   17.451793  \n",
       "40        0.645992    8.012530  \n",
       "28        0.644557    5.625120  \n",
       "91        0.641013   16.278363  \n",
       "143       0.746835    9.253996  \n",
       "90        0.638481   15.031495  \n",
       "27        0.640506    4.819912  \n",
       "39        0.638228    7.183477  \n",
       "88        0.632574   13.861901  \n",
       "38        0.629367    6.412395  \n",
       "86        0.624388   12.652347  \n",
       "153       0.731308    4.541728  \n",
       "97        0.659578    2.612590  \n",
       "..             ...         ...  \n",
       "32        0.525823    1.625310  \n",
       "127       0.512574   15.625380  \n",
       "69        0.672911    2.314086  \n",
       "125       0.620759    4.227787  \n",
       "48        0.494093    6.365857  \n",
       "2         0.483291    8.853109  \n",
       "31        0.473840    0.840305  \n",
       "46        0.458143    4.797040  \n",
       "9         0.456118    8.028947  \n",
       "96        0.486835    1.123082  \n",
       "84        0.557975    1.247300  \n",
       "44        0.428608    3.253160  \n",
       "141       0.762110    6.242048  \n",
       "124       0.622700    2.879770  \n",
       "23        0.610211    1.608776  \n",
       "152       0.731392    2.281325  \n",
       "42        0.404557    1.695190  \n",
       "122       0.432658   40.437156  \n",
       "121       0.431983   38.982371  \n",
       "120       0.431308   37.616300  \n",
       "119       0.430464   36.292585  \n",
       "118       0.429536   34.924899  \n",
       "113       0.429114   33.540665  \n",
       "117       0.428776   32.154849  \n",
       "110       0.428101   30.803165  \n",
       "116       0.426076   29.328593  \n",
       "68        0.693671    1.187153  \n",
       "105       0.425401   28.022108  \n",
       "115       0.424557   26.673722  \n",
       "41        0.304726    0.841745  \n",
       "114       0.419494   25.264237  \n",
       "112       0.414852   22.553295  \n",
       "111       0.408776   21.142184  \n",
       "108       0.394093   18.364748  \n",
       "107       0.368523   16.913385  \n",
       "5         0.334177    4.781629  \n",
       "103       0.338481   14.227843  \n",
       "139       0.767764    3.185793  \n",
       "109       0.329114   12.886392  \n",
       "106       0.301013    8.269593  \n",
       "138       0.749030    1.629867  \n",
       "104       0.276034    5.504664  \n",
       "123       0.616878    1.471498  \n",
       "3         0.276962    3.191427  \n",
       "102       0.269789    2.793978  \n",
       "101       0.271392    1.376509  \n",
       "1         0.192827    1.596510  \n",
       "53        0.181603    1.093403  \n",
       "22        0.610886    0.769770  \n",
       "0         0.195696    0.807331  \n",
       "\n",
       "[158 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.852Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.855Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8908  0.1092]\n",
      " [ 0.0802  0.9198]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAfu0wxyznnIBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5c8ff7f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.6817  0.3183]\n",
      " [ 0.1061  0.8939]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5e2f59eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583259</td>\n",
       "      <td>0.823207</td>\n",
       "      <td>1.737384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.853132</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>2.746407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.873181</td>\n",
       "      <td>0.817215</td>\n",
       "      <td>3.780096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885247</td>\n",
       "      <td>0.815274</td>\n",
       "      <td>4.823596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.886533</td>\n",
       "      <td>0.812489</td>\n",
       "      <td>5.901322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.552778</td>\n",
       "      <td>0.696460</td>\n",
       "      <td>0.657553</td>\n",
       "      <td>1.041274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.776393</td>\n",
       "      <td>0.669705</td>\n",
       "      <td>2.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.681746</td>\n",
       "      <td>0.800435</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>3.030958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.834634</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>4.018355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.738228</td>\n",
       "      <td>4.996470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.855305</td>\n",
       "      <td>0.746414</td>\n",
       "      <td>5.974369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.861693</td>\n",
       "      <td>0.757637</td>\n",
       "      <td>7.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.759409</td>\n",
       "      <td>8.021910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">32</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.724603</td>\n",
       "      <td>0.734120</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>1.510834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.771825</td>\n",
       "      <td>0.750665</td>\n",
       "      <td>0.542785</td>\n",
       "      <td>2.517899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.911905</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.651308</td>\n",
       "      <td>11.737579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.830952</td>\n",
       "      <td>0.786152</td>\n",
       "      <td>0.606245</td>\n",
       "      <td>4.757215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.814807</td>\n",
       "      <td>0.652827</td>\n",
       "      <td>12.685921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.623460</td>\n",
       "      <td>6.851718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.931349</td>\n",
       "      <td>0.815871</td>\n",
       "      <td>0.654430</td>\n",
       "      <td>13.714334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.806379</td>\n",
       "      <td>0.640928</td>\n",
       "      <td>8.793038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.905952</td>\n",
       "      <td>0.809173</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>9.762101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.648692</td>\n",
       "      <td>10.796256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">122</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>0.778819</td>\n",
       "      <td>0.966112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.798810</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>0.768692</td>\n",
       "      <td>2.016091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.879702</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>3.078858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.892965</td>\n",
       "      <td>0.813502</td>\n",
       "      <td>4.012818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.896425</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>5.040853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.538492</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.323962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">8</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.486508</td>\n",
       "      <td>0.553584</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>1.417094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.734127</td>\n",
       "      <td>0.799104</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>2.576081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.841820</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>3.832223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.814430</td>\n",
       "      <td>5.245420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.861382</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>6.504427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.843651</td>\n",
       "      <td>0.871540</td>\n",
       "      <td>0.804473</td>\n",
       "      <td>7.869504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.880988</td>\n",
       "      <td>0.799578</td>\n",
       "      <td>9.173052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>0.796540</td>\n",
       "      <td>10.463114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.749202</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>1.831090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.629705</td>\n",
       "      <td>3.321462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.803983</td>\n",
       "      <td>0.643122</td>\n",
       "      <td>4.869611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.882937</td>\n",
       "      <td>0.805492</td>\n",
       "      <td>0.641857</td>\n",
       "      <td>6.326309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.736072</td>\n",
       "      <td>0.587342</td>\n",
       "      <td>2.207851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.842486</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>4.287650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.868701</td>\n",
       "      <td>0.771308</td>\n",
       "      <td>6.335766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.553968</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.559099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.423016</td>\n",
       "      <td>0.321061</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>1.659954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.414257</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>3.252145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.620238</td>\n",
       "      <td>0.485318</td>\n",
       "      <td>0.471561</td>\n",
       "      <td>5.009747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.679762</td>\n",
       "      <td>0.507630</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>7.373743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.724206</td>\n",
       "      <td>0.560903</td>\n",
       "      <td>0.440506</td>\n",
       "      <td>8.960562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.696726</td>\n",
       "      <td>0.439156</td>\n",
       "      <td>32.222431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">32</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.616971</td>\n",
       "      <td>0.426414</td>\n",
       "      <td>1.195384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826587</td>\n",
       "      <td>0.726712</td>\n",
       "      <td>0.527595</td>\n",
       "      <td>2.318904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.803983</td>\n",
       "      <td>0.632321</td>\n",
       "      <td>12.117837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.753637</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>4.590592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.815250</td>\n",
       "      <td>0.653840</td>\n",
       "      <td>13.141996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.915476</td>\n",
       "      <td>0.776171</td>\n",
       "      <td>0.586751</td>\n",
       "      <td>6.918323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.909524</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.584979</td>\n",
       "      <td>8.676533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.931746</td>\n",
       "      <td>0.826961</td>\n",
       "      <td>0.675359</td>\n",
       "      <td>15.049278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.796709</td>\n",
       "      <td>0.618734</td>\n",
       "      <td>10.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.822347</td>\n",
       "      <td>0.667004</td>\n",
       "      <td>14.017167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.829179</td>\n",
       "      <td>0.679578</td>\n",
       "      <td>16.208178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.829933</td>\n",
       "      <td>0.681013</td>\n",
       "      <td>17.381170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.944048</td>\n",
       "      <td>0.830199</td>\n",
       "      <td>0.681266</td>\n",
       "      <td>19.179069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.830554</td>\n",
       "      <td>0.681772</td>\n",
       "      <td>20.313943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.630412</td>\n",
       "      <td>0.659747</td>\n",
       "      <td>1.532302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.846825</td>\n",
       "      <td>0.834812</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>3.066649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.839026</td>\n",
       "      <td>0.709030</td>\n",
       "      <td>5.453015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>0.844571</td>\n",
       "      <td>0.716878</td>\n",
       "      <td>6.928237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.879762</td>\n",
       "      <td>0.846833</td>\n",
       "      <td>0.718481</td>\n",
       "      <td>8.421714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.934921</td>\n",
       "      <td>0.852067</td>\n",
       "      <td>0.724641</td>\n",
       "      <td>16.195730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>0.853886</td>\n",
       "      <td>0.728017</td>\n",
       "      <td>17.664799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.521032</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.331508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">8</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.405436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.450397</td>\n",
       "      <td>0.569508</td>\n",
       "      <td>0.818734</td>\n",
       "      <td>2.777832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.863068</td>\n",
       "      <td>0.795612</td>\n",
       "      <td>14.802773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>0.571505</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>5.437043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.866306</td>\n",
       "      <td>0.795781</td>\n",
       "      <td>16.250514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.676984</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>7.986766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.866971</td>\n",
       "      <td>0.795274</td>\n",
       "      <td>17.737847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.838893</td>\n",
       "      <td>0.794346</td>\n",
       "      <td>10.757020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.845413</td>\n",
       "      <td>0.792152</td>\n",
       "      <td>12.104453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>0.851712</td>\n",
       "      <td>0.793502</td>\n",
       "      <td>13.462587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.874423</td>\n",
       "      <td>0.776962</td>\n",
       "      <td>22.464884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">32</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>0.569331</td>\n",
       "      <td>0.818397</td>\n",
       "      <td>1.640981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.818312</td>\n",
       "      <td>3.389922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.574255</td>\n",
       "      <td>0.820084</td>\n",
       "      <td>4.984748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.507540</td>\n",
       "      <td>0.586276</td>\n",
       "      <td>0.820844</td>\n",
       "      <td>6.491150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.707540</td>\n",
       "      <td>0.777945</td>\n",
       "      <td>0.801013</td>\n",
       "      <td>8.023460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.788492</td>\n",
       "      <td>0.853974</td>\n",
       "      <td>0.812743</td>\n",
       "      <td>9.607423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.881521</td>\n",
       "      <td>0.822363</td>\n",
       "      <td>11.145439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.889727</td>\n",
       "      <td>0.817131</td>\n",
       "      <td>12.703154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.875794</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.819494</td>\n",
       "      <td>14.221701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">122</th>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.693254</td>\n",
       "      <td>0.619012</td>\n",
       "      <td>0.620591</td>\n",
       "      <td>2.205522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.798016</td>\n",
       "      <td>0.789567</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>4.312284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.863778</td>\n",
       "      <td>0.809030</td>\n",
       "      <td>6.395371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.874823</td>\n",
       "      <td>0.809536</td>\n",
       "      <td>8.521467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.876553</td>\n",
       "      <td>0.806835</td>\n",
       "      <td>10.668059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.877839</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>12.819965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.903968</td>\n",
       "      <td>0.884271</td>\n",
       "      <td>0.796793</td>\n",
       "      <td>14.985993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  test_score_20  \\\n",
       "no_of_features hidden_layers                                                  \n",
       "1              1                  2     0.500000    0.583259       0.823207   \n",
       "               1                  3     0.773810    0.853132       0.821350   \n",
       "               1                  4     0.819444    0.873181       0.817215   \n",
       "               1                  5     0.857143    0.885247       0.815274   \n",
       "               1                  6     0.858730    0.886533       0.812489   \n",
       "8              1                  2     0.552778    0.696460       0.657553   \n",
       "               1                  3     0.641667    0.776393       0.669705   \n",
       "               1                  4     0.681746    0.800435       0.688439   \n",
       "               1                  5     0.791270    0.834634       0.718987   \n",
       "               1                  6     0.851587    0.848829       0.738228   \n",
       "               1                  7     0.872222    0.855305       0.746414   \n",
       "               1                  8     0.892063    0.861693       0.757637   \n",
       "               1                  9     0.909921    0.862890       0.759409   \n",
       "32             1                  2     0.724603    0.734120       0.525485   \n",
       "               1                  3     0.771825    0.750665       0.542785   \n",
       "               1                  4     0.911905    0.814319       0.651308   \n",
       "               1                  5     0.830952    0.786152       0.606245   \n",
       "               1                  6     0.922619    0.814807       0.652827   \n",
       "               1                  7     0.868651    0.796576       0.623460   \n",
       "               1                  8     0.931349    0.815871       0.654430   \n",
       "               1                  9     0.894048    0.806379       0.640928   \n",
       "               1                 10     0.905952    0.809173       0.644810   \n",
       "               1                 11     0.922222    0.812544       0.648692   \n",
       "122            1                  2     0.679365    0.775328       0.778819   \n",
       "               1                  3     0.798810    0.852466       0.768692   \n",
       "               1                  4     0.860317    0.879702       0.804810   \n",
       "               1                  5     0.885714    0.892965       0.813502   \n",
       "               1                  6     0.894048    0.896425       0.816203   \n",
       "1              3                  2     0.538492    0.430758       0.181603   \n",
       "8              3                  2     0.486508    0.553584       0.777300   \n",
       "               3                  3     0.734127    0.799104       0.784473   \n",
       "               3                  4     0.774603    0.841820       0.804979   \n",
       "               3                  5     0.785714    0.857745       0.814430   \n",
       "               3                  6     0.811111    0.861382       0.805823   \n",
       "               3                  7     0.843651    0.871540       0.804473   \n",
       "               3                  8     0.869841    0.880988       0.799578   \n",
       "               3                  9     0.894048    0.884847       0.796540   \n",
       "32             3                  2     0.776190    0.749202       0.626414   \n",
       "               3                  3     0.842857    0.786773       0.629705   \n",
       "               3                  4     0.875000    0.803983       0.643122   \n",
       "               3                  5     0.882937    0.805492       0.641857   \n",
       "122            3                  2     0.722222    0.736072       0.587342   \n",
       "               3                  3     0.858730    0.842486       0.737890   \n",
       "               3                  4     0.903571    0.868701       0.771308   \n",
       "1              5                  2     0.553968    0.430758       0.181603   \n",
       "8              5                  2     0.423016    0.321061       0.499325   \n",
       "               5                  3     0.540079    0.414257       0.487511   \n",
       "               5                  4     0.620238    0.485318       0.471561   \n",
       "               5                  6     0.679762    0.507630       0.443966   \n",
       "               5                  7     0.724206    0.560903       0.440506   \n",
       "...                             ...          ...         ...            ...   \n",
       "               3                 33     0.905556    0.696726       0.439156   \n",
       "32             3                  2     0.691667    0.616971       0.426414   \n",
       "               3                  3     0.826587    0.726712       0.527595   \n",
       "               3                  4     0.930556    0.803983       0.632321   \n",
       "               3                  5     0.883333    0.753637       0.553333   \n",
       "               3                  6     0.939286    0.815250       0.653840   \n",
       "               3                  7     0.915476    0.776171       0.586751   \n",
       "               3                  9     0.909524    0.778256       0.584979   \n",
       "               3                 10     0.931746    0.826961       0.675359   \n",
       "               3                 11     0.924603    0.796709       0.618734   \n",
       "               3                  8     0.941667    0.822347       0.667004   \n",
       "               3                 12     0.934127    0.829179       0.679578   \n",
       "               3                 14     0.935714    0.829933       0.681013   \n",
       "               3                 18     0.944048    0.830199       0.681266   \n",
       "               3                 20     0.939286    0.830554       0.681772   \n",
       "122            3                  2     0.633333    0.630412       0.659747   \n",
       "               3                  3     0.846825    0.834812       0.706667   \n",
       "               3                  5     0.869048    0.839026       0.709030   \n",
       "               3                  6     0.863889    0.844571       0.716878   \n",
       "               3                  7     0.879762    0.846833       0.718481   \n",
       "               3                  8     0.934921    0.852067       0.724641   \n",
       "               3                 10     0.948016    0.853886       0.728017   \n",
       "1              5                  2     0.521032    0.430758       0.181603   \n",
       "8              5                  2     0.455556    0.569242       0.818397   \n",
       "               5                  3     0.450397    0.569508       0.818734   \n",
       "               5                  4     0.793651    0.863068       0.795612   \n",
       "               5                  5     0.459524    0.571505       0.821350   \n",
       "               5                  6     0.810714    0.866306       0.795781   \n",
       "               5                  7     0.676984    0.793426       0.813165   \n",
       "               5                  8     0.829365    0.866971       0.795274   \n",
       "               5                  9     0.755556    0.838893       0.794346   \n",
       "               5                 10     0.776190    0.845413       0.792152   \n",
       "               5                 11     0.789286    0.851712       0.793502   \n",
       "               5                 20     0.877381    0.874423       0.776962   \n",
       "32             5                  2     0.467857    0.569331       0.818397   \n",
       "               5                  3     0.469048    0.570263       0.818312   \n",
       "               5                  4     0.478571    0.574255       0.820084   \n",
       "               5                  5     0.507540    0.586276       0.820844   \n",
       "               5                  6     0.707540    0.777945       0.801013   \n",
       "               5                  7     0.788492    0.853974       0.812743   \n",
       "               5                  8     0.833333    0.881521       0.822363   \n",
       "               5                  9     0.858333    0.889727       0.817131   \n",
       "               5                 10     0.875794    0.895050       0.819494   \n",
       "122            5                  2     0.693254    0.619012       0.620591   \n",
       "               5                  3     0.798016    0.789567       0.769114   \n",
       "               5                  4     0.859127    0.863778       0.809030   \n",
       "               5                  5     0.880952    0.874823       0.809536   \n",
       "               5                  6     0.880952    0.876553       0.806835   \n",
       "               5                  7     0.891270    0.877839       0.798650   \n",
       "               5                  8     0.903968    0.884271       0.796793   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                1.737384  \n",
       "               1                2.746407  \n",
       "               1                3.780096  \n",
       "               1                4.823596  \n",
       "               1                5.901322  \n",
       "8              1                1.041274  \n",
       "               1                2.013830  \n",
       "               1                3.030958  \n",
       "               1                4.018355  \n",
       "               1                4.996470  \n",
       "               1                5.974369  \n",
       "               1                7.018800  \n",
       "               1                8.021910  \n",
       "32             1                1.510834  \n",
       "               1                2.517899  \n",
       "               1               11.737579  \n",
       "               1                4.757215  \n",
       "               1               12.685921  \n",
       "               1                6.851718  \n",
       "               1               13.714334  \n",
       "               1                8.793038  \n",
       "               1                9.762101  \n",
       "               1               10.796256  \n",
       "122            1                0.966112  \n",
       "               1                2.016091  \n",
       "               1                3.078858  \n",
       "               1                4.012818  \n",
       "               1                5.040853  \n",
       "1              3                1.323962  \n",
       "8              3                1.417094  \n",
       "               3                2.576081  \n",
       "               3                3.832223  \n",
       "               3                5.245420  \n",
       "               3                6.504427  \n",
       "               3                7.869504  \n",
       "               3                9.173052  \n",
       "               3               10.463114  \n",
       "32             3                1.831090  \n",
       "               3                3.321462  \n",
       "               3                4.869611  \n",
       "               3                6.326309  \n",
       "122            3                2.207851  \n",
       "               3                4.287650  \n",
       "               3                6.335766  \n",
       "1              5                1.559099  \n",
       "8              5                1.659954  \n",
       "               5                3.252145  \n",
       "               5                5.009747  \n",
       "               5                7.373743  \n",
       "               5                8.960562  \n",
       "...                                  ...  \n",
       "               3               32.222431  \n",
       "32             3                1.195384  \n",
       "               3                2.318904  \n",
       "               3               12.117837  \n",
       "               3                4.590592  \n",
       "               3               13.141996  \n",
       "               3                6.918323  \n",
       "               3                8.676533  \n",
       "               3               15.049278  \n",
       "               3               10.999747  \n",
       "               3               14.017167  \n",
       "               3               16.208178  \n",
       "               3               17.381170  \n",
       "               3               19.179069  \n",
       "               3               20.313943  \n",
       "122            3                1.532302  \n",
       "               3                3.066649  \n",
       "               3                5.453015  \n",
       "               3                6.928237  \n",
       "               3                8.421714  \n",
       "               3               16.195730  \n",
       "               3               17.664799  \n",
       "1              5                1.331508  \n",
       "8              5                1.405436  \n",
       "               5                2.777832  \n",
       "               5               14.802773  \n",
       "               5                5.437043  \n",
       "               5               16.250514  \n",
       "               5                7.986766  \n",
       "               5               17.737847  \n",
       "               5               10.757020  \n",
       "               5               12.104453  \n",
       "               5               13.462587  \n",
       "               5               22.464884  \n",
       "32             5                1.640981  \n",
       "               5                3.389922  \n",
       "               5                4.984748  \n",
       "               5                6.491150  \n",
       "               5                8.023460  \n",
       "               5                9.607423  \n",
       "               5               11.145439  \n",
       "               5               12.703154  \n",
       "               5               14.221701  \n",
       "122            5                2.205522  \n",
       "               5                4.312284  \n",
       "               5                6.395371  \n",
       "               5                8.521467  \n",
       "               5               10.668059  \n",
       "               5               12.819965  \n",
       "               5               14.985993  \n",
       "\n",
       "[1177 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex(levels=[[1, 8, 32, 122], [1, 3, 5]],\n",
       "           labels=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
       "           names=['no_of_features', 'hidden_layers'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.754711\n",
       "                3                0.430758\n",
       "                5                0.430758\n",
       "8               1                0.744528\n",
       "                3                0.707654\n",
       "                5                0.638838\n",
       "32              1                0.756613\n",
       "                3                0.773092\n",
       "                5                0.698526\n",
       "122             1                0.769095\n",
       "                3                0.789415\n",
       "                5                0.815644\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.mean().test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-19T14:38:30.923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                0.092557\n",
       "                3                0.000000\n",
       "                5                0.000000\n",
       "8               1                0.113589\n",
       "                3                0.121685\n",
       "                5                0.134545\n",
       "32              1                0.070748\n",
       "                3                0.094213\n",
       "                5                0.115432\n",
       "122             1                0.076050\n",
       "                3                0.089459\n",
       "                5                0.101644\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std().test_score"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
