{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:45.922343Z",
     "start_time": "2017-07-20T21:57:45.503712Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:45.968162Z",
     "start_time": "2017-07-20T21:57:45.923806Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:47.553525Z",
     "start_time": "2017-07-20T21:57:45.970093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train__2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:47.560426Z",
     "start_time": "2017-07-20T21:57:47.555053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:47.565658Z",
     "start_time": "2017-07-20T21:57:47.561917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:48.178314Z",
     "start_time": "2017-07-20T21:57:47.567236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 122)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:54.340480Z",
     "start_time": "2017-07-20T21:57:48.179820Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:54.535014Z",
     "start_time": "2017-07-20T21:57:54.342023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:54.827015Z",
     "start_time": "2017-07-20T21:57:54.536741Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd-/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    \n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:57:54.921072Z",
     "start_time": "2017-07-20T21:57:54.828516Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        print(\"********************************** Training ******************************\")\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 12, 24, 48, 122]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [5]\n",
    "        lrs = [1e-5, 1e-6]\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "        past_scores.to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.427909Z",
     "start_time": "2017-07-20T21:57:54.922527Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.704859 | Validation Accuracy: 0.678968\n",
      "Accuracy on Test data: 0.6729950308799744, 0.448860764503479\n",
      "Step 2 | Training Loss: 0.638593 | Validation Accuracy: 0.741667\n",
      "Accuracy on Test data: 0.700762927532196, 0.4810970425605774\n",
      "Step 3 | Training Loss: 0.584574 | Validation Accuracy: 0.793651\n",
      "Accuracy on Test data: 0.7201472520828247, 0.4945991635322571\n",
      "Step 4 | Training Loss: 0.570982 | Validation Accuracy: 0.831746\n",
      "Accuracy on Test data: 0.7365152835845947, 0.5159493684768677\n",
      "Step 5 | Training Loss: 0.519881 | Validation Accuracy: 0.862302\n",
      "Accuracy on Test data: 0.7663236260414124, 0.5675105452537537\n",
      "Step 1 | Training Loss: 0.513082 | Validation Accuracy: 0.874206\n",
      "Accuracy on Test data: 0.7675656676292419, 0.5693671107292175\n",
      "Step 2 | Training Loss: 0.510145 | Validation Accuracy: 0.868254\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5698733925819397\n",
      "Step 3 | Training Loss: 0.529031 | Validation Accuracy: 0.855159\n",
      "Accuracy on Test data: 0.7686302065849304, 0.5710548758506775\n",
      "Step 4 | Training Loss: 0.499858 | Validation Accuracy: 0.873016\n",
      "Accuracy on Test data: 0.7706707119941711, 0.5743460059165955\n",
      "Step 5 | Training Loss: 0.499836 | Validation Accuracy: 0.871825\n",
      "Accuracy on Test data: 0.7716465592384338, 0.5759493708610535\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:12\n",
      "Step 1 | Training Loss: 0.695790 | Validation Accuracy: 0.460714\n",
      "Accuracy on Test data: 0.5574432015419006, 0.7873417735099792\n",
      "Step 2 | Training Loss: 0.602079 | Validation Accuracy: 0.650000\n",
      "Accuracy on Test data: 0.7164655923843384, 0.7700421810150146\n",
      "Step 3 | Training Loss: 0.593868 | Validation Accuracy: 0.832143\n",
      "Accuracy on Test data: 0.8544623851776123, 0.7656540274620056\n",
      "Step 4 | Training Loss: 0.537484 | Validation Accuracy: 0.860317\n",
      "Accuracy on Test data: 0.8686568737030029, 0.7843037843704224\n",
      "Step 5 | Training Loss: 0.503493 | Validation Accuracy: 0.889683\n",
      "Accuracy on Test data: 0.8700762987136841, 0.7817721366882324\n",
      "Step 1 | Training Loss: 0.534738 | Validation Accuracy: 0.897222\n",
      "Accuracy on Test data: 0.8694552779197693, 0.7801687717437744\n",
      "Step 2 | Training Loss: 0.532649 | Validation Accuracy: 0.890079\n",
      "Accuracy on Test data: 0.8693222403526306, 0.7792404890060425\n",
      "Step 3 | Training Loss: 0.533821 | Validation Accuracy: 0.896429\n",
      "Accuracy on Test data: 0.8695440292358398, 0.7785654067993164\n",
      "Step 4 | Training Loss: 0.515652 | Validation Accuracy: 0.894841\n",
      "Accuracy on Test data: 0.8695440292358398, 0.7775527238845825\n",
      "Step 5 | Training Loss: 0.533061 | Validation Accuracy: 0.890079\n",
      "Accuracy on Test data: 0.8712295889854431, 0.7766245007514954\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:24\n",
      "Step 1 | Training Loss: 0.626033 | Validation Accuracy: 0.629762\n",
      "Accuracy on Test data: 0.6639460325241089, 0.7231223583221436\n",
      "Step 2 | Training Loss: 0.589082 | Validation Accuracy: 0.734921\n",
      "Accuracy on Test data: 0.7754169702529907, 0.7435442805290222\n",
      "Step 3 | Training Loss: 0.536754 | Validation Accuracy: 0.835714\n",
      "Accuracy on Test data: 0.828646183013916, 0.7381434440612793\n",
      "Step 4 | Training Loss: 0.514874 | Validation Accuracy: 0.848810\n",
      "Accuracy on Test data: 0.8309971690177917, 0.7205907106399536\n",
      "Step 5 | Training Loss: 0.498009 | Validation Accuracy: 0.885714\n",
      "Accuracy on Test data: 0.8138307332992554, 0.6690295338630676\n",
      "Step 1 | Training Loss: 0.533622 | Validation Accuracy: 0.884921\n",
      "Accuracy on Test data: 0.8143630027770996, 0.6683544516563416\n",
      "Step 2 | Training Loss: 0.477143 | Validation Accuracy: 0.890873\n",
      "Accuracy on Test data: 0.8145847916603088, 0.6676793098449707\n",
      "Step 3 | Training Loss: 0.525298 | Validation Accuracy: 0.880556\n",
      "Accuracy on Test data: 0.8146735429763794, 0.6661603450775146\n",
      "Step 4 | Training Loss: 0.510468 | Validation Accuracy: 0.890079\n",
      "Accuracy on Test data: 0.8143630027770996, 0.6647257208824158\n",
      "Step 5 | Training Loss: 0.501308 | Validation Accuracy: 0.894841\n",
      "Accuracy on Test data: 0.8152058124542236, 0.6648945212364197\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:48\n",
      "Step 1 | Training Loss: 0.678602 | Validation Accuracy: 0.635714\n",
      "Accuracy on Test data: 0.7136710286140442, 0.6208438873291016\n",
      "Step 2 | Training Loss: 0.689762 | Validation Accuracy: 0.759127\n",
      "Accuracy on Test data: 0.7632185816764832, 0.6230379939079285\n",
      "Step 3 | Training Loss: 0.590445 | Validation Accuracy: 0.784921\n",
      "Accuracy on Test data: 0.7706263065338135, 0.6195780634880066\n",
      "Step 4 | Training Loss: 0.549522 | Validation Accuracy: 0.812698\n",
      "Accuracy on Test data: 0.7673882246017456, 0.600337564945221\n",
      "Step 5 | Training Loss: 0.537873 | Validation Accuracy: 0.868651\n",
      "Accuracy on Test data: 0.7890791296958923, 0.6254852414131165\n",
      "Step 1 | Training Loss: 0.522742 | Validation Accuracy: 0.866270\n",
      "Accuracy on Test data: 0.7920067310333252, 0.6296202540397644\n",
      "Step 2 | Training Loss: 0.518537 | Validation Accuracy: 0.890873\n",
      "Accuracy on Test data: 0.7945351600646973, 0.6324050426483154\n",
      "Step 3 | Training Loss: 0.542599 | Validation Accuracy: 0.882540\n",
      "Accuracy on Test data: 0.7963981628417969, 0.6340084671974182\n",
      "Step 4 | Training Loss: 0.532806 | Validation Accuracy: 0.889683\n",
      "Accuracy on Test data: 0.797507107257843, 0.6348523497581482\n",
      "Step 5 | Training Loss: 0.533293 | Validation Accuracy: 0.897619\n",
      "Accuracy on Test data: 0.7991039752960205, 0.6365400552749634\n",
      "Current Layer Attributes - epochs:5 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.690460 | Validation Accuracy: 0.761111\n",
      "Accuracy on Test data: 0.5943488478660583, 0.40590718388557434\n",
      "Step 2 | Training Loss: 0.615858 | Validation Accuracy: 0.823016\n",
      "Accuracy on Test data: 0.6348917484283447, 0.47409284114837646\n",
      "Step 3 | Training Loss: 0.571126 | Validation Accuracy: 0.850794\n",
      "Accuracy on Test data: 0.7276880741119385, 0.49544304609298706\n",
      "Step 4 | Training Loss: 0.539070 | Validation Accuracy: 0.882143\n",
      "Accuracy on Test data: 0.7323899865150452, 0.5029535889625549\n",
      "Step 5 | Training Loss: 0.535039 | Validation Accuracy: 0.901587\n",
      "Accuracy on Test data: 0.7434794306755066, 0.5209282636642456\n",
      "Step 1 | Training Loss: 0.490484 | Validation Accuracy: 0.895635\n",
      "Accuracy on Test data: 0.7453867793083191, 0.5237974524497986\n",
      "Step 2 | Training Loss: 0.539194 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.7463626861572266, 0.5253164768218994\n",
      "Step 3 | Training Loss: 0.511202 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.7470723986625671, 0.5265823006629944\n",
      "Step 4 | Training Loss: 0.519965 | Validation Accuracy: 0.892460\n",
      "Accuracy on Test data: 0.7479152083396912, 0.5280168652534485\n",
      "Step 5 | Training Loss: 0.472532 | Validation Accuracy: 0.896825\n",
      "Accuracy on Test data: 0.7488910555839539, 0.5297890305519104\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.693097 | Validation Accuracy: 0.535714\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692988 | Validation Accuracy: 0.536508\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693227 | Validation Accuracy: 0.540873\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693047 | Validation Accuracy: 0.542857\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692922 | Validation Accuracy: 0.530556\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692970 | Validation Accuracy: 0.541667\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692702 | Validation Accuracy: 0.534127\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692694 | Validation Accuracy: 0.539286\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.693125 | Validation Accuracy: 0.546429\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.693124 | Validation Accuracy: 0.544048\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:12\n",
      "Step 1 | Training Loss: 0.701191 | Validation Accuracy: 0.686508\n",
      "Accuracy on Test data: 0.7278211712837219, 0.5504641532897949\n",
      "Step 2 | Training Loss: 0.704741 | Validation Accuracy: 0.758333\n",
      "Accuracy on Test data: 0.7593151330947876, 0.5847257375717163\n",
      "Step 3 | Training Loss: 0.691565 | Validation Accuracy: 0.765873\n",
      "Accuracy on Test data: 0.7739975452423096, 0.6024472713470459\n",
      "Step 4 | Training Loss: 0.679824 | Validation Accuracy: 0.778968\n",
      "Accuracy on Test data: 0.7867282032966614, 0.6206750869750977\n",
      "Step 5 | Training Loss: 0.688822 | Validation Accuracy: 0.807936\n",
      "Accuracy on Test data: 0.7853530645370483, 0.6124050617218018\n",
      "Step 1 | Training Loss: 0.676469 | Validation Accuracy: 0.802381\n",
      "Accuracy on Test data: 0.7861515283584595, 0.6135020852088928\n",
      "Step 2 | Training Loss: 0.657949 | Validation Accuracy: 0.803175\n",
      "Accuracy on Test data: 0.786550760269165, 0.6141772270202637\n",
      "Step 3 | Training Loss: 0.710668 | Validation Accuracy: 0.789286\n",
      "Accuracy on Test data: 0.7876597046852112, 0.6160337328910828\n",
      "Step 4 | Training Loss: 0.675461 | Validation Accuracy: 0.799603\n",
      "Accuracy on Test data: 0.7886355519294739, 0.6174683570861816\n",
      "Step 5 | Training Loss: 0.692655 | Validation Accuracy: 0.801190\n",
      "Accuracy on Test data: 0.7903655171394348, 0.6200844049453735\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:24\n",
      "Step 1 | Training Loss: 0.727939 | Validation Accuracy: 0.597619\n",
      "Accuracy on Test data: 0.5993168950080872, 0.6648945212364197\n",
      "Step 2 | Training Loss: 0.693773 | Validation Accuracy: 0.711111\n",
      "Accuracy on Test data: 0.7116749286651611, 0.6626160144805908\n",
      "Step 3 | Training Loss: 0.683454 | Validation Accuracy: 0.820238\n",
      "Accuracy on Test data: 0.8024308085441589, 0.6535021066665649\n",
      "Step 4 | Training Loss: 0.629838 | Validation Accuracy: 0.832937\n",
      "Accuracy on Test data: 0.8149396777153015, 0.6678481101989746\n",
      "Step 5 | Training Loss: 0.638545 | Validation Accuracy: 0.861508\n",
      "Accuracy on Test data: 0.8269162774085999, 0.6891983151435852\n",
      "Step 1 | Training Loss: 0.624529 | Validation Accuracy: 0.854365\n",
      "Accuracy on Test data: 0.8281139135360718, 0.6913924217224121\n",
      "Step 2 | Training Loss: 0.629730 | Validation Accuracy: 0.863492\n",
      "Accuracy on Test data: 0.8316625356674194, 0.6978058815002441\n",
      "Step 3 | Training Loss: 0.638014 | Validation Accuracy: 0.843651\n",
      "Accuracy on Test data: 0.8360539674758911, 0.7059915661811829\n",
      "Step 4 | Training Loss: 0.647867 | Validation Accuracy: 0.863492\n",
      "Accuracy on Test data: 0.838449239730835, 0.7103797197341919\n",
      "Step 5 | Training Loss: 0.678058 | Validation Accuracy: 0.867460\n",
      "Accuracy on Test data: 0.8431955575942993, 0.7193248867988586\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:48\n",
      "Step 1 | Training Loss: 0.634718 | Validation Accuracy: 0.750000\n",
      "Accuracy on Test data: 0.7867725491523743, 0.6574683785438538\n",
      "Step 2 | Training Loss: 0.606195 | Validation Accuracy: 0.787302\n",
      "Accuracy on Test data: 0.8173350095748901, 0.7060759663581848\n",
      "Step 3 | Training Loss: 0.609876 | Validation Accuracy: 0.794841\n",
      "Accuracy on Test data: 0.8232345581054688, 0.7119831442832947\n",
      "Step 4 | Training Loss: 0.655573 | Validation Accuracy: 0.806746\n",
      "Accuracy on Test data: 0.823145866394043, 0.7030379772186279\n",
      "Step 5 | Training Loss: 0.614171 | Validation Accuracy: 0.861111\n",
      "Accuracy on Test data: 0.8462561964988708, 0.7326582074165344\n",
      "Step 1 | Training Loss: 0.592474 | Validation Accuracy: 0.878968\n",
      "Accuracy on Test data: 0.8482522964477539, 0.7354430556297302\n",
      "Step 2 | Training Loss: 0.571366 | Validation Accuracy: 0.893254\n",
      "Accuracy on Test data: 0.8494055867195129, 0.7369620203971863\n",
      "Step 3 | Training Loss: 0.649811 | Validation Accuracy: 0.876587\n",
      "Accuracy on Test data: 0.8506919741630554, 0.7386497855186462\n",
      "Step 4 | Training Loss: 0.607397 | Validation Accuracy: 0.894048\n",
      "Accuracy on Test data: 0.8518896102905273, 0.7400000095367432\n",
      "Step 5 | Training Loss: 0.620668 | Validation Accuracy: 0.891270\n",
      "Accuracy on Test data: 0.8524662852287292, 0.7406750917434692\n",
      "Current Layer Attributes - epochs:5 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.715050 | Validation Accuracy: 0.721429\n",
      "Accuracy on Test data: 0.7898331880569458, 0.6759493947029114\n",
      "Step 2 | Training Loss: 0.648456 | Validation Accuracy: 0.765079\n",
      "Accuracy on Test data: 0.8562366962432861, 0.7789029479026794\n",
      "Step 3 | Training Loss: 0.653581 | Validation Accuracy: 0.809921\n",
      "Accuracy on Test data: 0.8815205693244934, 0.8066666722297668\n",
      "Step 4 | Training Loss: 0.638820 | Validation Accuracy: 0.861508\n",
      "Accuracy on Test data: 0.8974893689155579, 0.8333333134651184\n",
      "Step 5 | Training Loss: 0.569691 | Validation Accuracy: 0.881349\n",
      "Accuracy on Test data: 0.9033445715904236, 0.8422784805297852\n",
      "Step 1 | Training Loss: 0.611969 | Validation Accuracy: 0.877381\n",
      "Accuracy on Test data: 0.9036994576454163, 0.8427004218101501\n",
      "Step 2 | Training Loss: 0.583786 | Validation Accuracy: 0.880159\n",
      "Accuracy on Test data: 0.9039655923843384, 0.8427848219871521\n",
      "Step 3 | Training Loss: 0.584890 | Validation Accuracy: 0.886111\n",
      "Accuracy on Test data: 0.9038768410682678, 0.8422784805297852\n",
      "Step 4 | Training Loss: 0.580338 | Validation Accuracy: 0.900794\n",
      "Accuracy on Test data: 0.9041873812675476, 0.8427004218101501\n",
      "Step 5 | Training Loss: 0.579043 | Validation Accuracy: 0.879762\n",
      "Accuracy on Test data: 0.8927874565124512, 0.8206751346588135\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 10\n",
    "#capture\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-18T20:12:01.591604Z",
     "start_time": "2017-06-18T20:12:01.586451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.431810Z",
     "start_time": "2017-07-20T21:59:07.429362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "#df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.462032Z",
     "start_time": "2017-07-20T21:59:07.433139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "#df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.491683Z",
     "start_time": "2017-07-20T21:59:07.463357Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.497325Z",
     "start_time": "2017-07-20T21:59:07.493036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train.predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.835009Z",
     "start_time": "2017-07-20T21:59:07.498621Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores-.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:07.908290Z",
     "start_time": "2017-07-20T21:59:07.836533Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.109437Z",
     "start_time": "2017-07-20T21:59:07.909693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.113488Z",
     "start_time": "2017-07-20T21:59:08.110917Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.118835Z",
     "start_time": "2017-07-20T21:59:08.114794Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.153176Z",
     "start_time": "2017-07-20T21:59:08.120137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.904187</td>\n",
       "      <td>0.918907</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.907125</td>\n",
       "      <td>8.973320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880159</td>\n",
       "      <td>0.903966</td>\n",
       "      <td>0.918868</td>\n",
       "      <td>0.842785</td>\n",
       "      <td>0.907373</td>\n",
       "      <td>7.430262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.877381</td>\n",
       "      <td>0.903699</td>\n",
       "      <td>0.918741</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.907448</td>\n",
       "      <td>6.369843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.861508</td>\n",
       "      <td>0.897489</td>\n",
       "      <td>0.914315</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.903020</td>\n",
       "      <td>4.242011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889683</td>\n",
       "      <td>0.870076</td>\n",
       "      <td>0.885635</td>\n",
       "      <td>0.781772</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>2.840094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890079</td>\n",
       "      <td>0.871230</td>\n",
       "      <td>0.885470</td>\n",
       "      <td>0.776625</td>\n",
       "      <td>0.859359</td>\n",
       "      <td>4.722867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.868657</td>\n",
       "      <td>0.885130</td>\n",
       "      <td>0.784304</td>\n",
       "      <td>0.866192</td>\n",
       "      <td>2.250494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765079</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.880920</td>\n",
       "      <td>0.778903</td>\n",
       "      <td>0.871190</td>\n",
       "      <td>2.193426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.854462</td>\n",
       "      <td>0.874565</td>\n",
       "      <td>0.765654</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>1.711961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>0.866928</td>\n",
       "      <td>0.740675</td>\n",
       "      <td>0.833685</td>\n",
       "      <td>8.744209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.851890</td>\n",
       "      <td>0.866531</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.833414</td>\n",
       "      <td>7.848405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.876587</td>\n",
       "      <td>0.850692</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.738650</td>\n",
       "      <td>0.832821</td>\n",
       "      <td>6.886408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.893254</td>\n",
       "      <td>0.849406</td>\n",
       "      <td>0.864843</td>\n",
       "      <td>0.736962</td>\n",
       "      <td>0.832212</td>\n",
       "      <td>5.971076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.878968</td>\n",
       "      <td>0.848252</td>\n",
       "      <td>0.864068</td>\n",
       "      <td>0.735443</td>\n",
       "      <td>0.831606</td>\n",
       "      <td>5.028126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.828646</td>\n",
       "      <td>0.849190</td>\n",
       "      <td>0.738143</td>\n",
       "      <td>0.833342</td>\n",
       "      <td>1.666660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.843196</td>\n",
       "      <td>0.849080</td>\n",
       "      <td>0.719325</td>\n",
       "      <td>0.804261</td>\n",
       "      <td>8.582286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.848810</td>\n",
       "      <td>0.830997</td>\n",
       "      <td>0.847356</td>\n",
       "      <td>0.720591</td>\n",
       "      <td>0.818227</td>\n",
       "      <td>2.194746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.838449</td>\n",
       "      <td>0.843905</td>\n",
       "      <td>0.710380</td>\n",
       "      <td>0.796923</td>\n",
       "      <td>7.708073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.787302</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.843410</td>\n",
       "      <td>0.706076</td>\n",
       "      <td>0.820566</td>\n",
       "      <td>1.843658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.843651</td>\n",
       "      <td>0.836054</td>\n",
       "      <td>0.841292</td>\n",
       "      <td>0.705992</td>\n",
       "      <td>0.793283</td>\n",
       "      <td>6.840965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.836401</td>\n",
       "      <td>0.697806</td>\n",
       "      <td>0.786349</td>\n",
       "      <td>5.959406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.854365</td>\n",
       "      <td>0.828114</td>\n",
       "      <td>0.832461</td>\n",
       "      <td>0.691392</td>\n",
       "      <td>0.780874</td>\n",
       "      <td>5.088723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.832937</td>\n",
       "      <td>0.814940</td>\n",
       "      <td>0.818577</td>\n",
       "      <td>0.667848</td>\n",
       "      <td>0.762003</td>\n",
       "      <td>3.417473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.721429</td>\n",
       "      <td>0.789833</td>\n",
       "      <td>0.817291</td>\n",
       "      <td>0.675949</td>\n",
       "      <td>0.797147</td>\n",
       "      <td>1.059048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734921</td>\n",
       "      <td>0.775417</td>\n",
       "      <td>0.813442</td>\n",
       "      <td>0.743544</td>\n",
       "      <td>0.839045</td>\n",
       "      <td>1.136762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.812058</td>\n",
       "      <td>0.657468</td>\n",
       "      <td>0.781762</td>\n",
       "      <td>0.942473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.799104</td>\n",
       "      <td>0.797641</td>\n",
       "      <td>0.636540</td>\n",
       "      <td>0.729239</td>\n",
       "      <td>5.745863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889683</td>\n",
       "      <td>0.797507</td>\n",
       "      <td>0.796233</td>\n",
       "      <td>0.634852</td>\n",
       "      <td>0.728118</td>\n",
       "      <td>5.177340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.796398</td>\n",
       "      <td>0.795400</td>\n",
       "      <td>0.634008</td>\n",
       "      <td>0.727763</td>\n",
       "      <td>4.528214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.794535</td>\n",
       "      <td>0.793712</td>\n",
       "      <td>0.632405</td>\n",
       "      <td>0.726588</td>\n",
       "      <td>3.943557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866270</td>\n",
       "      <td>0.792007</td>\n",
       "      <td>0.791628</td>\n",
       "      <td>0.629620</td>\n",
       "      <td>0.725121</td>\n",
       "      <td>3.296505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.716466</td>\n",
       "      <td>0.786990</td>\n",
       "      <td>0.770042</td>\n",
       "      <td>0.864394</td>\n",
       "      <td>1.128677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759127</td>\n",
       "      <td>0.763219</td>\n",
       "      <td>0.786035</td>\n",
       "      <td>0.623038</td>\n",
       "      <td>0.750712</td>\n",
       "      <td>1.162712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.778968</td>\n",
       "      <td>0.786728</td>\n",
       "      <td>0.783755</td>\n",
       "      <td>0.620675</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>3.363642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>0.790366</td>\n",
       "      <td>0.783171</td>\n",
       "      <td>0.620084</td>\n",
       "      <td>0.709548</td>\n",
       "      <td>7.018722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799603</td>\n",
       "      <td>0.788636</td>\n",
       "      <td>0.781091</td>\n",
       "      <td>0.617468</td>\n",
       "      <td>0.707076</td>\n",
       "      <td>6.185080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>0.787660</td>\n",
       "      <td>0.779938</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.705654</td>\n",
       "      <td>5.342499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765873</td>\n",
       "      <td>0.773998</td>\n",
       "      <td>0.769842</td>\n",
       "      <td>0.602447</td>\n",
       "      <td>0.701552</td>\n",
       "      <td>2.538861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871825</td>\n",
       "      <td>0.771647</td>\n",
       "      <td>0.757467</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.664978</td>\n",
       "      <td>6.343552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.770671</td>\n",
       "      <td>0.756224</td>\n",
       "      <td>0.574346</td>\n",
       "      <td>0.663329</td>\n",
       "      <td>5.769164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.759315</td>\n",
       "      <td>0.755211</td>\n",
       "      <td>0.584726</td>\n",
       "      <td>0.687297</td>\n",
       "      <td>1.708174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.768630</td>\n",
       "      <td>0.753590</td>\n",
       "      <td>0.571055</td>\n",
       "      <td>0.659932</td>\n",
       "      <td>5.233289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868254</td>\n",
       "      <td>0.767920</td>\n",
       "      <td>0.752601</td>\n",
       "      <td>0.569873</td>\n",
       "      <td>0.658538</td>\n",
       "      <td>4.685276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.767566</td>\n",
       "      <td>0.752129</td>\n",
       "      <td>0.569367</td>\n",
       "      <td>0.657999</td>\n",
       "      <td>4.072639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629762</td>\n",
       "      <td>0.663946</td>\n",
       "      <td>0.747484</td>\n",
       "      <td>0.723122</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>0.545619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.711675</td>\n",
       "      <td>0.746431</td>\n",
       "      <td>0.662616</td>\n",
       "      <td>0.764074</td>\n",
       "      <td>1.679697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.713671</td>\n",
       "      <td>0.738717</td>\n",
       "      <td>0.620844</td>\n",
       "      <td>0.754896</td>\n",
       "      <td>0.560952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.686508</td>\n",
       "      <td>0.727821</td>\n",
       "      <td>0.729334</td>\n",
       "      <td>0.550464</td>\n",
       "      <td>0.666875</td>\n",
       "      <td>0.858689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.748891</td>\n",
       "      <td>0.724405</td>\n",
       "      <td>0.529789</td>\n",
       "      <td>0.610948</td>\n",
       "      <td>5.939553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892460</td>\n",
       "      <td>0.747915</td>\n",
       "      <td>0.722956</td>\n",
       "      <td>0.528017</td>\n",
       "      <td>0.608744</td>\n",
       "      <td>5.366831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.747072</td>\n",
       "      <td>0.721772</td>\n",
       "      <td>0.526582</td>\n",
       "      <td>0.607088</td>\n",
       "      <td>4.742599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.746363</td>\n",
       "      <td>0.720774</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.605568</td>\n",
       "      <td>4.184102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895635</td>\n",
       "      <td>0.745387</td>\n",
       "      <td>0.719370</td>\n",
       "      <td>0.523797</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>3.566281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.460714</td>\n",
       "      <td>0.557443</td>\n",
       "      <td>0.712337</td>\n",
       "      <td>0.787342</td>\n",
       "      <td>0.880103</td>\n",
       "      <td>0.591269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831746</td>\n",
       "      <td>0.736515</td>\n",
       "      <td>0.709592</td>\n",
       "      <td>0.515949</td>\n",
       "      <td>0.597530</td>\n",
       "      <td>2.934487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.732390</td>\n",
       "      <td>0.700670</td>\n",
       "      <td>0.502954</td>\n",
       "      <td>0.578624</td>\n",
       "      <td>2.379202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.597619</td>\n",
       "      <td>0.599317</td>\n",
       "      <td>0.681409</td>\n",
       "      <td>0.664895</td>\n",
       "      <td>0.768414</td>\n",
       "      <td>0.883788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.700763</td>\n",
       "      <td>0.668403</td>\n",
       "      <td>0.481097</td>\n",
       "      <td>0.558420</td>\n",
       "      <td>1.802713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.678968</td>\n",
       "      <td>0.672995</td>\n",
       "      <td>0.658672</td>\n",
       "      <td>0.448861</td>\n",
       "      <td>0.564397</td>\n",
       "      <td>1.213549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823016</td>\n",
       "      <td>0.634892</td>\n",
       "      <td>0.538957</td>\n",
       "      <td>0.474093</td>\n",
       "      <td>0.538712</td>\n",
       "      <td>1.168637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.594349</td>\n",
       "      <td>0.461711</td>\n",
       "      <td>0.405907</td>\n",
       "      <td>0.446193</td>\n",
       "      <td>0.589620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "61     10             122              3     0.900794    0.904187  0.918907   \n",
       "60      6             122              3     0.880159    0.903966  0.918868   \n",
       "58      4             122              3     0.877381    0.903699  0.918741   \n",
       "59      5             122              3     0.861508    0.897489  0.914315   \n",
       "12      6              12              1     0.889683    0.870076  0.885635   \n",
       "13     12              12              1     0.890079    0.871230  0.885470   \n",
       "11      5              12              1     0.860317    0.868657  0.885130   \n",
       "57      3             122              3     0.765079    0.856237  0.880920   \n",
       "10      4              12              1     0.832143    0.854462  0.874565   \n",
       "55     12              48              3     0.891270    0.852466  0.866928   \n",
       "54     10              48              3     0.894048    0.851890  0.866531   \n",
       "53      8              48              3     0.876587    0.850692  0.865672   \n",
       "52      6              48              3     0.893254    0.849406  0.864843   \n",
       "51      4              48              3     0.878968    0.848252  0.864068   \n",
       "16      4              24              1     0.835714    0.828646  0.849190   \n",
       "48     12              24              3     0.867460    0.843196  0.849080   \n",
       "17      5              24              1     0.848810    0.830997  0.847356   \n",
       "47     10              24              3     0.863492    0.838449  0.843905   \n",
       "50      3              48              3     0.787302    0.817335  0.843410   \n",
       "46      8              24              3     0.843651    0.836054  0.841292   \n",
       "45      6              24              3     0.863492    0.831663  0.836401   \n",
       "43      4              24              3     0.854365    0.828114  0.832461   \n",
       "44      5              24              3     0.832937    0.814940  0.818577   \n",
       "56      2             122              3     0.721429    0.789833  0.817291   \n",
       "15      3              24              1     0.734921    0.775417  0.813442   \n",
       "49      2              48              3     0.750000    0.786773  0.812058   \n",
       "24     12              48              1     0.897619    0.799104  0.797641   \n",
       "23     10              48              1     0.889683    0.797507  0.796233   \n",
       "22      8              48              1     0.882540    0.796398  0.795400   \n",
       "21      6              48              1     0.890873    0.794535  0.793712   \n",
       "20      4              48              1     0.866270    0.792007  0.791628   \n",
       "9       3              12              1     0.650000    0.716466  0.786990   \n",
       "19      3              48              1     0.759127    0.763219  0.786035   \n",
       "37      5              12              3     0.778968    0.786728  0.783755   \n",
       "40     12              12              3     0.801190    0.790366  0.783171   \n",
       "39     10              12              3     0.799603    0.788636  0.781091   \n",
       "38      8              12              3     0.789286    0.787660  0.779938   \n",
       "36      4              12              3     0.765873    0.773998  0.769842   \n",
       "7      12               1              1     0.871825    0.771647  0.757467   \n",
       "6      10               1              1     0.873016    0.770671  0.756224   \n",
       "35      3              12              3     0.758333    0.759315  0.755211   \n",
       "5       8               1              1     0.855159    0.768630  0.753590   \n",
       "4       6               1              1     0.868254    0.767920  0.752601   \n",
       "2       4               1              1     0.874206    0.767566  0.752129   \n",
       "14      2              24              1     0.629762    0.663946  0.747484   \n",
       "42      3              24              3     0.711111    0.711675  0.746431   \n",
       "18      2              48              1     0.635714    0.713671  0.738717   \n",
       "34      2              12              3     0.686508    0.727821  0.729334   \n",
       "32     12             122              1     0.896825    0.748891  0.724405   \n",
       "31     10             122              1     0.892460    0.747915  0.722956   \n",
       "30      8             122              1     0.896825    0.747072  0.721772   \n",
       "29      6             122              1     0.896825    0.746363  0.720774   \n",
       "27      4             122              1     0.895635    0.745387  0.719370   \n",
       "8       2              12              1     0.460714    0.557443  0.712337   \n",
       "3       5               1              1     0.831746    0.736515  0.709592   \n",
       "28      5             122              1     0.882143    0.732390  0.700670   \n",
       "41      2              24              3     0.597619    0.599317  0.681409   \n",
       "1       3               1              1     0.741667    0.700763  0.668403   \n",
       "0       2               1              1     0.678968    0.672995  0.658672   \n",
       "26      3             122              1     0.823016    0.634892  0.538957   \n",
       "25      2             122              1     0.761111    0.594349  0.461711   \n",
       "33      2               1              3     0.535714    0.430758  0.000000   \n",
       "\n",
       "    test_score_20  f1_score_20  time_taken  \n",
       "61       0.842700     0.907125    8.973320  \n",
       "60       0.842785     0.907373    7.430262  \n",
       "58       0.842700     0.907448    6.369843  \n",
       "59       0.833333     0.903020    4.242011  \n",
       "12       0.781772     0.863880    2.840094  \n",
       "13       0.776625     0.859359    4.722867  \n",
       "11       0.784304     0.866192    2.250494  \n",
       "57       0.778903     0.871190    2.193426  \n",
       "10       0.765654     0.856819    1.711961  \n",
       "55       0.740675     0.833685    8.744209  \n",
       "54       0.740000     0.833414    7.848405  \n",
       "53       0.738650     0.832821    6.886408  \n",
       "52       0.736962     0.832212    5.971076  \n",
       "51       0.735443     0.831606    5.028126  \n",
       "16       0.738143     0.833342    1.666660  \n",
       "48       0.719325     0.804261    8.582286  \n",
       "17       0.720591     0.818227    2.194746  \n",
       "47       0.710380     0.796923    7.708073  \n",
       "50       0.706076     0.820566    1.843658  \n",
       "46       0.705992     0.793283    6.840965  \n",
       "45       0.697806     0.786349    5.959406  \n",
       "43       0.691392     0.780874    5.088723  \n",
       "44       0.667848     0.762003    3.417473  \n",
       "56       0.675949     0.797147    1.059048  \n",
       "15       0.743544     0.839045    1.136762  \n",
       "49       0.657468     0.781762    0.942473  \n",
       "24       0.636540     0.729239    5.745863  \n",
       "23       0.634852     0.728118    5.177340  \n",
       "22       0.634008     0.727763    4.528214  \n",
       "21       0.632405     0.726588    3.943557  \n",
       "20       0.629620     0.725121    3.296505  \n",
       "9        0.770042     0.864394    1.128677  \n",
       "19       0.623038     0.750712    1.162712  \n",
       "37       0.620675     0.717171    3.363642  \n",
       "40       0.620084     0.709548    7.018722  \n",
       "39       0.617468     0.707076    6.185080  \n",
       "38       0.616034     0.705654    5.342499  \n",
       "36       0.602447     0.701552    2.538861  \n",
       "7        0.575949     0.664978    6.343552  \n",
       "6        0.574346     0.663329    5.769164  \n",
       "35       0.584726     0.687297    1.708174  \n",
       "5        0.571055     0.659932    5.233289  \n",
       "4        0.569873     0.658538    4.685276  \n",
       "2        0.569367     0.657999    4.072639  \n",
       "14       0.723122     0.831700    0.545619  \n",
       "42       0.662616     0.764074    1.679697  \n",
       "18       0.620844     0.754896    0.560952  \n",
       "34       0.550464     0.666875    0.858689  \n",
       "32       0.529789     0.610948    5.939553  \n",
       "31       0.528017     0.608744    5.366831  \n",
       "30       0.526582     0.607088    4.742599  \n",
       "29       0.525316     0.605568    4.184102  \n",
       "27       0.523797     0.603750    3.566281  \n",
       "8        0.787342     0.880103    0.591269  \n",
       "3        0.515949     0.597530    2.934487  \n",
       "28       0.502954     0.578624    2.379202  \n",
       "41       0.664895     0.768414    0.883788  \n",
       "1        0.481097     0.558420    1.802713  \n",
       "0        0.448861     0.564397    1.213549  \n",
       "26       0.474093     0.538712    1.168637  \n",
       "25       0.405907     0.446193    0.589620  \n",
       "33       0.181603     0.000000    0.901495  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.173729Z",
     "start_time": "2017-07-20T21:59:08.154489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.904187</td>\n",
       "      <td>0.918907</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.907125</td>\n",
       "      <td>8.973320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.890079</td>\n",
       "      <td>0.871230</td>\n",
       "      <td>0.885470</td>\n",
       "      <td>0.776625</td>\n",
       "      <td>0.859359</td>\n",
       "      <td>4.722867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.891270</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>0.866928</td>\n",
       "      <td>0.740675</td>\n",
       "      <td>0.833685</td>\n",
       "      <td>8.744209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">24</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.843196</td>\n",
       "      <td>0.849080</td>\n",
       "      <td>0.719325</td>\n",
       "      <td>0.804261</td>\n",
       "      <td>8.582286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.848810</td>\n",
       "      <td>0.830997</td>\n",
       "      <td>0.847356</td>\n",
       "      <td>0.720591</td>\n",
       "      <td>0.818227</td>\n",
       "      <td>2.194746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.799104</td>\n",
       "      <td>0.797641</td>\n",
       "      <td>0.636540</td>\n",
       "      <td>0.729239</td>\n",
       "      <td>5.745863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.801190</td>\n",
       "      <td>0.790366</td>\n",
       "      <td>0.783171</td>\n",
       "      <td>0.620084</td>\n",
       "      <td>0.709548</td>\n",
       "      <td>7.018722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.871825</td>\n",
       "      <td>0.771647</td>\n",
       "      <td>0.757467</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.664978</td>\n",
       "      <td>6.343552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.748891</td>\n",
       "      <td>0.724405</td>\n",
       "      <td>0.529789</td>\n",
       "      <td>0.610948</td>\n",
       "      <td>5.939553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "122            3                 10     0.900794    0.904187  0.918907   \n",
       "12             1                 12     0.890079    0.871230  0.885470   \n",
       "48             3                 12     0.891270    0.852466  0.866928   \n",
       "24             3                 12     0.867460    0.843196  0.849080   \n",
       "               1                  5     0.848810    0.830997  0.847356   \n",
       "48             1                 12     0.897619    0.799104  0.797641   \n",
       "12             3                 12     0.801190    0.790366  0.783171   \n",
       "1              1                 12     0.871825    0.771647  0.757467   \n",
       "122            1                 12     0.896825    0.748891  0.724405   \n",
       "1              3                  2     0.535714    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "122            3                   0.842700     0.907125    8.973320  \n",
       "12             1                   0.776625     0.859359    4.722867  \n",
       "48             3                   0.740675     0.833685    8.744209  \n",
       "24             3                   0.719325     0.804261    8.582286  \n",
       "               1                   0.720591     0.818227    2.194746  \n",
       "48             1                   0.636540     0.729239    5.745863  \n",
       "12             3                   0.620084     0.709548    7.018722  \n",
       "1              1                   0.575949     0.664978    6.343552  \n",
       "122            1                   0.529789     0.610948    5.939553  \n",
       "1              3                   0.181603     0.000000    0.901495  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.192661Z",
     "start_time": "2017-07-20T21:59:08.175034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>3</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.834392</td>\n",
       "      <td>0.875902</td>\n",
       "      <td>0.894840</td>\n",
       "      <td>0.802729</td>\n",
       "      <td>0.882217</td>\n",
       "      <td>5.044652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>3</th>\n",
       "      <td>6.428571</td>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>0.854787</td>\n",
       "      <td>0.722182</td>\n",
       "      <td>0.823724</td>\n",
       "      <td>5.323479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>1</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.763823</td>\n",
       "      <td>0.789722</td>\n",
       "      <td>0.838355</td>\n",
       "      <td>0.777623</td>\n",
       "      <td>0.865125</td>\n",
       "      <td>2.207560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>3</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.804266</td>\n",
       "      <td>0.787926</td>\n",
       "      <td>0.806195</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.782023</td>\n",
       "      <td>5.020051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>1</th>\n",
       "      <td>6.428571</td>\n",
       "      <td>0.831689</td>\n",
       "      <td>0.779492</td>\n",
       "      <td>0.785624</td>\n",
       "      <td>0.630187</td>\n",
       "      <td>0.734634</td>\n",
       "      <td>3.487878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>1</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.762302</td>\n",
       "      <td>0.774752</td>\n",
       "      <td>0.814368</td>\n",
       "      <td>0.731350</td>\n",
       "      <td>0.830578</td>\n",
       "      <td>1.385947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>3</th>\n",
       "      <td>6.285714</td>\n",
       "      <td>0.768537</td>\n",
       "      <td>0.773503</td>\n",
       "      <td>0.768906</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.699311</td>\n",
       "      <td>3.859381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.824355</td>\n",
       "      <td>0.744588</td>\n",
       "      <td>0.726085</td>\n",
       "      <td>0.538312</td>\n",
       "      <td>0.628140</td>\n",
       "      <td>4.006833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>1</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.868105</td>\n",
       "      <td>0.712157</td>\n",
       "      <td>0.663827</td>\n",
       "      <td>0.502057</td>\n",
       "      <td>0.574953</td>\n",
       "      <td>3.492103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "122            3              5.000000     0.834392    0.875902  0.894840   \n",
       "48             3              6.428571     0.853061    0.836688  0.854787   \n",
       "12             1              5.333333     0.763823    0.789722  0.838355   \n",
       "24             3              6.250000     0.804266    0.787926  0.806195   \n",
       "48             1              6.428571     0.831689    0.779492  0.785624   \n",
       "24             1              3.500000     0.762302    0.774752  0.814368   \n",
       "12             3              6.285714     0.768537    0.773503  0.768906   \n",
       "1              1              6.250000     0.824355    0.744588  0.726085   \n",
       "122            1              6.250000     0.868105    0.712157  0.663827   \n",
       "1              3              2.000000     0.535714    0.430758  0.000000   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "122            3                   0.802729     0.882217    5.044652  \n",
       "48             3                   0.722182     0.823724    5.323479  \n",
       "12             1                   0.777623     0.865125    2.207560  \n",
       "24             3                   0.690032     0.782023    5.020051  \n",
       "48             1                   0.630187     0.734634    3.487878  \n",
       "24             1                   0.731350     0.830578    1.385947  \n",
       "12             3                   0.601700     0.699311    3.859381  \n",
       "1              1                   0.538312     0.628140    4.006833  \n",
       "122            1                   0.502057     0.574953    3.492103  \n",
       "1              3                   0.181603     0.000000    0.901495  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.235642Z",
     "start_time": "2017-07-20T21:59:08.193977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions-__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.264992Z",
     "start_time": "2017-07-20T21:59:08.237136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465612</td>\n",
       "      <td>0.534388</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466659</td>\n",
       "      <td>0.533341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498665</td>\n",
       "      <td>0.501335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494189</td>\n",
       "      <td>0.505811</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415961</td>\n",
       "      <td>0.584039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536884</td>\n",
       "      <td>0.463116</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536820</td>\n",
       "      <td>0.463180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.417525</td>\n",
       "      <td>0.582475</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535082</td>\n",
       "      <td>0.464918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356850</td>\n",
       "      <td>0.643150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.482441</td>\n",
       "      <td>0.517559</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537098</td>\n",
       "      <td>0.462902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465069</td>\n",
       "      <td>0.534931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434034</td>\n",
       "      <td>0.565966</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470738</td>\n",
       "      <td>0.529262</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538382</td>\n",
       "      <td>0.461618</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535323</td>\n",
       "      <td>0.464677</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538310</td>\n",
       "      <td>0.461690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509452</td>\n",
       "      <td>0.490548</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463275</td>\n",
       "      <td>0.536725</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564154</td>\n",
       "      <td>0.435846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.387141</td>\n",
       "      <td>0.612859</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.464531</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530856</td>\n",
       "      <td>0.469144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381297</td>\n",
       "      <td>0.618703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501955</td>\n",
       "      <td>0.498045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531282</td>\n",
       "      <td>0.468718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528853</td>\n",
       "      <td>0.471146</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509347</td>\n",
       "      <td>0.490653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558312</td>\n",
       "      <td>0.441688</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527934</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534991</td>\n",
       "      <td>0.465009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501781</td>\n",
       "      <td>0.498219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.492546</td>\n",
       "      <td>0.507454</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.426652</td>\n",
       "      <td>0.573348</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534537</td>\n",
       "      <td>0.465463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.490374</td>\n",
       "      <td>0.509626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>0.462069</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504402</td>\n",
       "      <td>0.495598</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455980</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532966</td>\n",
       "      <td>0.467034</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532548</td>\n",
       "      <td>0.467452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532546</td>\n",
       "      <td>0.467454</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.505683</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532479</td>\n",
       "      <td>0.467521</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380570</td>\n",
       "      <td>0.619430</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528479</td>\n",
       "      <td>0.471521</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460451</td>\n",
       "      <td>0.539549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512679</td>\n",
       "      <td>0.487321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645221</td>\n",
       "      <td>0.354779</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532355</td>\n",
       "      <td>0.467645</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.429927</td>\n",
       "      <td>0.570073</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466919</td>\n",
       "      <td>0.533081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.464128</td>\n",
       "      <td>0.535873</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528099</td>\n",
       "      <td>0.471901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496140</td>\n",
       "      <td>0.503860</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563572</td>\n",
       "      <td>0.436428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451583</td>\n",
       "      <td>0.548417</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.492547</td>\n",
       "      <td>0.507453</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.429602</td>\n",
       "      <td>0.570398</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545056</td>\n",
       "      <td>0.454944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.480360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.529768</td>\n",
       "      <td>0.470232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.488050</td>\n",
       "      <td>0.511950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486881</td>\n",
       "      <td>0.513119</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525610</td>\n",
       "      <td>0.474390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.388912</td>\n",
       "      <td>0.611088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>0.588330</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458073</td>\n",
       "      <td>0.541928</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.472831</td>\n",
       "      <td>0.527170</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532044</td>\n",
       "      <td>0.467956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471912</td>\n",
       "      <td>0.528088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.484008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.484347</td>\n",
       "      <td>0.515653</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474474</td>\n",
       "      <td>0.525526</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500105</td>\n",
       "      <td>0.499895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527857</td>\n",
       "      <td>0.472143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535306</td>\n",
       "      <td>0.464694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22524</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>0.529640</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22525</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550045</td>\n",
       "      <td>0.449955</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525333</td>\n",
       "      <td>0.474667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22527</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528801</td>\n",
       "      <td>0.471199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447227</td>\n",
       "      <td>0.552773</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517336</td>\n",
       "      <td>0.482664</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460007</td>\n",
       "      <td>0.539993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450713</td>\n",
       "      <td>0.549287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534652</td>\n",
       "      <td>0.465348</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538359</td>\n",
       "      <td>0.461641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465699</td>\n",
       "      <td>0.534301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530789</td>\n",
       "      <td>0.469211</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22536</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.490663</td>\n",
       "      <td>0.509337</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540228</td>\n",
       "      <td>0.459772</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455775</td>\n",
       "      <td>0.544225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542064</td>\n",
       "      <td>0.457936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531799</td>\n",
       "      <td>0.468201</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526129</td>\n",
       "      <td>0.473871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534190</td>\n",
       "      <td>0.465810</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22543</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533462</td>\n",
       "      <td>0.466538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22544 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.465612     0.534388         1.0\n",
       "1         1.0     0.466659     0.533341         1.0\n",
       "2         0.0     0.498665     0.501335         1.0\n",
       "3         1.0     0.494189     0.505811         1.0\n",
       "4         1.0     0.415961     0.584039         1.0\n",
       "5         0.0     0.536884     0.463116         0.0\n",
       "6         0.0     0.536820     0.463180         0.0\n",
       "7         1.0     0.417525     0.582475         1.0\n",
       "8         0.0     0.535082     0.464918         0.0\n",
       "9         1.0     0.356850     0.643150         1.0\n",
       "10        1.0     0.482441     0.517559         1.0\n",
       "11        0.0     0.537098     0.462902         0.0\n",
       "12        1.0     0.465069     0.534931         1.0\n",
       "13        1.0     0.434034     0.565966         1.0\n",
       "14        0.0     0.470738     0.529262         1.0\n",
       "15        0.0     0.538382     0.461618         0.0\n",
       "16        0.0     0.535323     0.464677         0.0\n",
       "17        0.0     0.538310     0.461690         0.0\n",
       "18        0.0     0.509452     0.490548         0.0\n",
       "19        1.0     0.463275     0.536725         1.0\n",
       "20        1.0     0.564154     0.435846         0.0\n",
       "21        1.0     0.387141     0.612859         1.0\n",
       "22        0.0     0.535469     0.464531         0.0\n",
       "23        0.0     0.530856     0.469144         0.0\n",
       "24        1.0     0.381297     0.618703         1.0\n",
       "25        1.0     0.501955     0.498045         0.0\n",
       "26        0.0     0.531282     0.468718         0.0\n",
       "27        0.0     0.535897     0.464103         0.0\n",
       "28        1.0     0.528853     0.471146         0.0\n",
       "29        0.0     0.509347     0.490653         0.0\n",
       "30        1.0     0.558312     0.441688         0.0\n",
       "31        0.0     0.527934     0.472066         0.0\n",
       "32        0.0     0.534991     0.465009         0.0\n",
       "33        0.0     0.501781     0.498219         0.0\n",
       "34        1.0     0.492546     0.507454         1.0\n",
       "35        1.0     0.426652     0.573348         1.0\n",
       "36        0.0     0.534537     0.465463         0.0\n",
       "37        1.0     0.490374     0.509626         1.0\n",
       "38        0.0     0.537931     0.462069         0.0\n",
       "39        0.0     0.504402     0.495598         0.0\n",
       "40        1.0     0.455980     0.544021         1.0\n",
       "41        0.0     0.532966     0.467034         0.0\n",
       "42        0.0     0.532548     0.467452         0.0\n",
       "43        0.0     0.532546     0.467454         0.0\n",
       "44        1.0     0.494318     0.505683         1.0\n",
       "45        0.0     0.532479     0.467521         0.0\n",
       "46        1.0     0.380570     0.619430         1.0\n",
       "47        1.0     0.528479     0.471521         0.0\n",
       "48        1.0     0.460451     0.539549         1.0\n",
       "49        0.0     0.512679     0.487321         0.0\n",
       "...       ...          ...          ...         ...\n",
       "22494     1.0     0.645221     0.354779         0.0\n",
       "22495     0.0     0.532355     0.467645         0.0\n",
       "22496     1.0     0.429927     0.570073         1.0\n",
       "22497     1.0     0.466919     0.533081         1.0\n",
       "22498     1.0     0.464128     0.535873         1.0\n",
       "22499     0.0     0.528099     0.471901         0.0\n",
       "22500     1.0     0.463415     0.536585         1.0\n",
       "22501     1.0     0.496140     0.503860         1.0\n",
       "22502     1.0     0.563572     0.436428         0.0\n",
       "22503     1.0     0.451583     0.548417         1.0\n",
       "22504     1.0     0.492547     0.507453         1.0\n",
       "22505     1.0     0.429602     0.570398         1.0\n",
       "22506     0.0     0.545056     0.454944         0.0\n",
       "22507     0.0     0.519640     0.480360         0.0\n",
       "22508     0.0     0.529768     0.470232         0.0\n",
       "22509     1.0     0.488050     0.511950         1.0\n",
       "22510     1.0     0.486881     0.513119         1.0\n",
       "22511     0.0     0.525610     0.474390         0.0\n",
       "22512     1.0     0.388912     0.611088         1.0\n",
       "22513     1.0     0.411670     0.588330         1.0\n",
       "22514     0.0     0.458073     0.541928         1.0\n",
       "22515     1.0     0.472831     0.527170         1.0\n",
       "22516     0.0     0.532044     0.467956         0.0\n",
       "22517     1.0     0.471912     0.528088         1.0\n",
       "22518     0.0     0.515992     0.484008         0.0\n",
       "22519     1.0     0.484347     0.515653         1.0\n",
       "22520     1.0     0.474474     0.525526         1.0\n",
       "22521     1.0     0.500105     0.499895         0.0\n",
       "22522     1.0     0.527857     0.472143         0.0\n",
       "22523     0.0     0.535306     0.464694         0.0\n",
       "22524     1.0     0.470360     0.529640         1.0\n",
       "22525     1.0     0.550045     0.449955         0.0\n",
       "22526     0.0     0.525333     0.474667         0.0\n",
       "22527     0.0     0.528801     0.471199         0.0\n",
       "22528     1.0     0.447227     0.552773         1.0\n",
       "22529     0.0     0.517336     0.482664         0.0\n",
       "22530     1.0     0.460007     0.539993         1.0\n",
       "22531     1.0     0.450713     0.549287         1.0\n",
       "22532     0.0     0.534652     0.465348         0.0\n",
       "22533     0.0     0.538359     0.461641         0.0\n",
       "22534     1.0     0.465699     0.534301         1.0\n",
       "22535     0.0     0.530789     0.469211         0.0\n",
       "22536     1.0     0.490663     0.509337         1.0\n",
       "22537     1.0     0.540228     0.459772         0.0\n",
       "22538     1.0     0.455775     0.544225         1.0\n",
       "22539     0.0     0.542064     0.457936         0.0\n",
       "22540     0.0     0.531799     0.468201         0.0\n",
       "22541     1.0     0.526129     0.473871         0.0\n",
       "22542     0.0     0.534190     0.465810         0.0\n",
       "22543     1.0     0.533462     0.466538         0.0\n",
       "\n",
       "[22544 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoch_nof_hidden\n",
    "Train.predictions[\"12_12_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.293307Z",
     "start_time": "2017-07-20T21:59:08.266283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.489643</td>\n",
       "      <td>0.510357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479050</td>\n",
       "      <td>0.520950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684754</td>\n",
       "      <td>0.315246</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509620</td>\n",
       "      <td>0.490380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.520928</td>\n",
       "      <td>0.479072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.522766</td>\n",
       "      <td>0.477234</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430324</td>\n",
       "      <td>0.569676</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465598</td>\n",
       "      <td>0.534402</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510822</td>\n",
       "      <td>0.489178</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510271</td>\n",
       "      <td>0.489729</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451928</td>\n",
       "      <td>0.548072</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.453405</td>\n",
       "      <td>0.546595</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498727</td>\n",
       "      <td>0.501273</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504635</td>\n",
       "      <td>0.495365</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644952</td>\n",
       "      <td>0.355048</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450944</td>\n",
       "      <td>0.549056</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.480414</td>\n",
       "      <td>0.519585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.601628</td>\n",
       "      <td>0.398372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647314</td>\n",
       "      <td>0.352686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499609</td>\n",
       "      <td>0.500391</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648090</td>\n",
       "      <td>0.351909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495108</td>\n",
       "      <td>0.504892</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481593</td>\n",
       "      <td>0.518407</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503205</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.475136</td>\n",
       "      <td>0.524864</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504299</td>\n",
       "      <td>0.495701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528865</td>\n",
       "      <td>0.471135</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.426849</td>\n",
       "      <td>0.573151</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537462</td>\n",
       "      <td>0.462538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.467430</td>\n",
       "      <td>0.532570</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496381</td>\n",
       "      <td>0.503619</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499153</td>\n",
       "      <td>0.500847</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543968</td>\n",
       "      <td>0.456032</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505080</td>\n",
       "      <td>0.494920</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514504</td>\n",
       "      <td>0.485496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512005</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460380</td>\n",
       "      <td>0.539620</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471058</td>\n",
       "      <td>0.528942</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514421</td>\n",
       "      <td>0.485579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575477</td>\n",
       "      <td>0.424523</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548408</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.513449</td>\n",
       "      <td>0.486551</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516054</td>\n",
       "      <td>0.483946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553068</td>\n",
       "      <td>0.446932</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465958</td>\n",
       "      <td>0.534042</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.541120</td>\n",
       "      <td>0.458880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511318</td>\n",
       "      <td>0.488682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504134</td>\n",
       "      <td>0.495866</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536648</td>\n",
       "      <td>0.463352</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.454423</td>\n",
       "      <td>0.545577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11800</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495605</td>\n",
       "      <td>0.504395</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511366</td>\n",
       "      <td>0.488634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510542</td>\n",
       "      <td>0.489458</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504050</td>\n",
       "      <td>0.495950</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496547</td>\n",
       "      <td>0.503453</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11805</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499716</td>\n",
       "      <td>0.500284</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543988</td>\n",
       "      <td>0.456012</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507104</td>\n",
       "      <td>0.492896</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391811</td>\n",
       "      <td>0.608189</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.408784</td>\n",
       "      <td>0.591216</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501277</td>\n",
       "      <td>0.498723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507633</td>\n",
       "      <td>0.492367</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11813</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500593</td>\n",
       "      <td>0.499407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11815</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463372</td>\n",
       "      <td>0.536628</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.502815</td>\n",
       "      <td>0.497185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545882</td>\n",
       "      <td>0.454118</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517556</td>\n",
       "      <td>0.482444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540466</td>\n",
       "      <td>0.459534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505555</td>\n",
       "      <td>0.494445</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.412664</td>\n",
       "      <td>0.587336</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.485760</td>\n",
       "      <td>0.514240</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539608</td>\n",
       "      <td>0.460392</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546513</td>\n",
       "      <td>0.453487</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.529273</td>\n",
       "      <td>0.470727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.463633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.524294</td>\n",
       "      <td>0.475706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.502533</td>\n",
       "      <td>0.497467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526854</td>\n",
       "      <td>0.473146</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495059</td>\n",
       "      <td>0.504941</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.537140</td>\n",
       "      <td>0.462860</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503573</td>\n",
       "      <td>0.496427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.491526</td>\n",
       "      <td>0.508474</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544218</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.410477</td>\n",
       "      <td>0.589523</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.483483</td>\n",
       "      <td>0.516517</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283149</td>\n",
       "      <td>0.716851</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490015</td>\n",
       "      <td>0.509985</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540075</td>\n",
       "      <td>0.459925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.551674</td>\n",
       "      <td>0.448326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.449982</td>\n",
       "      <td>0.550018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498836</td>\n",
       "      <td>0.501164</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.453554</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527059</td>\n",
       "      <td>0.472941</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.412297</td>\n",
       "      <td>0.587703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527740</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505555</td>\n",
       "      <td>0.494445</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11850 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.489643     0.510357         1.0\n",
       "1         1.0     0.479050     0.520950         1.0\n",
       "2         1.0     0.684754     0.315246         0.0\n",
       "3         0.0     0.509620     0.490380         0.0\n",
       "4         1.0     0.520928     0.479072         0.0\n",
       "5         1.0     0.522766     0.477234         0.0\n",
       "6         1.0     0.430324     0.569676         1.0\n",
       "7         1.0     0.465598     0.534402         1.0\n",
       "8         0.0     0.510822     0.489178         0.0\n",
       "9         0.0     0.510271     0.489729         0.0\n",
       "10        1.0     0.451928     0.548072         1.0\n",
       "11        1.0     0.453405     0.546595         1.0\n",
       "12        1.0     0.498727     0.501273         1.0\n",
       "13        1.0     0.504635     0.495365         0.0\n",
       "14        1.0     0.644952     0.355048         0.0\n",
       "15        1.0     0.450944     0.549056         1.0\n",
       "16        1.0     0.480414     0.519585         1.0\n",
       "17        1.0     0.601628     0.398372         0.0\n",
       "18        1.0     0.647314     0.352686         0.0\n",
       "19        1.0     0.499609     0.500391         1.0\n",
       "20        1.0     0.648090     0.351909         0.0\n",
       "21        1.0     0.495108     0.504892         1.0\n",
       "22        1.0     0.481593     0.518407         1.0\n",
       "23        1.0     0.503205     0.496795         0.0\n",
       "24        1.0     0.475136     0.524864         1.0\n",
       "25        1.0     0.504299     0.495701         0.0\n",
       "26        1.0     0.528865     0.471135         0.0\n",
       "27        1.0     0.426849     0.573151         1.0\n",
       "28        0.0     0.537462     0.462538         0.0\n",
       "29        1.0     0.467430     0.532570         1.0\n",
       "30        1.0     0.496381     0.503619         1.0\n",
       "31        1.0     0.499153     0.500847         1.0\n",
       "32        0.0     0.543968     0.456032         0.0\n",
       "33        0.0     0.505080     0.494920         0.0\n",
       "34        1.0     0.514504     0.485496         0.0\n",
       "35        0.0     0.512005     0.487996         0.0\n",
       "36        1.0     0.460380     0.539620         1.0\n",
       "37        1.0     0.471058     0.528942         1.0\n",
       "38        1.0     0.514421     0.485579         0.0\n",
       "39        1.0     0.575477     0.424523         0.0\n",
       "40        0.0     0.548408     0.451592         0.0\n",
       "41        1.0     0.513449     0.486551         0.0\n",
       "42        1.0     0.516054     0.483946         0.0\n",
       "43        0.0     0.553068     0.446932         0.0\n",
       "44        1.0     0.465958     0.534042         1.0\n",
       "45        1.0     0.541120     0.458880         0.0\n",
       "46        1.0     0.511318     0.488682         0.0\n",
       "47        1.0     0.504134     0.495866         0.0\n",
       "48        1.0     0.536648     0.463352         0.0\n",
       "49        1.0     0.454423     0.545577         1.0\n",
       "...       ...          ...          ...         ...\n",
       "11800     1.0     0.495605     0.504395         1.0\n",
       "11801     1.0     0.511366     0.488634         0.0\n",
       "11802     0.0     0.510542     0.489458         0.0\n",
       "11803     1.0     0.504050     0.495950         0.0\n",
       "11804     1.0     0.496547     0.503453         1.0\n",
       "11805     1.0     0.499716     0.500284         1.0\n",
       "11806     0.0     0.543988     0.456012         0.0\n",
       "11807     1.0     0.507104     0.492896         0.0\n",
       "11808     1.0     0.391811     0.608189         1.0\n",
       "11809     1.0     0.408784     0.591216         1.0\n",
       "11810     1.0     0.485714     0.514286         1.0\n",
       "11811     1.0     0.501277     0.498723         0.0\n",
       "11812     0.0     0.507633     0.492367         0.0\n",
       "11813     1.0     0.500593     0.499407         0.0\n",
       "11814     1.0     0.765074     0.234926         0.0\n",
       "11815     1.0     0.463372     0.536628         1.0\n",
       "11816     1.0     0.502815     0.497185         0.0\n",
       "11817     0.0     0.545882     0.454118         0.0\n",
       "11818     0.0     0.517556     0.482444         0.0\n",
       "11819     1.0     0.540466     0.459534         0.0\n",
       "11820     1.0     0.505555     0.494445         0.0\n",
       "11821     1.0     0.412664     0.587336         1.0\n",
       "11822     1.0     0.485760     0.514240         1.0\n",
       "11823     1.0     0.539608     0.460392         0.0\n",
       "11824     1.0     0.546513     0.453487         0.0\n",
       "11825     0.0     0.529273     0.470727         0.0\n",
       "11826     1.0     0.536367     0.463633         0.0\n",
       "11827     1.0     0.524294     0.475706         0.0\n",
       "11828     1.0     0.502533     0.497467         0.0\n",
       "11829     1.0     0.526854     0.473146         0.0\n",
       "11830     1.0     0.518400     0.481600         0.0\n",
       "11831     1.0     0.495059     0.504941         1.0\n",
       "11832     1.0     0.537140     0.462860         0.0\n",
       "11833     0.0     0.503573     0.496427         0.0\n",
       "11834     1.0     0.491526     0.508474         1.0\n",
       "11835     0.0     0.544218     0.455782         0.0\n",
       "11836     1.0     0.410477     0.589523         1.0\n",
       "11837     1.0     0.483483     0.516517         1.0\n",
       "11838     1.0     0.506849     0.493151         0.0\n",
       "11839     1.0     0.283149     0.716851         1.0\n",
       "11840     0.0     0.490015     0.509985         1.0\n",
       "11841     0.0     0.540075     0.459925         0.0\n",
       "11842     1.0     0.551674     0.448326         0.0\n",
       "11843     1.0     0.449982     0.550018         1.0\n",
       "11844     1.0     0.498836     0.501164         1.0\n",
       "11845     0.0     0.546445     0.453554         0.0\n",
       "11846     0.0     0.527059     0.472941         0.0\n",
       "11847     1.0     0.412297     0.587703         1.0\n",
       "11848     1.0     0.527740     0.472260         0.0\n",
       "11849     1.0     0.505555     0.494445         0.0\n",
       "\n",
       "[11850 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"12_12_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.301152Z",
     "start_time": "2017-07-20T21:59:08.294596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions[\"12_12_3\"].dropna()\n",
    "df_ = Train.predictions_[\"12_12_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.308730Z",
     "start_time": "2017-07-20T21:59:08.302473Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.353915Z",
     "start_time": "2017-07-20T21:59:08.310012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.790366</td>\n",
       "      <td>0.783171</td>\n",
       "      <td>0.952248</td>\n",
       "      <td>0.665082</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.620084</td>\n",
       "      <td>0.709548</td>\n",
       "      <td>0.947777</td>\n",
       "      <td>0.567024</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision    Recall      Scenario\n",
       "0  0.790366  0.783171   0.952248  0.665082  Train+/Test+\n",
       "1  0.620084  0.709548   0.947777  0.567024  Train+/Test-"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:08.360516Z",
     "start_time": "2017-07-20T21:59:08.355240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:09.701963Z",
     "start_time": "2017-07-20T21:59:08.361891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGhCAYAAAAJL0FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKiEf4fzM3PkkpyKv4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a5b2ba908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:09.709516Z",
     "start_time": "2017-07-20T21:59:09.703556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:09.976444Z",
     "start_time": "2017-07-20T21:59:09.710937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGhCAYAAAAJL0FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4VNXWx/HvSqFJ70hXQaRLFwFpKl4LKopYsHHhKlYQ\nsVxeO4q9l4siYgWsoAgiWFCRDlJUEKQoIL2DQJL1/jGHGAKEAJOEnPl97jMPM/u0da5J1qx99tnH\n3B0REZHcLi6nAxAREYkGJTQREQkFJTQREQkFJTQREQkFJTQREQkFJTQREQkFJTQREQkFJTQREQkF\nJTQREQmFhJwOQEREsk584cruSTuisi/fseYLd+8QlZ1lASU0EZEQ86Qd5D2xc1T29fesF0tGZUdZ\nRAlNRCTUDCw2ri7FxlmKiEjoqUITEQkzA8xyOopsoYQmIhJ26nIUERHJPVShiYiEnbocRUQk99Mo\nRxERkVxFFZqISNipy1FERHI9Q12OIiIiuYkqNBGRUDN1OYqISEioy1FERCT3UIUmIhJ26nIUEZHc\nTzdWi4iI5Cqq0EREwiyGHh+jCk1EREJBFZqISNjFyDU0JTQRkVDToBAREZFcRRWaiEjYxcXGoBAl\nNBGRMNNs+yIiIrmLKjQRkbCLkfvQlNBEREJNoxxFRERyFVVoIiJhpy5HEREJBXU5ioiI5B6q0ERE\nwswsZrocVaGJiEgoKKGJiISdxUXnlZlDmS0xszlmNsvMpgVtxc3sSzP7Lfi3WJr17zKzhWY238zO\nTNPeMNjPQjN7zuzgZaYSmoSameU3s0/NbJOZvX8E+7nczMZGM7acYmYtzWx+Tsch2WhPt+ORvjKv\njbvXd/dGwec7gfHuXg0YH3zGzGoCXYBaQAfgJTOLD7Z5GegOVAteHQ52UCU0OSqY2WVmNs3MtprZ\nSjMbbWYtorDri4AyQAl3v/hwd+Lu77j7GVGIJ0uZmZvZCRmt4+7fufuJ2RWTCNARGBK8HwKcn6Z9\nqLvvdPfFwEKgiZmVAwq7+yR3d+DNNNsckBKa5Dgz6w08AzxMJPlUAl4EzovC7isDC9w9KQr7yvXM\nTAPBYo5Fs8uxZPDFc8+rx34O6MA4M5ueZnkZd18ZvP+LyO85QHngjzTb/hm0lQ/ep2/PkBKa5Cgz\nKwI8ANzg7h+5+zZ33+3un7l732CdvGb2jJmtCF7PmFneYFlrM/vTzG4zs9VBdXdNsOx+4B7gkqDy\n62Zm95nZ22mOXyWoahKCz1eb2e9mtsXMFpvZ5Wnav0+zXXMzmxp0ZU41s+Zpln1jZg+a2Q/Bfsaa\nWckDnP+e+Pumif98M/uXmS0ws/Vmdnea9ZuY2Y9mtjFY9wUzyxMsmxCs9lNwvpek2f8dZvYXMHhP\nW7DN8cExGgSfjzWzNWbW+oj+w8rRJXpdjmvdvVGa18D9HK2Fu9cHzgJuMLNWaRcGFZdnxWkqoUlO\nOwXIB3ycwTr/BZoB9YF6QBOgX5rlZYEiRL7BdQNeNLNi7n4vkapvmLsXdPdBGQViZscAzwFnuXsh\noDkwaz/rFQdGBeuWAJ4CRplZiTSrXQZcA5QG8gB9Mjh0WSL/H5QnkoBfBa4AGgItgf8zs6rBuslA\nL6Akkf/v2gE9Adx9zx+OesH5Dkuz/+JEqtW9vlG7+yLgDuBtMysADAaGuPs3GcQrckDuvjz4dzWR\n3+smwKqgG5Hg39XB6suBimk2rxC0LQ/ep2/PkBKa5LQSRL71ZdQleDnwgLuvdvc1wP1A1zTLdwfL\nd7v758BW4HCvEaUAtc0sv7uvdPd5+1nnbOA3d3/L3ZPc/T3gV+DcNOsMdvcF7r4DGE4kGR/IbqC/\nu+8GhhJJVs+6+5bg+D8TSeS4+/TgukKSuy8B/geclolzuje4TrEj/UJ3f5XItYvJQDkiXyAkLPY8\nDy0bRjma2TFmVmjPe+AMYC4wErgqWO0qYETwfiTQJeiFqUpk8MeUoHtys5k1C0Y3XplmmwNSf7rk\ntHVE+uUTMkhqxwJL03xeGrSl7iPdttuBgocaiLtvM7NLiFRTg8zsB+A2d//1IPHsiSltH/9fhxDP\nOndPDt7vSTir0izfsWd7M6tOpCJsBBQg8js8PaPzAta4+98HWedVIn9cerj7zoOsK7lKts62Xwb4\nOBhhnwC86+5jzGwqMNzMuhH5XekM4O7zzGw4kS9tSUQuPez5XegJvAHkB0YHrwwpoUlO+xHYSWQE\n0wcHWGcFke6yPdVSpaDtcGwjkgj2KJt2obt/AXxhZvmBh4j8oW95gHjSqgSMOcyYDsXLwEzgUnff\nYma3EhnJmZEMr1eYWUEig3IGAfeZ2Yfuvj4q0UpMcfffCXoT0rWvI9I9vr9t+gP999M+Dah9KMdX\nl6PkKHffROS60YvBYIgCZpZoZmeZ2WPBau8B/cysVDC44h7g7QPt8yBmAa3MrFIwIOWuPQvMrIyZ\ndQy6SnYS6bpM2c8+PgeqW+RWg4SgqqsJfHaYMR2KQsBmYKuZ1QCuT7d8FXDcIe7zWWCau/+byLXB\nV444Sjm6ZP99aDlCCU1ynLs/CfQmMtBjDZFhvDcCnwSrPARMA2YDc4AZQdvhHOtLYFiwr+nsnYTi\ngjhWAOuJXJtKnzD2fNs8B7iNSJdpX+Acd197ODEdoj5EBpxsIVI9Dku3/D5gSDAKsvPBdmZmHYnc\nsLrnPHsDDfaM7pSQyMaZQnKSRUZQiohIGMUVrex5T7v74Ctmwt8jr5ueZvaPo46uoYmIhF0u6C6M\nBiU0EZEws2wd5ZijYuMsRUQk9FShiYiEnbocY0ORYiW89LEVD76iyEEUzhfzv04SJTNmTF/r7qVy\nOo7cJuZ/A0sfW5HnhofiMVeSw9qcWDqnQ5CQyJ9o6WeiOSKmCk1ERHI7I3YSmgaFiIhIKKhCExEJ\nMwteMUAJTUQk1ExdjiIiIrmJKjQRkZCLlQpNCU1EJORiJaGpy1FEREJBFZqISMjFSoWmhCYiEmYx\nNGxfXY4iIhIKqtBERELMYug+NCU0EZGQi5WEpi5HEREJBVVoIiIhpwpNREQkF1GFJiIScrFSoSmh\niYiEme5DExERyV1UoYmIhJy6HEVEJNeLpRur1eUoIiKhoApNRCTkYqVCU0ITEQm72Mhn6nIUEZFw\nUIUmIhJmpi5HEREJiVhJaOpyFBGRUFCFJiIScrFSoSmhiYiEmG6sFhERyWVUoYmIhF1sFGiq0ERE\nJBxUoYmIhJnuQxMRkbCIlYSmLkcREQkFVWgiIiEXKxWaEpqISNjFRj5Tl6OIiISDKjQRkZBTl6OI\niOR6Zpr6SkQk0/7++29anNKEJg3q0aBeLR68/97UZevXr+fsDqdT+6RqnN3hdDZs2ADA1ClTaNqw\nPk0b1qdJg3qM+OTjnApfQkIJTUSOWN68eRnz5VdMmfETk6fNYuwXY5g8aRIATzw2gNZt2zH3l99o\n3bYdTzw2AIBatWvzw+RpTJ4+ixGjxnBTz/+QlJSUk6cRWnuqtCN9He2U0ETkiJkZBQsWBGD37t0k\n7d6d+gfws09HcEXXqwC4outVfDryEwAKFChAQkLkqsfOv//OFX8wcyslNBGRQ5CcnEzThvWpdGxp\n2rY/nSZNmwKwetUqypUrB0DZsmVZvWpV6jZTJk+mQb1aNDq5Ds+9+EpqghM5HEpoIhIV8fHxTJ4+\ni4VL/mTa1CnMmzt3n3XSf9Nv0rQpM36ax/c/TuXxRx/h77//zs6QY4dF6XWUU0ITkagqWrQop7Vu\nw9ixYwAoXaYMK1euBGDlypWUKl16n21qnHQSBQsW3G8SFMksJTQROWJr1qxh48aNAOzYsYPx477k\nxBNrAHD2Oefx9ltDAHj7rSGcc25HAJYsXpw6CGTp0qXMn/8rlatUyf7gY0CsXENTh7WIHLG/Vq6k\n+7VXkZycTIqn0Omizvzr7HMA6NP3Tq64tDNDBg+iUqXKvP3ecAAm/vA9Tzw+gMSEROLi4nj2+Zco\nWbJkTp5GOOnxMSIimVenbl0mTZu532UlSpRg9Njx+7RfdkVXLruia1aHJjFECU1EJMQMiJECTQlN\nRCTccsf1r2jQoBAREQkFJbQY9HS/W7i0VU2uP7/VXu2Lfp1Lr8vO4sZObbm58xnMnzNjr+WrV/7J\nhY2r8uHgl1Lbvh39CT0vaM11HVvx+lMPZkv8cvhOPKEKjerXSZ1D8ceJEzNcv2TRgkd8zO7XXk2N\nalVp2rA+pzRuwKQffzzkfXz26UgeD6bMGjniE375+efUZQ/cdw9fjR93xHGGmVl0Xkc7JbQY1P78\nLjz4ytB92l9/8gEuu74PL3z4FV1v7MvrT+6doF597F4atWyX+nnzxvW8/uQDPDLoA14ZMYENa1cz\na9KELI9fjsyYcV8zefosJk+fxSnNm2fLMR8e8DiTp8/iwYcHcFPP/xzy9uecex63970TgE9HfMIv\nv/yT0O657wHatmsftVjDKFaG7SuhxaA6jU6hUJGi+7SbGdu3bgFg29bNFC9dJnXZxPGfU7Z8JSod\nf2Jq219/LOXYylUpUjwy1Lp+s1b88OWoLI5eom3r1q2cdUY7TmncgEb16/DpyBH7rLNy5Urat2lF\n04b1aVi/Nt9//x0A474cy2ktTuGUxg24rMvFbN26NcNjtWjZikWLFgLw06xZtDq1GY1Prkvniy5I\nnYX/xeef4+S6NWl8cl26Xt4FgLeGvMGtN9/IjxMnMuqzkdx95+00bVif3xctovu1V/PRhx8w9osx\nXNbl4tRjTfj2Gy7seM5hxSm5kxKapOpxx4O8/uQDXNnuZAY9cT9X3/pfAHZs38YHr7/AZT377LV+\nuUpV+XPJIlYtX0ZyUhI/fjWaNX8tz4nQ5RB0aN+Gpg3r07J5ZK7FfPnyMeyDj/lx6gzGjPuaO/ve\nhrvvtc2woe9y+hlnMnn6LKZM/4l69eqzdu1aBjz8EJ9/MY4fp86gQcNGPPfMUxkee9Rnn1Krdh0A\n/n3NlfR/5FGmzpxN7dp16P/g/QA88fgAJk2dydSZs3n+xVf22v6U5s05+5zzUiu+444/PnVZ23bt\nmTplMtu2bQPgg+HDuLhzl8OKM1Si1N2YCwo0jXKUf3w+7A263/EALU4/hwljRvDsPb14+LUPeOfF\nxzm/63/IX+CYvdYvVKQoN/7fozzSpwdxcXGcVL8xK/9YkjPBS6aNGff1Xjcwuzv39LubH76bQFxc\nHCuWL2fVqlWULVs2dZ1GjRrzn+7Xsnv3bs4973zq1a/PdxO+5ddffqZtq1MB2LV7F02bnrLfY959\n5+08+vBDlCxVilcGDmLTpk1s3LSRlq1OAyKz8F8eVFd16tTl6isv57zzzufcjudn+rwSEhI444wO\njPrsUy7sdBGjR4+i/4DHDinOMDIgLi4XZKMoUEKTVONGDuc/d/UHoOWZ5/Hsvb0BmD9nBt9/+Rmv\nP/Ug27ZswiyOPHnzcu5l3Wja+kyatj4TgNHvv0lcXHyOxS+HZ+i777B27RomTplOYmIiJ55QhZ3p\nJglu0bIVX341gTGfj6JHt6u5+dbeFC1WjLbtT+fNt9876DEeHvA4F3a6KPXzpk2bDrjuxyNH8f13\nExj12ac8OqA/02bOyfS5XHxJF15+6QWKFy9Og4aNKFSoEO6e6TglOswsHpgGLHf3c8ysODAMqAIs\nATq7+4Zg3buAbkAycLO7fxG0NwTeAPIDnwO3ePqug3TU5SipSpQqy5ypkVFvP03+jvKVjwPg8TdH\n8sbYabwxdhodr+jBJd1v4dzLugGwcd0aALZs2siooW9wZqfLcyZ4OWybNm2iVKnSJCYm8u03X7Ns\n6dJ91lm6dCllypTh2n935+pr/83MmTNo0rQZP078gUULI9fEtm3bxm8LFmTqmEWKFKFY0WKp1+Le\nfectWrQ6jZSUFP784w9Oa92G/o88yqZNm/a53lWwUCG2btmy3/22bHUas2bO4PVBr3Jx58j1tyOJ\nMyxyoMvxFuCXNJ/vBMa7ezVgfPAZM6sJdAFqAR2Al4JkCPAy0B2oFrw6HOygqtBi0KO3/4fZUyey\neeN6urarzxU9b+fMTpdz8/1P8r8B/UhOSiIxb15uuveJg+7rfwP68fv8yIizy67rTYUqxx9kCzna\ndLnscjqdfy6N6tehQcNGnFijxj7rfPftNzz91OMkJiRyTMGCDBr8JqVKleLVQW9w5RWXsmvnTgDu\nfeAhqlWvnqnjvvr6EG664Tp2bN9OleOOY+Brg0lOTuaaq65g86ZNOE7PG2+maNG9BzBd3LkLN1zf\nnZdeeI53h32w17L4+HjO+tc5vP3mG7z2emRC5CONMwyyc4SimVUAzgb6A72D5o5A6+D9EOAb4I6g\nfai77wQWm9lCoImZLQEKu/ukYJ9vAucDozM89kEquNCrVqu+Pzd8bE6HISHQ5sR9H4sicjjyJ9p0\nd28UlX2Vq+7Hd3sxGrtiXv8zlgJr0zQNdPeBadcxsw+AR4BCQJ+gy3GjuxcNlhuwwd2LmtkLwCR3\nfztYNohI0loCDHD39kF7S+AOdz8no/hUoYmIhFl0RyiuzSjRmtk5wGp3n25mrfe3jru7mWVJJaWE\nJiIi0XIqcJ6Z/QvIBxQ2s7eBVWZWzt1Xmlk5YHWw/nKgYprtKwRty4P36dszpEEhR7lbL+3AjZ3a\nclX7BnRpWZMbO7Xlxk5tWbV8WVSPs2LZYv5Vuwyjhr6R2vb8/bfz1acfHHijw7Bl0wZGDRuS+nnN\nyuU8clv3qB5DMq9l86Y0bVifasdVomK5UqlTYi1dsiRLjnffPf14/tlnALjmyisYOeKTfda55sor\nUqfKatqwPu1at8ySWGJFZLb97JkpxN3vcvcK7l6FyGCPr9z9CmAkcFWw2lXAnrv3RwJdzCyvmVUl\nMvhjiruvBDabWbOgi/LKNNsckCq0o9wz70UeY//lJ0P5bd5P9PzvI/tdLzk5mfj4IxsyX6xEKT5+\n6390uOgK4hOy5kdjy6aNjB4+hLMvifxslypXnruefDVLjiUH993EyUBkJo7p06fxzHMv5HBEEY89\n8TTnZXAPWlJSEglpfkbTf87sdrHhqJi2agAw3My6AUuBzgDuPs/MhgM/A0nADe6eHGzTk3+G7Y/m\nIANCQBVarpWclMTFp1TjfwP60fOC1iyYM4Ou7eqzdXPk/p5ff5rG3f+O3PezY/s2nvrvzdza5Uxu\nvKgdk7/5Yr/7LFayNLUbNGX8p+/vs2z50t/p1+MSbu58Ordf1ZE/lyxKbb/10g5cf8FpDHn2YS4+\npRoA27du4c5rL+Smi9vT84LWTP4mMvBm8NMP8eeSRdzYqS2Dn36QFcsWc2OntgDc3PkM/ly8MPWY\nfbqey6Jf52Y6fomeQa8O5M6+/8wMM/CVl7nrjttZtHAhDerVouvlXahf5yQuv7QzO3bsAGDa1Kmc\n3vY0mjdpSMdzzmLVqlVRjem+e/rR7eoradPqVLpfezWDB73GxZ3O58z2bTj3X2eSkpJC3z69aVi/\nNo3q1+GjDyO9C1+NH8cZ7VpzYcdzaHRynajGJAfm7t/sGcTh7uvcvZ27V3P39u6+Ps16/d39eHc/\n0d1Hp2mf5u61g2U3HuweNFBCy9W2bdlM7Yan8NLH33BS/cYHXO/dl5+k4alteGboFzwy6ENeffw+\ndu38e7/rXvzvm/lw8EukpKTs1f78fbdxQ79HeW74l1x9y395uf9dALzy8N10uronL3/8LcVL/TP3\nY568+bjnuSE8//44Hn7tfV597B4ArunVjwpVjueFD7/iml7/t9cxWnXoyIQvRgKw9q8VbNm0keNr\n1D6k+CU6Lr6kCyNHfExSUhIAbw4ZzFVXXwvALz//zI033cqsOb+QL28+Xhv4P3bu3Emf3rfw3vAP\nmThlOl0uu4IH7v2/jA6Rob59eqV2OXa7+srU9vnzf2X02PEMfvNtAH6aNZOh73/E6LHj+fCD95n/\n6y9Mmf4Tn435kr59erF6deRSzYzp03jm+ZeYNeeX/R4v7DT11REKRrE85e63BZ/7AAXd/b6sOuZ+\nYngD+Mzdo3sh6CiRkJiH5u3/ddD1Zk78hmnfj+f9Qc8DsHvnTlavXL7fe8bKVz6O42rUYsKYf65t\nbN28iV9nz6B/r2tT25KTI3/o5s+Zwf0vvwtA67Mv5M3nI4/4cJzBTz/EvBmTiYuLY81fK9i0YV2G\ncbbscB7339CVy67rzYQxI2h5xrmHHL9ER+HChWnRohVfjBlN1arHER8fT42TTmLRwoVUqVqVps2a\nAXDp5Vcw6LWBtDqtNb/8PI+zz4zMep+cnEz5ChUyOkSGDtTleO55HcmXL1/q5/btz6BYsWIATPzh\nezpfcinx8fGULVuW5qe2YMb0aeTJk4emzU6hUqVKhx1PbncUdDlmi6zsTN4JXGhmj7j72oOunY6Z\nJbh7UhbEFRp58+Xb6wc1Pj4B90hltecGUgjm6nt2COUqVcnUfrv0uJXH+l5PjXoNU7cvXLQ4L3z4\nVaZjGz9iONu2bub598cRn5BA13b12Z0mpv0pc2xF8hc4hmWL5jNhzCf07v/cYcUv0XH1tf/muWef\nonLlKlx51TWp7en/OJoZ7k7tOnUZ/813WRpTgXTziRY45pgDrJluu0yuJ7lbVnY5JgEDgV7pF5hZ\nFTP7ysxmm9l4M6sUtL9hZq+Y2WTgMTO7z8yGmNl3ZrbUzC40s8fMbI6ZjTGzxGC7e8xsqpnNNbOB\nFitfR9Ipc2xFfps3G4Afxn2W2t7g1DaMfPe11M+Lfsl4brzKJ9SgXMUqTPsuksAKFSlK8VKlmTju\ncwBSUlL4/dd5AFSvfTITx0favx39T1W3fesWihYvSXxCAjMmfsu6VSsByF/gGLZvO/CjO1p26Miw\nV59j965dqY+qOdT4JTqan3oqixct4qMP3+eizpekti9ZvJhpU6cCMOy9d2nevAUn1azJihXLmTpl\nCgC7du3i53nzsjXeU1u05P3hQ0lJSWHVqlX8OPEHGjSMyr3JuVsMzbaf1dfQXgQuN7Mi6dqfB4a4\ne13gHeC5NMsqAM3dfc+UKccDbYHzgLeBr929DrCDyPQqAC+4e2N3r01kREyGd5OH1eU9+/DSQ3dw\nyyVnkpCY55/262/j7x3buf6C07iuYyveeenxg+6ry3967fUomDse/x+fDx/CDRe24fqOrZjybWSQ\nx3V39eeDQc/T84LWrPpzGQUKFgag7bkX8fOsqVx/wWlMGP0xxwbzQhYrWZoTatXj+gtOY/DT+z7h\nuuWZ5/Ht5x/R8szzjih+iY4LOl1EixatKFLkn1/hGiedxHPPPkX9Oiexfcd2unXvQd68eXl36Afc\ncXtvGp9cl2aNT2bqlMmHfdy019CaNqxPcnLyQbe5sNNFVD+xBo0b1OXsM9vz6ONPUbq0Zm/JzmH7\nOS3Lpr4ys63uXtDMHgB2E0lABd39PjNbC5Rz991BlbXS3UsG17y+dvchwT7uA3a7e38ziwv2kS+4\n0/wBYL27P2NmnYC+QAGgOPC8uw840DU0M+sB9AAoXa5Cwze+nJ4l/x/Egr+3byNv/gKYGV99+gET\nx39Ov2dez+mwckQYp7467+wO3H7HXamPeVm0cCGXXXIRk6fPyuHIwi2aU18dU/5Er3HdKwdfMRNm\n3NM2anFlhey4IeMZYAYwOJPrb0v3eSeAu6eY2e40QzdTgAQzywe8BDRy9z+CJJiPDARzjw2EyFyO\nmYxL9mPB3FkMfPT/SElJoWCRovR68JmcDkmiYN26dZzWohkNGjZKTWaSe+WC4ioqsjyhufv64Ma5\nbsCer+4TidxF/hZwOXAkV5L3JK+1ZlYQuAgI5ajGo1HdJqce0mARyR1KlCjB3F9+26f9+BNOUHWW\nC+WG7sJoyK770J4ESqb5fBNwjZnNBroSeXbOYXH3jcCrwFzgC2DqEcQpIiK5VJZVaO5eMM37VUSu\nb+35vJTIQI/021yd7vN9GezzvjTv+wH9DrY/EZFYFCMFmuZyFBEJNVOXo4iISK6iCk1EJMQi96Hl\ndBTZQxWaiIiEgio0EZFQyx2zfESDEpqISMjFSD5Tl6OIiISDKjQRkZBTl6OIiOR+ueTRL9GgLkcR\nEQkFVWgiIiG253losUAJTUQk5GIloanLUUREQkEVmohIyMVIgaaEJiISdupyFBERyUVUoYmIhJnu\nQxMREcldVKGJiISYabZ9EREJixjJZ+pyFBGRcFCFJiIScnExUqIpoYmIhFyM5DN1OYqISDioQhMR\nCTGz2JkpRAlNRCTk4mIjn6nLUUREwkEVmohIyKnLUUREQiFG8pm6HEVEJBxUoYmIhJgRmc8xFiih\niYiEnEY5ioiI5CKq0EREwsxi5/ExqtBERCQUVKGJiIRcjBRoSmgiImFmxM7jY9TlKCIioaAKTUQk\n5GKkQFNCExEJO41yFBERyUVUoYmIhFjkAZ85HUX2UEITEQk5jXIUERHJRVShiYiEXGzUZxkkNDMr\nnNGG7r45+uGIiEi0xcoox4wqtHmAs3dy3/PZgUpZGJeIiMghOWBCc/eK2RmIiIhEX2Tqq5yOIntk\nalCImXUxs7uD9xXMrGHWhiUiIlERPD4mGq+j3UETmpm9ALQBugZN24FXsjIoERGRQ5WZUY7N3b2B\nmc0EcPf1ZpYni+MSEZEoyQXFVVRkpstxt5nFERkIgpmVAFKyNCoREZFDlJkK7UXgQ6CUmd0PdAbu\nz9KoREQkanLD9a9oOGhCc/c3zWw60D5outjd52ZtWCIiEg2xNMoxszOFxAO7iXQ7arosERE56mRm\nlON/gfeAY4EKwLtmdldWByYiItERK8P2M1OhXQmc7O7bAcysPzATeCQrAxMRkeg4+lNRdGSm+3Al\neye+hKBNREQklZnlM7MpZvaTmc0LBhJiZsXN7Esz+y34t1iabe4ys4VmNt/MzkzT3tDM5gTLnrNM\nlIgZTU78NJFrZuuBeWb2RfD5DGDq4Z+yiIhkF7NsfR7aTqCtu281s0TgezMbDVwIjHf3AWZ2J3An\ncIeZ1QS6ALWIXNYaZ2bV3T0ZeBnoDkwGPgc6AKMzOnhGXY57RjLOA0alaZ90qGcoIiI5J7vymbs7\nsDX4mBhScoACAAAgAElEQVS8HOgItA7ahwDfAHcE7UPdfSew2MwWAk3MbAlQ2N0nReK3N4HzOdyE\n5u6DDuuMREQkZplZPDAdOAF40d0nm1kZd99zqeovoEzwvjx7F0l/Bm27g/fp2zN00EEhZnY80B+o\nCeTb0+7u1Q+2rYiI5LwojlAsaWbT0nwe6O4D064QdBfWN7OiwMdmVjvdcjczj1ZAaWVmlOMbwEPA\nE8BZwDUE02CJiMjRL4pdjmvdvVFmVnT3jWb2NZFrX6vMrJy7rzSzcsDqYLXlQNpHlVUI2pYH79O3\nZygzoxwLuPsXQYCL3L0fkcQmIiKSysxKBZUZZpYfOB34FRgJXBWsdhUwIng/EuhiZnnNrCpQDZgS\ndE9uNrNmwejGK9Nsc0CZqdB2BpMTLzKz64hkyUKZPkMREckxhmXnKMdywJDgOlocMNzdPzOzH4Hh\nZtYNWEpkTmDcfZ6ZDQd+BpKAG4IuS4CeRHoI8xMZDJLhgBDIXELrBRwD3EzkWloR4NpMn56IiMQE\nd58NnLyf9nVAuwNs059IbknfPg2ove8WB5aZyYknB2+38M9DPkVEJDew2HkeWkY3Vn9MBoM/3P3C\nLIlIRESiKjfMwxgNGVVoL2RbFDlo2dpt3DBo2sFXFDmIlV9/ntMhiMS0jG6sHp+dgYiISNaIlWd+\nZfZ5aCIikgsZsdPlGCuJW0REQi7TFZqZ5Q0mkBQRkVwkLjYKtEw9sbqJmc0Bfgs+1zOz57M8MhER\niYo4i87raJeZLsfngHOAdQDu/hPQJiuDEhEROVSZ6XKMc/el6S4qJh9oZREROXqYxc6gkMwktD/M\nrAngwfxcNwELsjYsERGJltzQXRgNmelyvB7oDVQCVgHNgjYREZGjRmbmclwNdMmGWEREJAvESI9j\npp5Y/Sr7mdPR3XtkSUQiIhI1Btn5+JgclZlraOPSvM8HXAD8kTXhiIiIHJ7MdDkOS/vZzN4Cvs+y\niEREJKpiZUqowznPqkCZaAciIiJyJDJzDW0D/1xDiwPWA3dmZVAiIhI9MXIJLeOEZpG78eoBy4Om\nFHc/4EM/RUTk6GJmMTMoJMMuxyB5fe7uycFLyUxERI5KmbmGNsvMTs7ySEREJEtEpr868tfR7oBd\njmaW4O5JwMnAVDNbBGwjcluDu3uDbIpRRESOQKxMfZXRNbQpQAPgvGyKRURE5LBllNAMwN0XZVMs\nIiISZZopJKKUmfU+0EJ3fyoL4hERkSiLkXyWYUKLBwoSVGoiIiJHs4wS2kp3fyDbIhERkegzDQoB\nVWYiIqFgMfLnPKP70NplWxQiIiJH6IAVmruvz85AREQk+iKjHHM6iuyRmeehiYhILhYrCS1WHpMj\nIiIhpwpNRCTkLEZuRFOFJiIioaAKTUQkxDQoREREwiGXPPolGtTlKCIioaAKTUQk5DTbvoiI5Hqx\ndA1NXY4iIhIKqtBEREIuRnocldBERMLNiNNs+yIiIrmHKjQRkRAz1OUoIiJhoCdWS9jFGYzodSqr\nNu3k34OmAXBWvbLccmY1TihdkAuemcicPzcBkBhv9L+4DnUqFiHFnQc+/pnJiyKPyzu7fjluaH88\ncXHG1z+v5tHP5ufYOYlIbNM1tBh1TauqLFq9ba+2BSu3cP3gGUz5fe9nu3ZpVgmAsx7/jitfmcLd\n552EGRQtkMhd59bgipen0OGx7yhZKC/Nq5XItnMQkcyJM4vK62inhBaDyhbJR5uTSjFs0h97tS9a\nvY3Fa7bts/4JZQoy8be1AKzbuostO3ZTp2IRKpUowJI121i/bRcAPyxYS4e6ZbP+BERE9kMJLQb9\n3/knMeCzX0lxz9T6v6zYTPtaZYiPMyoUz0/tikU4tmh+lqzdRtXSx1C+WH7i44wz6pSlXNF8WRy9\niByKPYNCovE62ukaWoxpW7M067buYu6fm2l6fPFMbfP+lD85oUxBRvQ6leUbdjBjyQaSU5zNO5L4\nvw/m8fyVJ5PizowlG6hUokAWn4GIHKrc0F0YDUpoMaZh1WK0q1Wa1ieVIm9CPAXzJfDU5fXo/c5P\nB9wmOcV5aMQvqZ/fv+mU1K7Jr35ezVc/rwagS7OKJKdkruoTEYk2JbQY8/io+Tw+KjISsenxxene\n+rgMkxlAvsQ4zIwdu5JpUb0kySnOwlVbAShRMA/rtu6icP4Erji1Mje9OTPLz0FEDk2MFGhKaPKP\nM+qU4d4LalK8YB4GdW/Ez8s3c/XAqZQomJch/2lMisOqTX/T+91Zqdvcc35NahxbCIDnxy7c76AS\nEck5RuwMllBCi2GTF61PvZ8MYOycVYyds2qf9ZZv2EH7ARP2u49b3p6133YRkeymhCYiEmYGFiN9\njkpoIiIhFxvpLHa6VkVEJORUoR2lJvRrzbadyanD4O/5cC4zlmw84PpzHjmDOneNPaJjPtalLi1O\nLEnrh75hV3IKxY5JZESvU2n10DdHtN/0Tq9dhsVrtqWOlLy1QzWmLlrPD7+ti+pxJGv9Oup+tmzb\nSXJKCknJKbS4/LG9lt/StS0Del9IhTZ3sG7jNhIT4nmh36U0qFmJFE+hz2Mf8t303wC46IwG9O12\nJvHxcYyeMJd+z43IiVMKJUP3oclR4LKXJrFh2+5sPWZKinNx0wq8M3FZlh3j9Npl+Prn1akJ7Zkx\nv2XZsSRrdejxLOs27juytUKZorRrdhLLVv4z6OjaC08FoHHnhylVrCCfvNCTFlc8TrHCBXj41vNp\nfvljrN2wlVcf6ErrJtX5ZsqCbDuPsIuNdKaElqsUyBPPwGsbUrhAIonxcTz5+XzGzVu91zqlCuXl\n+StPpmC+BOLjjHs+mMvUxRtoUb0kt3aoRp6EOJat3U7fobPZvit5n2MMnrCEa1tVZWi6eR4Burep\nytn1ypEnIY6xc1bxzBeRRHTj6SdwfsNjWb91Fys3/s2cPzfx2jeLuaRZRS5tVpHEhDiWrt1O73dm\nUbN8YdrXLk3T44tzw+kn0PONGdx4+gl8/fNqtu1KonOTitwY3Mu25z65fw+alun45ejwWJ9O/PfZ\nT3j/6R6pbTWOK8s3UyP3QK7ZsJVNW3bQsGYl3J2Fy9awdkPkC85Xk3/l/Hb1ldDkkCmhHcXe7dmM\n5BRnV1IKFz47kZ1JKVw3eAZbdyZR7JhEPryl+T4J7bwGxzJh/hpeGreIOIP8eeIpdkwiN55+Al1f\nmcKOXcn8p+1xdGtdlefHLtznmCs27GDa4vVc0LA843/+Zwh/i+olqVLyGM5/ZiJm8Oq1jWh8XDF2\n7k6hQ92y/OuJ70mMNz7t3SL1sTNfzP4rdQLk3mdVp3PTirz5/VLGzV3N1z+vZvTsv/Y69g8L1vHw\nxXXInyeeHbuSOad+OT6dteKQ4pfs4+6MeuUmkpNTGPThD7z+0Q8AnNO6DitWb2TOguV7rT9nwXLO\nOa0Ow8dMp0KZYpxcsyIVyhbjmynzqV6lNJXKFWf56o2c16YeiQnxOXFKoRUjPY5KaEez9F2OBvQ5\nuzpNjitOikdmzS9ZKA9rt+xKXWf2Hxt5tEtdEuPjGDvnL35ZsYWmx5fghDIFef+mU4DI881mLj3w\n9biXxy/if9c25Otf/kmWLU8sScsTS/LZbS0AKJA3nqqljuGYvAmMm7uKXUkp7EqC8WkSbPVyhbjt\nrOoUzp9IgTzxfDd/bYbnm5zifPvrGtrVLM3o2X/RpmZpBnz26yHHL9mj3TVPs2LNJkoVK8hnr9zI\n/CV/MePnZfS99kzO6fnCPusPGfEjNaqW4Yd3+rJs5Xom/bSY5OQUNm7Zwc0PD+PtR68lxZ1JP/3O\ncRVK5sAZhZVp2L4cfTo2PJbix+ThvKd+ICnFmdCvNXnTfZOd+vsGurwwiTY1S/P4pfUY9O1iNm3f\nzQ8L1mb6Jugla7fzy4ot/Kt+udQ2s0iie+/Hvbsir2lV5YD7ebxLXf4zeDq/rthCp8blaXr8wZ+V\n9tnMlVzZojIbt+9mzh+b2LYz0q14KPFL9lixJlKJr9mwlZFfzaZxrSps3LyDyuVLMGXYXQCUL12U\nH9+9g5ZdH2fVui30ffKj1O2/fqM3vy2LfAH6fMJcPp8wF4hca0tOTsnms5Ew0LD9XKRQvkTWbd1F\nUorT7ITiVCi+78z2xxbLx9otOxk26Q+GTf6D2hUKM2vpRhpWLUblkpH18+eJVFcZefHLhXRvXTX1\n84Rf13Jxk4oUyBNJoGWK5KVEwTxMX7yBtrVKkychjgJ54mlbs3TqNsfkTWDN5p0kxBkdG5RPbd+2\nM4lj8u7/u9TkReuoVaEwXZpV5NOZKwAOK37JWgXy5aFggbyp79ufUoN5i1Ywb+EKKre7ixpn30uN\ns+9l+eqNnHLZo6xat4X8+RIpkC8PAG2b1iApOYVff490O5cqVhCAooXy06NzSwZ//GPOnFgI7Zn6\nKhqvo50qtFxkxIzlvNqtEaNvb8mcPzaljhJMq9nxJeje5jiSklPYviuZ2979ifXbdnH7e7N59or6\n5EmI/Fg+OXpBhvMu/rZqK/P+3EytCoUB+H7BWk4oU5APb2kORJJS73d+YvYfmxg/bzWj+7Rk7dad\nzP9rC1v+TgLg6TEL+OiW5qzftotZSzemJrHPZq7k4c61uaplZW4YsvdkxikemcG/U+MK9HkvMmny\n4cQvWat0iUIMe6o7AAnx8QwbPY0vJ/6S4TalihXi05duICXFWbFmI936DUld9kTfi6hTPfKl55GB\nY1i4bPWBdiNyQOaZfMhjWOUtU83LX/5sToeRqxXIE8/2XcnkS4xj2I2ncPfwOcxbvjmnw8p2K7/+\nPKdDkJD4e9aL0929UTT2dXzNev7Iu6OjsSsuObl81OLKCqrQ5Ig93LkOJ5QpSN6EOD6atjwmk5nI\n0Sw2hoQooUkU3KrBGiJyFFBCy6U+uqU5eRLiKFogkbyJcazatBOA/7w+neUbdkT9eL3Pqs6GbbsY\nPGHJPu0XNanA+q3/3DpwyQs/po5OlKPThDf7kCdPAsULFyBfvkRWrI6MWOzca+Bes3scqeMqlmTa\n8LtZsHQ1eRLj+Xbqb/QaMPyQ9zPyxRu47PbXSEyIp9MZDXjtg++ByIwkj/S6gK53Do5azKGTjbPt\nm1lF4E2gDODAQHd/1syKA8OAKsASoLO7bwi2uQvoBiQDN7v7F0F7Q+ANID/wOXCLH+QamRJaLnXh\nsxMB6NS4PHUqFuG+j37OsVhe/fr3fRJdWvFxljon5f4+Z8QMYvwyb5ZodeUTAFxxblMa1qxEr0ff\n3+96cXFGSib/Wx3IgqWradZlAAkJcYx99RbOPq0Oo76dc0j7OO+GF4FIgvz3RS1SE9qfqzYqmR1E\nNj/gMwm4zd1nmFkhYLqZfQlcDYx39wFmdidwJ3CHmdUEugC1gGOBcWZW3d2TgZeB7sBkIgmtA5Dh\nxcDcMBJTDkGXZhW569waqZ8vb16JO8+tQeWSBRjTtyXPdq3P2Dta8fyVJ5M3MfKfv27FIrx3Q1NG\n9DqV17s3omTBPEccR+emFXjlmga807MpQ3o05tRqJXi3Z1NeC0ZpAvRocxyjb2/J6NtbcmWLygBU\nLlmAL/q25OnL6/FF35aULpz3iGORzIuPj2PlhMd4vE8npgy7i8a1q7BwzIMUKZgfgCZ1qjDqlRsB\nOCZ/HgbefwXfvdWHH9+7g3+1qp3hvpOSUpg8ewnHVyyFmfHobRcy7f27mTr8bi5oXx+AY0sVYfzr\nvZg09E6mvX83zepFbh3ZE8NDN3ekeuXSTBp6Jw/efB7HVSzJpKF3AvD9O32pVvmf20bGv96LutXL\nH3KccvjcfaW7zwjebwF+AcoDHYE9w1qHAOcH7zsCQ919p7svBhYCTcysHFDY3ScFVdmbabY5IFVo\nIfPZzJV8etupPDZqPskpzkVNKtDn3cjw9+plC3HnsDnMWrqRJy6ty2WnVOKdH5bxf+fXpMfr09iw\nbTcdGxxLr7Oq89/352b6mN3bHEenxhUA2LBtF11fmQJArfJFOPvJ79i8I4lTq5WgTsUinPnoBFZs\n/Jt6lYrQseGxnP/MDyTEGR/feiqTF63n793JHF+6IH3enZ06hZZkr6KFCvD9jIXc/sSHGa53d4+z\n+HLiL/S4922KFsrPhLduZ/ykX9m5K2m/6xfIl4fTGlen37Mj6HT6yZxYtQxNLnmEUsUK8v3bffl+\n+kIuPbsxn0+Yw5NvjCMuzsifN3GvffR7bgTHVSxFsy4DgEjFtseHX0yn0xkNGPDqGMqXLkqxIgWY\nvWA5/W/peEhxhlEUuxxLmtm0NJ8HuvvAAxyzCnAykQqrjLuvDBb9RaRLEiLJblKazf4M2nYH79O3\nZyjbE5qZnQ98DJzk7r8GJ93c3d8NltcHjnX3wxoDbWZLgEbunvE8SyG1dWcSU3/fwGk1SvHHuu0k\npziLVm+jcskCLFu3nVnBlFGfTF/BpadUZNLCdVQvW5C3rmsKRLoDV248tGtwB+py/G7BGjbv+OeP\nxswlG1mx8W8AGlUtzpjZf7Fzdwo7gS/nrqJx1WJ8t2Aty9ZtVzLLQTt37WbEVz8ddL12p5zEGafW\n4rZrTgcgX54EKpYtvs89ZHsqqpQUZ+TXP/HV5F956o6LGT5mOikpzqp1W5g4axENalVi2rxlvNCv\nC3nzJPLpN7P3mQ8yIx9+OYMPnrmOAa+O4aIzG/DRlzMPKc4wi+IVtLWZGbZvZgWBD4Fb3X1z2oTq\n7m5mWXIhIScqtEuB74N/7yVykfAy4N1geX2gEZE+UzkMwyb9QbfWVflz/XY+mPLPl5z011PdI9/c\nfl25hUtemJR+N0dsR7rZ8Ldn8huxZtHPWTt27v3IoqTkFOLiIn+Q8ub5p2Iyg869B7L4z4y/O+65\nhpYZ305dwJn/fpYOLWvz2oNdefqNcQwdPe3gGwLLVm5g246d1DiuLBed0YDu9759SHFKdJhZIpFk\n9o6775nrbJWZlXP3lUF34p5vE8uBimk2rxC0LQ/ep2/PULZeQwuydgsiI1q6BM0DgJZmNsvM7gAe\nAC4JPl9iZk3M7Eczm2lmE83sxGBf8Wb2hJnNNbPZZnZTumPlN7PRZtY9G0/xqDB9yQYqlSjAv+qV\nY9SslantFYsXoG7FIkBkVv5pizew8K+tlCmSj7qVIu2J8Ua1MgWzPMapi9dzRu0y5E2MTJnVvlZp\npi7ekOXHlUO3dMV6Tj6pEkDqtS6AcRN/oWeX01I/1zuxwj7bHsgPMxZy8ZkNMTNKFy/EKfWOY8a8\nZVQqV4y/1m3m9Y9+4K0Rk6hXo+Je223dtpNCBQ58XfWDL2Zw+zVnkCdPQuq0WkcSZ1iYRed18OOY\nAYOAX9z9qTSLRgJXBe+vAkakae9iZnnNrCpQDZgSdE9uNrNmwT6vTLPNAWV3hdYRGOPuC8xsXTAs\n806gj7ufA2Bmq4h0Gd4YfC4MtHT3JDNrDzwMdAJ6EKnu6gfLiqc5TkFgKPCmu7+ZXSd3NBn900qO\nL1MwdRoqgIWrt9LttKqcVL4w81du4b1Jy9iVnMINb8zg3gtqUjBfAnFxxqBvFvPbfqbVOpC019AA\nug86+Dfq2cs28enMlXxya+Shj+9MXMb8lVtS52uUo8dDr3zOS/dcyqYtO/h+xj+P7On/v9E8fnsn\npg6/m7g4Y9Efa+jca7+XU/bx0bhZNKlblanD78Id7njqI9Zs2MqVHZtx8xVt2Z2UzNbtO/eaHgtg\n9fotzPzlD6YOv5sx389l8McT0+13Jo/ediEPvDwqKnGGQWSUY7bdWn0q0BWYY2Z7blC9m0jhMtzM\nugFLgc4A7j7PzIYDPxMZIXlDMMIRoCf/DNsfzUFGOEI2T31lZp8Bz7r7l2Z2M1AJ+Iy9E9rV7J3Q\nKgLPEcncDiS6ew0z+xB4xd2/THeMJcAm4DF3f+cAcfQgkhCJL1SqYaV/vxHtU81xg3s05uXxi5iy\nKHJPUeWSBXjxqgac8+T3ORxZeGnqK4mWaE59Va1WPX9q6Nho7Irz6pbV1FcAQQXVFqgTXBCMJ5Kg\nRmW4ITwIfO3uFwQDSL7JxOF+ADqY2bv7uxEvGJUzECJzOWb2HHKDogUS+eiW5sz5Y1NqMhOR2BYj\nj0PL1mtoFwFvuXtld6/i7hWBxUAKUCjNelvSfS7CPxcDr07T/iXwHzNLgNSEucc9wAbgxaieQS6w\ncftu2j7y7T7PDlu6druqM5GYZFH739EuOxPapUSG66f1IZHBIclm9pOZ9QK+BmruGRQCPAY8YmYz\n2buifA1YBsw2s5+IjJRM6xYgv5k9lgXnIiIiR5ls63J09zb7aXvuAKs3Tve5epr3/YJtk4DewSvt\nPquk+XjNIQcqIhIy6nIUERHJRTT1lYhIiGXzsP0cpYQmIhJmmbwpOgzU5SgiIqGgCk1EJORipUJT\nQhMRCbnccA9ZNKjLUUREQkEVmohIiBkQFxsFmhKaiEjYqctRREQkF1GFJiISchrlKCIioaAuRxER\nkVxEFZqISIjF0ihHVWgiIhIKqtBEREItdzxtOhqU0EREwkyz7YuIiOQuqtBEREIuRgo0JTQRkTCL\njHKMjZSmLkcREQkFVWgiIiEXG/WZEpqISPjFSEZTl6OIiISCKjQRkZDTjdUiIhIKMTLIUV2OIiIS\nDqrQRERCLkYKNCU0EZHQi5GMpi5HEREJBVVoIiIhZsTOKEdVaCIiEgqq0EREwiyGnoemhCYiEnIx\nks/U5SgiIuGgCk1EJOxipERTQhMRCTXTKEcREZHcRBWaiEjIaZSjiIjkekbMXEJTl6OIiISDKjQR\nkbCLkRJNCU1EJOQ0ylFERCQXUYUmIhJyGuUoIiKhECP5TF2OIiISDqrQRETCLIZuRFOFJiIioaAK\nTUQk5GJl2L4SmohIiBmxM8pRXY4iIhIKqtBEREIuRgo0JTQRkdCLkYymLkcREQkFVWgiIiGnUY4i\nIhIKGuUoIiKSi6hCExEJuRgp0JTQRERCL0YymrocRUQkFFShiYiEWGSy/dgo0VShiYhIKCihiYiE\nmUWG7UfjddBDmb1uZqvNbG6atuJm9qWZ/Rb8WyzNsrvMbKGZzTezM9O0NzSzOcGy58wyd+OBEpqI\nSMhZlF6Z8AbQIV3bncB4d68GjA8+Y2Y1gS5ArWCbl8wsPtjmZaA7UC14pd/nfimhiYhIVLj7BGB9\nuuaOwJDg/RDg/DTtQ919p7svBhYCTcysHFDY3Se5uwNvptkmQxoUIiISdtEbE1LSzKal+TzQ3Qce\nZJsy7r4yeP8XUCZ4Xx6YlGa9P4O23cH79O0HpYQmIhJqFs1RjmvdvdHhbuzubmYerWDSi/mEtmv1\nwrWLnz57aU7HkQuUBNbmdBASCvpZOrjKOR1AFK0ys3LuvjLoTlwdtC8HKqZZr0LQtjx4n779oGI+\nobl7qZyOITcws2lH8s1MZA/9LGW/HJ6ceCRwFTAg+HdEmvZ3zewp4Fgigz+muHuymW02s2bAZOBK\n4PnMHCjmE5qISJgdwgjFIz+W2XtAayLX2v4E7iWSyIabWTdgKdAZwN3nmdlw4GcgCbjB3ZODXfUk\nMmIyPzA6eB2UEpqIiESFu196gEXtDrB+f6D/ftqnAbUP9fhKaJJZBxvJJJJZ+lnKbrEx85USmmRO\nJobmimSKfpayn+ZyFBERyUVUoYmIhFwOj3LMNkpoIiIhFyP5TF2OcuTM7CQza2tmiTkdi+ROmZ1N\nXSQjqtAkGroQueM/2cwmuvvunA5IcpdgElqCm2mXuPtfORxSeGTy0S9hoApNouF+YAlwCdBClZpk\nlpmdbGZ5gvfHE7knKSlno5LcSglNDkvaLiJ3TyHyh2glSmpyaO4DPg2S2mJgE7ALwMzi0jwfS45I\nNj4RLQcpockhMzNL00V0hpm1BooCDwHLiCS15kpqciBmFgfg7h2BDcBwoCCRSr9AsCwFyJNDIYaG\nkX1PrM5puoYmhyxNMusNXEBkLrbuwGvu/rCZ3QH0AJKB73MsUDkqBV+IUoL3pdy9i5mNAH4k8jNT\nzsySgURgpZnd5e47cjBkySWU0OSwmFl7oI27tzSzR4AmwKVmhrs/ama9iDyBVmQvab4Q3Qw0MrPr\n3b2jmb1CZM6/x4B4IlX/fCWzI5cLiquoUEKTTEnbzRj4A7jJzK4GGgP/Ap4G7jOzRHd/OgfClFzC\nzC4g8iiRc9x9G4C7X2dm7wMPAue7uwaHRElu6C6MBl1Dk4NKd82sqZkVAxa7+xIizzB6OXjE+mzg\nJ2BWjgUrucVxwMjgoY+Je663uvvFwCoiz8cSOSSq0OSg0iSz64DbgXnAWDMbCswFhphZA+BCIt+4\nVx9wZxJz9lPdQ+QJxC3NrLC7bw7W6wz86e7dsj3IkIuVyYmV0OSA0lVmpYG6RK6VNQJOB7oBLxAZ\nat0UuNDdF+VQuHIUSvczdCGwBdgKjAUuB641s/lErpf9Fzg3p2INtdjIZ0posn/p/hDdCJQFarn7\nOuCLYNh1e6Av8Ky7f55z0crRKt0AkMuIPAutL5EnEvcAbiTyJSkfcKm7L86hUCUEdA1N9ivdt+qr\ngClABTMbFiwfDUwgMrQ6Rr7/yeEws5OBjkBroAKwGngNaOru/3X3y4Ar3X1OzkUZbrFxW7USmqST\ndgYQM2tIpFtooLuPBE4AqpvZewDuPgLoH1RtIgCYWdFgGivMrC6wA7iUSFI73d1bAa8Cw8zsCgB3\n35pT8YZdtG6qzg0jJdXlKKnSdTNeBJxEZBaH1mY2xd1/CgZ//G5mb7j71XuGXIsAmFkCUB04x8zK\nASWBy919ezA69t1g1fXAU8CknIlUwkgJTVKlSWYd/r+9u4/VuqzjOP7+SFg8BU0Xbq0CH9BQg8ko\n0NaYMdAKxh/RfCySoZw2l1YUS2u1tWVzbeXwISuzViNr+ZhzDGmpERhGQlY8VKyyTKEl5lOUfvrj\nus52cwZygCP3Ob/f58Xucc59/+7fdZ2zs33v6+n7paxxzKUEtYuA+ZJertNCEyVN7F5PYzCqH4j+\nV/Ppr3gAAAZ1SURBVDd5fAaYCXzK9vP1ktcAcyWdTNn8Mcv2X7vU3VZpyy7HTDnGXmpexh5gg+3/\n2t4M3AWMAi6QdCpAFu+jUx19nVO/nUTJyXg9cIakeQC2VwC3U84qzk8wO4JasoiWEVrL7eOM0A5K\n1vzjJU2xvcn22nrw9WzKodeIvoYDZ0n6HIDtmZKOpexsnCfpaUo6qz3Ayt5cjhEDKQGtxfqsmc2j\n1KF6Grgc+BqwsHea0fbPJD2cvHrRSdJxtv9h+ylJTwKTKaMwbO+SdA/l7+rTwBTgPQlmR94QGFwN\niEw5BpI+SinS+S7gFuDK+hgHLJI0GSDBLDpJOgX4u6SvSroAuImyk3GnpBvqB6YdwGrgEmCG7W1d\n7HI0XAJaC0l6i6RRtl0zgHyQshPtKuBMYCmwkFK0cxjl3FBEX88Cv6BMUS8GbgTGAquAZ4AVki6m\nfDh6xvbfutXRtmvLtv0EtJaRNB74BNAjaXTNu7iLWiXY9r+AK4DTa8LhZbZ3da3DMWjZfpxy4P4M\nyo7YNcDFlGz59wDHAIuAFbZf7FI3Aw3Yv8EuAa19dgIbKNnMP1IPUv8B+EE9QwTwVkpWkGGU9Y+I\nvXQcwF8OmHLe7AlgGvAbyjrs48CHbf+uK52M1smmkJaQdBJwlO2tkr5PSSh8LrDE9nJJNwIPStpM\nSTR8oe2XutjlGMTqdHVvUNsOfIUSzK60fWddX3uyjviji8TQmC4cCAloLSDpGGArsEvSFyhl7m+m\nrHecKOky2z2S3klJEvvlnDOLA6k7ZPdI+h7wAHC97Tvra1u62rlopQS0FrD9T0mzgfsp08xTgNso\ni/p7gNPrp+1v2/5P93oaQ1Ed9S8HJkga2ZEZJOKISkBrCds/lTQXuI4S0MZTDkqfRynfcTKwEkhA\ni0OxnlLgNQahTDlG49heLemTlCrTM2x/R9LdlCwPI23v7m4PY6iyvUXSeRmdDU5DYYfiQEhAaxnb\n90p6GVgvaWZKv8RASTCLbktAayHb90k6Grhf0rSkIoposCFyKHogJKC1lO27JK1JMItotiGSKH9A\n5GB1i6VKcEQ0SUZoERFN15IhWkZoERHRCBmhRUQ0XFu27WeEFkOWpJckPSrpMUk/kjTyMO41S9JP\n6tfza+aL/V07rtaQO9g2Pl/PAfbr+T7X3CrpAwfR1gRJjx1sH6OZUj4mYvB7wfZU26dRUngt7XxR\nxUH/jdu+2/Y1r3DJOOCgA1pEvLoS0KIpHqIkWp4gaauk71IyorxZ0hxJ6yRtrCO50QCSzpG0RdJG\nOtI2SVokaUX9erykOyRtqo8zgWuAE+ro8Np63TJJGyRtrgmge+91laRtkn5OSS/2iiQtqffZJOnH\nfUadsyU9Uu/3/nr9MEnXdrR92eH+IqN5NECPwS4BLYa8WsftXEodLoCTgBtsnwo8B1wNzLZ9BvAI\n8HFJrwO+AcyjlD05bj+3vw54wPYUSiHL31JqgP2xjg6XSZpT23wHMBWYJundkqZRcmVOBd4LTO/H\nj3O77em1vd9TKkH3mlDbeB9wU/0ZFgO7bU+v918iaWI/2ok2aUlEy6aQGMpGSHq0fv0Q8C1K4dI/\n215fn58BTAbW1vJdRwPrgFOAHba3A9QSKJfuo42zgQ8B1PpwuyW9oc81c+rj1/X70ZQANwa4ozcl\nVM2beSCnSfoiZVpzNLCq47Uf1oPw2yX9qf4Mc4C3d6yvja1tb+tHWxGNkoAWQ9kLtqd2PlGD1nOd\nTwGrbZ/f57q93neYBHzJ9tf7tHHFIdzrVmCB7U2SFgGzOl5zn2td277cdmfgQ9KEQ2g7Giq7HCOa\nYT1wlqQTASSNkjQJ2EKp33VCve78/bx/DdBT3ztM0ljg35TRV69VwCUda3NvkvRG4EFggaQRksZQ\npjcPZAzwhKThwIV9Xlso6aja5+MpRVtXAT31eiRNkjSqH+1ES/RWrG7DLseM0KLRbO+sI52Vkl5b\nn77a9jZJlwL3SnqeMmU5Zh+3+Bhws6TFlErfPbbXSVpbt8XfV9fR3gasqyPEZ4GLbG+UdBuwCXgK\n2NCPLn8WeBjYWf/v7NNfgF8CrweW2n5R0jcpa2sba5HWncCC/v12og02bvzVqhHDdewA3W7XAN3n\nVaFSRT0iImJoy5RjREQ0QgJaREQ0QgJaREQ0QgJaREQ0QgJaREQ0QgJaREQ0QgJaREQ0QgJaREQ0\nQgJaREQ0wv8Bn362NkPsp34AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a4079c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T21:59:10.015444Z",
     "start_time": "2017-07-20T21:59:09.977807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                (0.644345804225, 0.807823907992)\n",
       "                3                                      (nan, nan)\n",
       "12              1                (0.695739626089, 0.980969584784)\n",
       "                3                (0.729312236349, 0.808499713691)\n",
       "24              1                (0.721225461548, 0.907510092589)\n",
       "                3                (0.688149903636, 0.924239269828)\n",
       "48              1                   (0.7443988358, 0.82684817156)\n",
       "                3                (0.814410813939, 0.895163382698)\n",
       "122             1                (0.461421770976, 0.866231834507)\n",
       "                3                (0.814914932778, 0.974765955175)\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
