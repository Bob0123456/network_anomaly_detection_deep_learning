{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "y_train_labels = pd.read_pickle(\"dataset/kdd_train_2labels_y.pkl\")\n",
    "y_test_labels = pd.read_pickle(\"dataset/kdd_test_2labels_y.pkl\")\n",
    "\n",
    "output_columns_2labels = ['is_Attack','is_Normal']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "x_input = kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "#y_output = kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "x_input = ss.fit_transform(x_input)\n",
    "\n",
    "le = pp.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_labels).reshape(-1, 1)\n",
    "y_test = le.transform(y_test_labels).reshape(-1, 1)\n",
    "\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(x_input, \n",
    "                              y_train, \n",
    "                              test_size=0.2)\n",
    "#x_valid, x_test, y_valid, y_test = ms.train_test_split(x_valid, y_valid, test_size = 0.4)\n",
    "\n",
    "x_test = kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "#y_test = kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "x_train = np.hstack((x_train, y_train))\n",
    "x_valid = np.hstack((x_valid, y_valid))\n",
    "\n",
    "x_test = np.hstack((x_test, np.random.normal(loc = 0, scale = 0.05, size = y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 123\n",
    "intermediate_dim = 80\n",
    "latent_dim = 32\n",
    "batch_size = 1409\n",
    "epochs = 5\n",
    "hidden_layers = 8\n",
    "\n",
    "class Train:\n",
    "    def train():\n",
    "        Train.x = Input(shape=(input_dim,))\n",
    "        \n",
    "        hidden_encoder = Train.x\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_encoder = Dense(intermediate_dim, activation='relu')(hidden_encoder)\n",
    "\n",
    "        mean_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        logvar_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        def get_distrib(args):\n",
    "\n",
    "            mean_encoder, logvar_encoder = args\n",
    "\n",
    "            # Sample epsilon\n",
    "            epsilon = np.random.normal(loc=0.0, scale=0.05, size = (batch_size, latent_dim))\n",
    "\n",
    "            # Sample latent variable\n",
    "            z = mean_encoder + K.exp(logvar_encoder / 2) * epsilon\n",
    "            return z\n",
    "\n",
    "        z = Lambda(get_distrib)([mean_encoder, logvar_encoder])\n",
    "\n",
    "        hidden_decoder = z\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_decoder = Dense(intermediate_dim, activation=\"relu\")(hidden_decoder)\n",
    "\n",
    "        Train.x_ = Dense(input_dim, activation=None)(hidden_decoder)\n",
    "\n",
    "def get_loss(x, x_):\n",
    "    xent_loss = input_dim * metrics.binary_crossentropy(x, x_) \n",
    "    kl_loss = - 0.5 * K.sum(1 + logvar_encoder - K.square(mean_encoder) - K.exp(logvar_encoder), axis=-1)\n",
    "    return K.abs(K.mean(xent_loss + kl_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:2\n",
      "14090/22544 [=================>............] - ETA: 0s\n",
      " Train Accuracy: 0.4765512855613933, Test Accuracy: 1.3745645955204964\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:4\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.2449467015617034, Test Accuracy: 1.0052113756537437\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:8\n",
      "14090/22544 [=================>............] - ETA: 0s\n",
      " Train Accuracy: 0.30440372403930216, Test Accuracy: 0.9470128770917654\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:16\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.517290942809161, Test Accuracy: 1.4346611201763153\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:32\n",
      "14090/22544 [=================>............] - ETA: 0s\n",
      " Train Accuracy: 2.0855190052705654, Test Accuracy: 1.6945202499628067\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:64\n",
      "12681/22544 [===============>..............] - ETA: 0s\n",
      " Train Accuracy: 10.142303547438454, Test Accuracy: 8.463559940457344\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:128\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Accuracy: 0.7214698616196128, Test Accuracy: 2.9785360619425774\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:2 features count:256\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 1.5460096071748173, Test Accuracy: 752.0524385273457\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:2\n",
      "12681/22544 [===============>..............] - ETA: 0s\n",
      " Train Accuracy: 0.33716556254555197, Test Accuracy: 1.296068623661995\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:4\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.21362129116759582, Test Accuracy: 0.7171539925038815\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:8\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Accuracy: 0.3949201036902035, Test Accuracy: 1.1195846125483513\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:16\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.4778965676532072, Test Accuracy: 1.269591648131609\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:32\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Accuracy: 0.3575937502524432, Test Accuracy: 0.8807482719421387\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:64\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Accuracy: 0.4171255599049961, Test Accuracy: 1.1699876375496387\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:128\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.5242233083528631, Test Accuracy: 1.4632056169211864\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:4 features count:256\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.33421776575200696, Test Accuracy: 0.8600492011755705\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:2\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Accuracy: 0.32392115102094765, Test Accuracy: 1.1267166398465633\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:4\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.21746347231023452, Test Accuracy: 0.8958723749965429\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:8\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.5144776070819181, Test Accuracy: 1.362423114478588\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:16\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.362480103969574, Test Accuracy: 1.030649159103632\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:32\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.3544086445780361, Test Accuracy: 0.9851055610924959\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:64\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.6118444940623116, Test Accuracy: 1.6307689771056175\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:128\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.3813183447893928, Test Accuracy: 0.9855190496891737\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:6 features count:256\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.1943781884277568, Test Accuracy: 0.6905734539031982\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:2\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.4159203645061044, Test Accuracy: 1.1776256002485752\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:4\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Accuracy: 0.3457191306002, Test Accuracy: 1.1375652700662613\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:8\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.4058668087510502, Test Accuracy: 1.2224897425621748\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:16\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.45808501103345084, Test Accuracy: 1.3234910182654858\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:32\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.3024302098680945, Test Accuracy: 1.0437155161052942\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:64\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.3118121396092808, Test Accuracy: 1.1136272437870502\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:128\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Accuracy: 0.3615816445911632, Test Accuracy: 1.1607743985950947\n",
      " \n",
      " Current Layer Attributes - epochs:40 hidden layers:10 features count:256\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Accuracy: 0.4201052592081182, Test Accuracy: 1.3202529810369015\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "epoch_arr = [40]\n",
    "\n",
    "score = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "scores = []\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for e, h, f in itertools.product(epoch_arr, hidden_layers_arr, features_arr):\n",
    "    \n",
    "    print(\" \\n Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "    latent_dim = f\n",
    "    epochs = e\n",
    "    hidden_layers = h\n",
    "\n",
    "    Train.train()\n",
    "\n",
    "    vae_model = Model(inputs = Train.x, outputs = Train.x_ )\n",
    "    vae_model.compile(optimizer = \"adam\", loss = \"mean_squared_error\" )\n",
    "\n",
    "    train_size = x_train.shape[0] - x_train.shape[0]%batch_size\n",
    "    valid_size = x_valid.shape[0] - x_valid.shape[0]%batch_size\n",
    "\n",
    "    vae_model.fit(x = x_train[:train_size,:], y = x_train[:train_size,:], \n",
    "                  shuffle=True, epochs=epochs, \n",
    "                  batch_size = batch_size, \n",
    "                  validation_data = (x_valid[:valid_size,:], x_valid[:valid_size,:]),\n",
    "                  verbose = 0)\n",
    "    score_train = vae_model.evaluate(x_valid[:valid_size,:], y = x_valid[:valid_size,:],\n",
    "                               batch_size = batch_size,\n",
    "                               verbose = 0)\n",
    "    score_test = vae_model.evaluate(x_test, y = x_test,\n",
    "                           batch_size = batch_size,\n",
    "                           verbose = 1)\n",
    "    y_test_pred = vae_model.predict(x_test, batch_size=batch_size)\n",
    "    \n",
    "    y_pred = y_test_pred[:,-1]\n",
    "    \n",
    "    y_pred = y_test_pred[:,-1]\n",
    "    y_pred[y_pred >= y_test_pred[:,-1].mean()] = 1\n",
    "    y_pred[y_pred < y_test_pred[:,-1].mean()] = 0\n",
    "    #print (y_pred)\n",
    "    \n",
    "    scores.append(score(e,f,h,score_train, score_test))\n",
    "    curr_pred = pd.DataFrame({\"{}_{}_{}\".format(e,f,h):y_pred},)\n",
    "    predictions = pd.concat([predictions, curr_pred], axis = 1)\n",
    "    \n",
    "    print(\"\\n Train Accuracy: {}, Test Accuracy: {}\".format(score_train, score_test)  )\n",
    "    \n",
    "scores = pd.DataFrame(scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.476551</td>\n",
       "      <td>1.374565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.244947</td>\n",
       "      <td>1.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.304404</td>\n",
       "      <td>0.947013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.517291</td>\n",
       "      <td>1.434661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2.085519</td>\n",
       "      <td>1.694520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>10.142304</td>\n",
       "      <td>8.463560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.721470</td>\n",
       "      <td>2.978536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>1.546010</td>\n",
       "      <td>752.052439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.337166</td>\n",
       "      <td>1.296069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.213621</td>\n",
       "      <td>0.717154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.394920</td>\n",
       "      <td>1.119585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.477897</td>\n",
       "      <td>1.269592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.357594</td>\n",
       "      <td>0.880748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.417126</td>\n",
       "      <td>1.169988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.524223</td>\n",
       "      <td>1.463206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.860049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.323921</td>\n",
       "      <td>1.126717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.217463</td>\n",
       "      <td>0.895872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.514478</td>\n",
       "      <td>1.362423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.362480</td>\n",
       "      <td>1.030649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.354409</td>\n",
       "      <td>0.985106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>0.611844</td>\n",
       "      <td>1.630769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.381318</td>\n",
       "      <td>0.985519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>0.194378</td>\n",
       "      <td>0.690573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.415920</td>\n",
       "      <td>1.177626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.345719</td>\n",
       "      <td>1.137565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.405867</td>\n",
       "      <td>1.222490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.458085</td>\n",
       "      <td>1.323491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.302430</td>\n",
       "      <td>1.043716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.311812</td>\n",
       "      <td>1.113627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.361582</td>\n",
       "      <td>1.160774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>0.420105</td>\n",
       "      <td>1.320253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "0      40               2              2     0.476551    1.374565\n",
       "1      40               4              2     0.244947    1.005211\n",
       "2      40               8              2     0.304404    0.947013\n",
       "3      40              16              2     0.517291    1.434661\n",
       "4      40              32              2     2.085519    1.694520\n",
       "5      40              64              2    10.142304    8.463560\n",
       "6      40             128              2     0.721470    2.978536\n",
       "7      40             256              2     1.546010  752.052439\n",
       "8      40               2              4     0.337166    1.296069\n",
       "9      40               4              4     0.213621    0.717154\n",
       "10     40               8              4     0.394920    1.119585\n",
       "11     40              16              4     0.477897    1.269592\n",
       "12     40              32              4     0.357594    0.880748\n",
       "13     40              64              4     0.417126    1.169988\n",
       "14     40             128              4     0.524223    1.463206\n",
       "15     40             256              4     0.334218    0.860049\n",
       "16     40               2              6     0.323921    1.126717\n",
       "17     40               4              6     0.217463    0.895872\n",
       "18     40               8              6     0.514478    1.362423\n",
       "19     40              16              6     0.362480    1.030649\n",
       "20     40              32              6     0.354409    0.985106\n",
       "21     40              64              6     0.611844    1.630769\n",
       "22     40             128              6     0.381318    0.985519\n",
       "23     40             256              6     0.194378    0.690573\n",
       "24     40               2             10     0.415920    1.177626\n",
       "25     40               4             10     0.345719    1.137565\n",
       "26     40               8             10     0.405867    1.222490\n",
       "27     40              16             10     0.458085    1.323491\n",
       "28     40              32             10     0.302430    1.043716\n",
       "29     40              64             10     0.311812    1.113627\n",
       "30     40             128             10     0.361582    1.160774\n",
       "31     40             256             10     0.420105    1.320253"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.to_pickle(\"dataset/vae_only_predictions.pkl\")\n",
    "scores.to_pickle(\"dataset/vae_only_scores.pkl\")"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Final Hyper parameter tuning",
    "public": false
   },
   "id": "33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
