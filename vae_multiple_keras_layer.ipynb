{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "y_train_labels = pd.read_pickle(\"dataset/kdd_train_2labels_y.pkl\")\n",
    "y_test_labels = pd.read_pickle(\"dataset/kdd_test_2labels_y.pkl\")\n",
    "\n",
    "output_columns_2labels = ['is_Attack','is_Normal']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "x_input = kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "#y_output = kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "x_input = ss.fit_transform(x_input)\n",
    "\n",
    "le = pp.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_labels).reshape(-1, 1)\n",
    "y_test = le.transform(y_test_labels).reshape(-1, 1)\n",
    "\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(x_input, \n",
    "                              y_train, \n",
    "                              test_size=0.2)\n",
    "#x_valid, x_test, y_valid, y_test = ms.train_test_split(x_valid, y_valid, test_size = 0.4)\n",
    "\n",
    "x_test = kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "#y_test = kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "x_train = np.hstack((x_train, y_train))\n",
    "x_valid = np.hstack((x_valid, y_valid))\n",
    "\n",
    "x_test = np.hstack((x_test, np.random.normal(loc = 0, scale = 0.05, size = y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 123\n",
    "intermediate_dim = 80\n",
    "latent_dim = 32\n",
    "batch_size = 1409\n",
    "epochs = 5\n",
    "hidden_layers = 8\n",
    "\n",
    "class Train:\n",
    "    def train():\n",
    "        Train.x = Input(shape=(input_dim,))\n",
    "        \n",
    "        hidden_encoder = Train.x\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_encoder = Dense(intermediate_dim, activation='relu')(hidden_encoder)\n",
    "\n",
    "        mean_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        logvar_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        def get_distrib(args):\n",
    "\n",
    "            mean_encoder, logvar_encoder = args\n",
    "\n",
    "            # Sample epsilon\n",
    "            epsilon = np.random.normal(loc=0.0, scale=0.05, size = (batch_size, latent_dim))\n",
    "\n",
    "            # Sample latent variable\n",
    "            z = mean_encoder + K.exp(logvar_encoder / 2) * epsilon\n",
    "            return z\n",
    "\n",
    "        z = Lambda(get_distrib)([mean_encoder, logvar_encoder])\n",
    "\n",
    "        hidden_decoder = z\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_decoder = Dense(intermediate_dim, activation=\"relu\")(hidden_decoder)\n",
    "\n",
    "        Train.x_ = Dense(input_dim, activation=None)(hidden_decoder)\n",
    "\n",
    "def get_loss(x, x_):\n",
    "    xent_loss = input_dim * metrics.binary_crossentropy(x, x_) \n",
    "    kl_loss = - 0.5 * K.sum(1 + logvar_encoder - K.square(mean_encoder) - K.exp(logvar_encoder), axis=-1)\n",
    "    return K.abs(K.mean(xent_loss + kl_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:4\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Loss: 0.865976894603056, Test Loss: 1.9359437078237534\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:8\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Loss: 0.8005657160983366, Test Loss: 1.8731894120573997\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:16\n",
      "15499/22544 [===================>..........] - ETA: 0s\n",
      " Train Loss: 0.8843785594491398, Test Loss: 2.8732849955558777\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:32\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Loss: 0.8616735549534068, Test Loss: 1.9559364691376686\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:256\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 1.8556335919043596, Test Loss: 1443.4688436612487\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:2 features count:1024\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.7048630118370056, Test Loss: 15.560515195131302\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:4\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.7588324091013741, Test Loss: 1.8503988049924374\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:8\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.7609234872986289, Test Loss: 1.8457798957824707\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:16\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.7682473203715157, Test Loss: 1.8372743837535381\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:32\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Loss: 0.7811250546399284, Test Loss: 1.8598576188087463\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:256\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Loss: 0.7576144968762117, Test Loss: 1.8286715783178806\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:6 features count:1024\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.6987065041766447, Test Loss: 1.7861705608665943\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:4\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Loss: 0.822284716017106, Test Loss: 1.9243359491229057\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:8\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Loss: 0.8330686197561377, Test Loss: 1.9305750951170921\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:16\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Loss: 0.8073793614611906, Test Loss: 1.8968224823474884\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:32\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Loss: 0.762059723629671, Test Loss: 1.8355343118309975\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:256\n",
      "18317/22544 [=======================>......] - ETA: 0s\n",
      " Train Loss: 0.7984178802546333, Test Loss: 1.8995782658457756\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:10 features count:1024\n",
      "21135/22544 [===========================>..] - ETA: 0s\n",
      " Train Loss: 0.8166557550430298, Test Loss: 1.9179733842611313\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:4\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.914388078100541, Test Loss: 1.9815089106559753\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:8\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.914393400444704, Test Loss: 1.9811769500374794\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:16\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.9143729209899902, Test Loss: 1.9810647442936897\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:32\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.914381791563595, Test Loss: 1.9816859737038612\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:256\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.9143872997340035, Test Loss: 1.9821809381246567\n",
      " \n",
      " Current Layer Attributes - epochs:1 hidden layers:100 features count:1024\n",
      "22544/22544 [==============================] - 1s     \n",
      "\n",
      " Train Loss: 0.9143500748802634, Test Loss: 1.9815848618745804\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "features_arr = [4, 8, 16, 32, 256, 1024]\n",
    "hidden_layers_arr = [2, 6, 10, 100]\n",
    "\n",
    "epoch_arr = [1]\n",
    "\n",
    "score = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "scores = []\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for e, h, f in itertools.product(epoch_arr, hidden_layers_arr, features_arr):\n",
    "    \n",
    "    print(\" \\n Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "    latent_dim = f\n",
    "    epochs = e\n",
    "    hidden_layers = h\n",
    "\n",
    "    Train.train()\n",
    "\n",
    "    vae_model = Model(inputs = Train.x, outputs = Train.x_ )\n",
    "    vae_model.compile(optimizer = \"adam\", loss = \"mean_squared_error\" )\n",
    "\n",
    "    train_size = x_train.shape[0] - x_train.shape[0]%batch_size\n",
    "    valid_size = x_valid.shape[0] - x_valid.shape[0]%batch_size\n",
    "\n",
    "    vae_model.fit(x = x_train[:train_size,:], y = x_train[:train_size,:], \n",
    "                  shuffle=True, epochs=epochs, \n",
    "                  batch_size = batch_size, \n",
    "                  validation_data = (x_valid[:valid_size,:], x_valid[:valid_size,:]),\n",
    "                  verbose = 0)\n",
    "    score_train = vae_model.evaluate(x_valid[:valid_size,:], y = x_valid[:valid_size,:],\n",
    "                               batch_size = batch_size,\n",
    "                               verbose = 0)\n",
    "    score_test = vae_model.evaluate(x_test, y = x_test,\n",
    "                           batch_size = batch_size,\n",
    "                           verbose = 1)\n",
    "    y_test_pred = vae_model.predict(x_test, batch_size=batch_size)\n",
    "    \n",
    "    y_pred = y_test_pred[:,-1]\n",
    "    \n",
    "    y_pred = y_test_pred[:,-1]\n",
    "    y_pred[y_pred >= y_test_pred[:,-1].mean()] = 1\n",
    "    y_pred[y_pred < y_test_pred[:,-1].mean()] = 0\n",
    "    #print (y_pred)\n",
    "    \n",
    "    scores.append(score(e,f,h,score_train, score_test))\n",
    "    curr_pred = pd.DataFrame({\"{}_{}_{}\".format(e,f,h):y_pred},)\n",
    "    predictions = pd.concat([predictions, curr_pred], axis = 1)\n",
    "    \n",
    "    print(\"\\n Train Loss: {}, Test Loss: {}\".format(score_train, score_test)  )\n",
    "    \n",
    "scores = pd.DataFrame(scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865977</td>\n",
       "      <td>1.935944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.800566</td>\n",
       "      <td>1.873189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.884379</td>\n",
       "      <td>2.873285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861674</td>\n",
       "      <td>1.955936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>1.855634</td>\n",
       "      <td>1443.468844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>0.704863</td>\n",
       "      <td>15.560515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.758832</td>\n",
       "      <td>1.850399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.760923</td>\n",
       "      <td>1.845780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.768247</td>\n",
       "      <td>1.837274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.781125</td>\n",
       "      <td>1.859858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>0.757614</td>\n",
       "      <td>1.828672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>6</td>\n",
       "      <td>0.698707</td>\n",
       "      <td>1.786171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.822285</td>\n",
       "      <td>1.924336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833069</td>\n",
       "      <td>1.930575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807379</td>\n",
       "      <td>1.896822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.762060</td>\n",
       "      <td>1.835534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>0.798418</td>\n",
       "      <td>1.899578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816656</td>\n",
       "      <td>1.917973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914388</td>\n",
       "      <td>1.981509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914393</td>\n",
       "      <td>1.981177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914373</td>\n",
       "      <td>1.981065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914382</td>\n",
       "      <td>1.981686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914387</td>\n",
       "      <td>1.982181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>100</td>\n",
       "      <td>0.914350</td>\n",
       "      <td>1.981585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score   test_score\n",
       "0       1               4              2     0.865977     1.935944\n",
       "1       1               8              2     0.800566     1.873189\n",
       "2       1              16              2     0.884379     2.873285\n",
       "3       1              32              2     0.861674     1.955936\n",
       "4       1             256              2     1.855634  1443.468844\n",
       "5       1            1024              2     0.704863    15.560515\n",
       "6       1               4              6     0.758832     1.850399\n",
       "7       1               8              6     0.760923     1.845780\n",
       "8       1              16              6     0.768247     1.837274\n",
       "9       1              32              6     0.781125     1.859858\n",
       "10      1             256              6     0.757614     1.828672\n",
       "11      1            1024              6     0.698707     1.786171\n",
       "12      1               4             10     0.822285     1.924336\n",
       "13      1               8             10     0.833069     1.930575\n",
       "14      1              16             10     0.807379     1.896822\n",
       "15      1              32             10     0.762060     1.835534\n",
       "16      1             256             10     0.798418     1.899578\n",
       "17      1            1024             10     0.816656     1.917973\n",
       "18      1               4            100     0.914388     1.981509\n",
       "19      1               8            100     0.914393     1.981177\n",
       "20      1              16            100     0.914373     1.981065\n",
       "21      1              32            100     0.914382     1.981686\n",
       "22      1             256            100     0.914387     1.982181\n",
       "23      1            1024            100     0.914350     1.981585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.to_pickle(\"dataset/vae_only_predictions.pkl\")\n",
    "scores.to_pickle(\"dataset/vae_only_scores.pkl\")"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Final Hyper parameter tuning",
    "public": false
   },
   "id": "33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
