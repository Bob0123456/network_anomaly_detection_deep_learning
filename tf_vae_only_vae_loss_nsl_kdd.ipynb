{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:28.735285Z",
     "start_time": "2017-05-31T00:37:28.102354Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:28.856454Z",
     "start_time": "2017-05-31T00:37:28.739211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:28.866702Z",
     "start_time": "2017-05-31T00:37:28.859291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:28.873212Z",
     "start_time": "2017-05-31T00:37:28.868301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:29.941605Z",
     "start_time": "2017-05-31T00:37:28.875864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:31.630107Z",
     "start_time": "2017-05-31T00:37:29.944519Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:32.410997Z",
     "start_time": "2017-05-31T00:37:31.638477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T00:37:32.827238Z",
     "start_time": "2017-05-31T00:37:32.421626Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    \n",
    "                    def train_batch():\n",
    "                        nonlocal train_loss\n",
    "                        _, train_loss = sess.run([net.train_op, \n",
    "                                                   net.regularized_loss, \n",
    "                                                   ], #net.summary_op\n",
    "                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                             net.y_: y_train[i,:], \n",
    "                                                             net.keep_prob:1})\n",
    "                    train_batch()\n",
    "                        \n",
    "                    count = 10\n",
    "                    while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count < 1):\n",
    "                        print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                        net.saver.restore(sess, \n",
    "                                          tf.train.latest_checkpoint('dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                     .format(epochs,h,f)))\n",
    "                        train_batch()\n",
    "                        count -=1\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    #if(train_loss > 1e9):\n",
    "                    \n",
    "                    #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "                    \n",
    "                #print(\"\")\n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                \n",
    "                accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                               net.pred, \n",
    "                                                               net.actual, net.y], \n",
    "                                                              feed_dict={net.x: preprocess.x_test, \n",
    "                                                                         net.y_: preprocess.y_test, \n",
    "                                                                         net.keep_prob:1})\n",
    "                #print(\"*************** \\n\")\n",
    "                print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, test_loss, accuracy))\n",
    "                #print(\"*************** \\n\")\n",
    "                #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "                \n",
    "                if accuracy > Train.best_acc_global:\n",
    "                    Train.best_acc_global = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                if accuracy > Train.best_acc:\n",
    "                    \n",
    "                    #net.saver.save(sess, \"dataset/tf_vae_only_vae_loss_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                    #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                    #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                    #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "                    Train.best_acc = accuracy\n",
    "                    \n",
    "                    if(not np.isnan(train_loss)):\n",
    "                        net.saver.save(sess, \n",
    "                                   \"dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}/model\"\n",
    "                                   .format(epochs,h,f), \n",
    "                                   global_step = epoch, \n",
    "                                   write_meta_graph=False)\n",
    "                    \n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):\n",
    "                                              (curr_pred, \n",
    "                                               Train.result(epochs, f, h,valid_accuracy, accuracy, time.perf_counter() - start_time))})\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.625726Z",
     "start_time": "2017-05-31T00:37:32.829322Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.000071 | Test Loss: 0.004926 | Test Accuracy: 0.746274\n",
      "Step 2 | Training Loss: 0.000312 | Test Loss: 0.006968 | Test Accuracy: 0.784333\n",
      "Step 3 | Training Loss: 0.000391 | Test Loss: 13932.844727 | Test Accuracy: 0.773510\n",
      "Step 4 | Training Loss: 0.000555 | Test Loss: 0.062941 | Test Accuracy: 0.790232\n",
      "Step 5 | Training Loss: 0.001124 | Test Loss: 0.008172 | Test Accuracy: 0.743568\n",
      "Step 6 | Training Loss: 0.000632 | Test Loss: 0.008513 | Test Accuracy: 0.756343\n",
      "Step 7 | Training Loss: 0.000269 | Test Loss: 0.007665 | Test Accuracy: 0.787083\n",
      "Step 8 | Training Loss: 0.000140 | Test Loss: 0.007296 | Test Accuracy: 0.773155\n",
      "Step 9 | Training Loss: 0.000089 | Test Loss: 0.008836 | Test Accuracy: 0.719171\n",
      "Step 10 | Training Loss: 0.000516 | Test Loss: 0.009324 | Test Accuracy: 0.761489\n",
      "Step 11 | Training Loss: 0.000262 | Test Loss: 0.008095 | Test Accuracy: 0.730483\n",
      "Step 12 | Training Loss: 0.000188 | Test Loss: 0.009603 | Test Accuracy: 0.724317\n",
      "Step 13 | Training Loss: 0.000582 | Test Loss: 0.009431 | Test Accuracy: 0.722764\n",
      "Step 14 | Training Loss: 0.000822 | Test Loss: 0.012452 | Test Accuracy: 0.735096\n",
      "Step 15 | Training Loss: 0.000335 | Test Loss: 0.008940 | Test Accuracy: 0.736471\n",
      "Step 16 | Training Loss: 0.000124 | Test Loss: 0.008710 | Test Accuracy: 0.742548\n",
      "Step 17 | Training Loss: 0.000429 | Test Loss: 0.009089 | Test Accuracy: 0.727156\n",
      "Step 18 | Training Loss: 0.000034 | Test Loss: 0.007379 | Test Accuracy: 0.730926\n",
      "Step 19 | Training Loss: 0.000494 | Test Loss: 0.008665 | Test Accuracy: 0.717974\n",
      "Step 20 | Training Loss: 0.000210 | Test Loss: 0.009897 | Test Accuracy: 0.705110\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.000483 | Test Loss: 0.014876 | Test Accuracy: 0.793471\n",
      "Step 2 | Training Loss: 0.000115 | Test Loss: 0.010262 | Test Accuracy: 0.820440\n",
      "Step 3 | Training Loss: 0.000199 | Test Loss: 0.011794 | Test Accuracy: 0.840002\n",
      "Step 4 | Training Loss: 0.006303 | Test Loss: 0.040629 | Test Accuracy: 0.769961\n",
      "Step 5 | Training Loss: 0.001843 | Test Loss: 0.025718 | Test Accuracy: 0.834634\n",
      "Step 6 | Training Loss: 0.000271 | Test Loss: 0.013993 | Test Accuracy: 0.841643\n",
      "Step 7 | Training Loss: 0.009856 | Test Loss: 0.031298 | Test Accuracy: 0.832993\n",
      "Step 8 | Training Loss: 0.001682 | Test Loss: 0.032042 | Test Accuracy: 0.821194\n",
      "Step 9 | Training Loss: 0.003598 | Test Loss: 0.040264 | Test Accuracy: 0.834191\n",
      "Step 10 | Training Loss: 0.003227 | Test Loss: 0.021675 | Test Accuracy: 0.793426\n",
      "Step 11 | Training Loss: 0.000943 | Test Loss: 0.020875 | Test Accuracy: 0.840756\n",
      "Step 12 | Training Loss: 0.006643 | Test Loss: 0.014202 | Test Accuracy: 0.842264\n",
      "Step 13 | Training Loss: 0.001187 | Test Loss: 0.012997 | Test Accuracy: 0.815694\n",
      "Step 14 | Training Loss: 0.018450 | Test Loss: 0.094865 | Test Accuracy: 0.766945\n",
      "Step 15 | Training Loss: 0.002265 | Test Loss: 0.033658 | Test Accuracy: 0.745165\n",
      "Step 16 | Training Loss: 0.001517 | Test Loss: 0.041319 | Test Accuracy: 0.793471\n",
      "Step 17 | Training Loss: 0.012331 | Test Loss: 0.053635 | Test Accuracy: 0.706663\n",
      "Step 18 | Training Loss: 0.016425 | Test Loss: 4033049001984.000000 | Test Accuracy: 0.729019\n",
      "Step 19 | Training Loss: 0.012410 | Test Loss: 0.188181 | Test Accuracy: 0.431068\n",
      "Step 20 | Training Loss: 0.011420 | Test Loss: 0.073225 | Test Accuracy: 0.776881\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.021120 | Test Loss: 0.043221 | Test Accuracy: 0.767876\n",
      "Step 2 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 3 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 4 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 5 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 6 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 7 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 8 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 9 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 11 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 12 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 13 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 14 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 15 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:20 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.001070 | Test Loss: 0.007429 | Test Accuracy: 0.734741\n",
      "Step 2 | Training Loss: 0.000366 | Test Loss: 0.004888 | Test Accuracy: 0.823279\n",
      "Step 3 | Training Loss: 0.000079 | Test Loss: 0.006929 | Test Accuracy: 0.794003\n",
      "Step 4 | Training Loss: 0.000046 | Test Loss: 0.007767 | Test Accuracy: 0.779941\n",
      "Step 5 | Training Loss: 0.000485 | Test Loss: 0.007815 | Test Accuracy: 0.776792\n",
      "Step 6 | Training Loss: 0.000308 | Test Loss: 0.008929 | Test Accuracy: 0.754746\n",
      "Step 7 | Training Loss: 0.000224 | Test Loss: 0.009713 | Test Accuracy: 0.725825\n",
      "Step 8 | Training Loss: 0.000157 | Test Loss: 0.010503 | Test Accuracy: 0.728265\n",
      "Step 9 | Training Loss: 0.000197 | Test Loss: 0.010105 | Test Accuracy: 0.713982\n",
      "Step 10 | Training Loss: 0.000102 | Test Loss: 0.012489 | Test Accuracy: 0.694863\n",
      "Step 11 | Training Loss: 0.000245 | Test Loss: 0.012058 | Test Accuracy: 0.726136\n",
      "Step 12 | Training Loss: 0.000214 | Test Loss: 0.014364 | Test Accuracy: 0.649840\n",
      "Step 13 | Training Loss: 0.000365 | Test Loss: 0.013549 | Test Accuracy: 0.687810\n",
      "Step 14 | Training Loss: 0.000022 | Test Loss: 0.013948 | Test Accuracy: 0.700452\n",
      "Step 15 | Training Loss: 0.000355 | Test Loss: 0.014387 | Test Accuracy: 0.704844\n",
      "Step 16 | Training Loss: 0.000339 | Test Loss: 0.016560 | Test Accuracy: 0.695130\n",
      "Step 17 | Training Loss: 0.000251 | Test Loss: 0.020710 | Test Accuracy: 0.698501\n",
      "Step 18 | Training Loss: 0.000382 | Test Loss: 0.021077 | Test Accuracy: 0.677209\n",
      "Step 19 | Training Loss: 0.000340 | Test Loss: 0.018595 | Test Accuracy: 0.656405\n",
      "Step 20 | Training Loss: 0.000211 | Test Loss: 0.021443 | Test Accuracy: 0.683730\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.007921 | Test Loss: 0.010531 | Test Accuracy: 0.484697\n",
      "Step 2 | Training Loss: 0.000014 | Test Loss: 0.007530 | Test Accuracy: 0.633339\n",
      "Step 3 | Training Loss: 0.001783 | Test Loss: 0.003626 | Test Accuracy: 0.783534\n",
      "Step 4 | Training Loss: 0.000451 | Test Loss: 0.001998 | Test Accuracy: 0.816670\n",
      "Step 5 | Training Loss: 0.001039 | Test Loss: 0.003999 | Test Accuracy: 0.804294\n",
      "Step 6 | Training Loss: 0.001137 | Test Loss: 0.003607 | Test Accuracy: 0.842841\n",
      "Step 7 | Training Loss: 0.000067 | Test Loss: 0.002935 | Test Accuracy: 0.830465\n",
      "Step 8 | Training Loss: 0.000187 | Test Loss: 0.002856 | Test Accuracy: 0.862535\n",
      "Step 9 | Training Loss: 0.000724 | Test Loss: 0.002231 | Test Accuracy: 0.822347\n",
      "Step 10 | Training Loss: 0.000913 | Test Loss: 0.001687 | Test Accuracy: 0.817779\n",
      "Step 11 | Training Loss: 0.000370 | Test Loss: 0.003866 | Test Accuracy: 0.733676\n",
      "Step 12 | Training Loss: 0.000404 | Test Loss: 0.004246 | Test Accuracy: 0.626730\n",
      "Step 13 | Training Loss: 0.000139 | Test Loss: 0.005917 | Test Accuracy: 0.656716\n",
      "Step 14 | Training Loss: 0.000530 | Test Loss: 0.008642 | Test Accuracy: 0.578912\n",
      "Step 15 | Training Loss: 0.000484 | Test Loss: 0.005196 | Test Accuracy: 0.610229\n",
      "Step 16 | Training Loss: 0.000170 | Test Loss: 0.006030 | Test Accuracy: 0.585034\n",
      "Step 17 | Training Loss: 0.000020 | Test Loss: 0.005343 | Test Accuracy: 0.588361\n",
      "Step 18 | Training Loss: 0.000183 | Test Loss: 0.005251 | Test Accuracy: 0.584901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19 | Training Loss: 0.000095 | Test Loss: 0.007873 | Test Accuracy: 0.560992\n",
      "Step 20 | Training Loss: 0.000209 | Test Loss: 0.005156 | Test Accuracy: 0.630456\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.001057 | Test Loss: 0.005686 | Test Accuracy: 0.705066\n",
      "Step 2 | Training Loss: 0.001158 | Test Loss: 0.007990 | Test Accuracy: 0.772401\n",
      "Step 3 | Training Loss: 0.001176 | Test Loss: 0.005418 | Test Accuracy: 0.684927\n",
      "Step 4 | Training Loss: 0.000124 | Test Loss: 0.005720 | Test Accuracy: 0.696593\n",
      "Step 5 | Training Loss: 0.000005 | Test Loss: 0.005581 | Test Accuracy: 0.692956\n",
      "Step 6 | Training Loss: 0.001437 | Test Loss: 0.006385 | Test Accuracy: 0.666430\n",
      "Step 7 | Training Loss: 0.000549 | Test Loss: 0.003972 | Test Accuracy: 0.702848\n",
      "Step 8 | Training Loss: 0.000717 | Test Loss: 0.004411 | Test Accuracy: 0.707284\n",
      "Step 9 | Training Loss: 0.000162 | Test Loss: 0.004703 | Test Accuracy: 0.672951\n",
      "Step 10 | Training Loss: 0.000768 | Test Loss: 0.003701 | Test Accuracy: 0.703025\n",
      "Step 11 | Training Loss: 0.000557 | Test Loss: 0.003090 | Test Accuracy: 0.715179\n",
      "Step 12 | Training Loss: 0.001241 | Test Loss: 0.004829 | Test Accuracy: 0.714336\n",
      "Step 13 | Training Loss: 0.000134 | Test Loss: 0.004487 | Test Accuracy: 0.704533\n",
      "Step 14 | Training Loss: 0.000057 | Test Loss: 0.003654 | Test Accuracy: 0.716377\n",
      "Step 15 | Training Loss: 0.000287 | Test Loss: 0.003411 | Test Accuracy: 0.716199\n",
      "Step 16 | Training Loss: 0.000194 | Test Loss: 0.004806 | Test Accuracy: 0.716599\n",
      "Step 17 | Training Loss: 0.001045 | Test Loss: 0.005267 | Test Accuracy: 0.689407\n",
      "Step 18 | Training Loss: 0.000158 | Test Loss: 0.004276 | Test Accuracy: 0.715711\n",
      "Step 19 | Training Loss: 0.000111 | Test Loss: 0.003871 | Test Accuracy: 0.712917\n",
      "Step 20 | Training Loss: 0.000096 | Test Loss: 0.004471 | Test Accuracy: 0.694375\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.001258 | Test Loss: 0.007794 | Test Accuracy: 0.712340\n",
      "Step 2 | Training Loss: 0.001058 | Test Loss: 0.005593 | Test Accuracy: 0.735850\n",
      "Step 3 | Training Loss: 0.000449 | Test Loss: 0.005369 | Test Accuracy: 0.732035\n",
      "Step 4 | Training Loss: 0.000036 | Test Loss: 0.005398 | Test Accuracy: 0.739088\n",
      "Step 5 | Training Loss: 0.000410 | Test Loss: 0.004212 | Test Accuracy: 0.783712\n",
      "Step 6 | Training Loss: 0.000094 | Test Loss: 0.005434 | Test Accuracy: 0.696726\n",
      "Step 7 | Training Loss: 0.000411 | Test Loss: 0.005406 | Test Accuracy: 0.713715\n",
      "Step 8 | Training Loss: 0.000313 | Test Loss: 0.004612 | Test Accuracy: 0.723829\n",
      "Step 9 | Training Loss: 0.000201 | Test Loss: 0.004254 | Test Accuracy: 0.729196\n",
      "Step 10 | Training Loss: 0.000828 | Test Loss: 0.004127 | Test Accuracy: 0.727511\n",
      "Step 11 | Training Loss: 0.000484 | Test Loss: 0.005051 | Test Accuracy: 0.698589\n",
      "Step 12 | Training Loss: 0.000081 | Test Loss: 0.005059 | Test Accuracy: 0.694154\n",
      "Step 13 | Training Loss: 0.000235 | Test Loss: 0.004056 | Test Accuracy: 0.704578\n",
      "Step 14 | Training Loss: 0.000103 | Test Loss: 0.004739 | Test Accuracy: 0.766501\n",
      "Step 15 | Training Loss: 0.000404 | Test Loss: 0.004605 | Test Accuracy: 0.740907\n",
      "Step 16 | Training Loss: 0.000903 | Test Loss: 0.004846 | Test Accuracy: 0.691625\n",
      "Step 17 | Training Loss: 0.000023 | Test Loss: 0.004071 | Test Accuracy: 0.698323\n",
      "Step 18 | Training Loss: 0.000202 | Test Loss: 0.004798 | Test Accuracy: 0.708703\n",
      "Step 19 | Training Loss: 0.000167 | Test Loss: 0.004847 | Test Accuracy: 0.824876\n",
      "Step 20 | Training Loss: 0.000851 | Test Loss: 0.005362 | Test Accuracy: 0.798749\n",
      "Current Layer Attributes - epochs:20 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.001085 | Test Loss: 0.003555 | Test Accuracy: 0.787083\n",
      "Step 2 | Training Loss: 0.000137 | Test Loss: 0.003698 | Test Accuracy: 0.774131\n",
      "Step 3 | Training Loss: 0.000323 | Test Loss: 0.007124 | Test Accuracy: 0.657248\n",
      "Step 4 | Training Loss: 0.000859 | Test Loss: 0.006628 | Test Accuracy: 0.708304\n",
      "Step 5 | Training Loss: 0.000310 | Test Loss: 0.003778 | Test Accuracy: 0.809218\n",
      "Step 6 | Training Loss: 0.000764 | Test Loss: 0.005518 | Test Accuracy: 0.761311\n",
      "Step 7 | Training Loss: 0.000637 | Test Loss: 0.007917 | Test Accuracy: 0.703158\n",
      "Step 8 | Training Loss: 0.000151 | Test Loss: 0.006606 | Test Accuracy: 0.736293\n",
      "Step 9 | Training Loss: 0.000244 | Test Loss: 0.003483 | Test Accuracy: 0.710256\n",
      "Step 10 | Training Loss: 0.001177 | Test Loss: 0.004413 | Test Accuracy: 0.764461\n",
      "Step 11 | Training Loss: 0.001179 | Test Loss: 0.004079 | Test Accuracy: 0.747960\n",
      "Step 12 | Training Loss: 0.000592 | Test Loss: 0.003447 | Test Accuracy: 0.764594\n",
      "Step 13 | Training Loss: 0.000099 | Test Loss: 0.003756 | Test Accuracy: 0.752839\n",
      "Step 14 | Training Loss: 0.000097 | Test Loss: 0.003328 | Test Accuracy: 0.753105\n",
      "Step 15 | Training Loss: 0.000304 | Test Loss: 0.003695 | Test Accuracy: 0.733277\n",
      "Step 16 | Training Loss: 0.000052 | Test Loss: 0.003444 | Test Accuracy: 0.727511\n",
      "Step 17 | Training Loss: 0.000124 | Test Loss: 0.003232 | Test Accuracy: 0.748181\n",
      "Step 18 | Training Loss: 0.000678 | Test Loss: 0.004007 | Test Accuracy: 0.756254\n",
      "Step 19 | Training Loss: 0.000488 | Test Loss: 0.004145 | Test Accuracy: 0.783801\n",
      "Step 20 | Training Loss: 0.000200 | Test Loss: 0.003238 | Test Accuracy: 0.782115\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.002930 | Test Loss: 0.003436 | Test Accuracy: 0.753371\n",
      "Step 2 | Training Loss: 0.000185 | Test Loss: 0.004045 | Test Accuracy: 0.829223\n",
      "Step 3 | Training Loss: 0.000856 | Test Loss: 0.004980 | Test Accuracy: 0.851313\n",
      "Step 4 | Training Loss: 0.000625 | Test Loss: 0.002723 | Test Accuracy: 0.812589\n",
      "Step 5 | Training Loss: 0.001356 | Test Loss: 0.005157 | Test Accuracy: 0.849539\n",
      "Step 6 | Training Loss: 0.000241 | Test Loss: 0.002396 | Test Accuracy: 0.869633\n",
      "Step 7 | Training Loss: 0.000482 | Test Loss: 0.003121 | Test Accuracy: 0.779941\n",
      "Step 8 | Training Loss: 0.000785 | Test Loss: 0.001205 | Test Accuracy: 0.792583\n",
      "Step 9 | Training Loss: 0.001196 | Test Loss: 0.001156 | Test Accuracy: 0.719571\n",
      "Step 10 | Training Loss: 0.001490 | Test Loss: 0.001427 | Test Accuracy: 0.846789\n",
      "Step 11 | Training Loss: 0.000465 | Test Loss: 0.002956 | Test Accuracy: 0.778966\n",
      "Step 12 | Training Loss: 0.000441 | Test Loss: 0.000287 | Test Accuracy: 0.845946\n",
      "Step 13 | Training Loss: 0.000412 | Test Loss: 0.000383 | Test Accuracy: 0.848696\n",
      "Step 14 | Training Loss: 0.000999 | Test Loss: 0.001251 | Test Accuracy: 0.824876\n",
      "Step 15 | Training Loss: 0.000262 | Test Loss: 0.000534 | Test Accuracy: 0.842264\n",
      "Step 16 | Training Loss: 0.002592 | Test Loss: 0.001074 | Test Accuracy: 0.759359\n",
      "Step 17 | Training Loss: 0.000063 | Test Loss: 0.005993 | Test Accuracy: 0.660664\n",
      "Step 18 | Training Loss: 0.000430 | Test Loss: 0.004681 | Test Accuracy: 0.611959\n",
      "Step 19 | Training Loss: 0.000918 | Test Loss: 0.006001 | Test Accuracy: 0.616395\n",
      "Step 20 | Training Loss: 0.000442 | Test Loss: 0.003886 | Test Accuracy: 0.562633\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.001421 | Test Loss: 0.008431 | Test Accuracy: 0.773687\n",
      "Step 2 | Training Loss: 0.000982 | Test Loss: 0.003735 | Test Accuracy: 0.758073\n",
      "Step 3 | Training Loss: 0.000760 | Test Loss: 0.004115 | Test Accuracy: 0.764194\n",
      "Step 4 | Training Loss: 0.000149 | Test Loss: 0.002275 | Test Accuracy: 0.761799\n",
      "Step 5 | Training Loss: 0.000867 | Test Loss: 0.001478 | Test Accuracy: 0.814807\n",
      "Step 6 | Training Loss: 0.000602 | Test Loss: 0.002391 | Test Accuracy: 0.819287\n",
      "Step 7 | Training Loss: 0.000437 | Test Loss: 0.001697 | Test Accuracy: 0.832638\n",
      "Step 8 | Training Loss: 0.001018 | Test Loss: 0.002106 | Test Accuracy: 0.833126\n",
      "Step 9 | Training Loss: 0.000323 | Test Loss: 0.001999 | Test Accuracy: 0.818621\n",
      "Step 10 | Training Loss: 0.000680 | Test Loss: 0.001883 | Test Accuracy: 0.797285\n",
      "Step 11 | Training Loss: 0.000135 | Test Loss: 0.002997 | Test Accuracy: 0.770893\n",
      "Step 12 | Training Loss: 0.000526 | Test Loss: 0.001588 | Test Accuracy: 0.808419\n",
      "Step 13 | Training Loss: 0.001171 | Test Loss: 0.002525 | Test Accuracy: 0.763529\n",
      "Step 14 | Training Loss: 0.001386 | Test Loss: 0.003705 | Test Accuracy: 0.507541\n",
      "Step 15 | Training Loss: 0.000367 | Test Loss: 0.004064 | Test Accuracy: 0.515836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.008269 | Test Loss: 0.001023 | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: 0.000361 | Test Loss: 0.007268 | Test Accuracy: 0.544402\n",
      "Step 18 | Training Loss: 0.002619 | Test Loss: 0.003488 | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: 0.000967 | Test Loss: 0.001948 | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: 0.000580 | Test Loss: 0.000870 | Test Accuracy: 0.458481\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 0.000799 | Test Loss: 0.003681 | Test Accuracy: 0.743879\n",
      "Step 2 | Training Loss: 0.000995 | Test Loss: 0.002404 | Test Accuracy: 0.827227\n",
      "Step 3 | Training Loss: 0.001711 | Test Loss: 0.001770 | Test Accuracy: 0.826517\n",
      "Step 4 | Training Loss: 0.000200 | Test Loss: 0.003523 | Test Accuracy: 0.781317\n",
      "Step 5 | Training Loss: 0.001462 | Test Loss: 0.002775 | Test Accuracy: 0.823678\n",
      "Step 6 | Training Loss: 0.000023 | Test Loss: 0.002873 | Test Accuracy: 0.790321\n",
      "Step 7 | Training Loss: 0.000355 | Test Loss: 0.002590 | Test Accuracy: 0.798527\n",
      "Step 8 | Training Loss: 0.001500 | Test Loss: 0.002271 | Test Accuracy: 0.823678\n",
      "Step 9 | Training Loss: 0.000884 | Test Loss: 0.001581 | Test Accuracy: 0.833969\n",
      "Step 10 | Training Loss: 0.000051 | Test Loss: 0.000639 | Test Accuracy: 0.840845\n",
      "Step 11 | Training Loss: 0.000169 | Test Loss: 0.000628 | Test Accuracy: 0.839913\n",
      "Step 12 | Training Loss: 0.001784 | Test Loss: 0.000945 | Test Accuracy: 0.836719\n",
      "Step 13 | Training Loss: 0.000644 | Test Loss: 0.002279 | Test Accuracy: 0.837828\n",
      "Step 14 | Training Loss: 0.000760 | Test Loss: 0.001082 | Test Accuracy: 0.829312\n",
      "Step 15 | Training Loss: 0.006609 | Test Loss: 0.011578 | Test Accuracy: 0.455110\n",
      "Step 16 | Training Loss: 0.006330 | Test Loss: 0.008801 | Test Accuracy: 0.450896\n",
      "Step 17 | Training Loss: 0.007918 | Test Loss: 0.007780 | Test Accuracy: 0.431955\n",
      "Step 18 | Training Loss: 0.003999 | Test Loss: 0.006740 | Test Accuracy: 0.434306\n",
      "Step 19 | Training Loss: 0.001305 | Test Loss: 0.007036 | Test Accuracy: 0.431467\n",
      "Step 20 | Training Loss: 0.007345 | Test Loss: 0.005528 | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:20 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.001359 | Test Loss: 0.003180 | Test Accuracy: 0.845413\n",
      "Step 2 | Training Loss: 0.004567 | Test Loss: 0.009158 | Test Accuracy: 0.629303\n",
      "Step 3 | Training Loss: 0.000443 | Test Loss: 0.003809 | Test Accuracy: 0.765082\n",
      "Step 4 | Training Loss: 0.000475 | Test Loss: 0.003679 | Test Accuracy: 0.774485\n",
      "Step 5 | Training Loss: 0.000567 | Test Loss: 0.005786 | Test Accuracy: 0.737713\n",
      "Step 6 | Training Loss: 0.007452 | Test Loss: 0.007961 | Test Accuracy: 0.430758\n",
      "Step 7 | Training Loss: 0.007280 | Test Loss: 0.008254 | Test Accuracy: 0.430758\n",
      "Step 8 | Training Loss: 0.007085 | Test Loss: 0.008269 | Test Accuracy: 0.430758\n",
      "Step 9 | Training Loss: 0.007080 | Test Loss: 0.008260 | Test Accuracy: 0.430758\n",
      "Step 10 | Training Loss: 0.007000 | Test Loss: 0.008264 | Test Accuracy: 0.430758\n",
      "Step 11 | Training Loss: 0.007498 | Test Loss: 0.008358 | Test Accuracy: 0.430758\n",
      "Step 12 | Training Loss: 0.009271 | Test Loss: 0.008289 | Test Accuracy: 0.430758\n",
      "Step 13 | Training Loss: 0.006858 | Test Loss: 0.008548 | Test Accuracy: 0.430758\n",
      "Step 14 | Training Loss: 0.006951 | Test Loss: 0.008321 | Test Accuracy: 0.430758\n",
      "Step 15 | Training Loss: 0.007368 | Test Loss: 0.008569 | Test Accuracy: 0.430758\n",
      "Step 16 | Training Loss: 0.007001 | Test Loss: 0.008384 | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: 0.007234 | Test Loss: 0.008571 | Test Accuracy: 0.430758\n",
      "Step 18 | Training Loss: 0.007215 | Test Loss: 0.008503 | Test Accuracy: 0.430758\n",
      "Step 19 | Training Loss: 0.007586 | Test Loss: 0.008455 | Test Accuracy: 0.430758\n",
      "Step 20 | Training Loss: 0.007193 | Test Loss: 0.008589 | Test Accuracy: 0.430758\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [20]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.632568Z",
     "start_time": "2017-05-31T01:05:48.627317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.699647Z",
     "start_time": "2017-05-31T01:05:48.634806Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.706249Z",
     "start_time": "2017-05-31T01:05:48.701276Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.722532Z",
     "start_time": "2017-05-31T01:05:48.707776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.856326</td>\n",
       "      <td>0.869633</td>\n",
       "      <td>58.462562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.865375</td>\n",
       "      <td>0.862535</td>\n",
       "      <td>62.101604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.904509</td>\n",
       "      <td>0.845413</td>\n",
       "      <td>7.644736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.920067</td>\n",
       "      <td>0.842264</td>\n",
       "      <td>63.524530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.778060</td>\n",
       "      <td>0.840845</td>\n",
       "      <td>73.350619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.844658</td>\n",
       "      <td>0.833126</td>\n",
       "      <td>77.473783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.905223</td>\n",
       "      <td>0.824876</td>\n",
       "      <td>135.412426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.950548</td>\n",
       "      <td>0.823279</td>\n",
       "      <td>11.461275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.903477</td>\n",
       "      <td>0.809218</td>\n",
       "      <td>36.302240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943086</td>\n",
       "      <td>0.790232</td>\n",
       "      <td>21.424091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911256</td>\n",
       "      <td>0.772401</td>\n",
       "      <td>13.949796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.953564</td>\n",
       "      <td>0.767876</td>\n",
       "      <td>5.662451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  time_taken\n",
       "8      20               4              6     0.856326    0.869633   58.462562\n",
       "4      20               4              4     0.865375    0.862535   62.101604\n",
       "11     20              32              6     0.904509    0.845413    7.644736\n",
       "1      20               8              2     0.920067    0.842264   63.524530\n",
       "10     20              16              6     0.778060    0.840845   73.350619\n",
       "9      20               8              6     0.844658    0.833126   77.473783\n",
       "6      20              16              4     0.905223    0.824876  135.412426\n",
       "3      20              32              2     0.950548    0.823279   11.461275\n",
       "7      20              32              4     0.903477    0.809218   36.302240\n",
       "0      20               4              2     0.943086    0.790232   21.424091\n",
       "5      20               8              4     0.911256    0.772401   13.949796\n",
       "2      20              16              2     0.953564    0.767876    5.662451"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.738707Z",
     "start_time": "2017-05-31T01:05:48.724305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:48.801886Z",
     "start_time": "2017-05-31T01:05:48.740439Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:05:49.128196Z",
     "start_time": "2017-05-31T01:05:48.803439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.9017  0.0983]\n",
      " [ 0.1727  0.8273]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCx3HNzMzCx3HNzMzCx3HNzMzCx3/j8sAE16SbFFwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae9c3eb518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
