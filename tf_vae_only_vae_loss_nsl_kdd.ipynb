{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:45:55.216303Z",
     "start_time": "2017-07-19T21:45:54.812696Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:45:55.227206Z",
     "start_time": "2017-07-19T21:45:55.217789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:45:58.120943Z",
     "start_time": "2017-07-19T21:45:58.019631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}, '20140118': {'x': 'dataset/Kyoto2016/2014/01/20140118_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140118_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:45:59.501387Z",
     "start_time": "2017-07-19T21:45:58.122336Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:45:59.922368Z",
     "start_time": "2017-07-19T21:45:59.502998Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 42\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:46:00.780436Z",
     "start_time": "2017-07-19T21:46:00.553437Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                            train_batch()\n",
    "\n",
    "                            count = 10\n",
    "                            while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                                print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                             .format(epochs,h,f)))\n",
    "                                train_batch()\n",
    "                                count -=1\n",
    "\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            #if(train_loss > 1e9):\n",
    "\n",
    "                            #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "\n",
    "                        #print(\"\")\n",
    "                        valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                             net.y_: y_valid, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    end_time = time.perf_counter()\n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "\n",
    "                        accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                       net.pred, \n",
    "                                                                       net.actual, net.y], \n",
    "                                                                      feed_dict={net.x: x_test, \n",
    "                                                                                 net.y_: y_test, \n",
    "                                                                                 net.keep_prob:1, net.lr:lr})\n",
    "                        f1_score = me.f1_score(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Train Accuracy: {:.6f} | Test Accuracy: {:.6f}, f1_score: {}\".format(key, train_loss, valid_accuracy, accuracy, f1_score))\n",
    "                       \n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = accuracy\n",
    "\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(key, f, h,valid_accuracy, accuracy, f1_score, end_time - start_time))})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T21:46:00.846490Z",
     "start_time": "2017-07-19T21:46:00.782106Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        print(\"********************************** Training ******************************\")\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-5]\n",
    "        print(\"***************************** Entering Loop **********************\")\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - hidden layers:{} features count:{}\".format(h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = df_results#temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results, ignore_index=True).to_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:05:25.634266Z",
     "start_time": "2017-07-19T21:46:00.847925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "***************************** Entering Loop **********************\n",
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20151224 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.986750, f1_score: 0.9933310021881523\n",
      "Key 20151216 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.978838, f1_score: 0.9893056189838565\n",
      "Key 20151222 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.986742, f1_score: 0.9933266422386666\n",
      "Key 20151214 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.984972, f1_score: 0.9924291376120528\n",
      "Key 20151202 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.953520, f1_score: 0.976207067357339\n",
      "Key 20151227 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.988197, f1_score: 0.9940636182902585\n",
      "Key 20151203 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.906253, f1_score: 0.9508214683813164\n",
      "Key 20151223 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.986512, f1_score: 0.9932103564794799\n",
      "Key 20151229 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.972761, f1_score: 0.9861923758059297\n",
      "Key 20151208 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.688859, f1_score: 0.8157684951148574\n",
      "Key 20151206 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.986692, f1_score: 0.993301487415298\n",
      "Key 20151217 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.964218, f1_score: 0.9817829292197555\n",
      "Key 20151215 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.980809, f1_score: 0.9903112932175426\n",
      "Key 20151213 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.990597, f1_score: 0.9952762415118019\n",
      "Key 20151209 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.982295, f1_score: 0.9910686368557518\n",
      "Key 20151228 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.988685, f1_score: 0.9943105724311051\n",
      "Key 20151226 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.978831, f1_score: 0.9893020210498383\n",
      "Key 20151231 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.925385, f1_score: 0.9612469016510523\n",
      "Key 20151212 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.978191, f1_score: 0.988975405233407\n",
      "Key 20151211 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.989978, f1_score: 0.994963862147604\n",
      "Key 20151221 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.986713, f1_score: 0.9933120326694206\n",
      "Key 20151201 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.969552, f1_score: 0.984540836630071\n",
      "Key 20151220 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n",
      "Key 20151230 | Training Loss: 0.009191 | Train Accuracy: 0.994156 | Test Accuracy: 0.948910, f1_score: 0.9737850931567794\n",
      "Current Layer Attributes - hidden layers:1 features count:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.017113, f1_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.013250, f1_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.021162, f1_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.013258, f1_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.015028, f1_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.046480, f1_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.011803, f1_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.093747, f1_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.034788, f1_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.013488, f1_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.027239, f1_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.016649, f1_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.311141, f1_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.014715, f1_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.014154, f1_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.013308, f1_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.033611, f1_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.035782, f1_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.019191, f1_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.009403, f1_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.017705, f1_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.011314, f1_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.097530, f1_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.021169, f1_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.074615, f1_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.021809, f1_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.010022, f1_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.013287, f1_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.030448, f1_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.061712, f1_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.005937 | Test Accuracy: 0.051090, f1_score: 0.0\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.017113, f1_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.013250, f1_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.021162, f1_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.013258, f1_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.015028, f1_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.046480, f1_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.011803, f1_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.093747, f1_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.034788, f1_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.013488, f1_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.027239, f1_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.016649, f1_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.311141, f1_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.014715, f1_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.014154, f1_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.013308, f1_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.033611, f1_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.035782, f1_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.019191, f1_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.009403, f1_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.017705, f1_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.011314, f1_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.097530, f1_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.021169, f1_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.074615, f1_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.021809, f1_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.010022, f1_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.013287, f1_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.030448, f1_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.061712, f1_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.005984 | Test Accuracy: 0.051090, f1_score: 0.0\n",
      "Current Layer Attributes - hidden layers:1 features count:16\n",
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.017113, f1_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.013250, f1_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.021162, f1_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.013258, f1_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.015028, f1_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.046480, f1_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.011803, f1_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.093747, f1_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.034788, f1_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.013488, f1_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.027239, f1_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.016649, f1_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.311141, f1_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.014715, f1_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.014154, f1_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.013308, f1_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.033611, f1_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.035782, f1_score: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.019191, f1_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.009403, f1_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.017705, f1_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.011314, f1_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.097530, f1_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.021169, f1_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.074615, f1_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.021809, f1_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.010022, f1_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.013287, f1_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.030448, f1_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.061712, f1_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.005657 | Test Accuracy: 0.051090, f1_score: 0.0\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.017113, f1_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.013250, f1_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.021162, f1_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.013258, f1_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.015028, f1_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.046480, f1_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.011803, f1_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.093747, f1_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.034788, f1_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.013488, f1_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.027239, f1_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.016649, f1_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.311141, f1_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.014715, f1_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.014154, f1_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.013308, f1_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.033611, f1_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.035782, f1_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.019191, f1_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.009403, f1_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.017705, f1_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.011314, f1_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.097530, f1_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.021169, f1_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.074615, f1_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.021809, f1_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.010022, f1_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.013287, f1_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.030448, f1_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.061712, f1_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.007480 | Test Accuracy: 0.051090, f1_score: 0.0\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20151224 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.986750, f1_score: 0.9933310021881523\n",
      "Key 20151216 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.978838, f1_score: 0.9893056189838565\n",
      "Key 20151222 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.986742, f1_score: 0.9933266422386666\n",
      "Key 20151214 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.984972, f1_score: 0.9924291376120528\n",
      "Key 20151202 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.953520, f1_score: 0.976207067357339\n",
      "Key 20151227 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.988197, f1_score: 0.9940636182902585\n",
      "Key 20151203 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.906253, f1_score: 0.9508214683813164\n",
      "Key 20151223 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.986512, f1_score: 0.9932103564794799\n",
      "Key 20151229 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.972761, f1_score: 0.9861923758059297\n",
      "Key 20151208 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.688859, f1_score: 0.8157684951148574\n",
      "Key 20151206 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.986692, f1_score: 0.993301487415298\n",
      "Key 20151217 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.964218, f1_score: 0.9817829292197555\n",
      "Key 20151215 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.980809, f1_score: 0.9903112932175426\n",
      "Key 20151213 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.990597, f1_score: 0.9952762415118019\n",
      "Key 20151209 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.982295, f1_score: 0.9910686368557518\n",
      "Key 20151228 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.988685, f1_score: 0.9943105724311051\n",
      "Key 20151226 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.978831, f1_score: 0.9893020210498383\n",
      "Key 20151231 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.925385, f1_score: 0.9612469016510523\n",
      "Key 20151212 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.978191, f1_score: 0.988975405233407\n",
      "Key 20151211 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.989978, f1_score: 0.994963862147604\n",
      "Key 20151221 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.986713, f1_score: 0.9933120326694206\n",
      "Key 20151201 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.969552, f1_score: 0.984540836630071\n",
      "Key 20151220 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151230 | Training Loss: 0.001588 | Train Accuracy: 0.993081 | Test Accuracy: 0.948910, f1_score: 0.9737850931567794\n",
      "Current Layer Attributes - hidden layers:3 features count:4\n",
      "Key 20151224 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.986750, f1_score: 0.9933310021881523\n",
      "Key 20151216 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.978838, f1_score: 0.9893056189838565\n",
      "Key 20151222 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.986742, f1_score: 0.9933266422386666\n",
      "Key 20151214 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.984972, f1_score: 0.9924291376120528\n",
      "Key 20151202 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.953520, f1_score: 0.976207067357339\n",
      "Key 20151227 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.988197, f1_score: 0.9940636182902585\n",
      "Key 20151203 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.906253, f1_score: 0.9508214683813164\n",
      "Key 20151223 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.986512, f1_score: 0.9932103564794799\n",
      "Key 20151229 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.972761, f1_score: 0.9861923758059297\n",
      "Key 20151208 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.688859, f1_score: 0.8157684951148574\n",
      "Key 20151206 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.986692, f1_score: 0.993301487415298\n",
      "Key 20151217 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.964218, f1_score: 0.9817829292197555\n",
      "Key 20151215 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.980809, f1_score: 0.9903112932175426\n",
      "Key 20151213 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.990597, f1_score: 0.9952762415118019\n",
      "Key 20151209 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.982295, f1_score: 0.9910686368557518\n",
      "Key 20151228 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.988685, f1_score: 0.9943105724311051\n",
      "Key 20151226 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.978831, f1_score: 0.9893020210498383\n",
      "Key 20151231 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.925385, f1_score: 0.9612469016510523\n",
      "Key 20151212 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.978191, f1_score: 0.988975405233407\n",
      "Key 20151211 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.989978, f1_score: 0.994963862147604\n",
      "Key 20151221 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.986713, f1_score: 0.9933120326694206\n",
      "Key 20151201 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.969552, f1_score: 0.984540836630071\n",
      "Key 20151220 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n",
      "Key 20151230 | Training Loss: 0.000357 | Train Accuracy: 0.993081 | Test Accuracy: 0.948910, f1_score: 0.9737850931567794\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20151224 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.986750, f1_score: 0.9933310021881523\n",
      "Key 20151216 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.978838, f1_score: 0.9893056189838565\n",
      "Key 20151222 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.986742, f1_score: 0.9933266422386666\n",
      "Key 20151214 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.984972, f1_score: 0.9924291376120528\n",
      "Key 20151202 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.953520, f1_score: 0.976207067357339\n",
      "Key 20151227 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.988197, f1_score: 0.9940636182902585\n",
      "Key 20151203 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.906253, f1_score: 0.9508214683813164\n",
      "Key 20151223 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.986512, f1_score: 0.9932103564794799\n",
      "Key 20151229 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.972761, f1_score: 0.9861923758059297\n",
      "Key 20151208 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.688859, f1_score: 0.8157684951148574\n",
      "Key 20151206 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.986692, f1_score: 0.993301487415298\n",
      "Key 20151217 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.964218, f1_score: 0.9817829292197555\n",
      "Key 20151215 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.980809, f1_score: 0.9903112932175426\n",
      "Key 20151213 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.990597, f1_score: 0.9952762415118019\n",
      "Key 20151209 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.982295, f1_score: 0.9910686368557518\n",
      "Key 20151228 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.988685, f1_score: 0.9943105724311051\n",
      "Key 20151226 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.978831, f1_score: 0.9893020210498383\n",
      "Key 20151231 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.925385, f1_score: 0.9612469016510523\n",
      "Key 20151212 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.978191, f1_score: 0.988975405233407\n",
      "Key 20151211 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.989978, f1_score: 0.994963862147604\n",
      "Key 20151221 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.986713, f1_score: 0.9933120326694206\n",
      "Key 20151201 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.969552, f1_score: 0.984540836630071\n",
      "Key 20151220 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n",
      "Key 20151230 | Training Loss: 0.003940 | Train Accuracy: 0.994203 | Test Accuracy: 0.948910, f1_score: 0.9737850931567794\n",
      "Current Layer Attributes - hidden layers:3 features count:16\n",
      "Key 20151224 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.986750, f1_score: 0.9933310021881523\n",
      "Key 20151216 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.978838, f1_score: 0.9893056189838565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151222 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.986742, f1_score: 0.9933266422386666\n",
      "Key 20151214 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.984972, f1_score: 0.9924291376120528\n",
      "Key 20151202 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.953520, f1_score: 0.976207067357339\n",
      "Key 20151227 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.988197, f1_score: 0.9940636182902585\n",
      "Key 20151203 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.906253, f1_score: 0.9508214683813164\n",
      "Key 20151223 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.986512, f1_score: 0.9932103564794799\n",
      "Key 20151229 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.972761, f1_score: 0.9861923758059297\n",
      "Key 20151208 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.688859, f1_score: 0.8157684951148574\n",
      "Key 20151206 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.986692, f1_score: 0.993301487415298\n",
      "Key 20151217 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.964218, f1_score: 0.9817829292197555\n",
      "Key 20151215 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.980809, f1_score: 0.9903112932175426\n",
      "Key 20151213 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.990597, f1_score: 0.9952762415118019\n",
      "Key 20151209 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.982295, f1_score: 0.9910686368557518\n",
      "Key 20151228 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.988685, f1_score: 0.9943105724311051\n",
      "Key 20151226 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.978831, f1_score: 0.9893020210498383\n",
      "Key 20151231 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.925385, f1_score: 0.9612469016510523\n",
      "Key 20151212 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.978191, f1_score: 0.988975405233407\n",
      "Key 20151211 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.989978, f1_score: 0.994963862147604\n",
      "Key 20151221 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.986713, f1_score: 0.9933120326694206\n",
      "Key 20151201 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.969552, f1_score: 0.984540836630071\n",
      "Key 20151220 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n",
      "Key 20151230 | Training Loss: 0.032663 | Train Accuracy: 0.992988 | Test Accuracy: 0.948910, f1_score: 0.9737850931567794\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20151224 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.982887, f1_score: 0.9913694351355774\n",
      "Key 20151204 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.986743, f1_score: 0.9933273732705604\n",
      "Key 20151216 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.978829, f1_score: 0.9893011377240198\n",
      "Key 20151222 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.986734, f1_score: 0.9933229480312881\n",
      "Key 20151214 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.984964, f1_score: 0.9924248961817184\n",
      "Key 20151202 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.953516, f1_score: 0.9762049185317025\n",
      "Key 20151227 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.988193, f1_score: 0.9940616184127602\n",
      "Key 20151203 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.906249, f1_score: 0.9508188430838734\n",
      "Key 20151223 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.965212, f1_score: 0.9822981257134811\n",
      "Key 20151205 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.986501, f1_score: 0.9932047518491165\n",
      "Key 20151229 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.972761, f1_score: 0.9861923255949263\n",
      "Key 20151208 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.983351, f1_score: 0.9916055228822305\n",
      "Key 20151219 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.688864, f1_score: 0.8157707146394154\n",
      "Key 20151206 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.985285, f1_score: 0.9925881318704952\n",
      "Key 20151225 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.985846, f1_score: 0.9928723653251537\n",
      "Key 20151210 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.986696, f1_score: 0.9933034154522233\n",
      "Key 20151217 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.966389, f1_score: 0.9829074174423308\n",
      "Key 20151207 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.964212, f1_score: 0.9817802173803036\n",
      "Key 20151215 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.980809, f1_score: 0.9903112580860383\n",
      "Key 20151213 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.990594, f1_score: 0.9952749905686571\n",
      "Key 20151209 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.982287, f1_score: 0.9910643448001548\n",
      "Key 20151228 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.988683, f1_score: 0.9943091966665024\n",
      "Key 20151226 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.902470, f1_score: 0.9487350726924209\n",
      "Key 20151218 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.978813, f1_score: 0.9892932754154913\n",
      "Key 20151231 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.925379, f1_score: 0.9612434104723285\n",
      "Key 20151212 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.978188, f1_score: 0.9889735128467654\n",
      "Key 20151211 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.989975, f1_score: 0.9949623699577302\n",
      "Key 20151221 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.986699, f1_score: 0.9933048954849991\n",
      "Key 20151201 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.969548, f1_score: 0.9845384595577132\n",
      "Key 20151220 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.938288, f1_score: 0.9681618023553507\n",
      "Key 20151230 | Training Loss: 0.005553 | Train Accuracy: 0.993128 | Test Accuracy: 0.948914, f1_score: 0.9737871184294692\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:15.334068Z",
     "start_time": "2017-07-19T22:05:25.635823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:15.403648Z",
     "start_time": "2017-07-19T22:06:15.336200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Positive\", \"\\n False Negative \\n Type II Error\"],\n",
    "             [\"\\n False Positive \\n Type I Error\", \"\\n True Negative\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:15.412650Z",
     "start_time": "2017-07-19T22:06:15.405085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:15.456044Z",
     "start_time": "2017-07-19T22:06:15.413905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>20151213</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>20151213</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>20151213</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.990594</td>\n",
       "      <td>0.995275</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>20151213</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.990594</td>\n",
       "      <td>0.995275</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>20151211</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>20151211</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>20151211</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>20151211</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>20151211</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>20151211</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.994964</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.989975</td>\n",
       "      <td>0.994962</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.989975</td>\n",
       "      <td>0.994962</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>20151228</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>20151228</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>20151228</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>20151228</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988685</td>\n",
       "      <td>0.994311</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.988683</td>\n",
       "      <td>0.994309</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.988683</td>\n",
       "      <td>0.994309</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>20151227</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>20151227</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>20151227</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>20151227</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.988197</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.988193</td>\n",
       "      <td>0.994062</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.988193</td>\n",
       "      <td>0.994062</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>20151204</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.986750</td>\n",
       "      <td>0.993331</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>20151204</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.986750</td>\n",
       "      <td>0.993331</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>20151228</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20151225</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.930623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.311141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>20151213</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>20151215</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>20151207</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>20151208</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>20151210</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>20151206</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.311141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>20151220</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>20151201</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.030448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>20151221</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.930623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>20151204</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>20151216</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>20151222</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>20151214</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>20151202</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>20151223</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>20151205</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>20151229</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>20151208</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.311141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>20151206</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>20151225</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>20151210</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>20151217</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>20151207</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>20151215</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>20151213</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>20151209</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>20151226</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>20151218</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>20151231</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>20151212</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.021809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "267  20151213              16              3     0.992988    0.990597   \n",
       "546  20151213               8              3     0.994203    0.990597   \n",
       "329  20151213               1              1     0.994156    0.990597   \n",
       "19   20151213               1              1     0.994156    0.990597   \n",
       "174  20151213               1              3     0.993081    0.990597   \n",
       "484  20151213               1              3     0.993081    0.990597   \n",
       "205  20151213               4              3     0.993081    0.990597   \n",
       "577  20151213              16              3     0.992988    0.990597   \n",
       "515  20151213               4              3     0.993081    0.990597   \n",
       "236  20151213               8              3     0.994203    0.990597   \n",
       "298  20151213              42              3     0.993128    0.990594   \n",
       "608  20151213              42              3     0.993128    0.990594   \n",
       "584  20151211              16              3     0.992988    0.989978   \n",
       "553  20151211               8              3     0.994203    0.989978   \n",
       "181  20151211               1              3     0.993081    0.989978   \n",
       "522  20151211               4              3     0.993081    0.989978   \n",
       "212  20151211               4              3     0.993081    0.989978   \n",
       "491  20151211               1              3     0.993081    0.989978   \n",
       "274  20151211              16              3     0.992988    0.989978   \n",
       "336  20151211               1              1     0.994156    0.989978   \n",
       "26   20151211               1              1     0.994156    0.989978   \n",
       "243  20151211               8              3     0.994203    0.989978   \n",
       "615  20151211              42              3     0.993128    0.989975   \n",
       "305  20151211              42              3     0.993128    0.989975   \n",
       "21   20151228               1              1     0.994156    0.988685   \n",
       "331  20151228               1              1     0.994156    0.988685   \n",
       "548  20151228               8              3     0.994203    0.988685   \n",
       "176  20151228               1              3     0.993081    0.988685   \n",
       "486  20151228               1              3     0.993081    0.988685   \n",
       "269  20151228              16              3     0.992988    0.988685   \n",
       "579  20151228              16              3     0.992988    0.988685   \n",
       "207  20151228               4              3     0.993081    0.988685   \n",
       "238  20151228               8              3     0.994203    0.988685   \n",
       "517  20151228               4              3     0.993081    0.988685   \n",
       "610  20151228              42              3     0.993128    0.988683   \n",
       "300  20151228              42              3     0.993128    0.988683   \n",
       "192  20151227               4              3     0.993081    0.988197   \n",
       "316  20151227               1              1     0.994156    0.988197   \n",
       "254  20151227              16              3     0.992988    0.988197   \n",
       "564  20151227              16              3     0.992988    0.988197   \n",
       "502  20151227               4              3     0.993081    0.988197   \n",
       "223  20151227               8              3     0.994203    0.988197   \n",
       "161  20151227               1              3     0.993081    0.988197   \n",
       "6    20151227               1              1     0.994156    0.988197   \n",
       "471  20151227               1              3     0.993081    0.988197   \n",
       "533  20151227               8              3     0.994203    0.988197   \n",
       "285  20151227              42              3     0.993128    0.988193   \n",
       "595  20151227              42              3     0.993128    0.988193   \n",
       "497  20151204               4              3     0.993081    0.986750   \n",
       "528  20151204               8              3     0.994203    0.986750   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "78   20151217               8              1     0.005984    0.033611   \n",
       "362  20151228               4              1     0.005937    0.011314   \n",
       "77   20151210               8              1     0.005984    0.013308   \n",
       "76   20151225               8              1     0.005984    0.014154   \n",
       "432  20151220              16              1     0.005657    0.061712   \n",
       "75   20151206               8              1     0.005984    0.014715   \n",
       "74   20151219               8              1     0.005984    0.311141   \n",
       "360  20151213               4              1     0.005937    0.009403   \n",
       "359  20151215               4              1     0.005937    0.019191   \n",
       "358  20151207               4              1     0.005937    0.035782   \n",
       "73   20151208               8              1     0.005984    0.016649   \n",
       "357  20151217               4              1     0.005937    0.033611   \n",
       "72   20151229               8              1     0.005984    0.027239   \n",
       "356  20151210               4              1     0.005937    0.013308   \n",
       "355  20151225               4              1     0.005937    0.014154   \n",
       "354  20151206               4              1     0.005937    0.014715   \n",
       "353  20151219               4              1     0.005937    0.311141   \n",
       "71   20151205               8              1     0.005984    0.013488   \n",
       "463  20151220              42              1     0.007480    0.061712   \n",
       "462  20151201              42              1     0.007480    0.030448   \n",
       "461  20151221              42              1     0.007480    0.013287   \n",
       "460  20151211              42              1     0.007480    0.010022   \n",
       "433  20151230              16              1     0.005657    0.051090   \n",
       "434  20151224              42              1     0.007480    0.017113   \n",
       "435  20151204              42              1     0.007480    0.013250   \n",
       "436  20151216              42              1     0.007480    0.021162   \n",
       "437  20151222              42              1     0.007480    0.013258   \n",
       "438  20151214              42              1     0.007480    0.015028   \n",
       "439  20151202              42              1     0.007480    0.046480   \n",
       "440  20151227              42              1     0.007480    0.011803   \n",
       "441  20151203              42              1     0.007480    0.093747   \n",
       "442  20151223              42              1     0.007480    0.034788   \n",
       "443  20151205              42              1     0.007480    0.013488   \n",
       "444  20151229              42              1     0.007480    0.027239   \n",
       "445  20151208              42              1     0.007480    0.016649   \n",
       "446  20151219              42              1     0.007480    0.311141   \n",
       "447  20151206              42              1     0.007480    0.014715   \n",
       "448  20151225              42              1     0.007480    0.014154   \n",
       "449  20151210              42              1     0.007480    0.013308   \n",
       "450  20151217              42              1     0.007480    0.033611   \n",
       "451  20151207              42              1     0.007480    0.035782   \n",
       "452  20151215              42              1     0.007480    0.019191   \n",
       "453  20151213              42              1     0.007480    0.009403   \n",
       "454  20151209              42              1     0.007480    0.017705   \n",
       "455  20151228              42              1     0.007480    0.011314   \n",
       "456  20151226              42              1     0.007480    0.097530   \n",
       "457  20151218              42              1     0.007480    0.021169   \n",
       "458  20151231              42              1     0.007480    0.074615   \n",
       "459  20151212              42              1     0.007480    0.021809   \n",
       "393  20151228               8              1     0.005984    0.011314   \n",
       "\n",
       "     f1_score  time_taken  \n",
       "267  0.995276   37.000459  \n",
       "546  0.995276   36.420131  \n",
       "329  0.995276   27.816367  \n",
       "19   0.995276   27.816367  \n",
       "174  0.995276   35.381034  \n",
       "484  0.995276   35.381034  \n",
       "205  0.995276   35.247175  \n",
       "577  0.995276   37.000459  \n",
       "515  0.995276   35.247175  \n",
       "236  0.995276   36.420131  \n",
       "298  0.995275   39.844855  \n",
       "608  0.995275   39.844855  \n",
       "584  0.994964   37.000459  \n",
       "553  0.994964   36.420131  \n",
       "181  0.994964   35.381034  \n",
       "522  0.994964   35.247175  \n",
       "212  0.994964   35.247175  \n",
       "491  0.994964   35.381034  \n",
       "274  0.994964   37.000459  \n",
       "336  0.994964   27.816367  \n",
       "26   0.994964   27.816367  \n",
       "243  0.994964   36.420131  \n",
       "615  0.994962   39.844855  \n",
       "305  0.994962   39.844855  \n",
       "21   0.994311   27.816367  \n",
       "331  0.994311   27.816367  \n",
       "548  0.994311   36.420131  \n",
       "176  0.994311   35.381034  \n",
       "486  0.994311   35.381034  \n",
       "269  0.994311   37.000459  \n",
       "579  0.994311   37.000459  \n",
       "207  0.994311   35.247175  \n",
       "238  0.994311   36.420131  \n",
       "517  0.994311   35.247175  \n",
       "610  0.994309   39.844855  \n",
       "300  0.994309   39.844855  \n",
       "192  0.994064   35.247175  \n",
       "316  0.994064   27.816367  \n",
       "254  0.994064   37.000459  \n",
       "564  0.994064   37.000459  \n",
       "502  0.994064   35.247175  \n",
       "223  0.994064   36.420131  \n",
       "161  0.994064   35.381034  \n",
       "6    0.994064   27.816367  \n",
       "471  0.994064   35.381034  \n",
       "533  0.994064   36.420131  \n",
       "285  0.994062   39.844855  \n",
       "595  0.994062   39.844855  \n",
       "497  0.993331   35.247175  \n",
       "528  0.993331   36.420131  \n",
       "..        ...         ...  \n",
       "78   0.000000   27.265537  \n",
       "362  0.000000   27.103205  \n",
       "77   0.000000   27.265537  \n",
       "76   0.000000   27.265537  \n",
       "432  0.000000   29.930623  \n",
       "75   0.000000   27.265537  \n",
       "74   0.000000   27.265537  \n",
       "360  0.000000   27.103205  \n",
       "359  0.000000   27.103205  \n",
       "358  0.000000   27.103205  \n",
       "73   0.000000   27.265537  \n",
       "357  0.000000   27.103205  \n",
       "72   0.000000   27.265537  \n",
       "356  0.000000   27.103205  \n",
       "355  0.000000   27.103205  \n",
       "354  0.000000   27.103205  \n",
       "353  0.000000   27.103205  \n",
       "71   0.000000   27.265537  \n",
       "463  0.000000   31.452015  \n",
       "462  0.000000   31.452015  \n",
       "461  0.000000   31.452015  \n",
       "460  0.000000   31.452015  \n",
       "433  0.000000   29.930623  \n",
       "434  0.000000   31.452015  \n",
       "435  0.000000   31.452015  \n",
       "436  0.000000   31.452015  \n",
       "437  0.000000   31.452015  \n",
       "438  0.000000   31.452015  \n",
       "439  0.000000   31.452015  \n",
       "440  0.000000   31.452015  \n",
       "441  0.000000   31.452015  \n",
       "442  0.000000   31.452015  \n",
       "443  0.000000   31.452015  \n",
       "444  0.000000   31.452015  \n",
       "445  0.000000   31.452015  \n",
       "446  0.000000   31.452015  \n",
       "447  0.000000   31.452015  \n",
       "448  0.000000   31.452015  \n",
       "449  0.000000   31.452015  \n",
       "450  0.000000   31.452015  \n",
       "451  0.000000   31.452015  \n",
       "452  0.000000   31.452015  \n",
       "453  0.000000   31.452015  \n",
       "454  0.000000   31.452015  \n",
       "455  0.000000   31.452015  \n",
       "456  0.000000   31.452015  \n",
       "457  0.000000   31.452015  \n",
       "458  0.000000   31.452015  \n",
       "459  0.000000   31.452015  \n",
       "393  0.000000   27.265537  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:15.475104Z",
     "start_time": "2017-07-19T22:06:15.457445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.990597</td>\n",
       "      <td>0.995276</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.990594</td>\n",
       "      <td>0.995275</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151226</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151220</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151222</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.930623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151216</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "1              1              20151213     0.994156    0.990597  0.995276   \n",
       "               3              20151213     0.993081    0.990597  0.995276   \n",
       "4              3              20151213     0.993081    0.990597  0.995276   \n",
       "8              3              20151213     0.994203    0.990597  0.995276   \n",
       "16             3              20151213     0.992988    0.990597  0.995276   \n",
       "42             3              20151213     0.993128    0.990594  0.995275   \n",
       "4              1              20151226     0.005937    0.097530  0.000000   \n",
       "8              1              20151220     0.005984    0.061712  0.000000   \n",
       "16             1              20151222     0.005657    0.013258  0.000000   \n",
       "42             1              20151216     0.007480    0.021162  0.000000   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1               27.816367  \n",
       "               3               35.381034  \n",
       "4              3               35.247175  \n",
       "8              3               36.420131  \n",
       "16             3               37.000459  \n",
       "42             3               39.844855  \n",
       "4              1               27.103205  \n",
       "8              1               27.265537  \n",
       "16             1               29.930623  \n",
       "42             1               31.452015  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='f1_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:18.576455Z",
     "start_time": "2017-07-19T22:06:15.476338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:04.813411Z",
     "start_time": "2017-07-19T23:29:04.791293Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151228_1_1'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:05.446738Z",
     "start_time": "2017-07-19T23:29:05.440390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:05.970072Z",
     "start_time": "2017-07-19T23:29:05.917088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943105724311051"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:06.013849Z",
     "start_time": "2017-07-19T23:29:06.002094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0      4159\n",
       "1.0    363423\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:07.036893Z",
     "start_time": "2017-07-19T23:29:06.470531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGkCAYAAACy1WveAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a900475f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:07.051510Z",
     "start_time": "2017-07-19T23:29:07.038289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.994156</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.979593</td>\n",
       "      <td>27.816367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.979593</td>\n",
       "      <td>35.381034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.979593</td>\n",
       "      <td>35.247175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.994203</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.979593</td>\n",
       "      <td>36.420131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.979593</td>\n",
       "      <td>37.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.961738</td>\n",
       "      <td>0.979591</td>\n",
       "      <td>39.844855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.103205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.930623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.452015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "1              1                 0.994156    0.961742  0.979593   27.816367\n",
       "               3                 0.993081    0.961742  0.979593   35.381034\n",
       "4              3                 0.993081    0.961742  0.979593   35.247175\n",
       "8              3                 0.994203    0.961742  0.979593   36.420131\n",
       "16             3                 0.992988    0.961742  0.979593   37.000459\n",
       "42             3                 0.993128    0.961738  0.979591   39.844855\n",
       "4              1                 0.005937    0.038258  0.000000   27.103205\n",
       "8              1                 0.005984    0.038258  0.000000   27.265537\n",
       "16             1                 0.005657    0.038258  0.000000   29.930623\n",
       "42             1                 0.007480    0.038258  0.000000   31.452015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:07.067991Z",
     "start_time": "2017-07-19T23:29:07.052870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055448</td>\n",
       "      <td>0.032585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "1              1                      0.0    0.055450  0.032586         0.0\n",
       "               3                      0.0    0.055450  0.032586         0.0\n",
       "4              1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.055450  0.032586         0.0\n",
       "8              1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.055450  0.032586         0.0\n",
       "16             1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.055450  0.032586         0.0\n",
       "42             1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.055448  0.032585         0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:07.089735Z",
     "start_time": "2017-07-19T23:29:07.069512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                (0.915726182876, 1.04345904093)\n",
       "                3                (0.915726182876, 1.04345904093)\n",
       "4               1                                     (nan, nan)\n",
       "                3                (0.915726182876, 1.04345904093)\n",
       "8               1                                     (nan, nan)\n",
       "                3                (0.915726182876, 1.04345904093)\n",
       "16              1                                     (nan, nan)\n",
       "                3                (0.915726182876, 1.04345904093)\n",
       "42              1                                     (nan, nan)\n",
       "                3                 (0.915725991234, 1.0434555485)\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
