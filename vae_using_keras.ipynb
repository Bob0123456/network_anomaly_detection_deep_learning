{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 123)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 123)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "\n",
    "    from sklearn import model_selection as ms\n",
    "    from sklearn import preprocessing as pp\n",
    "\n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "\n",
    "    x_valid, x_test, y_valid, y_test = ms.train_test_split(x_test, \n",
    "                                  y_test, \n",
    "                                  test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 121\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 40\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 40\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = tf.nn.relu)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = y))\n",
    "\n",
    "            loss = tf.reduce_mean(BCE + KLD + softmax_loss)\n",
    "\n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=0.01\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(y, 1)\n",
    "        self.actual = tf.argmax(self.y_, 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        # saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Train:    \n",
    "    def train(epochs, net):\n",
    "        batch_iterations = 100\n",
    "\n",
    "        batch_indices = np.array_split(np.arange(preprocess.x_train.shape[0]), \n",
    "                                   batch_iterations)\n",
    "        with tf.Session() as sess:\n",
    "            summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(0, epochs):\n",
    "                for i in batch_indices:\n",
    "                    _, train_loss, summary_str = sess.run([net.optimizer, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           net.summary_op],\n",
    "                                                          feed_dict={net.x: preprocess.x_train[i,:], \n",
    "                                                                     net.y_: preprocess.y_train[i,:], \n",
    "                                                                     net.keep_prob:0.8})\n",
    "                    summary_writer_train.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                accuracy, summary_str = sess.run([net.tf_accuracy, net.summary_op], \n",
    "                                                      feed_dict={net.x: preprocess.x_valid, \n",
    "                                                                 net.y_: preprocess.y_valid, \n",
    "                                                                 net.keep_prob:1})\n",
    "                summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.4f} | Validation Accuracy: {:.4f}\".format(epoch, train_loss, accuracy))\n",
    "\n",
    "            accuracy, pred_value, actual_value = sess.run([net.tf_accuracy, \n",
    "                                                           net.pred, \n",
    "                                                           net.actual], \n",
    "                                                          feed_dict={net.x: preprocess.x_test, \n",
    "                                                                     net.y_: preprocess.y_test, \n",
    "                                                                     net.keep_prob:1})\n",
    "\n",
    "\n",
    "            print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "            return accuracy, pred_value, actual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:4\n",
      "Step 0 | Training Loss: 0.7344 | Validation Accuracy: 0.7362\n",
      "Step 10 | Training Loss: 0.1223 | Validation Accuracy: 0.7997\n",
      "Step 20 | Training Loss: 0.1391 | Validation Accuracy: 0.8415\n",
      "Step 30 | Training Loss: 0.1309 | Validation Accuracy: 0.8580\n",
      "Accuracy on Test data: 0.7179293632507324\n",
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:8\n",
      "Step 0 | Training Loss: 0.2584 | Validation Accuracy: 0.6944\n",
      "Step 10 | Training Loss: 0.1443 | Validation Accuracy: 0.7798\n",
      "Step 20 | Training Loss: 0.1634 | Validation Accuracy: 0.8222\n",
      "Step 30 | Training Loss: 0.1166 | Validation Accuracy: 0.8502\n",
      "Accuracy on Test data: 0.7103886008262634\n",
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:16\n",
      "Step 0 | Training Loss: 0.0911 | Validation Accuracy: 0.6500\n",
      "Step 10 | Training Loss: 0.1064 | Validation Accuracy: 0.7824\n",
      "Step 20 | Training Loss: 0.1576 | Validation Accuracy: 0.8338\n",
      "Step 30 | Training Loss: 0.1343 | Validation Accuracy: 0.8752\n",
      "Accuracy on Test data: 0.7230748534202576\n",
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:32\n",
      "Step 0 | Training Loss: 0.2199 | Validation Accuracy: 0.6259\n",
      "Step 10 | Training Loss: 0.1761 | Validation Accuracy: 0.7958\n",
      "Step 20 | Training Loss: 0.1627 | Validation Accuracy: 0.8589\n",
      "Step 30 | Training Loss: 0.1576 | Validation Accuracy: 0.9056\n",
      "Accuracy on Test data: 0.7378460168838501\n",
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:64\n",
      "Step 0 | Training Loss: 0.3188 | Validation Accuracy: 0.6107\n",
      "Step 10 | Training Loss: 0.1840 | Validation Accuracy: 0.7956\n",
      "Step 20 | Training Loss: 0.1661 | Validation Accuracy: 0.8971\n",
      "Step 30 | Training Loss: 0.1164 | Validation Accuracy: 0.9343\n",
      "Accuracy on Test data: 0.7476046681404114\n",
      "Current Layer Attributes - epochs:40 hidden layers:2 features count:128\n",
      "Step 0 | Training Loss: 0.2317 | Validation Accuracy: 0.5831\n",
      "Step 10 | Training Loss: 0.2018 | Validation Accuracy: 0.8382\n",
      "Step 20 | Training Loss: 0.1741 | Validation Accuracy: 0.9285\n",
      "Step 30 | Training Loss: 0.1003 | Validation Accuracy: 0.9506\n",
      "Accuracy on Test data: 0.7616660594940186\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:4\n",
      "Step 0 | Training Loss: 0.8961 | Validation Accuracy: 0.5045\n",
      "Step 10 | Training Loss: 0.7868 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7759 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7653 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:8\n",
      "Step 0 | Training Loss: 0.8986 | Validation Accuracy: 0.5050\n",
      "Step 10 | Training Loss: 0.7918 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7750 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7675 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:16\n",
      "Step 0 | Training Loss: 0.8766 | Validation Accuracy: 0.5083\n",
      "Step 10 | Training Loss: 0.7885 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7759 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7691 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:32\n",
      "Step 0 | Training Loss: 0.9104 | Validation Accuracy: 0.5118\n",
      "Step 10 | Training Loss: 0.7877 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7753 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7681 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:64\n",
      "Step 0 | Training Loss: 0.8981 | Validation Accuracy: 0.5109\n",
      "Step 10 | Training Loss: 0.7878 | Validation Accuracy: 0.5357\n",
      "Step 20 | Training Loss: 0.7739 | Validation Accuracy: 0.5358\n",
      "Step 30 | Training Loss: 0.7701 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.4309350550174713\n",
      "Current Layer Attributes - epochs:40 hidden layers:4 features count:128\n",
      "Step 0 | Training Loss: 0.9096 | Validation Accuracy: 0.5117\n",
      "Step 10 | Training Loss: 0.7866 | Validation Accuracy: 0.5329\n",
      "Step 20 | Training Loss: 0.7770 | Validation Accuracy: 0.5344\n",
      "Step 30 | Training Loss: 0.7652 | Validation Accuracy: 0.5357\n",
      "Accuracy on Test data: 0.4328868091106415\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:4\n",
      "Step 0 | Training Loss: 0.8539 | Validation Accuracy: 0.5119\n",
      "Step 10 | Training Loss: 0.7950 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7868 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7745 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:8\n",
      "Step 0 | Training Loss: 0.8688 | Validation Accuracy: 0.5080\n",
      "Step 10 | Training Loss: 0.7936 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7936 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7774 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:16\n",
      "Step 0 | Training Loss: 0.8957 | Validation Accuracy: 0.5083\n",
      "Step 10 | Training Loss: 0.7901 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7865 | Validation Accuracy: 0.5359\n",
      "Step 30 | Training Loss: 0.7742 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:32\n",
      "Step 0 | Training Loss: 0.8725 | Validation Accuracy: 0.5114\n",
      "Step 10 | Training Loss: 0.8001 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7826 | Validation Accuracy: 0.5358\n",
      "Step 30 | Training Loss: 0.7749 | Validation Accuracy: 0.5359\n",
      "Accuracy on Test data: 0.43075764179229736\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:64\n",
      "Step 0 | Training Loss: 0.8820 | Validation Accuracy: 0.5080\n",
      "Step 10 | Training Loss: 0.7964 | Validation Accuracy: 0.5359\n",
      "Step 20 | Training Loss: 0.7806 | Validation Accuracy: 0.5358\n",
      "Step 30 | Training Loss: 0.7736 | Validation Accuracy: 0.5358\n",
      "Accuracy on Test data: 0.4308907091617584\n",
      "Current Layer Attributes - epochs:40 hidden layers:6 features count:128\n",
      "Step 0 | Training Loss: 0.8765 | Validation Accuracy: 0.5094\n",
      "Step 10 | Training Loss: 0.7872 | Validation Accuracy: 0.5342\n",
      "Step 20 | Training Loss: 0.7885 | Validation Accuracy: 0.5349\n",
      "Step 30 | Training Loss: 0.7758 | Validation Accuracy: 0.5354\n",
      "Accuracy on Test data: 0.4323101341724396\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "class Hyperparameters:\n",
    "    features_arr = [4, 8, 16, 32, 64, 128]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "    epochs = [40]\n",
    "    result = collections.namedtuple(\"result\", [\"epochs\", \"hidden_layers\", \"feature_count\",\n",
    "                                  \"accuracy\"])\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        acc, pred, actual = Train.train(e, n)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            pred_value = pred\n",
    "            actual_value = actual\n",
    "        results.append(result(e, h, f,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Hyperparameters.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>feature_count</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.717929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.710389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.723075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.737846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.747605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.761666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.432887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>0.430891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>0.432310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  hidden_layers  feature_count  accuracy\n",
       "0       40              2              4  0.717929\n",
       "1       40              2              8  0.710389\n",
       "2       40              2             16  0.723075\n",
       "3       40              2             32  0.737846\n",
       "4       40              2             64  0.747605\n",
       "5       40              2            128  0.761666\n",
       "6       40              4              4  0.430758\n",
       "..     ...            ...            ...       ...\n",
       "11      40              4            128  0.432887\n",
       "12      40              6              4  0.430758\n",
       "13      40              6              8  0.430758\n",
       "14      40              6             16  0.430758\n",
       "15      40              6             32  0.430758\n",
       "16      40              6             64  0.430891\n",
       "17      40              6            128  0.432310\n",
       "\n",
       "[18 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.6347  0.3653]\n",
      " [ 0.0705  0.9295]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc17d212f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_2labels = confusion_matrix(y_pred = Hyperparameters.pred_value, y_true = Hyperparameters.actual_value)\n",
    "plt.figure(figsize=[6,6])\n",
    "plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
