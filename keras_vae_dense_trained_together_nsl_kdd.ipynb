{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "\n",
    "#y_train_labels = pd.read_pickle(\"dataset/kdd_train_2labels_y.pkl\")\n",
    "#y_train_labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "#y_test_labels = pd.read_pickle(\"dataset/kdd_test_2labels_y.pkl\")\n",
    "\n",
    "output_columns_2labels = ['is_Attack','is_Normal']\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "x_input = kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "y_output = kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "x_input = ss.fit_transform(x_input)\n",
    "\n",
    "#le = pp.LabelEncoder()\n",
    "#y_train = le.fit_transform(y_train_labels).reshape(-1, 1)\n",
    "#y_test = le.transform(y_test_labels).reshape(-1, 1)\n",
    "\n",
    "y_train = kdd_train_2labels.loc[:,output_columns_2labels].values\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = ms.train_test_split(x_input, \n",
    "                              y_train, \n",
    "                              test_size=0.1)\n",
    "#x_valid, x_test, y_valid, y_test = ms.train_test_split(x_valid, y_valid, test_size = 0.4)\n",
    "\n",
    "x_test = kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "y_test = kdd_test_2labels.loc[:,output_columns_2labels].values\n",
    "\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "#x_train = np.hstack((x_train, y_train))\n",
    "#x_valid = np.hstack((x_valid, y_valid))\n",
    "\n",
    "#x_test = np.hstack((x_test, np.random.normal(loc = 0, scale = 0.01, size = y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 122\n",
    "intermediate_dim = 80\n",
    "latent_dim = 32\n",
    "batch_size = 1409\n",
    "epochs = 5\n",
    "hidden_layers = 8\n",
    "classes = 2\n",
    "\n",
    "class Train:\n",
    "    def train():\n",
    "        Train.x = Input(shape=(input_dim,))\n",
    "        \n",
    "        hidden_encoder = Train.x\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_encoder = Dense(intermediate_dim, activation='relu')(hidden_encoder)\n",
    "\n",
    "        mean_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        logvar_encoder = Dense(latent_dim, activation=None)(hidden_encoder)\n",
    "\n",
    "        def get_distrib(args):\n",
    "\n",
    "            mean_encoder, logvar_encoder = args\n",
    "\n",
    "            # Sample epsilon\n",
    "            epsilon = np.random.normal(loc=0.0, scale=0.05, size = (batch_size, latent_dim))\n",
    "\n",
    "            # Sample latent variable\n",
    "            z = mean_encoder + K.exp(logvar_encoder / 2) * epsilon\n",
    "            return z\n",
    "\n",
    "        z = Lambda(get_distrib)([mean_encoder, logvar_encoder])\n",
    "\n",
    "        hidden_decoder = z\n",
    "        for i in range(hidden_layers):\n",
    "            hidden_decoder = Dense(intermediate_dim, activation=\"relu\")(hidden_decoder)\n",
    "\n",
    "        Train.x_ = Dense(input_dim, activation=None, name='vae_output')(hidden_decoder)\n",
    "        \n",
    "        Train.y = Dense(classes, activation='softmax', name='softmax_output')(z)\n",
    "        \n",
    "\n",
    "def get_loss(x, x_):\n",
    "    xent_loss = input_dim * metrics.binary_crossentropy(x, x_) \n",
    "    kl_loss = - 0.5 * K.sum(1 + logvar_encoder - K.square(mean_encoder) - K.exp(logvar_encoder), axis=-1)\n",
    "    \n",
    "    return K.abs(K.mean(xent_loss + kl_loss + label_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:2 features count:4\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0426 - acc: 0.9488 - val_loss: 0.2015 - val_acc: 0.7716\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0152 - acc: 0.9836 - val_loss: 0.2027 - val_acc: 0.7728\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0137 - acc: 0.9853 - val_loss: 0.2074 - val_acc: 0.7685\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0129 - acc: 0.9860 - val_loss: 0.2084 - val_acc: 0.7693\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0120 - acc: 0.9869 - val_loss: 0.1997 - val_acc: 0.7742\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0069 - acc: 0.9924 - val_loss: 0.1935 - val_acc: 0.7914\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0053 - acc: 0.9939 - val_loss: 0.1918 - val_acc: 0.7877\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0050 - acc: 0.9944 - val_loss: 0.1904 - val_acc: 0.7913\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0047 - acc: 0.9948 - val_loss: 0.1970 - val_acc: 0.7867\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0044 - acc: 0.9950 - val_loss: 0.1997 - val_acc: 0.7876\n",
      " 1409/22544 [>.............................] - ETA: 0s\n",
      " Train Acc: 0.9939673542976379, Test Acc: 0.7875709682703018\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:2 features count:16\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0346 - acc: 0.9608 - val_loss: 0.1859 - val_acc: 0.7655\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0147 - acc: 0.9840 - val_loss: 0.2135 - val_acc: 0.7618\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0131 - acc: 0.9858 - val_loss: 0.2128 - val_acc: 0.7649\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0097 - acc: 0.9887 - val_loss: 0.2069 - val_acc: 0.7775\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0056 - acc: 0.9938 - val_loss: 0.2071 - val_acc: 0.7814\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0050 - acc: 0.9941 - val_loss: 0.2022 - val_acc: 0.7866\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0048 - acc: 0.9943 - val_loss: 0.2006 - val_acc: 0.7910\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0046 - acc: 0.9946 - val_loss: 0.2063 - val_acc: 0.7859\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0043 - acc: 0.9950 - val_loss: 0.2030 - val_acc: 0.7889\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0041 - acc: 0.9953 - val_loss: 0.1989 - val_acc: 0.7934\n",
      " 1409/22544 [>.............................] - ETA: 0s\n",
      " Train Acc: 0.9953868016600609, Test Acc: 0.7934261858463287\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:2 features count:32\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0380 - acc: 0.9526 - val_loss: 0.1901 - val_acc: 0.7862\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0144 - acc: 0.9847 - val_loss: 0.1927 - val_acc: 0.7854\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0122 - acc: 0.9860 - val_loss: 0.1940 - val_acc: 0.7832\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0075 - acc: 0.9917 - val_loss: 0.1887 - val_acc: 0.7949\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0057 - acc: 0.9935 - val_loss: 0.1947 - val_acc: 0.7939\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0052 - acc: 0.9941 - val_loss: 0.1869 - val_acc: 0.8009\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0050 - acc: 0.9945 - val_loss: 0.1866 - val_acc: 0.8036\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0048 - acc: 0.9944 - val_loss: 0.1930 - val_acc: 0.7967\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0045 - acc: 0.9947 - val_loss: 0.1995 - val_acc: 0.7939\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0044 - acc: 0.9949 - val_loss: 0.1903 - val_acc: 0.8007\n",
      " 1409/22544 [>.............................] - ETA: 0s\n",
      " Train Acc: 0.9953868016600609, Test Acc: 0.8007008507847786\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:6 features count:4\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0424 - acc: 0.9524 - val_loss: 0.2270 - val_acc: 0.7677\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0188 - acc: 0.9804 - val_loss: 0.2394 - val_acc: 0.7529\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0180 - acc: 0.9814 - val_loss: 0.2403 - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0174 - acc: 0.9821 - val_loss: 0.2382 - val_acc: 0.7576\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0174 - acc: 0.9820 - val_loss: 0.2345 - val_acc: 0.7597\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0145 - acc: 0.9846 - val_loss: 0.2229 - val_acc: 0.7724\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0131 - acc: 0.9863 - val_loss: 0.2269 - val_acc: 0.7676\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0133 - acc: 0.9861 - val_loss: 0.2215 - val_acc: 0.7711\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0128 - acc: 0.9863 - val_loss: 0.2085 - val_acc: 0.7835\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0077 - acc: 0.9909 - val_loss: 0.2165 - val_acc: 0.7791\n",
      "22544/22544 [==============================] - 0s     \n",
      "\n",
      " Train Acc: 0.9929027631878853, Test Acc: 0.7790986448526382\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:6 features count:16\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0420 - acc: 0.9558 - val_loss: 0.2172 - val_acc: 0.7760\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0205 - acc: 0.9786 - val_loss: 0.2316 - val_acc: 0.7593\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0182 - acc: 0.9809 - val_loss: 0.2428 - val_acc: 0.7504\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0179 - acc: 0.9814 - val_loss: 0.2219 - val_acc: 0.7697\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0149 - acc: 0.9839 - val_loss: 0.2305 - val_acc: 0.7572\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0126 - acc: 0.9864 - val_loss: 0.2183 - val_acc: 0.7720\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0104 - acc: 0.9879 - val_loss: 0.2117 - val_acc: 0.7792\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0059 - acc: 0.9931 - val_loss: 0.2058 - val_acc: 0.7805\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0056 - acc: 0.9936 - val_loss: 0.2061 - val_acc: 0.7871\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0050 - acc: 0.9945 - val_loss: 0.2126 - val_acc: 0.7786\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Acc: 0.9944109246134758, Test Acc: 0.7786107249557972\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:6 features count:32\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0366 - acc: 0.9591 - val_loss: 0.2125 - val_acc: 0.7786\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0146 - acc: 0.9845 - val_loss: 0.2089 - val_acc: 0.7801\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0143 - acc: 0.9847 - val_loss: 0.2175 - val_acc: 0.7681\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112720/112720 [==============================] - 0s - loss: 0.0133 - acc: 0.9858 - val_loss: 0.2047 - val_acc: 0.7828\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0080 - acc: 0.9907 - val_loss: 0.2039 - val_acc: 0.7825\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0059 - acc: 0.9933 - val_loss: 0.1966 - val_acc: 0.7929\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0059 - acc: 0.9933 - val_loss: 0.2124 - val_acc: 0.7785\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0051 - acc: 0.9939 - val_loss: 0.2047 - val_acc: 0.7826\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0045 - acc: 0.9946 - val_loss: 0.1950 - val_acc: 0.7959\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0042 - acc: 0.9951 - val_loss: 0.1883 - val_acc: 0.7959\n",
      "19726/22544 [=========================>....] - ETA: 0s\n",
      " Train Acc: 0.9958303719758987, Test Acc: 0.7958658635616302\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:10 features count:4\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0539 - acc: 0.9459 - val_loss: 0.2370 - val_acc: 0.7578\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0158 - acc: 0.9835 - val_loss: 0.2248 - val_acc: 0.7700\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0144 - acc: 0.9848 - val_loss: 0.2265 - val_acc: 0.7680\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0146 - acc: 0.9844 - val_loss: 0.2274 - val_acc: 0.7546\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0138 - acc: 0.9854 - val_loss: 0.2258 - val_acc: 0.7581\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0081 - acc: 0.9902 - val_loss: 0.2190 - val_acc: 0.7701\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0068 - acc: 0.9927 - val_loss: 0.2217 - val_acc: 0.7714\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0062 - acc: 0.9928 - val_loss: 0.2187 - val_acc: 0.7737\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0050 - acc: 0.9940 - val_loss: 0.2202 - val_acc: 0.7724\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0052 - acc: 0.9938 - val_loss: 0.2128 - val_acc: 0.7808\n",
      "16908/22544 [=====================>........] - ETA: 0s\n",
      " Train Acc: 0.9939673617482185, Test Acc: 0.7807842381298542\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:10 features count:16\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0464 - acc: 0.9518 - val_loss: 0.2186 - val_acc: 0.7748\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0195 - acc: 0.9792 - val_loss: 0.2267 - val_acc: 0.7597\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0140 - acc: 0.9844 - val_loss: 0.2271 - val_acc: 0.7505\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0121 - acc: 0.9864 - val_loss: 0.2197 - val_acc: 0.7639\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0070 - acc: 0.9916 - val_loss: 0.2167 - val_acc: 0.7747\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0055 - acc: 0.9937 - val_loss: 0.2237 - val_acc: 0.7689\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0055 - acc: 0.9935 - val_loss: 0.2103 - val_acc: 0.7778\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0049 - acc: 0.9942 - val_loss: 0.2018 - val_acc: 0.7910\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0048 - acc: 0.9944 - val_loss: 0.2158 - val_acc: 0.7741\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0046 - acc: 0.9946 - val_loss: 0.2166 - val_acc: 0.7757\n",
      "15499/22544 [===================>..........] - ETA: 0s\n",
      " Train Acc: 0.9945883676409721, Test Acc: 0.7757274769246578\n",
      " \n",
      " Current Layer Attributes - epochs:10 hidden layers:10 features count:32\n",
      "Train on 112720 samples, validate on 22544 samples\n",
      "Epoch 1/10\n",
      "112720/112720 [==============================] - 1s - loss: 0.0465 - acc: 0.9498 - val_loss: 0.2217 - val_acc: 0.7746\n",
      "Epoch 2/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0177 - acc: 0.9812 - val_loss: 0.2289 - val_acc: 0.7660\n",
      "Epoch 3/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0156 - acc: 0.9832 - val_loss: 0.2071 - val_acc: 0.7868\n",
      "Epoch 4/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0094 - acc: 0.9889 - val_loss: 0.2030 - val_acc: 0.7901\n",
      "Epoch 5/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0055 - acc: 0.9936 - val_loss: 0.1911 - val_acc: 0.8033\n",
      "Epoch 6/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0056 - acc: 0.9936 - val_loss: 0.2118 - val_acc: 0.7821\n",
      "Epoch 7/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0052 - acc: 0.9941 - val_loss: 0.2191 - val_acc: 0.7744\n",
      "Epoch 8/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0049 - acc: 0.9944 - val_loss: 0.2023 - val_acc: 0.7915\n",
      "Epoch 9/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0049 - acc: 0.9943 - val_loss: 0.2184 - val_acc: 0.7724\n",
      "Epoch 10/10\n",
      "112720/112720 [==============================] - 0s - loss: 0.0046 - acc: 0.9947 - val_loss: 0.1885 - val_acc: 0.8035\n",
      "15499/22544 [===================>..........] - ETA: 0s\n",
      " Train Acc: 0.9935237765312195, Test Acc: 0.8034510277211666\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "#features_arr = [4, 16, 32, 256, 1024]\n",
    "#hidden_layers_arr = [2, 6, 10, 100]\n",
    "\n",
    "features_arr = [4, 16, 32]\n",
    "hidden_layers_arr = [2, 6, 10]\n",
    "\n",
    "epoch_arr = [10]\n",
    "\n",
    "score = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "scores = []\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for e, h, f in itertools.product(epoch_arr, hidden_layers_arr, features_arr):\n",
    "    \n",
    "    print(\" \\n Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "    latent_dim = f\n",
    "    epochs = e\n",
    "    hidden_layers = h\n",
    "\n",
    "    Train.train()\n",
    "\n",
    "    vae_model = Model(inputs = Train.x, outputs = Train.y)\n",
    "    vae_model.compile(optimizer = \"adam\", loss = \"mean_squared_error\", metrics = ['accuracy'] )\n",
    "\n",
    "    train_size = x_train.shape[0] - x_train.shape[0]%batch_size\n",
    "    valid_size = x_valid.shape[0] - x_valid.shape[0]%batch_size\n",
    "\n",
    "    vae_model.fit(x = x_train[:train_size,:], y = y_train[:train_size,:], \n",
    "                  shuffle=True, epochs=epochs, \n",
    "                  batch_size = batch_size, \n",
    "                  #validation_data = (x_valid[:valid_size,:], y_valid[:valid_size,:]),\n",
    "                  validation_data = (x_test, y_test),\n",
    "                  verbose = 1)\n",
    "    \n",
    "    score_train = vae_model.evaluate(x_valid[:valid_size,:], y = y_valid[:valid_size,:],\n",
    "                               batch_size = batch_size,\n",
    "                               verbose = 1)\n",
    "    \n",
    "    score_test = vae_model.evaluate(x_test, y = y_test,\n",
    "                           batch_size = batch_size,\n",
    "                           verbose = 1)\n",
    "    \n",
    "    y_test_pred = vae_model.predict(x_test, batch_size=batch_size)\n",
    "    \n",
    "    y_pred = np.argmax(y_test_pred[:,-2:], axis = 1)\n",
    "    \n",
    "    curr_pred = pd.DataFrame({\"{}_{}_{}\".format(e,f,h):y_pred},)\n",
    "    predictions = pd.concat([predictions, curr_pred], axis = 1)\n",
    "    \n",
    "    scores.append(score(e,f,h,score_train[-1], score_test[-1])) #score_test[-1]))\n",
    "    \n",
    "    print(\"\\n Train Acc: {}, Test Acc: {}\".format(score_train[-1], \n",
    "                                                  score_test[-1])  )\n",
    "    \n",
    "scores = pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.993524</td>\n",
       "      <td>0.803451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.995387</td>\n",
       "      <td>0.800701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.995830</td>\n",
       "      <td>0.795866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.995387</td>\n",
       "      <td>0.793426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.787571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.780784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.992903</td>\n",
       "      <td>0.779099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.994411</td>\n",
       "      <td>0.778611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.994588</td>\n",
       "      <td>0.775727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "8     10              32             10     0.993524    0.803451\n",
       "2     10              32              2     0.995387    0.800701\n",
       "5     10              32              6     0.995830    0.795866\n",
       "1     10              16              2     0.995387    0.793426\n",
       "0     10               4              2     0.993967    0.787571\n",
       "6     10               4             10     0.993967    0.780784\n",
       "3     10               4              6     0.992903    0.779099\n",
       "4     10              16              6     0.994411    0.778611\n",
       "7     10              16             10     0.994588    0.775727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort_values(\"test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.to_pickle(\"dataset/vae_dense_keras_predictions.pkl\")\n",
    "scores.to_pickle(\"dataset/vae_dense_keras_scores.pkl\")"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Final Hyper parameter tuning",
    "public": false
   },
   "id": "33dcb1bcf3ca4a3461c4405a003a7591"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
